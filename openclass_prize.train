What is the Turing Test?	The original Turing test involved three entities communicating with each other through computer terminals in separate rooms - a computer program trying to pass itself off as a human, a human trying to prove that she or he is human, and a human judge deciding which of the competitors is human and which a machine. This is different from the popular idea that it's about a human judging whether one subject is human or machine. The reason it's different is that Turing's original proposal was not intended as a practical test, but rather as a thought experiment making a philosophical argument - that it is impossible to determine whether something is "intelligent" better than by judging its linguistic interactions, and that therefore a machine that sounds intelligent has to be considered truly intelligent. And by "intelligent," Turing meant, like a human, with consciousness.	Which of the following is NOT an objection to the Turing Test mentioned in the reading?	Intelligence is not the same thing as consciousness.	Many human beings might not pass the Turing test.|||Language is not necessarily the only way to demonstrate high intelligence.|||Cognitive ability is cognitive ability whether demonstrated through behavior or not.	The Turing Test web page||document||N/A|||The Turing test paragraph from Stanford Encyclopedia of Philosophy||image||N/A
Turing test	Alan Turing	Computational linguistics	Language philosophy	Artificial Intelligence	Philosophy of artificial intelligence
What is inflectional morphology?	Inflectional morphology is when we have a change in word form, for example "perform" and "performed."	What is the difference between inflectional and derivational morphology?	Derivational morphology is where we have a new word.	One is part of micro- and the other is part of macro-linguistics.|||Inflectional morphology means the suffix changes.|||Derivational morphology is where morphemes are derived from phonemes.	Inflection in Morphology presentation||slides||Inflection Bayu Jaka Magistra 180120130006 Indah  Mustika S. M . 180120130003  Inflection  Inflection  What is Inflection ?  Regular  & Irregular  Inflection  Forms of Nouns   Forms of Pronouns  & Determiners  Forms of Verbs  Forms of Adjectives   `   What is Inflection ?  What is Inflection?  1. The pianist performs in the local café  every month. 2. The pianist performed in the local café last night.  3. The performance was extraordinary.  What is Inflection? 1. The pianist performs in the local café  every month. 2. The pianist performed in the local café last night.  3. The performance was extraordinary.  What is Inflection? 1. The pianist  performs in the local café  every month. 2. The pianist  performed in the local café last night.  3. The  performance was extraordinary.  The words performs, performed & performance belong to the same root which is perform .  However, the word performs & performed in sentence ( 1 ) & ( 2 ) belong to the same word class i . e . verb .  The word Performance in sentence ( 3 ) , on the other hand, belongs to the different word class i . e . noun .  What is Inflection? 1. The pianist  performs in the local café  every month. 2. The pianist  performed in the local café last night.   What happens in sentence ( 1 ) and ( 2 ) is the process of word formation called inflection .  Inflection does not change the word class (parts of speech) and meaning of a word  Instead, Inflection is grammatically conditioned (McCarthy, 2002 ), or expresses grammatical categories like tense, mood, voice, aspect, person, number, gender and case . 1 1  Inflection . (2013, September 12).  Retrieved November 24, 2013,  from www.wikipedia.org : http://en.wikipedia.org/wiki/Inflection  What is Inflection? 1. The pianist  performs in the local café  every month. 2. The pianist  performed in the local café last night.  Grammatically condition or expresses grammatical categories like  tense, mood, voice, aspect, person, number, gender and case.  In sentence ( 1 ) the suffix - s is added to the root because of being grammatically conditioned by third - person singular subject the pianist .  In sentence ( 2 ) the suffix - ed is added to the root to express past tense .  What is Inflection? 3. The  performance was extraordinary.  What happens in sentence ( 3 ), on the other hand, is the process of word formation called derivation .  Derivation is the process of  new words by adding affixes to existing words .  ( Trask , 2007 ) .  Derivation changes the word class and/or meaning of the root .  What is Inflection? ROOT  Word class  Meaning Inflection Variants Variants Variants  What is Inflection? PERFORM  Verb  To execute Inflection Perform s Perform ing Perform ed  The variants still  belong to the  same word class  (verb), and have  the same  meaning  However, they are  grammatically  conditioned, or  express certain  grammatical  category grammatically  conditioned by third - person singular  subject Expressing continuous  and progressive  aspects Expressing past tense  What is Inflection? PERFORM  Verb  To execute Perform ance  Noun  The act of  performing Perform er  Noun  One who  performs Inflection Inflection Perform ance s Plural Perform er s Plural  Regular & Irregular Inflection Cats Guitars Hats Tables  Chairs Doors Windows  Regular & Irregular Inflection Cat s Guitar s Hat s Table s Chair s Door s Window s  Regular & Irregular Inflection Cat s Guitar s Hat s Table s Chair s Door s Window s  Adding suffix - s to a noun root is the regular method of forming plural .  Regular & Irregular Inflection Mice Children Women Teeth Oxen Men Knives are irregular plural forms of  Mouse Child Woman Tooth Ox Man Knife are allomorphs of   Regular & Irregular Inflection Went Better Worse are irregular inflection forms of  Go Good Bad Allomorphs ???  Regular & Irregular Inflection Went Better Worse Suppletion of  Go Good Bad Suppletion  Regular & Irregular Inflection Suppletion vs. Allomorph Allomorph Mice Children Women Teeth Oxen Men Knives Root Mouse Child Woman Tooth Ox Man Knife  Regular & Irregular Inflection Suppletion vs. Allomorph Allomorph M i ce Child ren Wom e n T ee th Ox en M e n Kni ves Root M ou se Child Wom a n T oo th Ox M a n Kni fe An allomorph has similar  phoneme(s) as its root  Regular & Irregular Inflection Suppletion vs. Allomorph Root Went Better Worse Suppletion Go Good Bad Suppletion and its root  does not have any similar  phoneme.  Forms of Nouns  Inflection in nouns expresses  grammatical category which is number.  Regular forms (adding the suffix  - s)  Irregular forms (Allophones, zero suffix  like  deer, fish, sheep )   4.4 Forms of Pronouns And  Determiners Open classes :  Nouns, Adjectives, Verbs,   Adverbs Determiners:  nouns , display a singular - plural  contrast  P ro - nouns  combine a singular - plural contrast  with contrast unique to them, between  subject and non - subject forms.   T he  distinction between this and  these.  These  are the singular and plural forms of the  determinest lexeme  this.  The  determiners THAT and THIS demonstrate  that number contrasts can have a grammatical  effect inside noun phrase as well as between  subject noun phrases and their accompanying  verbs.   In English, the same technique is used for one  small closed class of lexemes, namely personal  pronouns.  If  one replaces John and Mary with the  appropriate pronouns in these two examples,  the outcome is as in: 1. He  loves her. 2. She  loves him.   He and him are sometimes said to contrast in  case. 1. He belonging to the nominative  case 2. H im belonging to the accusative case .  It  is striking that the relationship between  nominative and accusative forms is  consistently  suppletive . >>  I/me, she/her, we/us, and they/them.    Corresponding words with a possessive  meaning: his and our, as well as my, her, your  and their.  Syntactically and semantically, these words  fulfill just the same role as noun phrases with  the apostrophe - s: 1.  His bicycle  means the bicycle belonging   to him. 2.   bicycle means the bicycle  belonging to that man.  4.5 Forms of  Verbs  In English, a verb lexeme has at most five distinct  forms, as illustrated here with GIVE .  Third person singular present tense e.g . Marry  gives a lecture every year.  Past tense e.g . Marry  gave a lecture last week.  Progressive participle e.g . Mary is  giving a lecture today.  Perfect or passive participle e.g . Mary has  given a lecture today.  Basic form (used everywhere else) e.g Mary wants to  give a lecture.  4.6  Forms of  Adjectives  Many  English adjectives exhibit three forms,  for example GREEN here: 1. Grass  is  green . 2. The  grass is  greener now than in winter. 3. The  grass is  greenest in early summer.  Other adjectives with  similar forms: Positive Comprative Superlative Happy happier happiest Long longer longest Pure purer purest Untidy untidier untidiest Good better best All  these exhibit a regular pattern of suffixation with   er and   est , except for better and best, which are  suppletive .
Inflectional morphology	Inflection	Derivational morphology	Morphology	Macrolinguistics
What English sentence could represent the following set of first order logic expressions?	The answer is “What game do you want to play with me today?”	Which of the following expressions could belong in an FOL representation of, âWhat do you want to play with?â	with(play, $qvar)	play(want, $qvar)|||want(play, $qvar)|||play(with, $qvar)	NONE
First-order logic	Predicate logic	Semantics	Sentence analysis
Classify the following sentence using the described classifier: "good good great bad terrible"	This sentence would have a feature vector as follows: x = [2, 1, 1, 1].  \( p(+|x) = P(Y = 1|x) = \sigma( w · x + b ) =  \sigma( [1.5, 3.2, -2.1, -4.0] · [2, 1, 1, 1] - 1.0 ) =  \sigma( -0.9 ) = 0.289 \) \( p(-|x) = P(Y = 0|x) = 1 - \sigma( w · x + b ) = 0.711 \)  Thus, our model would classify this sentence to be -.	Add another feature to our Logistic Regression model as follows, with a weight of 3.0: \( \begin{equation} X= \begin{cases} 1, & \text{if count("good")}\ > 1 \\ 0, & \text{otherwise} \end{cases} \end{equation} \). Does this change our prediction label?	Yes, our prediction would now be + because  \( p(+|x) > p(-|x) \) after factoring in this feature.	No, our prediction would still be - because our input sentence does not satisfy the requirement of the new feature.|||No, our prediction would still be - because \( p(-|x) > p(+|x) \) even factoring in this feature.	Speech and Language Processing: Logistic Regression||publication||4 C HAPTER 5  L OGISTIC R EGRESSION nearlylineararound0buthasasharpslopetowardtheends,ittendstosquashoutlier valuestoward0or1.Andit'sdifferentiable,whichaswe'llseeinSection 5.8 will behandyforlearning. We'realmostthere.Ifweapplythesigmoidtothesumoftheweightedfeatures, wegetanumberbetween0and1.Tomakeitaprobability,wejustneedtomake surethatthetwocases, p ( y = 1 ) and p ( y = 0 ) ,sumto1.Wecandothisasfollows: P ( y = 1 )= s ( w  x + b ) = 1 1 + e  ( w  x + b ) P ( y = 0 )= 1  s ( w  x + b ) = 1  1 1 + e  ( w  x + b ) = e  ( w  x + b ) 1 + e  ( w  x + b ) (5.5) Nowwehaveanalgorithmthatgivenaninstance x computestheprobability P ( y = 1 j x ) .Howdowemakeadecision?Foratestinstance x ,wesayyesiftheprobability P ( y = 1 j x ) ismorethan.5,andnootherwise.Wecall.5the decisionboundary : decision boundary ‹ y = ˆ 1if P ( y = 1 j x ) > 0 : 5 0otherwise 5.1.1Example:sentiment Let'shaveanexample.Supposewearedoingbinarysentimenton moviereviewtext,andwewouldliketoknowwhethertoassignthesentimentclass + or  toareviewdocument doc .We'llrepresenteachinputobservationbythe6 features x 1 ::: x 6 oftheinputshowninthefollowingtable;Fig. 5.2 showsthefeatures inasampleminitestdocument. VarValueinFig. 5.2 x 1 count(positivelexicon) 2 doc ) 3 x 2 count(negativelexicon) 2 doc ) 2 x 3 ˆ 1ifﬁnoﬂ 2 doc 0otherwise 1 x 4 count ( 1stand2ndpronouns 2 doc ) 3 x 5 ˆ 1ifﬁ!ﬂ 2 doc 0otherwise 0 x 6 log ( wordcountofdoc ) ln ( 66 )= 4 : 19 Let'sassumeforthemomentthatwe'vealreadylearnedareal-valuedweightfor eachofthesefeatures,andthatthe6weightscorrespondingtothe6featuresare [ 2 : 5 ;  5 : 0 ;  1 : 2 ; 0 : 5 ; 2 : 0 ; 0 : 7 ] ,while b =0.1.(We'lldiscussinthenextsectionhow theweightsarelearned.)Theweight w 1 ,forexampleindicateshowimportanta featurethenumberofpositivelexiconwords( great , nice , enjoyable ,etc.)isto apositivesentimentdecision,while w 2 tellsustheimportanceofnegativelexicon words.Notethat w 1 = 2 : 5ispositive,while w 2 =  5 : 0,meaningthatnegativewords arenegativelyassociatedwithapositivesentimentdecision,andareabouttwiceas importantaspositivewords.  5.1  C LASSIFICATION : THESIGMOID 5 Figure5.2 Asampleminitestdocumentshowingtheextractedfeaturesinthevector x . Giventhese6featuresandtheinputreview x , P (+ j x ) and P ( j x ) canbecom- putedusingEq. 5.5 : p (+ j x )= P ( Y = 1 j x )= s ( w  x + b ) = s ([ 2 : 5 ;  5 : 0 ;  1 : 2 ; 0 : 5 ; 2 : 0 ; 0 : 7 ]  [ 3 ; 2 ; 1 ; 3 ; 0 ; 4 : 19 ]+ 0 : 1 ) = s ( : 833 ) = 0 : 70 (5.6) p ( j x )= P ( Y = 0 j x )= 1  s ( w  x + b ) = 0 : 30 LogisticregressioniscommonlyappliedtoallsortsofNLPtasks,andanyproperty oftheinputcanbeafeature.Considerthetaskof perioddisambiguation :deciding ifaperiodistheendofasentenceorpartofaword,byclassifyingeachperiod intooneoftwoclassesEOS(end-of-sentence)andnot-EOS.Wemightusefeatures like x 1 belowexpressingthatthecurrentwordislowercaseandtheclassisEOS (perhapswithapositiveweight),orthatthecurrentwordisinourabbreviations dictionary(ﬁProf.ﬂ)andtheclassisEOS(perhapswithanegativeweight).Afeature canalsoexpressaquitecomplexcombinationofproperties.Forexampleaperiod followinganuppercasewordislikelytobeanEOS,butiftheworditselfis St. and thepreviouswordiscapitalized,thentheperiodislikelypartofashorteningofthe word street . x 1 = ˆ 1ifﬁ Case ( w i )= Lowerﬂ 0otherwise x 2 = ˆ 1ifﬁ w i 2 AcronymDictﬂ 0otherwise x 3 = ˆ 1ifﬁ w i = St.& Case ( w i  1 )= Capﬂ 0otherwise Designingfeatures: Featuresaregenerallydesignedbyexaminingthetraining setwithaneyetolinguisticintuitionsandthelinguisticliteratureonthedomain.A carefulerroranalysisonthetrainingsetordevsetofanearlyversionofasystem oftenprovidesinsightsintofeatures. Forsometasksitisespeciallyhelpfultobuildcomplexfeaturesthatarecombi- nationsofmoreprimitivefeatures.Wesawsuchafeatureforperioddisambiguation above,whereaperiodontheword St. waslesslikelytobetheendofthesentence ifthepreviouswordwascapitalized.ForlogisticregressionandnaiveBayesthese combinationfeaturesor featureinteractions havetobedesignedbyhand. feature interactions
Logistic regression	Text classification	Binary classification	Natural Language Processing (NLP)
In sentiment classification, what tends to be more important: if a word occurs or how many times a word occurs?	In many text classification problems, including sentiment classification, a word occurring or not in a given text is more significant than its frequency. Binary NB (Naive Bayes) accounts for this by only counting a word that occurs once per text. For instance, consider the following text with a positive sentiment:  "The movie included not only great acting and great direction, but the plot was also excellent."  A standard Naive Bayes classifier would count 'the' and 'great' as positive words twice, whereas a Binary NB would count 'the' and 'great' as positive words only once.	What is one possible reason Binary NB can be more effective than standard naive Bayes for sentiment analysis?	A Binary NB will reduce noise of common words with no bearing on sentiment. 	A positive word, such as 'great', is more likely to appear multiple times in a positive text than it is to appear even once in a negative text.|||A Binary NB will help keep our weights in our model smaller, and smaller weights are usually more accurate.	Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||4.3  W ORKEDEXAMPLE 7 4.3Workedexample Let'swalkthroughanexampleoftrainingandtestingnaiveBayeswithadd-one smoothing.We'lluseasentimentanalysisdomainwiththetwoclassespositive (+)andnegative(-),andtakethefollowingminiaturetrainingandtestdocuments fromactualmoviereviews. Cat Documents Training - justplainboring - entirelypredictableandlacksenergy - nosurprisesandveryfewlaughs + verypowerful + themostfunofthesummer Test ? predictablewithnofun Theprior P ( c ) forthetwoclassesiscomputedviaEq. 4.11 as N c N doc : P (  )= 3 5 P (+)= 2 5 Theword with doesn'toccurinthetrainingset,sowedropitcompletely(as mentionedabove,wedon'tuseunknownwordmodelsfornaiveBayes).Thelike- lihoodsfromthetrainingsetfortheremainingthreewordsﬁpredictableﬂ,ﬁnoﬂ,and ﬁfunﬂ,areasfollows,fromEq. 4.14 (computingtheprobabilitiesfortheremainder ofthewordsinthetrainingsetisleftasanexerciseforthereader): P ( ﬁpredictableﬂ j )= 1 + 1 14 + 20 P ( ﬁpredictableﬂ j +)= 0 + 1 9 + 20 P ( ﬁnoﬂ j )= 1 + 1 14 + 20 P ( ﬁnoﬂ j +)= 0 + 1 9 + 20 P ( ﬁfunﬂ j )= 0 + 1 14 + 20 P ( ﬁfunﬂ j +)= 1 + 1 9 + 20 ForthetestsentenceS=ﬁpredictablewithnofunﬂ,afterremovingtheword`with', thechosenclass,viaEq. 4.9 ,isthereforecomputedasfollows: P (  ) P ( S j )= 3 5  2  2  1 34 3 = 6 : 1  10  5 P (+) P ( S j +)= 2 5  1  1  2 29 3 = 3 : 2  10  5 Themodelthuspredictstheclass negative forthetestsentence. 4.4OptimizingforSentimentAnalysis WhilestandardnaiveBayestextcanworkwellforsentimentanalysis, somesmallchangesaregenerallyemployedthatimproveperformance. First,forsentimenttionandanumberofothertexttasks, whetherawordoccursornotseemstomattermorethanitsfrequency.Thusit oftenimprovesperformancetoclipthewordcountsineachdocumentat1(see theendofthechapterforpointerstotheseresults).Thisvariantiscalled binary  8 C HAPTER 4  N AIVE B AYESAND S ENTIMENT C LASSIFICATION multinomialnaiveBayes or binaryNB .ThevariantusesthesameEq. 4.10 except binaryNB thatforeachdocumentweremoveallduplicatewordsbeforeconcatenatingthem intothesinglebigdocument.Fig. 4.3 showsanexampleinwhichasetoffour documents(shortenedandtext-normalizedforthisexample)areremappedtobinary, withthecountsshowninthetableontheright.Theexampleisworked withoutadd-1smoothingtomakethedifferencesclearer.Notethattheresultscounts neednotbe1;theword great hasacountof2evenforBinaryNB,becauseitappears inmultipledocuments. Fouroriginaldocuments:  itwaspathetictheworstpartwasthe boxingscenes  noplottwistsorgreatscenes + andsatireandgreatplottwists + greatscenesgreat Afterper-documentbinarization:  itwaspathetictheworstpartboxing scenes  noplottwistsorgreatscenes + andsatiregreatplottwists + greatscenes NBBinary CountsCounts +  +  and 2 0 1 0 boxing0101 1010 great 3 1 2 1 it0101 no0101 or0101 part0101 pathetic0101 plot1111 satire1010 scenes1212 the 0 2 0 1 twists1111 was 0 2 0 1 worst0101 Figure4.3 AnexampleofbinarizationforthebinarynaiveBayesalgorithm. Asecondimportantadditioncommonlymadewhendoingtextfor sentimentistodealwithnegation.Considerthedifferencebetween Ireallylikethis movie (positive)and Ididn'tlikethismovie (negative).Thenegationexpressedby didn't completelyalterstheinferenceswedrawfromthepredicate like .Similarly, negationcanmodifyanegativewordtoproduceapositivereview( don'tdismissthis  , doesn'tletusgetbored ). Averysimplebaselinethatiscommonlyusedinsentimentanalysistodealwith negationisthefollowing:duringtextnormalization,prependthe NOT to everywordafteratokenoflogicalnegation( n't,not,no,never )untilthenextpunc- tuationmark.Thusthephrase didntlikethismovie,butI becomes didntNOT_likeNOT_thisNOT_movie,butI Newlyformed`words'like NOT like , NOT recommend willthusoccurmoreof- teninnegativedocumentandactascuesfornegativesentiment,whilewordslike NOT bored , NOT dismiss willacquirepositiveassociations.WewillreturninChap- ter17totheuseofparsingtodealmoreaccuratelywiththescoperelationshipbe- tweenthesenegationwordsandthepredicatestheymodify,butthissimplebaseline worksquitewellinpractice. Finally,insomesituationswemighthaveinsuflabeledtrainingdatato trainaccuratenaiveBayesusingallwordsinthetrainingsettoestimate positiveandnegativesentiment.Insuchcaseswecaninsteadderivethepositive
Naive Bayes	Text classification	Binary Naive Bayes (NB)	Sentiment analysis
What is a conceptual metaphor?	Conceptual metaphors are conceptual mappings - conceptualizing of something in one domain of thought in terms of another. For example, "understanding is vision."  Conceptual metaphors, like "understanding is vision" are not things people say; they are ideas that underlie what people say. So, we can say that ideas are clear, dim, or bright. We say "I see what you mean" and "let's look at the problem." Conceptual metaphors can generate many ways of speaking. Simple metaphors are individual phrases, like "the mind is a lantern."Before the discovery of conceptual metaphors, nobody realized that most of what we say is partly metaphorical. And they are significant because they support the ideas of cognitive semantics, an approach to meaning that contradicts traditional philosophical semantics.	Which of the following is NOT metaphorical according to George Lakoff and his references?	"The sun rose over the plain."	"Your theory crumbles under criticism."|||Grammar|||The way we think about causality	The Contemporary Theory of Metaphor||slides||The Contemporary Theory of MetaphorGeorge Lakoff(c) Copyright George Lakoff, 1992To Appear in Ortony, Andrew (ed.) Metaphor and Thought (2nd edition), CambridgeUniversity Press.Do not go gentle into that good night. -Dylan ThomasDeath is the mother of beauty . . . -Wallace Stevens, Sunday MorningIntroductionThese famous lines by Thomas and Stevens are examples of what classical theorists, atleast since Aristotle, have referred to as metaphor: instances of novel poetic language inwhich words like mother, go, and night are not used in their normal everyday senses. Inclassical theories of language, metaphor was seen as a matter of language not thought.Metaphorical expressions were assumed to be mutually exclusive with the realm ofordinary everyday language: everyday language had no metaphor, and metaphor usedmechanisms outside the realm of everyday conventional language. The classical theorywas taken so much for granted over the centuries that many people didn't realize that itwas just a theory. The theory was not merely taken to be true, but came to be taken asdefinitional. The word metaphor was defined as a novel or poetic linguistic expressionwhere one or more words for a concept are used outside of its normal conventionalmeaning to express a similar concept. But such issues are not matters for definitions; theyare empirical questions. As a cognitive scientist and a linguist, one asks: What are thegeneralizations governing the linguistic expressions re ferred to classically as poeticmetaphors? When this question is answered rigorously, the classical theory turns out to befalse. The generalizations governing poetic metaphorical expressions are not in language,but in thought: They are general map pings across conceptual domains. Moreover, thesegeneral princi ples which take the form of conceptual mappings, apply not just to novelpoetic expressions, but to much of ordinary everyday language. In short, the locus ofmetaphor is not in language at all, but in the way we conceptualize one mental domain interms of another. The general theory of metaphor is given by characterizing such cross-domain mappings. And in the process, everyday abstract concepts like time, states,change, causation, and pur pose also turn out to be metaphorical. The result is thatmetaphor (that is, cross-domain mapping) is absolutely central to ordinary naturallanguage semantics, and that the study of literary metaphor is an extension of the study ofeveryday metaphor. Everyday metaphor is characterized by a huge system of thousands ofcross-domain mappings, and this system is made use of in novel metaphor. Because ofthese empirical results, the word metaphor has come to be used differently incontemporary metaphor research. The word metaphor has come to mean a cross-domainmapping in the conceptual system. The term metaphorical expression refers to a linguistic expression (a word, phrase, or sentence) that is the surface realization of such a cross-domain mapping (this is what the word metaphor referred to in the old theory). I willadopt the contemporary usage throughout this chapter. Experimental resultsdemonstrating the cognitive reali ty of the extensive system of metaphorical mappings arediscussed by Gibbs (this volume). Mark Turner's 1987 book, Death is the mother ofbeauty, whose title comes from Stevens' great line, demonstrates in detail how that lineuses the ordinary system of everyday mappings. For further examples of how literarymetaphor makes use of the ordinary metaphor system, see More Than Cool Reason: AField Guide to Poetic Metaphor, by Lakoff and Turner (1989) and Reading Minds: TheStudy of English in the Age of Cognitive Science, by Turner (1991). Since the everydaymetaphor system is central to the understanding of poetic metaphor, we will begin withthe everyday system and then turn to poetic examples.Homage To ReddyThe contemporary theory that metaphor is primarily conceptual, conventional, and part ofthe ordinary system of thought and language can be traced to Michael Reddy's (thisvolume) now classic paper, The Conduit Metaphor, which first appeared in the firstedition of this collection. Reddy did far more in that paper than he modestly suggested.With a single, thoroughly analyzed example, he allowed us to see, albeit in a restricteddomain, that ordinary everyday English is largely metaphorical, dispelling once and forall the traditional view that metaphor is primarily in the realm of poetic or figurativelanguage. Reddy showed, for a single very significant case, that the locus of metaphor isthought, not language, that metaphor is a major and indispensable part of our ordinary,conventional way of conceptualizing the world, and that our everyday behavior reflectsour metaphorical understanding of experience. Though other theorists had noticed someof these characteristics of metaphor, Reddy was the first to demonstrate it by rigorouslinguistic analysis, stating generalizations over voluminous examples. Reddy's chapter onhow we conceptualize the concept of communication by metaphor gave us a tiny glimpseof an enormous system of conceptual metaphor. Since its appearance, an entire branch oflinguis tics and cognitive science has developed to study systems of metaphorical thoughtthat we use to reason, that we base our actions on, and that underlie a great deal of thestructure of language. The bulk of the chapters in this book were written before thedevelopment of the contemporary field of metaphor research. My chapter will thereforecontradict much that appears in the others, many of which make certain assumptions thatwere widely taken for granted in 1977. A major assumption that is challenged bycontemporary research is the traditional division between literal and figurative language,with metaphor as a kind of figurative language. This entails, by definition, that: What isliteral is not metaphorical. In fact, the word literal has traditionally been used with one ormore of a set of assumptions that have since proved to be false:Traditional false assumptions· All everyday conventional language is literal, and none is metaphorical.· All subject matter can be comprehended literally, without metaphor.· Only literal language can be contingently true or false.· All definitions given in the lexicon of a language are literal, not metaphorical.· The concepts used in the grammar of a language are all literal; none aremetaphorical. The big difference between the contemporary theory and views of metaphor prior toReddy's work lies in this set of assumptions. The reason for the difference is that, in theintervening years, a huge system of everyday, convention al, conceptual metaphors hasbeen discovered. It is a system of metaphor that structures our everyday conceptualsystem, including most abstract concepts, and that lies behind much of everydaylanguage. The discovery of this enormous metaphor system has destroyed the traditionalliteral-figurative distinction, since the term literal, as used in defining the traditionaldistinction, carries with it all those false assumptions. A major difference between thecontemporary theory and the classical one is based on the old literal-figurative distinction.Given that distinction, one might think that one arrives at a metaphorical interpretation ofa sentence by starting with the literal meaning and applying some algorithmic process toit (see Searle, this volume). Though there do exist cases where something like thishappens, this is not in general how metaphor works, as we shall see shortly.What is not metaphoricalAlthough the old literal-metaphorical distinction was based on assumptions that haveproved to be false, one can make a different sort of literal-metaphorical distinction: thoseconcepts that are not comprehended via conceptual metaphor might be called literal.Thus, while I will argue that a great many common concepts like causation and purposeare metaphorical, there is nonetheless an extensive range of nonmetaphorical concepts.Thus, a sentence like The balloon went up is not metaphorical, nor is the old philosopher'sfavorite The cat is on the mat. But as soon as one gets away from concrete physicalexperience and starting talking about abstractions or emotions, metaphoricalunderstanding is the norm.The Contemporary Theory: SomeExamplesLet us now turn to some examples that are illustrative of contemporary metaphorresearch. They will mostly come from the domain of everyday conventional metaphor,since that has been the main focus of the research. I will turn to the discussion of poeticmetaphor only after I have discussed the conventional system, since knowledge of theconventional system is needed to make sense of most of the poetic cases. The evidencefor the existence of a system of conventional conceptual metaphors is of five types:-Generalizations governing polysemy, that is, the use of words with a number ofrelated meanings.-Generalizations governing inference patterns, that is, cases where a pattern ofinferences from one conceptual domain is used in another domain.-Generalizations governing novel metaphorical language (see, Lakoff & Turner,1989).-Generalizations governing patterns of semantic change (see, Sweetser, 1990).-Psycholinguistic experiments (see, Gibbs, 1990, this volume).We will primarily be discussing the first three of these sources of evidence, since they arethe most robust.
Conceptual metaphors	Semantics	Cognitive semantics	George Lakoff	Literalness	Figurative language	Conceptual meaning
What are langue and parole?	This distinction was introduced by Ferdinand de Saussure and they refer to "language" in two very different senses. Langue, "tongue," refers to language in the abstract – the system which makes it possible to speak a particular language and understand it. Parole refers to "speech" – what people actually say. Langue is made of general patterns. Each instance of parole is unique. Saussure made the distinction so that he could explain that he was mainly only going to write about langue.  Other famous linguists made the same distinction in other words.  Most people now call these "knowledge of a grammar" (langue) and "performance" (parole). Like Saussure, most linguists were not very interested in parole, except as a way to probe langue, until recent decades.	Which of the following is NOT true of parole and langue?	Parole follows no patterns or rules. Langue is mostly rules.	Every instance of parole is potentially unique. Langue is mainly about generalizations.|||Parole is empirical. Langue is abstraction.|||Parole depends on knowledge of langue.	Langue and Parole||publication||Langue and Parole  John Phillips  The distinction between the French words,  langue (language or tongue) and parole (speech), enters the vocabulary of theoretical linguistics with Ferdinand de Saussure™s  Course in General Linguistics, which was published posthumously in 1915 after  having been collocated from student notes.  La langue denotes the abstract systematic  principles of a language, without which no meaningful utterance ( parole) would be  possible.  The  Course manifests a shift from the search  for origins and ideals, typical  of nineteenth century science, to the establishment of  systems.  The modern notion of system is reflected in the title of the course:  General Linguistics.  Saussure in this way indicates that the course will be about language  in general: not this  or that particular  language (Chinese or French) and not this or that aspect (phonetics or semantics).  A  general linguistics would be impossible  by empirical means because there exist  innumerable objects that can be considered  linguistic.  Instead Saussure™s  methodology allows him to establish a coherent  object for linguistics in the distinction between langue and parole.  Langue represents the ﬁwork of a collective in telligence,ﬂ which is both internal to  each individual and collective, in so far as  it is beyond the will of  any individual to  change.  Parole, on the other hand, designates individual acts, statements and  utterances, events of language use mani festing each time a speaker™s ephemeral  individual will through his combination of concepts and his ﬁphonationﬂŠthe formal   aspects of the utterance.  Saussure points out that the single word ﬁlinguisticsﬂ  therefore covers two very different kinds of study.  The study of parole would be  entirely focused on individual utterances, using all the available  resources of formal and empirical study to analyze actual statem ents, usually within a specific language.   The study of langue would be focused instead on generally applicable  conditions of possibility.  The Course thus follows the second route in this inevitable ﬁbifurcation,ﬂ setting out the groundwork for all attempts to  grasp the basic conditions of possibility for language and language use generally.   There would be no coherent and meaningful utterance wit hout the institution of norms that Saussure calls  langue.  So it is this that forms the object  of study for modern linguistics.   Such an object could not   ever be made visible (as a stretch of text can) but one  can in principle establish the  rules and conditions that make it possible to  speak and write in meaningful ways.   Langue and parole has been translated by alternative semiotic categories like  system and process (A J Greimas) or code and message (Roman Jakobson), which interpret  Saussure™s distinction in specific ways.  The main assumptions of structuralism and  semiology (or semiotics) would be that for  every process (an utterance for instance) there is a system of underlying laws that  govern it; and that the system arises  contingently (there are no natural or necessary reasons for th e relations within it to be  as they are).   The scientific approach to systems, inheri ted by Saussure, assumes that the elements  which make them up correspond to organized and integrated unities.  Each element in  a system should be located in  its place on the web of relationships between elements.   The elements of the linguistic system ar e, however, the mental phenomena called  signs.  A sign is comprised of both a mental  image (signifier) and an idea (signified).   Saussure™s most famous statement concer ns how these signs ar e differentiated in themselves and related to each other.  ﬁIn language,ﬂ he says, ﬁthere are only  differences without positive terms.ﬂ  He di stinguishes between meaning and value to  get the point across.  ﬁWhat we find, instead of  ideas being given in advance, are values emanating from a linguistic system.  If we say that these values correspond to  certain concepts, it must be understood that  the concepts in question are purely differential.  That is to say they are con cepts defined not positively, in terms of their  content, but negatively by contrast with other items in the same system.  What  characterizes each most exactly is being whatever the others are notﬂ (CGL 115).   The notion of value thus designates a quality that is entirely  relative to other values in  the system.  The concept of a dog or a cat, a  virtue or a crime, gets its value as a  linguistic unit entirely relative to the values of all the other linguistic units.  So no linguistic unit can be regarded as a positive pre-existing entity or idea (whether   concept or mark).  To define a linguistic unit,  rather, is to specify in what ways it is  similar to or different from the othe r units within the system.  Two marks  a and b are not, despite appearances, grasped positivel y by our consciousness.  We grasp the  difference between  a and  b etc.  It is for this reason, Saussure says, that each sign  ﬁremains free to change in accordance with laws quite unconnected with their   signifying functionﬂ (116).  Linguistic item s are therefore always based, ultimately,   upon their non-coincidence with the others.  This what also allows considerable flexibility in their relationsŠthe  play between signifiers and between signifiers and signifieds, their difference .  Language can be analysed according to two different poles, or axes, which relate  precisely to the difference between parole and langue.  On the syntagmatic axis are found the visible or audible elements of the utterance  itself, e.g., ﬁO time, thy  pyramids.ﬂ  On the paradigmatic axis (from a Greek word,  paradeigma, meaning  ﬁexampleﬂ) the utterance remains tied to a nd governed by the system out of which it  is generated.  An utterance is an exampl e of one of the uncountab le possibilities that the system makes possible.  The system  governs all possible relations between signifiers and signifieds.  The linguist Roman Jakobson suggested that the functions  of language can be understood according to the way the paradigmatic and syntagmatic  axes of language interact, with the figurative dimension of me tonymy operating on  the syntagmatic axis and that of metaphor  on the paradigmatic one.  The syntagmatic  axis features contiguous elements, elements found next to each other, related by  association (as the Sun is related to Day).   The paradigmatic axis features elements  that are not present together but may be substituted for each other (as the Sun might  substitute for the clear light of Truth).     To the distinction between langue and parole, corresponds a further distinction between the subject of the énunciation (the exercise of language) and subject of the énoncé (the statement made).  To illustrate  this, the French linguist Émile Benveniste  focuses on the role and implications of  the ubiquitous first and second person pronouns, used at least implicitly in every language known to ma n and woman.  The  first person, ﬁI,ﬂ operates in a way quite unlike other pronouns because it is  essentially linked to the exercise of language.  In other words, the sign I links  Saussure™s two dimensions of language, the collective intelligence of  langue and the  ephemeral individual acts of  parole: ﬁit is this property that establishes the basis for  individual discourse, in which each speaker takes over all the resources of language   for his own behalfﬂ (220).  In fact the I not only links the otherwise heterogeneous dimensions of  langue and parole but it also keeps its speakers unaware of this  profound difference.  The signs I and you are essentially empty of meaning except  when they are being used, so the reality to which  I or you refers is solely a reality of   discourse.  These signs cannot be misused because they ﬁdo not assert anything, they are not subject to the condition of truth and escape all denialﬂ (220).  The implications are far reaching.  First by indicating the situation of the speaker yet by escaping the conditions normally attributed to language , the pronoun tells us something about the  relation of the human animal to the language  she speaks.  Language is not something  the human subject merely  uses (as is often asserted), but rather, the human subject is  something made possible  by language.  An irreducible division emerges between  enunciation and statement ( énoncé) corresponding to that between langue and parole.  Emphasis on the statement would draw attention to its content, its sense and  reference, whether it is true or false, and on what attributes or qualities are predicated  of what subject.  For instance the statement ﬁthis vase is yellowﬂ perhaps predicates  some actually existing ornament with a yellow  color and can thus be tested against the  actually existing ornament for  truth or falsity.  A shift of  emphasis onto the mode of  enunciation, away from merely what is being  said, would instead look at how, by saying it, the speaker is constituted institut ionally in some way or another according  to value and status.  Focus shifts to the  performance or the practice of speaking in this  way or thatŠthe role it plays in constituting or  perpetuating a particular world of  discourse.  The speaker is no longer a subject with autonomous feelings and thoughts but, rather, is constituted as this or that  according to a repeatable modality of  discourse.    Roland Barthes calls the different ways of addressing oneself to others ﬁimage- repertoires.ﬂ   In ﬁFrom Science to Litera tureﬂ Barthes uses lessons learned from the  failure of structuralism to turn itself into  the science of literature, to establish the  enunciative modality of science itself.  In this way he forces science (in his own  structuralist hands) to reflect on its own cond itions of being a science, thereby taking  it outside science (still understood on the model of predicative and cause-effect  oriented statements).  Science in his hands  thus becomes a kind  of literature as it begins to more and more rigorously attempt to  come to terms with  the impossibility of  grasping its own conditions of possibility, as a scientist would grasp the identifiable  qualities of some object.       In psychoanalytic theory the distinction corresponds to Sigmund Freud™s distinction  between consciousness and the unconscious.  Jacques Lacan argues that since the  subject comes into being through  language he does so through the exercise  of signifying articulation.  As soon as he come s into being he finds himself not as he  is (what Lacan would call the truth of his being) but as he  imagines himself to beŠthat is,  as a representation (at the level of the statemen t).  In order to discover the subject of the  unconscious the analyst must focus on the level of enunciation (performance,  expression) in order to recognize the truth of the subject in the articulation of language, its enunciation.  So the relation between statement and enunciation (the said and the  saying) actualizes the divided structure of the psychoanalytic subject and helps to clarify the difference between the imaginary  (fixed and complete  image of person) and  the symbolic (the constitutive function of  language).  The distinction between the  symbolic and the imaginary thus maps ont o that between langue and parole.  Lacan also argues that metaphor and metonymy, lo cated by Jacobson on the paradigmatic and  syntagmatic axes respectively, correspond to  Freud™s accounts of the displacements  and condensations of dreams, t hus providing the analyst a further resource in a kind of literary criticism.  A displacement censors  a dream by substituting relatively harmless  images or objects for those more likely to  cause anxiety.  Condensation merges images,  places them together, confusing them.       The concept of  langue must ultimately be regarded as  a triumph not of nineteenth  century science but of the theoretical imagination; that  is, langue should be regarded not as a reality (an ideality) but as a theoretical fiction, a catachresis that borrows the idea of system to account for the ways in which institutions like languages are   established and maintained in quasi syst ematic ways.  Jacques Derrida draws on  Saussure™s structuralism while at the same time exceeding it.  In  Of Grammatology, and with yet more technical precision in ﬁS ignature Event Context,ﬂ he demonstrates  how the notion of event (parole) exceeds the system ( langue) that theoretically makes  it possible.  Every system, ensemble, instit utionŠanything, that is, which brings its  elements together under a pr inciple or set of principlesŠcomes about on the basis of  certain conditions.  And such ensembles tend to produce moments when those  conditions are thematized, dramatized or  otherwise represented.  These are the  moments of self-identificati on, producing myths and ficti ons as factual narratives.   Saussure offers a privileged example, then, by producing a narrative (a parole)  about  the relationship between langue and parole.  Such representations can always be compared with the actual condi tions themselves, which are al ways also readable at the level of an ensemble™s performance, as repe atable and even sedimented principles or  axioms.  A forceful principle of interpre tation can therefore be  located in what  Derrida calls the  deconstruction of (the disjuncture or gap between) the statement  about and the performance of the relation between the ensemble and the conditions of  its own existence.
Ferdinand de Saussure	Langue	Parole	Performance	Knowledge of language
How, in general, are the meanings of ‘words’ (lexical units) most often represented in computational semantics today?	Almost all work in computational semantics today represents word-meanings in terms of vectors known as ‘embeddings’ which encode the frequencies, or probabilities, of other words, or sequences of words, occurring in the vicinity of the word whose meaning that vector represents. Or to put it another way, these models assign each word to a point in a vector space corresponding to a certain set of relations with other words. This embodies a relational theory of meaning, implying that the meaning of a word IS all of the ways in which it can be used, relative to other words. The truth-conditional theory of meaning does not represent the meanings of individual content words at all; they are called ‘non-logical terms’ and their meaning is defined as the sets of entities in the world that they refer to.	Which of the following is true about computational representations of word-meaning?	word-vectors and truth-conditions represent completely different aspects of meaning	logical terms can be thought of as representing ‘embeddings’|||word-vectors represent extensional meaning|||Logical predication is equivalent to vector multiplication	Chapter section on lexical meaning in computational semantics||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 6 VectorSemanticsandEmbed- dings TheasphaltthatLosAngelesisfamousforoccursmainlyonitsfreeways.Butinthe middleofthecityisanotherpatchofasphalt,theLaBreatarpits,andthisasphalt preservesmillionsoffossilbonesfromthelastoftheIceAgesofthePleistocene Epoch.Oneofthesefossilsisthe Smilodon ,orsabre-toothedtiger,instantlyrec- ognizablebyitslongcanines.Fivemillionyearsagoorso,acompletelydifferent sabre-toothtigercalled Thylacosmilus lived inArgentinaandotherpartsofSouthAmer- ica.Thylacosmiluswasamarsupialwhereas Smilodonwasaplacentalmammal,butThy- lacosmilushadthesamelonguppercanines and,likeSmilodon,hadaprotectivebone onthelowerjaw.Thesimilarityof thesetwomammalsisoneofmanyexamples ofparallelorconvergentevolution,inwhichparticularcontextsorenvironments leadtotheevolutionofverysimilarstructuresindifferentspecies (Gould,1980) . Theroleofcontextisalsoimportantinthesimilarityofalessbiologicalkind oforganism:theword.Wordsthatoccurin similarcontexts tendtohave similar meanings .Thislinkbetweensimilarityinhowwordsaredistributedandsimilarity inwhattheymeaniscalledthe distributionalhypothesis .Thehypothesiswas distributional hypothesis formulatedinthe1950sbylinguistslike Joos(1950) , Harris(1954) ,and Firth (1957) ,whonoticedthatwordswhicharesynonyms(like oculist and eye-doctor ) tendedtooccurinthesameenvironment(e.g.,nearwordslike eye or examined ) withtheamountofmeaningdifferencebetweentwowordsﬁcorrespondingroughly totheamountofdifferenceintheirenvironmentsﬂ (Harris,1954,157) . Inthischapterweintroduce vectorsemantics ,whichinstantiatesthislinguistic vector semantics hypothesisbylearningrepresentationsofthemeaningofwords,called embeddings , embeddings directlyfromtheirdistributionsintexts.Theserepresentationsareusedinevery naturallanguageprocessingapplicationthatmakesuseofmeaning,andunderliethe morepowerful contextualizedwordrepresentations like ELMo and BERT that wewillintroduceinChapter10. Thesewordrepresentationsarealsotheexampleinthisbookof repre- sentationlearning ,automaticallylearningusefulrepresentationsoftheinputtext. representation learning Findingsuch self-supervised waystolearnrepresentationsoftheinput,insteadof creatingrepresentationsbyhandvia featureengineering ,isanimportantfocusof NLPresearch (Bengioetal.,2013) . We'llbegin,however,byintroducingsomebasicprinciplesofwordmeaning, whichwillmotivatethevectorsemanticmodelsofthischapteraswellasextensions thatwe'llreturntoinChapter19,Chapter20,andChapter21.  2 C HAPTER 6  V ECTOR S EMANTICSAND E MBEDDINGS 6.1LexicalSemantics Howshouldwerepresentthemeaningofaword?IntheN-grammodelswesaw inChapter3,andinmanytraditionalNLPapplications,ouronlyrepresentationof awordisasastringofletters,orperhapsasanindexinavocabularylist.This representationisnotthatdifferentfromatraditioninphilosophy,perhapsyou've seenitinintroductorylogicclasses,inwhichthemeaningofwordsisrepresented byjustspellingthewordwithsmallcapitalletters;representingthemeaningof ﬁdogﬂas DOG ,andﬁcatﬂas CAT ). Representingthemeaningofawordbycapitalizingitisaprettyunsatisfactory model.Youmighthaveseentheoldphilosophyjoke: Q:What'sthemeaningoflife? A: LIFE Surelywecandobetterthanthis!Afterall,we'llwantamodelofwordmeaning todoallsortsofthingsforus.Itshouldtellusthatsomewordshavesimilarmean- ings( cat issimilarto dog ),otherwordsareantonyms( cold istheoppositeof hot ).It shouldknowthatsomewordshavepositiveconnotations( happy )whileothershave negativeconnotations( sad ).Itshouldrepresentthefactthatthemeaningsof buy , sell ,and pay offerdifferingperspectivesonthesameunderlyingpurchasingevent (IfIbuysomethingfromyou,you'veprobablysoldittome,andIlikelypaidyou). Moregenerally,amodelofwordmeaningshouldallowustodrawusefulinfer- encesthatwillhelpussolvemeaning-relatedtaskslikequestion-answering,sum- marization,detectingparaphrasesorplagiarism,anddialogue. Inthissectionwesummarizesomeofthesedesiderata,drawingonresultsinthe linguisticstudyofwordmeaning,whichiscalled lexicalsemantics ;we'llreturnto lexical semantics andexpandonthislistinChapter19. LemmasandSenses Let'sstartbylookingathowoneword(we'llchoose mouse ) mightbeinadictionary: 1 mouse(N) 1.anyofnumeroussmallrodents... 2.ahand-operateddevicethatcontrolsacursor... Heretheform mouse isthe lemma ,alsocalledthe citationform .Theform lemma citationform mouse wouldalsobethelemmafortheword mice ;dictionariesdon'thaveseparate forformslike mice .Similarly sing isthelemmafor sing , sang , sung .Inmanylanguagestheveformisusedasthelemmafortheverb,so Spanish dormir ﬁtosleepﬂisthelemmafor duermes ﬁyousleepﬂ.Theforms sung or carpets or sing or duermes arecalled wordforms . wordform Astheexampleaboveshows,eachlemmacanhavemultiplemeanings;the lemma mouse canrefertotherodentorthecursorcontroldevice.Wecalleach oftheseaspectsofthemeaningof mouse a wordsense .Thefactthatlemmascan be polysemous (havemultiplesenses)canmakeinterpretationdif(issomeone whotypesﬁmouseinfoﬂintoasearchenginelookingforapetoratool?).Chapter19 willdiscusstheproblemofpolysemy,andintroduce wordsensedisambiguation , thetaskofdeterminingwhichsenseofawordisbeingusedinaparticularcontext. Synonymy Oneimportantcomponentofwordmeaningistherelationshipbe- tweenwordsenses.Forexamplewhenonewordhasasensewhosemeaningis 1 ThisexampleshortenedfromtheonlinedictionaryWordNet,discussedinChapter19.  6.1  L EXICAL S EMANTICS 3 identicaltoasenseofanotherword,ornearlyidentical,wesaythetwosensesof thosetwowordsare synonyms .Synonymsincludesuchpairsas synonym couch/sofavomit/throwupcar/automobile Amoreformalofsynonymy(betweenwordsratherthansenses)isthat twowordsaresynonymousiftheyaresubstitutableonefortheotherinanysentence withoutchangingthe truthconditions ofthesentence,thesituationsinwhichthe sentencewouldbetrue.Weoftensayinthiscasethatthetwowordshavethesame propositionalmeaning . propositional meaning Whilesubstitutionsbetweensomepairsofwordslike car / automobile or wa- ter / H 2 O aretruthpreserving,thewordsarestillnotidenticalinmeaning.Indeed, probablynotwowordsareabsolutelyidenticalinmeaning.Oneofthefundamen- taltenetsofsemantics,calledthe principleofcontrast ( Girard1718 , Br ´ eal1897 , principleof contrast Clark1987 ),istheassumptionthatadifferenceinlinguisticformisalwaysassoci- atedwithatleastsomedifferenceinmeaning.Forexample,theword H 2 O isused incontextsandwouldbeinappropriateinahikingguideŠ water wouldbe moreappropriateŠandthisdifferenceingenreispartofthemeaningoftheword. Inpractice,theword synonym isthereforecommonlyusedtodescribearelationship ofapproximateorroughsynonymy. WordSimilarity Whilewordsdon'thavemanysynonyms,mostwordsdohave lotsof similar words. Cat isnotasynonymof dog ,but cats and dogs arecertainly similarwords.Inmovingfromsynonymytosimilarity,itwillbeusefultoshiftfrom talkingaboutrelationsbetweenwordsenses(likesynonymy)torelationsbetween words(likesimilarity).Dealingwithwordsavoidshavingtocommittoaparticular representationofwordsenses,whichwillturnouttosimplifyourtask. Thenotionofword similarity isveryusefulinlargersemantictasks.Know- similarity inghowsimilartwowordsarecanhelpincomputinghowsimilarthemeaningof twophrasesorsentencesare,averyimportantcomponentofnaturallanguageun- derstandingtaskslikequestionanswering,paraphrasing,andsummarization.One wayofgettingvaluesforwordsimilarityistoaskhumanstojudgehowsimilarone wordistoanother.Anumberofdatasetshaveresultedfromsuchexperiments.For exampletheSimLex-999dataset (Hilletal.,2015) givesvaluesonascalefrom0to 10,liketheexamplesbelow,whichrangefromnear-synonyms( vanish , disappear ) topairsthatscarcelyseemtohaveanythingincommon( hole , agreement ): vanish disappear 9.8 behave obey 7.3 belief impression 5.95 muscle bone 3.65 modest xible 0.98 hole agreement 0.3 WordRelatedness Themeaningoftwowordscanberelatedinwaysotherthan similarity.Onesuchclassofconnectionsiscalledword relatedness (Budanitsky relatedness andHirst,2006) ,alsotraditionallycalledword association inpsychology. association Considerthemeaningsofthewords coffee and cup .Coffeeisnotsimilartocup; theysharepracticallynofeatures(coffeeisaplantorabeverage,whileacupisa manufacturedobjectwithaparticularshape). Butcoffeeandcupareclearlyrelated;theyareassociatedbyco-participatingin aneverydayevent(theeventofdrinkingcoffeeoutofacup).Similarlythenouns|||Chapter section on truth-conditional semantics||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 16 LogicalRepresentationsof SentenceMeaning I SHMAEL : Surelyallthisisnotwithoutmeaning. HermanMelville, MobyDick Inthischapterweintroducetheideathatthemeaningoflinguisticexpressionscan becapturedinformalstructurescalled meaningrepresentations .Considertasks meaning representations thatrequiresomeformofsemanticprocessing,likelearningtouseanewpieceof softwarebyreadingthemanual,decidingwhattoorderatarestaurantbyreading amenu,orfollowingarecipe.Accomplishingthesetasksrequiresrepresentations thatlinkthelinguisticelementstothenecessarynon-linguistic knowledgeofthe world .Readingamenuanddecidingwhattoorder,givingadviceaboutwhereto gotodinner,followingarecipe,andgeneratingnewrecipesallrequireknowledge aboutfoodanditspreparation,whatpeopleliketoeat,andwhatrestaurantsarelike. Learningtouseapieceofsoftwarebyreadingamanual,orgivingadviceonusing software,requiresknowledgeaboutthesoftwareandsimilarapps,computers,and usersingeneral. Inthischapter,weassumethatlinguisticexpressionshavemeaningrepresenta- tionsthataremadeupofthe samekindofstuff thatisusedtorepresentthiskindof everydaycommon-senseknowledgeoftheworld.Theprocesswherebysuchrepre- sentationsarecreatedandassignedtolinguisticinputsiscalled semanticparsing or semantic parsing semanticanalysis ,andtheentireenterpriseofdesigningmeaningrepresentations andassociatedsemanticparsersisreferredtoas computationalsemantics . computational semantics 9 e ; yHaving ( e ) ^ Haver ( e ; Speaker ) ^ HadThing ( e ; y ) ^ Car ( y ) Figure16.1 Alistofsymbols,twodirectedgraphs,andarecordstructure:asamplerof meaningrepresentationsfor Ihaveacar. ConsiderFig. 16.1 ,whichshowsexamplemeaningrepresentationsforthesen- tence Ihaveacar usingfourcommonlyusedmeaningrepresentationlanguages. Thetoprowillustratesasentencein First-OrderLogic ,coveredindetailinSec- tion 16.3 ;thedirectedgraphanditscorrespondingtextualformisanexampleof an AbstractMeaningRepresentation(AMR) form (Banarescuetal.,2013) ,and ontherightisa frame-based or  representation,discussedinSection 16.5 andagaininChapter18.  2 C HAPTER 16  L OGICAL R EPRESENTATIONSOF S ENTENCE M EANING Whiletherearenon-trivialdifferencesamongtheseapproaches,theyallshare thenotionthatameaningrepresentationconsistsofstructurescomposedfroma setofsymbols,orrepresentationalvocabulary.Whenappropriatelyarranged,these symbolstructuresaretakento correspond toobjects,propertiesofobjects,andrela- tionsamongobjectsinsomestateofaffairsbeingrepresentedorreasonedabout.In thiscase,allfourrepresentationsmakeuseofsymbolscorrespondingtothespeaker, acar,andarelationdenotingthepossessionofonebytheother. Importantly,theserepresentationscanbeviewedfromatleasttwodistinctper- spectivesinalloftheseapproaches:asrepresentationsofthemeaningofthepar- ticularlinguisticinput Ihaveacar ,andasrepresentationsofthestateofaffairsin someworld.Itisthisdualperspectivethatallowstheserepresentationstobeused tolinklinguisticinputstotheworldandtoourknowledgeofit. Inthenextsectionswegivesomebackground:ourdesiderataforameaning representationlanguageandsomeguaranteesthattheserepresentationswillactually dowhatweneedthemtodoŠprovideacorrespondencetothestateofaffairsbeing represented.InSection 16.3 weintroduceFirst-OrderLogic,historicallytheprimary techniqueforinvestigatingnaturallanguagesemantics,andseeinSection 16.4 how itcanbeusedtocapturethesemanticsofeventsandstatesinEnglish.Chapter17 thenintroducestechniquesfor semanticparsing :generatingtheseformalmeaning representationsgivenlinguisticinputs. 16.1ComputationalDesiderataforRepresentations Let'sconsiderwhymeaningrepresentationsareneededandwhattheyshoulddofor us.Tofocusthisdiscussion,let'sconsiderasystemthatgivesrestaurantadviceto touristsbasedonaknowledgebase. V Considerthefollowingsimplequestion: (16.1) DoesMaharaniservevegetarianfood? Toanswerthisquestion,wehavetoknowwhatit'sasking,andknowwhetherwhat it'saskingistrueofMahariniornot. v isasystem'sabilitytocompare v thestateofaffairsdescribedbyarepresentationtothestateofaffairsinsomeworld asmodeledinaknowledgebase.Forexamplewe'llneedsomesortofrepresentation like Serves ( Maharani ; VegetarianFood ) ,whichasystemcancanmatchagainstits knowledgebaseoffactsaboutparticularrestaurants,andifitarepresentation matchingthisproposition,itcanansweryes.Otherwise,itmusteithersay No ifits knowledgeoflocalrestaurantsiscomplete,orsaythatitdoesn'tknowifitknows itsknowledgeisincomplete. UnambiguousRepresentations Semantics,likealltheotherdomainswehavestudied,issubjecttoambiguity.Words andsentenceshavedifferentmeaningrepresentationsindifferentcontexts.Consider thefollowingexample: (16.2) Iwannaeatsomeplacethat'scloseto ICSI . Thissentencecaneithermeanthatthespeakerwantstoeat at somenearbylocation, orunderaGodzilla-as-speakerinterpretation,thespeakermaywanttodevoursome  16.1  C OMPUTATIONAL D ESIDERATAFOR R EPRESENTATIONS 3 nearbylocation.Thesentenceisambiguous;asinglelinguisticexpressioncanhave oneoftwomeanings.Butour meaningrepresentations itselfcannotbeambiguous. Therepresentationofaninput'smeaningshouldbefreefromanyambiguity,sothat thethesystemcanreasonoverarepresentationthatmeanseitheronethingorthe otherinordertodecidehowtoanswer. Aconceptcloselyrelatedtoambiguityis vagueness :inwhichameaningrepre- vagueness sentationleavessomepartsofthemeaningVaguenessdoesnotgive risetomultiplerepresentations.Considerthefollowingrequest: (16.3) IwanttoeatItalianfood. While Italianfood mayprovideenoughinformationtoproviderecommendations,it isnevertheless vague astowhattheuserreallywantstoeat.Avaguerepresentation ofthemeaningofthisphrasemaybeappropriateforsomepurposes,whileamore representationmaybeneededforotherpurposes. CanonicalForm Thedoctrineof canonicalform saysthatdistinctinputsthatmeanthesamething canonicalform shouldhavethesamemeaningrepresentation.Thisapproachgreatlyrea- soning,sincesystemsneedonlydealwithasinglemeaningrepresentationfora potentiallywiderangeofexpressions. Considerthefollowingalternativewaysofexpressing( 16.1 ): (16.4) DoesMaharanihavevegetariandishes? (16.5) DotheyhavevegetarianfoodatMaharani? (16.6) ArevegetariandishesservedatMaharani? (16.7) DoesMaharaniservevegetarianfare? Despitethefactthesealternativesusedifferentwordsandsyntax,wewantthem tomaptoasinglecanonicalmeaningrepresentations.Iftheywerealldifferent, assumingthesystem'sknowledgebasecontainsonlyasinglerepresentationofthis fact,mostoftherepresentationswouldn'tmatch.Wecould,ofcourse,storeall possiblealternativerepresentationsofthesamefactintheknowledgebase,butdoing sowouldleadtoenormousdifinkeepingtheknowledgebaseconsistent. Canonicalformdoescomplicatethetaskofsemanticparsing.Oursystemmust concludethat vegetarianfare , vegetariandishes ,and vegetarianfood refertothe samething,that having and serving areequivalenthere,andthatalltheseparse structuresstillleadtothesamemeaningrepresentation.Orconsiderthispairof examples: (16.8) Maharaniservesvegetariandishes. (16.9) VegetariandishesareservedbyMaharani. Despitethedifferentplacementoftheargumentsto serve ,asystemmuststillassign Maharani and vegetariandishes tothesamerolesinthetwoexamplesbydraw- ingongrammaticalknowledge,suchastherelationshipbetweenactiveandpassive sentenceconstructions. InferenceandVariables Whataboutmorecomplexrequestssuchas: (16.10) CanvegetarianseatatMaharani? Thisrequestresultsinthesameanswerastheothersnotbecausetheymeanthesame thing,butbecausethereisacommon-senseconnectionbetweenwhatvegetarianseat  4 C HAPTER 16  L OGICAL R EPRESENTATIONSOF S ENTENCE M EANING andwhatvegetarianrestaurantsserve.Thisisafactabouttheworld.We'llneedto connectthemeaningrepresentationofthisrequestwiththisfactabouttheworldina knowledgebase.Asystemmustbeabletouse inference Štodrawvalidconclusions inference basedonthemeaningrepresentationofinputsanditsbackgroundknowledge.It mustbepossibleforthesystemtodrawconclusionsaboutthetruthofpropositions thatarenotexplicitlyrepresentedintheknowledgebasebutthatarenevertheless logicallyderivablefromthepropositionsthatarepresent. Nowconsiderthefollowingsomewhatmorecomplexrequest: (16.11) I'dliketoarestaurantwhereIcangetvegetarianfood. Thisrequestdoesnotmakereferencetoanyparticularrestaurant;theuserwantsin- formationaboutanunknownrestaurantthatservesvegetarianfood.Sincenorestau- rantsarenamed,simplematchingisnotgoingtowork.Answeringthisrequest requirestheuseof variables ,usingsomerepresentationlikethefollowing: variables Serves ( x ; VegetarianFood ) (16.12) Matchingsucceedsonlyifthevariable x canbereplacedbysomeobjectinthe knowledgebaseinsuchawaythattheentirepropositionwillthenmatch.Thecon- ceptthatissubstitutedforthevariablecanthenbeusedtotheuser'srequest. Itiscriticalforanymeaningrepresentationlanguagetobeabletohandlethesekinds ofreferences. Expressiveness Finally,ameaningrepresentationschememustbeexpressiveenoughtohandlea widerangeofsubjectmatter,ideallyanysensiblenaturallanguageutterance.Al- thoughthisisprobablytoomuchtoexpectfromanysinglerepresentationalsystem, First-OrderLogic,asdescribedinSection 16.3 ,isexpressiveenoughtohandlequite alotofwhatneedstoberepresented. 16.2Model-TheoreticSemantics Whatisitaboutaboutmeaningrepresentationlanguagesthatallowsthemto thesedesiderata,bridgingthegapfromformalrepresentationstorepresentationsthat tellussomethingaboutsomestateofaffairsintheworld? Theanswerisa model .Amodelisaformalconstructthatstandsforthepartic- model ularstateofaffairsintheworld.Expressionsinameaningrepresentationlanguage canbemappedtoelementsofthemodel,likeobjects,propertiesofobjects,and relationsamongobjects.Ifthemodelaccuratelycapturesthefactswe'reinterested in,thenaconsistentmappingbetweenthemeaningrepresentationandthemodel providesthebridgebetweenmeaningrepresentationandworld.Modelsprovidea surprisinglysimpleandpowerfulwaytogroundtheexpressionsinmeaningrepre- sentationlanguages. First,someterminology.Thevocabularyofameaningrepresentationconsistsof twoparts:thenon-logicalvocabularyandthelogicalvocabulary.The non-logical vocabulary consistsoftheopen-endedsetofnamesfortheobjects,properties,and  vocabulary relationsthatmakeuptheworldwe'retryingtorepresent.Theseappearinvarious schemesaspredicates,nodes,labelsonlinks,orlabelsinslotsinframes,The log- icalvocabulary consistsoftheclosedsetofsymbols,operators,links, logical vocabulary
Lexical semantics	Truth-conditions	Word vectors	Word embeddings
What are the 5 stages of Speech production	Stage 1 is Conceptual Preparation and at this stage an idea is formulated that the speaker wishes to communicate Stage 2 is Lexical Selection and at this stage certain words are selected that represent the idea that was initially formed Stage 3 is Morpho-Syntactic Encoding and at this stage the rules of grammar are applied  Stage 4 is Phonological Encoding and at this stage the speaker decides on which sounds will be required to produce the desired speech Stage 5 is Articulation and at this stage the speech is physically produced as a result of motor commands	At which stage could a semantic substitution occur?	Conceptual Preparation	Lexical Selection|||Articulation	5 Stages of Speech Production||youtube||N/A
Psycholinguistics	Applied linguistics	Speech production	Linguistic Theory	Speech errors
What is linguistic purism?	Linguistic purism is the attempt to maintain or re-create the "purity" of a language by purging it of all foreign elements, including elements foreign to the particular dialect considered the central language. It is associated with language standardization and nationalism. The french are famous for their linguistic purism, having an academy devoted to it, which attempts to replace foreign words (like "le weekend") borrowed into French with natively French substitutes. It is problematic because language naturally borrow words, and need to, and naturally change, and since all languages develop out of previous languages, the idea of linguistic purity itself is incoherent. It is also problematic if it is used to discriminate against speakers of "dialects."	Which of the following is NOT a historical example of linguistic purism?	Replacing "French fries" with "freedom fries."	Replacing "electricity" with "ghostfire."|||Saying that "y'all" is wrong.|||Saying that "Black English" is incorrect English	English Purist Tendencies in a Comparative Perspective||slides||English Purist Tendencies in a ComparativePerspectiveAndrás CserPázmány Péter Catholic University1. Introduction The purpose of this paper is to present and explain the contrast between the purist tendencies (here narrowly defined as the replacement of loanwords with lexical material based on native resources) that existed in England and those that existed in the speech communities of smaller European languages. It  is well known that in the case of English, purist tendencies were by and large unsuccessful (Barber  1976, Görlach 1991:154 ff., Nevalainen 1999:358 ff.), but the opposite is true for smaller languages  like Hungarian or Czech, as well as some of the major European languages, like German (Gardt 1999,  Cser 2006). To understand why, two types of purist attitude have to be distinguished both typologically  and historically (strictly in early modern and modern times). The first type is isolated and typically  earlier, the second type (typically later) is intimately connected to institutional aspects of the speech  community™s life, such as the expansion of schooling and universities and academies, the use of  standardised textbooks, and the emergence of a literary canon. First, a number of definitions will be  cited to show how the very notion of purism has been captured by various authors. Its typology will be  briefly discussed. Finally, its diachronic aspect will be touched upon and the central issue, the  difference between the two modern sub-periods of purism, will be explicated. 2. Definitions First of all, let us look at some of the definitions of purism, just to take a selection from the many possible sources. Adamson in her summary devoted specifically to the Early Modern English lexicon  defines purism as ﬁthe deliberate attempt at reducing the number of foreign words or avoiding their use  altogetherﬂ (1999:479). Renate Bartsch  gives (1987:66) the following description: ﬁLanguage purists  and entire puristic movements try to keep [the standard language,] this symbol of national- and group-  identity free from outside influences. They try to open native sources for lexical elaboration (preferably  from earlier stages of the language and not from ‚vulgar™ regional or social varieties) instead of non-  native resourcesﬂ Š this is found in a book devoted in its entirety to the issue of linguistic norms and  standardisation, cf. the qualifications referring to the standard and to the exclusion of regional varieties,  not in fact warranted by the various forms of purism all over Europe. The reference to ﬁnative sources  for lexical elaborationﬂ highlights what is probably the most spectacular aspect of purism, the  replacement of borrowed vocabulary by what is (perceived to be) native, inherited lexical material.  David Crystal in his Dictionary of Linguistics and Phonetics  defines purism  as ﬁa school of thought which sees a language as needing preservation from the external processes which might infiltrate it and  thus make it change, e.g., the pressures exercised by other dialects and languages (as in loan words) and  the variations introduced by colloquial speech–ﬂ (Crystal 1991, s.v. purism )The last definition proper is from Thomas (1991:12), the seminal work on purism. It is by far the most detailed definition that can be found in the literature; Thomas, characteristically, proposes it as a  working definition only, which he then elaborates throughout the book. Purism is the manifestation of a desire on the part of a speech community (or some section of  it) to preserve language from, or rid it of, putative foreign elements or other elements held to  be undesirable (including those originati ng in dialects, sociolects and styles of the same   language). It may be directed at all linguistic levels but primarily the lexicon. Above all, purism is an aspect of the codification, cultivation and planning of standard languages.  (Thomas 1991:12) The last citation is from the Art of English Poesie (1589; uncertain ascription to George Puttenham): [there are] many polysillables to sixe and seauen in one word, which we at this day vse in our most ordinarie language, and which corruption hath bene occasioned cheefly by the peevish  affectation not of the Normans them selues, but of clerks and scholers or secretaries long  since, who not content with the vsual Normane or Saxon word, would conuert the very Latine  and Greeke word into vulgar French, as to say innumerable for innombrable, reuocable,  irreuocable, irradiation, depopulation and such like, which are not naturall Normans nor yet  French, but altered Latines– which therefore were long time despised for inkehorne termes,  and now be reputed the best and most delicate of any other. Examples include Sir John Cheke™s  gainrising (=resurrection , sixteenth century), William Barnes™s fireghost (=electricity ) or gleemote (=concert , nineteenth century), or the Modern French neologisms courriel (courrier ‚mail™ +  électronique ‚electronic™) for e-mail and pourriel (poubelle ‚trash™ + courriel  orpourri ‚rotten™ + courriel ) for spam, the latter actually proscribed by the Académie française  but embraced by the Office québécois de la langue française. By the term purism I mean a narrower aspect of purism than is frequently thought of; I will not be concerned with its various stylistic and literary manifestations, but will concentrate on the puristic tendency to replace perceived foreignisms in the word stock. 3. TypologySeveral typologies of purism have been proposed in the literature, the most complete being perhaps that in Thomas (1991). The basic types or, I should rather say, attitudinal components he distinguishes  are the following (based on Thomas 1991:76Œ82): archaising, which relies on linguistic material from the past  ethnographic, which relies on rural dialects as its main source, such as the Finnish movement is reported to be élitist, which focuses on the aesthetic, prestige aspect of language and is characterised by a negative attitude to substandard and regional varieties reformist, which is concerned with coming to terms with resources accrued in earlier periods, adapting language to modern needs, and is forward-looking xenophobic, whose goal is the eradication of foreign elements or, perhaps more accurately, the eradication of what are perceived to be foreign elements, since, as is well known, the two  are not the same These attitudes are defined by three intersecting parameters or ﬁaxesﬂ, the social, the temporal and the perspectual. The two endpoints of the social axis are élitist and ethnographic purism, the former  focusing on the language use of the highest sections of society and that of the most highly educated  circles, whereas the latter focusses on rural dialects. The two endpoints of the temporal axis are  archaising and reformist, that is, backward-looking and forward-looking or modernising purism. The  two endpoints of the third, the perspectual axis are xenophobic and non-xenophobic, of which the latter  is not in fact a purist attittude at all. There is, in fact, one point at which an addition needs to be made to this picture, and that is the role of institutions. Comrie hints at the role of schools in his review of the book (Comrie 1994:845), and it is  crucial to an understanding of the nature of the English purist efforts, because Š and this is a point I  will come back to a little later Š the lack of a specifically dedicated institutional background is one of 37
Linguistic purism	Sociolinguistics	Typology	l'Academie Francaise	Language and nationalism
What is Natural Language Processing?	Natural Language Processing is a field of study combining computer science and linguistics together. It involves the breaking down of complete sentences into smaller segments in order for them to be easily processed.	What used to be a common error that was encountered early with NLP?	The problem was that computers could not deconstruct sentences into smaller segments	Computers could not process alphabetic symbols|||The problem was that computers did not understand the input language	NLP Crash Course clip||youtube||N/A
Computer science	Natural Language Processing (NLP)	Branches of linguistics	Linguistic Theory
Which of the following two branches of linguistics - neurolinguistics and applied linguistics - deals with overcoming language impairment?	The answer is applied linguistics, as it deals with finding solutions to problems with language use. Neurolinguistics typically studies language representation in the brain, neurological and brain damage affecting language use, but it does not focus on finding treatment.	Which of the following would neurolinguistics NOT cover?	The study of written and spoken comprehension and production of language	Eye movement control in developmental reading disorders|||Human brain mechanisms underlying language production|||Aphasic disruption of written language	"What is Applied Linguistics?" presentation from University Utara Malaysia||slides||What is Applied Linguistics 1  2 Awang Had Salleh School of Graduate Studies University Utara Malaysia, Malaysia mostafa.shalaby 1970 @gmail.com 11 / 2 / 2018 9 / 3 / 2018 IN GOD WE TRUST Moustafa Mohammad Shalabi MA. Applied Linguistics PhD. Scholar Corpus Linguistics  What is applied Linguistics   It is commonly known as the branch of linguistics concerned with practical applications of language studies, for example language teaching, translation, and speech therapy . 3  The term 'applied linguistics' refers to a broad range of activities which involve solving some language - related problem or addressing some language - related concern .  What is applied Linguistics  4 It appears as though applied linguistics, at least in North America, was first officially recognized as an independent course at the University of Michigan in 1946 . In those early days, the term was used both in the United States and in Great Britain to refer to applying a so - called 'scientific approach' to teaching foreign languages, including English for non - native speakers .  What is applied Linguistics  5 Early work to improve the quality of foreign language teaching by Professors Charles Fries (University of Michigan) and Robert Lado ( University of Michigan, then Georgetown University) helped to bring definition to the field as did the 1948 publication of a new journal, Language Learning : A Quarterly Journal of Applied Linguistics  What is applied Linguistics  6 Professor Anne Burns  Professor  of  TESOL, BA  (Hons), Diploma in  Adult TESOL, PhD,  MEd  Applied linguistics is notoriously hard to define. What sets it apart  from other areas of linguistics? How has it evolved over the years?  What do applied linguists do?   We asked ten leading and up - and - coming academics to give   us  their answer to the question:   Below  are their responses.  Take a look at  them  As an applied linguist,  primarily interested in offering people practical and illuminating insights into how language and communication contribute fundamentally to interaction between people .  What is applied Linguistics  7  Of course, several commentators have offered definitions of applied linguistics in recent decades , including Crystal ( 1980 : 20 ) , Richards et al , ( 1985 : 29 ) , Brumfit ( 1995 : 27 ) and Rampton ( 1997 : 11 ) .  For me, applied linguistics means taking language and language theories as the basis from which to elucidate how communication is actually carried out in real life, to identify problematic or challenging issues involving language in many different contexts, and to analyse them in order to draw out practical insights and implications that are useful for the people in those contexts .  What is applied Linguistics  8  Professor Richards' many successful publications include the Interchange series, Approaches and Methods in Language Teaching, and Curriculum Development in Language Teaching . Jack C. Richards  Professor Jack C. Richards is an internationally            recognized authority on English - language  acquisition, teacher  training, and materials design.   A well - known lecturer and consultant, he has taught at universities in the United States, China, Singapore, New Zealand, Canada, Indonesia, and Brazil .  What is applied Linguistics  9  More seriously, looking back at the term  applied linguistics  , it first emerged as an attempt to provide a theoretical basis for the activities of language teaching (witness Pit Corder  s book on the subject from 1973 ) .  A wit once described an applied linguist as  someone with a degree in linguistics who was  unable to get a job in a linguistics department.  Later, it became an umbrella term for a variety of  disciplines  which  focus on language issues in such fields as law,  speech  pathology , language planning, and forensic science.   What is applied Linguistics  10  Some years ago, many graduate programs in language teaching were labelled as programs in applied linguistics .  In the meantime, language teaching has evolved its own theoretical foundations, and these include second language acquisition, teacher cognition, pedagogical grammar, and so on, and there is a declining interest in viewing   as having any relevance to language teaching .  What is applied Linguistics  11  Today they are generally called programs in TESOL . Many specialists in language teaching, such as myself, don  t call themselves  applied linguists  . We are what we are  specialists in language teaching, and we don  t see that adding the label  applied linguistics  to our field adds any further understanding to what we do .    is of  course, something they need to  decide for themselves.  What is applied Linguistics  12  BA (Hons) Philosophy; MA Applied Linguistics;  PhD Applied Linguistics; Cambridge DELTA Dr Philip Durrant  Applied linguistics is any attempt to work with language in a critical and reflective way, with some ultimate practical goal in mind .  This includes(amongst other things ) : deliberately trying to learn (or teach) a foreign language or to develop your ability in your native language ; overcoming a language impairment ; translating from one language to another ; editing a piece of writing in a linguistically thoughtful way .  It also includes  doing any  research or developing any ideas or  tools which aim to help  people do  these sorts of things.  What is applied Linguistics  13  Professor of  Psycholinguistics,  University of Nottingham Zoltán Dörnyei    (AL) is one of several academic disciplines focusing on how language is acquired and used in the modern world .  It is a somewhat eclectic field that accommodates diverse theoretical approaches, and its interdisciplinary scope includes linguistic, psychological and educational topics . Although the field  s original focus was the study of foreign/second languages, this has been extended to cover first language issues, and nowadays many scholars would consider sociolinguistics and pragmatics to be part of the AL rubric .  Recently , AL conferences and journals have reflected the growing influence of psychology - based approaches, which in turn is a reflection of the increasing prevalence of cognitive (neuro)science in the study of human mental functions .  What is applied Linguistics  14  In my discipline (I am a Germanist), applied linguistics is perceived almost exclusively as research into the teaching and learning of the foreign - language , often resulting in the production of teaching materials . Professor Wini Davies Reader in German, Aberystwyth University  However, a broader definition (e . g . Dick Hudson  see references and below) sees applied linguistics as concerned with providing theoretical and empirical foundations for investigating and solving language - related problems in the   .  This definition would be relevant to some of my research  interests; for example, the problems facing speakers of non - standard dialects at schools in Germany.  What is applied Linguistics  15 Emeritus  Professor of Linguistics, University College London  Applied linguistics (AL) provides the theoretical  and descriptive foundations for the investigation and solution of language - related problems, especially those of language education (first - language, second - language and foreign language teaching and learning), but also problems of translation and interpretation, lexicography, forensic linguistics and (perhaps) clinical linguistics  Professor Richard Hudson  16  The main distinguishing characteristic of AL is its concern with professional activities whose aim is to solve  real -  language - based problems, which means that research touches on a particularly wide range of issues - psychological, pedagogical, social, political and economic as well as linguistic . As a consequence, AL research tends to be interdisciplinary . What is applied Linguistics   17  It is generally agreed that in spite of its name AL is not simply the  application  of research done in linguistics . On the one hand, AL has to look beyond linguistics for relevant research and theory, so AL research often involves the synthesis of research from a variety of disciplines, including linguistics . On the other hand, AL has been responsible for the development of original research in a number of areas of linguistics - e . g . bilingualism, literacy, genre .  Beyond this agreement, there is at least as much disagreement within AL as within linguistics about fundamental issues of theory and method, which leads (among other things ) to differences of opinion about the relationships between the two disciplines . What is applied Linguistics   18 What is applied Linguistics  Professor Andy Kirkpatrick  Professor, School of Humanities,  Languages Science Griffith  University  One way I can answer this broad question is by considering the Applied Linguistic issues that currently interest me, namely how languages interact and what differences we might expect when the languages concerned are not related to each other  19 What is applied Linguistics   For example, the Hong Kong language policy seeks to develop people who are trilingual in Cantonese, Putonghua and English . What specific linguistic difficulties will such learners face and how can we help them overcome them? What does it mean to be multilingual? Can we describe a multilingual model from which we could derive useful linguistic benchmarks for the language classroom?  20 What is applied Linguistics  Dr. Dawn Knight  Research Associate, University of Nottingham  Applied linguistics draws on a range of disciplines, including linguistics . In consequence, applied linguistics has applications in several areas of language study, including language learning and teaching, the psychology of language processing, discourse analysis, stylistics, corpus analysis, literacy studies and language planning and policies .  Applied linguistics is a discipline which explores the relations between theory and practice in language with particular reference to issues of language use .  It embraces contexts in which people use and learn languages and is a platform for systematically addressing problems involving the use of language and communication in real - world situations .  21 What is applied Linguistics   Applied linguistics is a broadly interdisciplinary field concerned with promoting our understanding of the role language plays in human life . At its centre are theoretical and empirical investigations of real - world issues in which language plays a leading role . Applied linguistics focuses on the relationship between theory and practice, using the insights gained from the theory - practice interface for solving language - related problems in a principled way . Juliane House  Professor of Foreign Language Teaching, Universität Hamburg  22 What is applied Linguistics   Applied linguistics is not  linguistics applied  , because it deals with many more issues than purely linguistic ones, and because disciplines such as psychology, sociology, ethnography, anthropology, educational research , communication and media studies also inform applied linguistic research . The result is a broad spectrum of themes in applied linguistics such as first, second and foreign language learning and teaching, bilingualism and multilingualism, discourse analysis, translation and interpreting, language policy and language planning, research methodology , language testing, stylistics, literature, rhetoric, literacy and other areas in which language - related decisions need to be taken .  23 What is applied Linguistics  Susan Hunston  Head of Department of English, University of Birmingham  The real - world concerns include language learning and teaching but also other issues such as professional communication, literacies, translation practices, language and legal or health issues, and many more . Applied linguistics is practically - oriented, but it is also theory driven and interdisciplinary . Models of how languages are learned and stored, for example, are  applied linguistics  , as are descriptions of individual language varieties that prioritise actual and contextualised language use .  One answer to this question is that it is the study of language in order to address real - world concerns . Another is that it is the study of language, and language - related topics, in specified situations .   References  Corder, P. ( 1973 )  Introducing Applied Linguistics,  Harmondsworth:  Penguin.  Crystal, D. ( 1980 )  A First Dictionary of Linguistics and Phonetics ,  London: André Deutsch.  Brumfit, C. ( 1995 )   Teacher professionalism and research  , in: Cook,  G. & Seidlhofer, B. (eds.) ( 1995 )  Principles and Practice in Applied Linguistics , Oxford: Oxford  University Press, pp 27 - 42 .  Hudson, R.  Applied Linguistics , available online at:  http://www.phon.ucl.ac.uk/home/dick/AL.html  Rampton, B. ( 1997 )   Retuning in applied linguistics?  ,  International  Journal of Applied Linguistics,  7 ( 1 ):  3 - 25 .  Richards, J.C., Platt, J. & Weber, H. ( 1985 )  Longman Dictionary of  Applied Linguistics,  London: Longman. 24
Applied linguistics	Neurolinguistics	Branches of linguistics	Neurological disorders	Language impairment
Why would 'Linguistics Ambiguity' pose a problem for NLP?	It is possible in some languages for some words to have more than one direct meaning. An example of this would be the homonym category. This phenomenon is too complicated at this stage for a digital dictionary to resolve on its own.	In order to help computers navigate ambiguity, what structure do they need to understand?	Grammar	Frequency	NLP Crash Course clip||youtube||N/A
Natural Language Processing (NLP)	Branches of linguistics	Syntax	Linguistic Theory
What can cause aphasia to occur?	Aphasia may be a result of a stroke (this is the most common cause of aphasia), degenerative disease (Parkinsons or Multiple Sclerosis), brain tumour (malignant or benign), a head injury (caused in a tragic accident), infectious disease (encephalitis), migraine or epilepsy.	Who was the first person to discover the location of language in the brain?	Paul Broca	Carl Wernicke|||Noam Chomsky|||Carl Jung	Understanding Aphasia Presentation||slides||˘ˇ˘   ˆ˙˝˚˛˝ ˝˚˝ ˛ ˆ˚˝!˘!˝ ˛ ˆ˛ ˝˛ !  '  ˆ˜˝˚˚ ˚˝ ˛'˛ *˘!˛˘˜        ˆ+˛ ˚  ! ˛! ˆ-˛ ! ˛! ˆ˚ !˛'  ! ˛!  ˘ ˝ ˆ˘! ˆ ˘ ˆ˘  ˆ˘! ˆ- ˆ˝˛ ˆ0   ˆ1ˇ˛˚0˜˘ 2&˜    ˛˜ ˆ1ˇ˛˚˘ ˙&˚˜ #!˚˝˛'1     ˆ ˜& ˘!˛˝! &#&˛ ˚ ˆ! ˛˝˚ & ˆ ˝ ˆ ˝ "&#&5 ˆ ˘!˝ #&"&#&˛$  ˚˜ ˆ6˘!  /˝ ˝˚˝˛ ˆ. !˛˘! ˆ !#˚˜& ˆ ˝˛ 7˛ 8˝ & !˘$ 7 8˜˛#!  ˛& !  ! ˆ*29::+˛˚˝˛ #˝˝˛ ;& ˆ ˝˝= ˜ ˝˛˚˚ ˛ ˆ ˝+˛˚˝  !12>:9˚ !˝ ˚˛  " ˆ?2˝˝ ˛˘!& ˆ/!!˙::4:::˚˛˜ ˛˝&"! * ˆ?˜˝˛ ˛˝˛˜! &       ˇ˙ ˝ ˛˙ ˜  ˆ    !"ˆˆˆ # ˜ % # ˛˙ (˘ˆ  ˙ˆ   +˘ ˛! ˜ˆ ˆ˚ˆ#  ˙  ˜ˆ 0 $ ˆ  ˆ˙˙ ,˙1     21 ˇ ˙ 3% ˇ 2% 4 5% 0 6˘ 3% 0˙ 2% !˙ ˇ ˘ ˚˘ ˙8$ ˆ 1.  ˚ 3% 9 2% ˆ 5%  :%  ˙   4˘7 ˘ $˙˘˙ $˚ 3% ( 2%  5%  ˆ :%  0 7  ˆ 1 1 3% 1  2%  5% 01 :% 1   0˙ ˘ˆ17 ˆ1<  ˙!ˆ˜ˆ 0$˚/; 7!˙ ˆˆˆ˙ ˆˆ 1<ˆ˜˙ ˆ$ '˝˘  ˆ 88 8' ˆ ˆ ˇˆ   1   1ˆ ˆ"==>#   1ˆ "=>=ˆ>#  ˇˆ  1 ˆ $ 0  +ˆˆ1 1 ˆ˚ ˘˛ "˚˙˙ ; # 6  ˆ ˙     ˘  ˙ %  ˆ ˙˝ < =ˆ1˙%> < =9ˆ1A1 ˆ%ˇˆ˘˚˚% 1ˇ1A> < =91˙B> < =ˇ˘˘''%> "(2CC5<3DD#  ˛ /;88  4  ˆ ˆˆˆ 1"   0  ˆ$  ˛ !˚ 3C73D 1ˆ˘# ˆ$ (  5     ( ˆ ˙ $ %%1 ˙   ˆ .ˆˆ #        ˘ ˘˘ ˘ˇˆ˙ ˝˘˚˜ ˘ ˇˆ˙ ˘˙˘ ˘˘ " ˘ ˇˆ˙˘ ˘˘  ˆˆ   ˛ ˚ ˚ ˚ ˆ˚ ˛  ˝  (˚)*#+#   ˘  -!./˘   ˚˚ 0 - ˚  ˛˛"        ˝˛˝ )˛˛ ˚   2  $˝ ˚˜˘ 3.  ˇ  ˚˚       ˆ +  4 ˝˘  +˝ ( ˚ ˘ ˛˝  ˚˚˚   ˇ ˇˆ˙˙  ˙ ˛˘˘˚ .˘! ˜ˆˆˆ˝˜˘˘ˆ ˜˜˜ ˜%˜&ˆ  ˛˘˜˘˜&. ˝˜˝ ˆ˜˘ ˘ ˝˜˜  ˜˘ ˜ ˘˛ ˜˚ ˜  #˜ˆ .˜˜˜˘ˆ˜ˆ  ˘˜˜˛ˆ˜˜˘ˆ˜˜ ˜˜˝˜˝ˆ˜ ˚˜˜˜    . ˛˜˜%˛˛&     % ˛˘ &. )˘˛ ˛#˛ 0˝˜˝˛˚ ˆ   ˜˝˚ ˛˘ ˛  ˚˛ ˛˝˜ ˝  ˛˚ 6 ˚˜˛ ˚ˆˆ 4   $   $ ˛ 8˚ ˜. 6 0  9 -  : ; < 2  ˛˚˜ ! 9 ˜  :   < 7 ,$)& = /      ˆ          ˜       ˜ ˛       ˆ˚ $˛        ˛ &   "˛ ˜˘ ˘  ˛ ˆ         ˛       ˛ ˘   ˘ !˘  ˘ (˛ )˛  +˛ , -˛ !  .˛ &  /˛  0˛   ˇ (˛ , )˛ &  +˛ 1     ,  -     !  3 .˛4                                     ˘
Neurolinguistics	Localization of the brain	Brain functions	Parts of the brain	Neuroanatomy
What does the meaning of a word consist of in a vector model, in both (a) concrete and philosophical (b) terms?	The meaning of a word in a vector model consists, concretely, of a list of numbers, which are proportional to the frequencies (counts) with which particular other words and morphemes appear in the environment of the word whose meaning is in question. Philosophically, this implies that the meanings of words are, or are represented well by, their distributions relative to other words and morphemes; this implies a relational theory of word meaning.	Which of the following is NOT an advantage of embeddings?	They automatically classify words well by part-of-speech 	They make it easy to calculate semantic similarity|||They imply fine-grained distinctions in word-meaning|||They can give results for words they were not trained on. 	Chapter Section on Word Vectors||slides||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 6 VectorSemanticsandEmbed- dings TheasphaltthatLosAngelesisfamousforoccursmainlyonitsfreeways.Butinthe middleofthecityisanotherpatchofasphalt,theLaBreatarpits,andthisasphalt preservesmillionsoffossilbonesfromthelastoftheIceAgesofthePleistocene Epoch.Oneofthesefossilsisthe Smilodon ,orsabre-toothedtiger,instantlyrec- ognizablebyitslongcanines.Fivemillionyearsagoorso,acompletelydifferent sabre-toothtigercalled Thylacosmilus lived inArgentinaandotherpartsofSouthAmer- ica.Thylacosmiluswasamarsupialwhereas Smilodonwasaplacentalmammal,butThy- lacosmilushadthesamelonguppercanines and,likeSmilodon,hadaprotectivebone onthelowerjaw.Thesimilarityof thesetwomammalsisoneofmanyexamples ofparallelorconvergentevolution,inwhichparticularcontextsorenvironments leadtotheevolutionofverysimilarstructuresindifferentspecies (Gould,1980) . Theroleofcontextisalsoimportantinthesimilarityofalessbiologicalkind oforganism:theword.Wordsthatoccurin similarcontexts tendtohave similar meanings .Thislinkbetweensimilarityinhowwordsaredistributedandsimilarity inwhattheymeaniscalledthe distributionalhypothesis .Thehypothesiswas distributional hypothesis formulatedinthe1950sbylinguistslike Joos(1950) , Harris(1954) ,and Firth (1957) ,whonoticedthatwordswhicharesynonyms(like oculist and eye-doctor ) tendedtooccurinthesameenvironment(e.g.,nearwordslike eye or examined ) withtheamountofmeaningdifferencebetweentwowordsﬁcorrespondingroughly totheamountofdifferenceintheirenvironmentsﬂ (Harris,1954,157) . Inthischapterweintroduce vectorsemantics ,whichinstantiatesthislinguistic vector semantics hypothesisbylearningrepresentationsofthemeaningofwords,called embeddings , embeddings directlyfromtheirdistributionsintexts.Theserepresentationsareusedinevery naturallanguageprocessingapplicationthatmakesuseofmeaning,andunderliethe morepowerful contextualizedwordrepresentations like ELMo and BERT that wewillintroduceinChapter10. Thesewordrepresentationsarealsotheexampleinthisbookof repre- sentationlearning ,automaticallylearningusefulrepresentationsoftheinputtext. representation learning Findingsuch self-supervised waystolearnrepresentationsoftheinput,insteadof creatingrepresentationsbyhandvia featureengineering ,isanimportantfocusof NLPresearch (Bengioetal.,2013) . We'llbegin,however,byintroducingsomebasicprinciplesofwordmeaning, whichwillmotivatethevectorsemanticmodelsofthischapteraswellasextensions thatwe'llreturntoinChapter19,Chapter20,andChapter21.  2 C HAPTER 6  V ECTOR S EMANTICSAND E MBEDDINGS 6.1LexicalSemantics Howshouldwerepresentthemeaningofaword?IntheN-grammodelswesaw inChapter3,andinmanytraditionalNLPapplications,ouronlyrepresentationof awordisasastringofletters,orperhapsasanindexinavocabularylist.This representationisnotthatdifferentfromatraditioninphilosophy,perhapsyou've seenitinintroductorylogicclasses,inwhichthemeaningofwordsisrepresented byjustspellingthewordwithsmallcapitalletters;representingthemeaningof ﬁdogﬂas DOG ,andﬁcatﬂas CAT ). Representingthemeaningofawordbycapitalizingitisaprettyunsatisfactory model.Youmighthaveseentheoldphilosophyjoke: Q:What'sthemeaningoflife? A: LIFE Surelywecandobetterthanthis!Afterall,we'llwantamodelofwordmeaning todoallsortsofthingsforus.Itshouldtellusthatsomewordshavesimilarmean- ings( cat issimilarto dog ),otherwordsareantonyms( cold istheoppositeof hot ).It shouldknowthatsomewordshavepositiveconnotations( happy )whileothershave negativeconnotations( sad ).Itshouldrepresentthefactthatthemeaningsof buy , sell ,and pay offerdifferingperspectivesonthesameunderlyingpurchasingevent (IfIbuysomethingfromyou,you'veprobablysoldittome,andIlikelypaidyou). Moregenerally,amodelofwordmeaningshouldallowustodrawusefulinfer- encesthatwillhelpussolvemeaning-relatedtaskslikequestion-answering,sum- marization,detectingparaphrasesorplagiarism,anddialogue. Inthissectionwesummarizesomeofthesedesiderata,drawingonresultsinthe linguisticstudyofwordmeaning,whichiscalled lexicalsemantics ;we'llreturnto lexical semantics andexpandonthislistinChapter19. LemmasandSenses Let'sstartbylookingathowoneword(we'llchoose mouse ) mightbeinadictionary: 1 mouse(N) 1.anyofnumeroussmallrodents... 2.ahand-operateddevicethatcontrolsacursor... Heretheform mouse isthe lemma ,alsocalledthe citationform .Theform lemma citationform mouse wouldalsobethelemmafortheword mice ;dictionariesdon'thaveseparate forformslike mice .Similarly sing isthelemmafor sing , sang , sung .Inmanylanguagestheveformisusedasthelemmafortheverb,so Spanish dormir ﬁtosleepﬂisthelemmafor duermes ﬁyousleepﬂ.Theforms sung or carpets or sing or duermes arecalled wordforms . wordform Astheexampleaboveshows,eachlemmacanhavemultiplemeanings;the lemma mouse canrefertotherodentorthecursorcontroldevice.Wecalleach oftheseaspectsofthemeaningof mouse a wordsense .Thefactthatlemmascan be polysemous (havemultiplesenses)canmakeinterpretationdif(issomeone whotypesﬁmouseinfoﬂintoasearchenginelookingforapetoratool?).Chapter19 willdiscusstheproblemofpolysemy,andintroduce wordsensedisambiguation , thetaskofdeterminingwhichsenseofawordisbeingusedinaparticularcontext. Synonymy Oneimportantcomponentofwordmeaningistherelationshipbe- tweenwordsenses.Forexamplewhenonewordhasasensewhosemeaningis 1 ThisexampleshortenedfromtheonlinedictionaryWordNet,discussedinChapter19.  6.1  L EXICAL S EMANTICS 3 identicaltoasenseofanotherword,ornearlyidentical,wesaythetwosensesof thosetwowordsare synonyms .Synonymsincludesuchpairsas synonym couch/sofavomit/throwupcar/automobile Amoreformalofsynonymy(betweenwordsratherthansenses)isthat twowordsaresynonymousiftheyaresubstitutableonefortheotherinanysentence withoutchangingthe truthconditions ofthesentence,thesituationsinwhichthe sentencewouldbetrue.Weoftensayinthiscasethatthetwowordshavethesame propositionalmeaning . propositional meaning Whilesubstitutionsbetweensomepairsofwordslike car / automobile or wa- ter / H 2 O aretruthpreserving,thewordsarestillnotidenticalinmeaning.Indeed, probablynotwowordsareabsolutelyidenticalinmeaning.Oneofthefundamen- taltenetsofsemantics,calledthe principleofcontrast ( Girard1718 , Br ´ eal1897 , principleof contrast Clark1987 ),istheassumptionthatadifferenceinlinguisticformisalwaysassoci- atedwithatleastsomedifferenceinmeaning.Forexample,theword H 2 O isused incontextsandwouldbeinappropriateinahikingguideŠ water wouldbe moreappropriateŠandthisdifferenceingenreispartofthemeaningoftheword. Inpractice,theword synonym isthereforecommonlyusedtodescribearelationship ofapproximateorroughsynonymy. WordSimilarity Whilewordsdon'thavemanysynonyms,mostwordsdohave lotsof similar words. Cat isnotasynonymof dog ,but cats and dogs arecertainly similarwords.Inmovingfromsynonymytosimilarity,itwillbeusefultoshiftfrom talkingaboutrelationsbetweenwordsenses(likesynonymy)torelationsbetween words(likesimilarity).Dealingwithwordsavoidshavingtocommittoaparticular representationofwordsenses,whichwillturnouttosimplifyourtask. Thenotionofword similarity isveryusefulinlargersemantictasks.Know- similarity inghowsimilartwowordsarecanhelpincomputinghowsimilarthemeaningof twophrasesorsentencesare,averyimportantcomponentofnaturallanguageun- derstandingtaskslikequestionanswering,paraphrasing,andsummarization.One wayofgettingvaluesforwordsimilarityistoaskhumanstojudgehowsimilarone wordistoanother.Anumberofdatasetshaveresultedfromsuchexperiments.For exampletheSimLex-999dataset (Hilletal.,2015) givesvaluesonascalefrom0to 10,liketheexamplesbelow,whichrangefromnear-synonyms( vanish , disappear ) topairsthatscarcelyseemtohaveanythingincommon( hole , agreement ): vanish disappear 9.8 behave obey 7.3 belief impression 5.95 muscle bone 3.65 modest xible 0.98 hole agreement 0.3 WordRelatedness Themeaningoftwowordscanberelatedinwaysotherthan similarity.Onesuchclassofconnectionsiscalledword relatedness (Budanitsky relatedness andHirst,2006) ,alsotraditionallycalledword association inpsychology. association Considerthemeaningsofthewords coffee and cup .Coffeeisnotsimilartocup; theysharepracticallynofeatures(coffeeisaplantorabeverage,whileacupisa manufacturedobjectwithaparticularshape). Butcoffeeandcupareclearlyrelated;theyareassociatedbyco-participatingin aneverydayevent(theeventofdrinkingcoffeeoutofacup).Similarlythenouns
Word embeddings	Lexical semantics
What is "Indo-European"?	This is a "genetic" classification; it refers to a "family" of languages descended from a common ancestor through language change and diversification. It is called Indo-European because its largest sub-branches include most of the European, and most of the Indo-Iranian languages. It was first proposed by Sir William Jones based on his studies of Sanskrit, Latin, and Greek. Their common ancestor is believed to have existed either in "Anatolia" (southern Turkey) or north of the Black Sea 6-8,000 years ago. The major groupings are the Indic language, Iranian languages, Celtic, Romance (Latinate), Germanic, Slavic, and Hellenic (Greek). Tocharian, Hittite, Armenian, and Anatolian are also groups but not major.	Which of the following languages is NOT Indo-European?	Hungarian	Slovenian|||Farsi|||Armenian	Indo-European Languages (from Carson-Newman University website)||image||N/A
Linguistic typology	Indo-European languages	Historical linguistics	Genetic classification
What is the difference between a "declarative" sentence and a "declarative" speech-act?	"Declarative sentence" describes the function of the sentence’s structure. A declarative sentence is one which, by virtue of its structure, could be true or false; it is a statement. Speech-act is not directly determined by sentence structure. It describes what the sentence is intended to accomplish, in the world. A "declarative" speech-act declares a circumstance to be so. For example, "My friend is a teacher" is a declarative sentence, but not a declarative speech-act; it describes something, but does not make it so. "I declare you man and wife" is a declarative speech-act, although it is only successful when spoken by an authorized person in prescribed circumstances.	Which of the following pairings of sentence-type and speech-act-type is NOT correct?	Declarative/assertive: "You are welcome!"	Declarative/declarative: "You're under arrest."|||Interrogative/directive: "What's your favorite color?"|||Declarative/commissive: "I promise to do a good job."	Speech Acts and Conversation webpage from the University of Pennsylvania||document||Speech Acts and Conversation Language Use: Functional Approaches to Syntax Handout for EDUC 537  Educational Linguistics  H. Schiffman, Instructor 1. Language in Use Having described various kinds of syntactic structures and what  they  mean  we see that people often don't seem to  say what  they mean.  They use languages differently from its apparent  meaning; it has functions  are different from the apparent  structure. Example:  Could I get you to open that window? How'd you like to hand me that wrench? Would it be too much trouble for me to ask you to hand me that wrench? I know this is an imposition, but could you possiblly open the window? instead of Open the window,  Hand me the wrench, etc. 2. Sentence Structure and the Function of utterances We are `used to' having questions being used to ask for  information, declarative sentences to state something, and imperative  sentences to give orders.  But the following may also occur: 1. [Form:  request:] Can I ask you to please refrain from smoking?  [Function: command:} (= Please stop smoking!) 2. [Form:  Statement:] We ask that you extinguish your cigarettes at this time, and bring  your tray tables and seatbacks to an upright position. [Function:  command:] (= Stop smoking  and sit up straight!) 3. [Form: question] Well, would you listen to that! [Function:  exclamation] (= That's really something to listen to.) 3. Speech Acts Speech acts are verbal actions that accomplish something: we  greet, insult, compliment, plead, ßirt, supply information, and get work  done. Types of Speech Acts  Representatives:  assertions, statements, claims,  hypotheses, descriptions, suggestions. Commissives: promises, oaths, pledges, threats,  vows. Directives: commands, requests, challenges,  invitations, orders, summons, entreaties, dares. Declarations:  blessings, Þrings, baptisms,  arrests, marrying, juridial speech acts such as sentencings, declaring a  mistrial, declaring s.o.out of order, etc. Expressives: Speech acts that make assessments of  psychological states or attitudes: greetings, apologies,  congratulations, condolences, thanksgivings... Verdictives: rankings, assessments, appraising,  condoning (combinations such as representational declarations:  You're  out!) Locutions and Illocutions Locutions:  the  utterance act.  Sentences have a  grammatical structure and a literal linguistic meaning; the bald,  literal force of the act:  what did the person  say?  (Not, what  did the person mean?) Illocution:  the speaker's  intention  of what  is to be accomplished by the speech act.  Compare:  How'd you like to hand me that wrench?  (locution: a  question) has the  illocutionary force  of a command: namely:  Hand me the wrench! Can I get you to open the window?  has a  structure (locutionary force) and a linguistic meaning (`will I be able to be successful in  getting your cooperation in opening the window?')  but its illocutionary  force  is different:  it has the force of a  polite imperative  : Please open the window! Every sentence has both a  locutionary force  and an  illocutionary force  .Distinguishing among speech acts How do we know what the force of a speech act is? By the  context  or the  setting  and by using their judgement and  background knowledge of the language and the culture.  If the Queen of Hearts (in  Alice in Wonderland  ) says `Off with their Heads!' it  has a different force than if someone else says it in another setting. Appropriateness conditions and Successful Declarations There are  conventions  that tell us that a particular  locution probably has a particular force.  People don't use language  inappropriately, or they get into trouble, or the act may be interpreted  asinvalid. utterance must be conventionally associated with the  speech act:  The preacher or ofÞciating judge says: I now pronounce you husband and wife  instead of Heybobareebob, you is hitched! Context must be conventionally recognized The above declaration must be in a setting that is appropriate,  like in a church or place of religious worship, etc. with people  gathered for that purpose, perhaps even dressed for the part.  Weddings  (e.g.) don't happen spontaneously during, e.g., a baptism or a  bar mitzvah. Speaker must be sincere: Person pronouncing the words must  believe  what s/he is saying Involved parties intend to create a marriage bond;  the  essential  condition Successful Promises:  (commissive): must be  recognized as a promise, must be sincere, essential; speaker must state  the intention of helping.  Preparatory condition:  speaker and hearer are  sane and responsible, speakers wishes to help, hearer wishes to be  helped, etc. (Speaker cannot have Þngers crossed behind her back...) 4. The Cooperative Principle there is unspoken agreement that people will cooperate in  communicating with each other, and speakers rely on this agreement. Grice:  Make your conversational contribution such as is  required, at the stage at which it occurs, by the accepted purpose or  direction of the talk exchange in which you are engaged. Maxim of Quantity Give as much information as is  necessary, but not more.  (Don't overdo it.) [Mary:]  Hi, John, how are ya? [John:] Oh, not so good, Mary.  I just had a tooth out, then  last week I had an epidural injection in my spine, followed by  restorative surgery on my little toe; you should have seen it, it was  horrible, and you wouldn't believe what the surgeon charged, I just got  the bill!  Our health care system is  outrageous,  and the trafÞc  on the way to work today! Unbelievable! (etc. etc. etc.) Maxim of  Relevance Be relevant; don't overload  the conversation with superßuous or irrelevant material (as in the previous exchange).  This requires speakers to organize their utterances  so that they are relevant to the ongoing context: Be relevant at the time  of the utterance. Maxim of Manner Be orderly and clear; avoid  ambiguity and obscurity. Maxim of Quality
Speech-Act Theory	Declarative	Sentence types	Pragmatics
What is the difference between a language and a dialect, according to linguists?	The difference between a language and a dialect is a purely social / political / legal categorization. There is no difference in kind between the two ways of speaking except that the one called a "language" is usually the one spoken by those with money and power, and usually becomes standardized.  Dialects are not incorrect ways of speaking languages; they are simply different, in terms of grammar, vocabulary, and other features – in the same ways two languages can be different. According to some, two ways of speaking are called "dialects" if they are mutually intelligible, and different "languages" if they are not. But, mutual intelligibility is asymmetrical and depends on social biases. For example, speakers of African-American Vernacular English understand Standard English easily, but not necessarily the other way around. One may also use geographical or temporal boundaries, but these are always fuzzy; there is no definite point in time at which Latin became Spanish, and no precise geographical boundary between Spanish and French.	Which of the following is NOT an incorrect assumption about dialects versus languages?	Two dialects can be distinguished from each other by their features.	Dialects develop out of the standardized language they are related to.|||The major language existed before any of its dialects.|||A dialect or language has one definite parent language.	Dialects Presentation||slides||Dialect vs. Language !!"What is the difference between a  dialect  and a  language ?!!"From a linguistic point of view, these terms are problematic !!"They might have a particular meaning from a socio-political  point of view !!"A 'language' tends to be associated with a standard language,  which is almost always written, and is almost always  associated with the speech of a wealthy, educated social class ! Dialects !!"From a linguistic point of view, there is no such thing !!"Linguistic variants can be separated geographically by  isoglosses !!"However, each isogloss will have a different geographic  distribution, yielding a huge number of 'dialects' (given  thousands of variants) !!"Similarly, variation along social dimensions is non-discrete ! Language !!"The concept of a 'language' is similarly problematic (e.g. the  Spanish language )!!"This problematic both temporally and geographically ! Temporal delimitation !!"Given that language change occurs item by item, in various  orders, there is no non-arbitrary point where, for example,  Latin gives way to Spanish !!"Nevertheless, there are two reasons to distinguish languages  temporally: !!"To label geographically distinct varieties !!"As a result of standardization ! Geographic delimitation !!"An artiÞcial method of delimiting the geographic distribution  of a language is through political boundaries - only relevant  for languages with some ofÞcial standardization !!"However, political boundaries and linguistic boundaries  rarely coincide !!"Mutual intelligibility is problematic, as it is non-discrete, and  often asymmetrical !!"Orthography is not necessarily keyed to similar varieties ! Languages and Dialects !!"What is wrong with saying " Andalucian  is a dialect of  Spanish"?  It is based on erroneous assumptions: !!"That a uniform standard language fragments into dialects !!"That the standard is somehow prior to the dialects (Castilian  was based on a variety spoken around Burgos, transplanted  to Toledo, then to Madrid, all for political reasons) !!"In addition, some varieties may share features with more  than one standard language (e.g. some dialects that share  features with Castilian and Catalan) !
Dialects	Typology
How are consonant sounds described scientifically?	Linguistic sounds are described in terms of their phonetic features, which for consonants, include a point of articulation, manner of articulation, and in English whether they are voiced or not. Other features, such as aspiration, or palatalization, may apply to specific allophones in English or to the phonemes of other languages. The point of articulation means where in the mouth, or with what parts of the mouth is the sound articulated. So "v" is a labio-dental sound; it is articulated between the lips and teeth.  The manner of articulation refers mainly to the nature of the airflow, such as the sound being a stop or a fricative. "v" is a fricative, meaning the airflow is not stopped, but obstructed enough to create friction (turbulance). Voicing refers to whether the sound is made with the vocal chords vibrating or not. "v" is voiced. So, altogether "v" is a "voiced labio-dental fricative."	Which of the following describes a real English consonant?	Voiceless post-alveolar fricative	Voiced interdental stop|||Voiceless lateral approximant|||Voiced bilabial fricative	Describing consonants||document||[f !" n #$! ks]Describing consonants What makes one consonant different from another? Producing a consonant involves making the vocal tract narrower at some location than it usually is. We call this narrowing a  constriction . Which consonant you're pronouncing depends on where in the vocal tract the constriction is and how narrow it is.  It also depends on a few other things, such as whether the vocal folds are vibrating and whether air is flowing through the nose. We classify consonants along three major dimensions: place of articulation manner of articulation voicing The  place of articulation  dimension specifies where in the vocal tract the constriction is.  The voicing parameter specifies whether the vocal folds are vibrating. The  manner of articulation dimesion is essentially everything else:  how narrow the constriction is, whether air is flowing through the nose, and whether the tongue is dropped down on one side. For example, for the sound  [d]:Place of articulation = alveolar.  (The narrowing of the vocal tract involves the tongue tip and the alveolar ridge.) Manner of articulation = oral stop.  (The narrowing is complete -- the tongue is completely blocking off airflow through the mouth.  There is also no airflow through the nose.) Voicing = voiced.  (The vocal folds are vibrating.) Voicing The vocal folds may be held against each other at just the right tension so that the air flowing past them from the lungs will cause them to vibrate against each other. We call this process  voicing.Sounds which are made with vocal fold vibration are said to be  voiced. Sounds made without vocal fold vibration are said to be  voiceless.There are several pairs of sounds in English which differ only in voicing -- that is, the two sounds have identical places and manners of articulation, but one has vocal fold vibration and the other doesn't.  The  [%] of thigh  and the  [&] of thy  are one such pair.  The others are: voiceless voiced [p][b][t][d][k] ['][f][v]  [%][&][s] [z] [(][)][t(][d)]The other sounds of English do not come in voiced/voiceless pairs.  [h] is voicess, and has no voiced counterpart. The other English consonants are all voiced:  [ *], [l], [w], [j], [m] , [n], and  [+]. This does not mean that it is physically impossible to say a sound that is exactly like, for example, an  [n]except without vocal fold vibration.  It is simply that English has chosen not to use such sounds in its set of distinctive sounds.  (It is possible even in English for one of these sounds to become voiceless under the influence of its neighbours, but this will never change the meaning of the word.)  Manners of articulation Stops A stop consonant completely cuts off the airflow through the mouth.  In the consonants  [t], [d], and [n], the tongue tip touches the alveolar ridge and cuts off the airflow at that point.  In [t] and [d], this means that there is no airflow at all for the duration of the stop. In  [n], there is no airflow through the mouth, but there is still airflow through the nose.  We distinguish between nasal stops , like  [n], which involve airflow through the nose, and oral stops , like  [t] and [d], which do not. Nasal stops are often simply called  nasals. Oral stops are often called plosives . Oral stops can beeither voiced or voiceless.  Nasal stops are almost always voiced.  (It is physically possible to produce a voiceless nasal stop, but English, like most languages, does not use such sounds.) Fricatives In the stop  [t], the tongue tip touches the alveolar ridge and cuts off the airflow.  In [s] , the tongue tip approaches the alveolar ridge but doesn't quite touch it. There is still enough of an opening for airflow to continue, but the opening is narrow enough that it causes the escaping air to become turbulent (hence the hissing sound of the  [s] ).  In a  fricative  consonant, the articulators involved in the constriction approach get close enough to each other to create a turbluent airstream. The fricatives of English are  [f], [v] , [%], [&], [s] , [z] , [(], and  [)].Approximants In an approximant, the articulators involved in the constriction are further apart still than they are for a fricative.  The articulators are still closer to each other than when the vocal tract is in its neutral position, but they are not even close enough to cause the air passing between them to become  turbulent. The approximants of English are  [w], [j], [ *], and  [l].Affricates An affricate is a single sound composed of a stop portion and a fricative portion.  In English  [t(], the airflow is first interuppted by a stop which is very similar to  [t] (though made a bit further back). But instead of finishing the articulation quickly and moving directly into the next sound, the tongue pulls away from the stop slowly, so that there is a period of time immediately after the stop where the constriction is narrow enough to cause a turbulent airstream.  In [t(], the period of turbulent airstream following the stop portion is the same as the fricative  [(]. English  [d)] is an affricate like [t(], but voiced. Laterals Pay attention to what you are doing with your tongue when you say the first consonant of  [lif] leaf .Your tongue tip is touching your alveolar ridge (or perhaps your upper teeth), but this doesn't make [l] a stop. Air is still flowing during an  [l] because the side of your tongue has dropped down and left an opening.  (Some people drop down the right side of their tongue during an  [l]; others drop down the left;  a few drop down both sides.) Sounds which involve airflow around the side of the tongue are called  laterals . Sounds which are not lateral are called  central .[l] is the only lateral in English.  The other sounds of Englihs, like most of the sounds of the world's languages, are central. More specifically,  [l] is a lateral  approximant . The opening left at the side of the tongue is wide enough that the air flowing through does not become turbulent. Places of articulation The place of articulation (or POA) of a consonant specifies where in the vocal tract the narrowing occurs.  From front to back, the POAs that English uses are: Bilabial In a bilabial consonant, the lower and upper lips approach or touch each other.  English  [p], [b], and  [m]  are bilabial stops. The diagram to the right shows the state of the vocal tract during a typical [p] or  [b]. (An  [m]  would look the same, but with the velum lowered to let out through the nasal passages.) The sound  [w] involves two constrictions of the vocal tract made simultaneously.  One of them is lip rounding, which you can think of as a bilabial approximant. Labiodental  In a labiodental consonant, the lower lip approaches or touches the upper teeth.  English  [f] and [v]  are bilabial fricatives. The diagram to the right shows the state of the vocal tract during a typical [f] or  [v] . Dental In a dental consonant, the tip or blade of the tongue approaches or touches  the upper teeth.  English  [%] and [&] are dental fricatives.  There are actually a couple of different ways of forming these sounds: The tongue tip can approach the back of the upper teeth, but not press against them so hard that the airflow is completely blocked. The blade of the tongue can touch the bottom of the upper teeth, with the tongue tip protruding between the teeth -- still leaving enough space for a turbulent airstream to escape.  This kind of  [%]and [&] is often called  interdental .The diagram to the right shows a typical interdental  [%] or  [&].Alveolar In an alveolar consonant, the tongue tip (or less often the tongue blade) approaches or touches the alveolar ridge, the ridge immediately behind the upper teeth.  The English stops  [t], [d], and  [n] are formed by completely blocking the airflow at this place of articulation.  The fricatives  [s]  and [z] are also at this place of articulation, as is the lateral approximant  [l].The diagram to the right shows the state of the vocal tract during plosive [t] or  [d]. Postalveolar In a postalveolar consonant, the constriction is made immediately behind the alveolar ridge.  The constriction can be made with either the tip or the blade of the tongue. The English fricatives  [(] and[)] are made at this POA, as are the corresponding affricates  [t(] and [d)].The diagram to the right shows the state of the vocal tract during the first half (the stop half) of an affricate  [t(] or  [d)]. Retroflex In a retroflex consonant, the tongue tip is curled backward in the mouth. English  [ *] is a retroflex approximant -- the tongue tip is curled up toward the postalveolar region (the area immediately behind the alveolar ridge). The diagram to the right shows a typical English retroflex  [ *].Both the sounds we've called "postalveolar" and the sounds we've called "retroflex" involve the region behind the alveolar ridge. In fact, at least for English, you can think of retroflexes as being a sub-type of postalveolars, specifically, the type of postalveolars that you make by curling your tongue tip backward. (In fact, the retroflexes and other postalveolars sound so similar that you can usually use either one in English without any noticeable effect on your accent. A substantial minority North American English speakers don't use a retroflex  [ *], but rather a "bunched" R -- sort of like a tongue-blade  [)]with an even wider opening. Similarly, a few people use a curled-up tongue tip rather than their tongue blades in making  [(] and [)].)Palatal In a palatal consonant, the body of the tongue approaches or touches the hard palate.  English  [j] is a palatal approximant -- the tongue body approaches the hard palate, but closely enough to create turbulence in the airstream. Velar In a velar consonant, the body of the tongue approaches or touches the soft palate, or velum. English  [k] , ['], and  [+] are stops made at this POA.  The  [x]  sound made at the end of the German name  Bach or the Scottish word  loch  is the voiceless fricative made at the velar POA. The diagram to the right shows a typical  [k]  or  ['] -- though where exactly on the velum the tongue body hits will vary a lot depending on the surrounding vowels. As we have seen, one of the two constrictions that form a  [w] is a bilabial approximant.  The other is  a velar approximant:  the tongue body approaches the soft palate, but does not get even as close as it does in an  [x] .GlottalThe glottis is the opening between the vocal folds. In an  [h], this opening is narrow enough to create some turbulence in the airstream flowing past the vocal folds. For this reason,  [h] is often classified as a glottal fricative.  Summary of English consonants [p]voiceless bilabial plosive [b]voiced bilabial plosive [t]voiceless alveolar plosive [d]voiced alveolar plosive [k] voiceless velar plosive [']voiced velar plosive [t(]voiceless postalveolar affricate [d)]voiced postalveolar affricate [m] voiced bilabial nasal[n]voiced alveolar nasal[+]voiced velar nasal[f]voiceless labiodental fricative [v] voiced labiodental fricative [%]voiceless dentalfricative [&]voiced dentalfricative [s] voiceless alveolar fricative [z] voiced alveolar fricative [(]voiceless postalveolar fricative [)]voiced postalveolar fricative [ *]voiced retroflex approximant [j]voiced palatalapproximant [w]voiced labial + velar approximant [l]voiced alveolar lateral approximant  [h]voiceless glottalfricative  Consonant charts It is often useful to display the consonants of a language in the form of a chart.  There is a conventional way of doing so: Columns show places of articulation, arranged (roughly) from the front of the vocal tract to the back. Rows show manners of articulation. Within each cell, the symbol for a voiceless sound is shown toward the left of the cell and the symbol for a voiced sound toward the right. The following is the chart for English consonants: bilabial labiodental dentalalveolar postalveolar retroflex palatalvelar glottalplosive pb  td   k' nasal m   n    + fricative  fv%&sz()   h approximant  (w)     * j (w)  lateral approximant     l     affricate     t(d)
Phonetics	Phonetic features	Phonetic analysis	English consonants
Provide an overview of macrolinguistics.	Macrolinguistics is a study area including anthropological linguistics, sociolinguistics, psycholinguistics, computational linguistics, stylistics, philosophical linguistics, forensic linguistics, discourse analysis.	Which of the following is not a branch of macro- or microlinguistics?	Diction	Semantics|||Pragmatics|||Ethnolinguistics	Branches of Linguistics presentation||slides||BRANCHES OF  LINGUISTICS  LINGUISTICS Linguistics is the scientific study of natural language . Someone who engages in this study is called a linguist .  BRANCHES OF LINGUISTICS  Language in general and language in particular can be studied from different points of view  The field of linguistics as a whole can be divided into several subfields according to the point of view that is adopted  FIRST DISTINCTION GENERAL LINGUISTICS  Studying language in  general  Supplies the concepts  and categories in terms  of which particular  languages are to be  analysed DESCRIPTIVE LINGUISTICS  Studying particular  languages  Provides the data  which confirm or refute  the propositions and  theories put forward in  general linguistics  SECOND DISTINCTION Diachronic (Historical)  Linguistics  Traces the historical  development of the  language and records  the changes that have  taken place in it between  successive points in time:   to historical  Of particular interest to  linguists throughout the  nineteenth century Synchronic Linguistics  Non - historical: presents  an account of the  language as it is at  some particular point in  time  THIRD DISTINCTION Theoretical Linguistics  Studies language and  languages with a view to  constructing a theory of  their structure and functions  and without regard to any  practical applications that  the investigation of  language and languages  might have  Goal: formulation of a  satisfactory theory of the  structure of language in  general Applied Linguistics  Application of the  concepts and findings  of linguistics to a variety  of practical tasks ,  including language  teaching  Concerned with both  the general and  descriptive branches of  the subject  FOURTH DISTINCTION Micro linguistics  Adopts the narrower  view  Concerned solely with  the structures of the  language system in  itself and for itself Macro linguistics  Adopts the broader view  Concerned with the way  languages are acquired,  stored in the brain and  used for various functions;  interdependence of  language and culture;  physiological and  psychological  mechanisms involved in  language behaviour  FOURTH DISCTINCTION (CONTD.) Micro Linguistics  Phonetics  Phonology  Morphology  Syntax  Semantics  Pragmatics Macro linguistics  Psycholinguistics  Sociolinguistics  Neurolinguistics  Discourse Analysis  Computational  Linguistics  Applied Linguistics   MICROLINGUISTICS  Phonetics is the scientific study of speech sounds . It studies how speech sounds are articulated, transmitted, and received .  Phonology is the study of how speech sounds function in a language, it studies the ways speech sounds are organized . It can be seen as the functional phonetics of a particular language .  Morphology is the study of the formation of words . It is a branch of linguistics which breaks words into morphemes . It can be considered as the grammar of words as syntax is the grammar of sentences .  MICROLINGUISTICS  Syntax deals with the combination of words into phrases, clauses and sentences . It is the grammar of sentence construction .  Semantics is a branch of linguistics which is concerned with the study of meaning in all its formal aspects . Words have several types of meaning  Pragmatics can be defined as the study of language in use in context .  MACROLINGUISTICS  Sociolinguistics studies the relations between language and society : how social factors influence the structure and use of language .  Psycholinguistics is the study of language and mind : the mental structures and processes which are involved in the acquisition, comprehension and production of language .  Neurolinguistics is the study of language processing and language representation in the brain . It typically studies the disturbances of language comprehension and production caused by the damage of certain areas of the brain .  MACROLINGUISTICS  Discourse analysis, or text linguistics is the study of the relationship between language and the contexts in which language is used . It deals with how sentences in spoken and written language form larger meaningful units .  Computational linguistics is an approach to linguistics which employs mathematical techniques, often with the help of a computer .  Applied linguistics is primarily concerned with the application of linguistic theories, methods and findings to the elucidation of language problems which have arisen in other areas of experience  MACROLINGUISTICS  Forensic linguistics, legal linguistics, or language and the law, is the application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure . It is a branch of applied linguistics .  There are principally three areas of application for linguists working in forensic contexts :  understanding language of the written law,  understanding language use in forensic and judicial processes, and  the provision of linguistic evidence
Macrolinguistics	Microlinguistics	Branches of linguistics
What is the basic and most common requirement for an English sentence to be to be correctly formed?	An English sentence needs to have a noun phrase followed by a verb phrase present.	Is the following sentence suitable to be a typical English sentence according to the resource: 'The dog's ugly tail'	No	Yes	NLP Crash Course clip||youtube||N/A
Natural Language Processing (NLP)	Branches of linguistics	Syntax	Phrases
Define symbolic communication.	"Use of arbitrary symbols whose definitation and usage are agreed upon by the community of users."	Which of the following is not an example of symbollic communication?	A baby crying to signify they are hungry.	The English word "chair" representing an object that is a seat for one person, typically with four legs.|||The Braille dots representing the word "cat".|||The sign language gesture that represents a bad smell.	NONE
Key terms in linguistics
Provide a label for the different lobes and regions of the brain labelled A-E in the image provided.	A - Frontal Lobe B - Occipital Lobe C - Cerebellum D - Parietal Lobe E - Temporal Lobe	Which of the following lobes in responsible for perception?	Parietal Lobe	Temporal Lobe|||Frontal Lobe|||Occipital Lobe	Brain Lobes presentation||slides||Brain Lobes Frontal, Temporal, Parietal,  Occiptal and Limbic  Brain  Brain Lateral  View  Brain -- Lateral  View - Lobes and functions  Brain Lobes and theirs functions  Brain , Medial  View  Five  Lobes  Medial  View of Brain  Brain  primitive emotions  Cerebral  Lobe dominance  Tha Thalamus  The Visceral  Brain  Thalamus and base  Nucleus  Basal  Ganglia  Substantia nigra  Thalamus and Hypothalamus  Limbic system (  memory and attention )  RNM  - Thalamus  Thalamus
Neurolinguistics	Localization of language	Brain functions	Parts of the brain	Neuroanatomy
Define "Learning a foreign language."	Learning a foreign language is generally used to make reference to acquisition of a language that is different from one's native language. Learning can be generated in a number of contexts, including the following: in an environment where the language is socially dominant or in one where it is not widely used in the learner's immediate social surrounding.	Learning a foreign language would be part of which branch of linguistics?	Applied linguistics	Microlinguistics|||Theoretical linguistics	Motivation and E-Learning English as a foreign language: A qualitative study||publication||MotivationandE-LearningEnglishasaforeignlanguage:Aqualitativestudy FredyGeovanniEscobarFandi ~noa,b,c,d,*,LuzDaryMu ~nozd,e,f,AngelaJulietteSilvaVelandia d,gaUniversidadDistritalFranciscoJos edeCaldas,Bogot a,Colombia bUniversidadNacionaldeColombia,Bogot a,Colombia cUniversidaddelosAndes,Bogot a,Colombia dCorporacionUniversitariaMinutodeDios ŒUNIMINUTO,AcademicDirection,Carrera73a#81b-70.Edi cioC.Piso8,Bogot a,Colombia eUniversidadPedag ogicaNacional,Bogot a,Colombia fUniversidadExternadodeColombia,Bogot a,Colombia gUniversidaddelaSalle,Bogot a,Colombia ARTICLEINFO Keywords:Psychology Education  CALLMotivation Meaningandbene tsE-learningABSTRACT Themotivationtolearnaforeignlanguageisacomplexprocess.AccordingtotheMinistryofNationalEducation ofColombia-MEN-,speakingEnglishisagenericcompetencetobedevelopedatalleducationallevels.Inthis regardandbasedontheTheoryofSelf-Determination-SDT-,thisqualitativephenomenologicalstudyaimsto  identifyandanalysetheaspectsrelatedtothemotivationtolearnEnglishinundergraduatestudentsoftheVirtual andDistancemodalities-E-Learning-.Themaininstrumentusedfordatacollectionwasasemi-structuredindi- vidualinterviews.Theparticipantsintheresearchwereagroupof16womenand3men.Theanalysiswascarried  outthroughsemanticcategorizationsandwiththesupportofNVivo11software,whichletsassumethatmoti-  vationforlearningEnglishisstronglyin uencedbyexternalfactors. 1.Introduction Learningaforeignlanguageisinalleducationallevels( Bhowmik,2015;Motteram,2013 ).AccordingtotheMinistryofNationalEducation ofColombia(hereinafter MEN,2004 ),Englishisthemostimportant foreignlanguage.CommunicatinginEnglishhasbecomeacorecompe- tencethatshouldbedevelopedinallacademiccontexts.However, althoughEnglishhasbeenincorporatedintomostundergraduatepro- gramsafollowupoftheresultsofSaberPro(asetofprofessionalcom- petencyfocusedtests)duringthelast5yearsisnotentirelysatisfactory, since1outofevery6studentsdoesnothaveabasicdevelopmentonthis competenceandthegeneralresultsarehighlydispersed( Icfes,2016 ).Thissituationisverycomplicatedbecauseitdirectlyaffectsthequalityof educationandthedevelopmentofthecountry.Atpresent,knowing Englishisconsideredanimportantrequirementtoreachsocial,academic andeconomicprogressaroundtheworld. Thesuccessonforeignlanguagelearningisinfusedwithmanyfactors asintelligence,attitudes,abilitiesandmotivation( Mantiri,2015 ;San-tanaetal.,2016 ;StøenandHaugan,2016 ).Amongthem,motivationis themainfactoraffectingforeignlanguagelearningsinceitmediatesthe attitudestowardthetargetlanguageandtheoutputsintheprocessof learningit( Mantiri,2015 ;Kazantsevaetal.,2016 ).Motivationisa complexpsychologicalprocessthatinvolvesmanyaspectssuchas cognition,behaviour,emotion,decision-makingprocessandbiological aspects( Gonzalez,2008 ;Marshall,2010 ;Woonetal.,2016 ).Although motivationisanessentialpartofthelearningprocess,especiallyan autonomousone,researchingonitisnotenough.Whilethevolumeof researchconcerningmotivationitselfinpsychologyandeducationis vast,therehavebeenveryfewknowingaboutlearningEnglishinOnline modalityandmotivationtolearnit.Eventhus,thissituationremainsa worldwideconcern( BusseandWalter,2013 ;Kazantsevaetal.,2016 ;Linetal.,2017 ;Pourfeiz,2016 ).Onthisbasis,UNIMINUTOasaChristian,ConfessionalandNational universityisnotunfamiliarwiththelackofsuccessinlearningEnglishas aforeignlanguage.Thiseducationalinstitutionwithheadquarters aroundColombiaandaninternationalone,locatedinIvoryCoast,Africa, offersitsmajorsinface-to-facemodalityandonlinemodality(hereinafter UVD).AslongasUVDisthebiggestheadquarterinColombia,itis  importantfortheinstitutiontoknowwhatmotivatesstudentstolearn EnglishsinceitisaNationalEducationalrequirementthatitisrelatedto aninternationalneed. KnowinghowanEnglishlearningprocessdevelopsinvirtualenvi- ronmentsisveryrelevantsinceastudentbecomesthemainactorinthe learningprocessandthecompletionoftheprocessremainsprimarilyhis/ *Correspondingauthor. E-mailaddress: fescobarfan@uniminuto.edu.co (F.G.EscobarFandi ~no).Contentslistsavailableat ScienceDirect Heliyonjournalhomepage: www.heliyon.com https://doi.org/10.1016/j.heliyon.2019.e02394 Received14March2019;Receivedinrevisedform13August2019;Accepted28August2019 2405-8440/ ©2019TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense( http://creativecommons.org/licenses/by- nc-nd/4.0/).Heliyon5(2019)e02394  herresponsibility,whichisakeyaspectwhenlearningaforeignlan- guage( BaniHani,2014 ;Burkle,2011 ;Contrerasetal.,2011 ;Liu,2013 ;Papaefthymiou-Lytra,2014 ;Rosario,2006 ).Therefore,theaimofthis studywastounderstandhowEnglishasforeignlanguageislearnedusing onlinemodality,whystudentslearnitandwhatkindofmotivationUVD studentshave.Thus,UNIMINUTOcouldadaptthecurriculumtothe motivationalpro lestoenhancethelearningexperience.Inordertoful lthisgoal,aqualitativestudyofphenomenologicaltypewasconducted with19UVDstudentsasasample. 2.Theory 2.1.Learningaforeignlanguage Learningaforeignlanguageisageneralexpressionusedtoreferto theappropriationofalanguageapartfromthemotherlanguage,andthis  learningcanbegeneratedindifferentcontexts,includingtwoinpartic- ular:the rstone,inwhichthelanguageissociallydominant,andthe secondone,inwhichtheforeignlanguageisnotwidelyusedinthe students'immediatesocialsurrounding( Lin,2008 ;Saville-Troike,2006 ).Learningaforeignlanguagerequiresamultidimensional,multifactorial andintegrativeframeworkconsolidatedundertwotypesofvariables:(a) thoserelatedtothecontext,and(b)thoserelatedtothesubject( Krashen,1989;Stern,1991 ).Contextvariablesencompassthelinguistic,socialandeducational contexts,includingfactorssuchasawidespreaduseoftheforeignlan- guage,thetimedevotedtoitsstudy,commonsocialbeliefsandattitudes concerningtheforeignlanguage,culturalsensitivity,socialnetworks, accesstoculturalgoods,educationalpolicy,theschoolenvironmentand teachingmethods( Krashen,1989 ;Richards,2014 ;Santanaetal.,2016 ;VermuntandDonche,2017 ).Variablesrelatedtothepersonincludeaptitudesandlearningpro- cess,aswellas,affectiveandmotivationalfactors.Aptitudesinvolvethe useofthelanguage,thedevelopmentofphonetic,grammaticaland pragmaticskills,alongwithlearningstrategies( Santanaetal.,2016 ).Theaffectiverealmembodiesarangeoffactorssuchasattitudes, empathy,self-esteem,extraversion,inhibition,imitation,anxietyand especially,motivation( Richards,2014 ;Santanaetal.,2016 ;VermuntandDonche,2017 ).Therefore,thisisaneedtounderstandmotivation processthatunderliestheentirelearningprocess. 2.2.Motivationasapsychologicalprocess Motivationisahighlycomplexpsychologicalprocess,involvingthe nervousactivity,cognition,emotionalrealmandthestablepersonality traitsthatallowpeopletointeractwiththeirenvironment.Thispsy- chologicalprocessaimsatful llingtheneedsofindividualsand,there- fore,triggerstheirbehaviour.Motivationisaforcethatactivates, encourage,directsandkeepsgoaldirectedbehaviour( Gonzalez,2008 ;Marshall,2010 ;Woonetal.,2016 ).Oneofthetheoreticalmodellingthatexplainthemotivationisthe Self-DeterminationTheory(SDThereinafter).SDTemphasizesonthe regulationoftheindividual'shumanbehaviourandhowmotivationcan bedifferentineachperson.Speci cally,thistheoryfocusesonhow personalmotivesareintegratedandregulatedintheindividual(Self), empoweringhim/herandallowingagoodfunctioning.Astheinteraction ofanindividualwiththeenvironmentneverends,SDThighlightshow  ideas,valuesandgoalsareinternalizedaccordingtothein uenceof numerousvariablesinthesocialcontext.Eachpersonregulateshis/her behaviourinaccordancewithhis/herpsychologicalneedsandcandoit onacontinuousbasisfromacompletelyexternalperspectivetoan internalizedandautonomousone( DeciandRyan,2014 ;RyanandDeci, 2000).IntheSDT,internalizationandtypesofregulationmaybeimple- mentedprogressivelyoverthelifespan,allowingasophisticateddiffer- entiationbetweenthemostextrinsiccharacteristicsandthemost intrinsicofthebehaviours.Externalaspectscauseextrinsicmotivation, whileintrinsicmotivationhastodowithautonomousaspects,whichis themostvaluable( Deci&Ryan,1985 ).Fig.1 presentsanoverviewofthe internalizationcontinuumamongtypesofmotivation. AccordingtoSDT,thedifferenttypesofmotivationarede nedbased ontheamountandlevelofcontroldisplayedbytheindividuals.Intrinsic motivationisanarchetypeofautonomy,whileextrinsiconeisrelatedto variablescontrolledbythecontext.Therearefourtypesofextrinsic motivation:(1)externalregulation,whichisbasicallycontrolledby environmentalcontingencies;(2)introjectedregulation,inwhichthe individualprimarilyinternalizescontingencies;(3)identi edregulation, inwhichtheindividualacknowledgestheimportanceoftheactivity,and (4)integratedregulation,inwhichtheindividualbehavesconsistently withhis/herneeds;thelatterisconsideredthemostautonomousofall. Thismodelcouldbecomparablewiththeoneabouttrainingpatternsof studentslearning( DeciandRyan,1985 ,2008;2014;Nu~nezandLe on,2015;RyanandDeci,2000 ;Vallerandetal.,1993 ;VermuntandDonche, 2017).Learningisanactivityofvitalimportanceforhumanityinwhich motivationplaysakeyrole.Naturally,themotivationforlearningmay varydependingonthetypeofstudy,butthispaperfocusesonvirtual modalitiesorE-Learning. 2.3.E-learningandCALL TheconceptofE-Learningisverybroad.Itwasconceivedattheend ofthe1990sasahugetechnologicaladvancethathasenhancedlearning mechanismsthroughtheInternet.Currently,itisconsideredthatE- Learninginvolvesawiderangeofmultimediatoolssuchasinternet, interactivetelevisionandallformsofelectronicsupport,amongothers, allofwhichfavourslearningandmakesitmore exibleandfriendly (Kakotyetal.,2011 ).Additionally,ithelpsstudentstogetknowledge, practiceandexperience( Burkle,2011 ;Contrerasetal.,2011 ;Liu,2013 ;Tavangarianetal.,2004 ).IntheE-learning eld,theComputerAssistedLanguageLearning (CALLhereinafter)standsout.Thismodelencompassesfourapproaches: traditional,explorative,multimediaandtheWorldWideWeb.Psycho- logicalschoolssuchasbehaviourism,cognitivism,constructivismand connectivismcontributetotheconstructionofCALL'sreferentialframe- work( Davies,2008 ;Levy,1997 ).CALLisanapproachthatcanplayakeyroleinthelanguageacqui- sitionprocess.Itisnotjustarepetitionofexercisescontrolledbya programsinceithasthepotentialformodifyingthelearners'cognitive schemes,aslongastheyareactivelyinvolved( FarooqandJavid,2012 ).CALLreliesontheuseofseveraltechnologicaltoolsandacontinuous organizedowofactivitiescloselyconnectedtoreallifesituations, whichmakesstudents'learningmoremeaningful.CALLallowsstudents todevelopdifferentskillssuchaslistening,reading,grammar,writing andspeaking,aswellastheapprehensionofnewvocabularyandthe  improvementofpronunciation( Hani,2014 ).Additionally,CALLfosters motivationandautonomyinstudents,sinceitletsthemtomonitorand evaluatetheirdevelopmentandunderstandingofthecoursecoretopics (ChapelleandJamieson,2008 ;Motteram,2013 ).Throughtheuseof CALL,technologystandardizationispromoted,buttutorsneedto incorporateskillstoeffectivelymanagetechnologyinteachingprocesses (ThomasandMotteram,2019 ).2.4.MotivationandCALL MotivationandCALLareintrinsicallyinterconnected.Attitudesand academicmotivationinteractextensivelywithlearningaforeignlan- guagewithoutadoubt( Pourfeiz,2016 ).Motivationisprobablythemost studiedfactoranditisthemainrequirementforpositiveresults (Kazantsevaetal.,2016 ;Zenots,2012 ).Apparently,integratinginthe curriculumactivitiesthatpromotemotivationandallowstudentsto experiencethesocioculturalenvironmentoftheforeignlanguage(as theydoononlineenvironments),couldimprovethelearningprocess F.G.EscobarFandi ~noetal. Heliyon5(2019)e02394 2 (Galishnikova,2014 ;Liu,2014 ).Intrinsicmotivationisenhancedbytheneedtointeract,tobe competentandtoachieveautonomy.Whenastudentconsciouslyiden- tieshimself/herselfwiththeongoingactionorwithitsvalue,thereisa highdegreeofperceivedautonomy( RyanandDeci,2000 ).Whenstu- dentshavehighlevelsofperceivedautonomy,theycanparticipateinthe denitionoftheirlearninggoals,thustests'resultsareimproved( Alietal.,2016 ;ButzandStupnisky,2017 ;Linetal.,2017 ;Shyan,2016 ;VanLoonetal.,2012 ).Inaddition,studentstendtoshowabetterdisposition tolearnwhentheyuseeffectivelyandpurposelythecomputer.Apro- ductiveandinteractivelearningenvironmentincreaseslearner'smoti- vation,especially,ifthestudentsfeelcomfortablewiththetechnology (Al-kahiry,2013 ;Baracketal.,2016 ;Huangetal.,2019 ;Kazantsevaetal.,2016 ;Lee,2016 ;Majidetal.,2012 ;Shyan,2016 ).Somestudies evenpointoutthatonlineapprenticesshowgreaterintrinsicmotivation thanthosewhofollowaface-to-facemethodology( Shroffetal.,2008 ;Firatetal.,2018 ).However,aslongasintrinsicmotivationdeclines, autonomylevelswillalsodoitaswellasgeneraleffortstolearnaforeign languageandstudentswillhaveahardtimetransferringknowledgeto otherareasofknowledge( BusseandWalter,2013 ;James,2012 ).Simi- larly,theefforttogetinvolvedinlearningaforeignlanguageisnotal-  wayscorrelatedwiththedesiretobecompetent( Kazantsevaetal., 2016).Motivationtolearnaforeignlanguageisalsorelatedtoextrinsic regulations,becausecontextualfactorsalsoin uencelearning,since aspectssuchasthecurriculum,resources,teacher,culturalcapitaland socioeconomicstatuscanin uencethesenseandthequalityofthe motivation( Al-kahiry,2013 ;Baracketal.,2016 ;Sergisetal.,2018 ).A stablesocioeconomicconditionseemstohaveadirectimpactonmoti- vationalaspects,asitisrelatedtogreateraffordabilityforinitiatingand completingstudies,aswellasputtingacquiredknowledgeintopractice (Alietal.,2016 ).Thepossibilitiestoaccesstechnologyandtheabilityto solvetechnicalaspectsalsoseemtopositivelyaffectthemotivationto learnaforeignlanguageinonlinemodality,becauseiftechnological toolsareavailableonapermanentbasis,themotivationtendstobe greater( Hani,2014 ;HammouriandAbu-Shanab,2018 ).Eventhen, researchrevieweddidnotrevealthatgenderfactorhadanyimpacton thelearningofaforeignlanguage( Kitcharkarn,2015 ).Apparently,the mostexternalaspectsordirectlyrelatedtothecontextwherethe teaching-learningprocessesofEnglishasaforeignlanguagearedevel- opedaffectinonewayoranotherthemotivationwithwhichstudents assumetheironlinecourses. MotivationisalsorelatedtoCALL.SeveralstudiesaboutCALL, frequentlyreportsigni cantbene tswhenstudyingthroughthismo- dality,however,agroundedtheoryhasnotbeenclearlyestablished (MutluandEr €oz-Tuga,2013 ;WangandV asquez,2012 ;Zareiand Hashemipour,2015 ).Nevertheless,agreatpartofthesuccessisgroun- dedinanexcellentstrategiesselectiontoenhancethelearningprocess andinanintensiveinstructor'sguidance,sothatstudentsunderstandthe taskandfeelsupported( MutluandEr €oz-Tuga,2013 ;SeguraandGreener, 2014).Inaddition,forCALLtobebene cial,animmediatefeedbackmustbe provided,asthisencouragesthemotivationtowardslearning.Asenseof entertainmentshouldbearoused,aswellasahighdegreeofinteraction andanincreaseinself-control( Hani,2014 ;Salgadoetal.,2015 ).Like- wise,thelearningenvironmentshouldbe exibletosolvetheactivities andstudentscantakeadvantageofteachingmaterials( Seguraand Greener,2014 ;Kessler,2018 ).Inconclusion,whentherearehighlevelsofintrinsicmotivation, especiallyconcernedwithahighperceivedautonomy,studentstendto controlmoreoftheirtasksandacademicactivities.Besides,ifthereare favourablecontextualconditions,motivationtolearnEnglishasaforeign languagetendstoremainhigh.Whatismore,followinganonline methodologyseemstobene tstudents'performance,aslongastheyfeel entertained,receivetimelyfeedbackandexperienceinteractionwith authenticresources,classmatesandthetutor. 3.Methodology Theresearchmethodologyisqualitativeofphenomenologicaltype. ThisresearchwasapprovedbytheGeneralCommitteeofResearchEthics ofUNIMINUTO,whichisrecognizedbytheAdministrativeDepartment ofScience,TechnologyandInnovationofColombia-COLCIENCIASand followstheHelsinkiDeclarationofethics.Allparticipantssignedthe informedconsenttoguaranteeandprotecttheircon dentiality.Asemi- structuredindividualinterviewwasconductedtoidentifyandunder- standthefactorsrelatedtomotivationasapsychologicalprocessthat inuencesthelearningofEnglishasaforeignlanguageinvirtualenvi- ronmentssinceitenablesacomprehensiveapproachtothephenomenon. Thisinstrumentwasappliedbecauseitallowsadiscourseandcontent analysisbysettingsemanticcategoriestoreduceinappropriatein- terpretationsandguaranteehighvalidity( Caceres,2003 ;Quecedoand Casta~no,2002 ;Sandoval,2002 ).3.1.Sample Thestudyincluded19volunteerswhowereinterviewed(see Table1 ).ParticipantswereUVDundergraduatestudentspursuingoneof thethreelevelsoftheEnglishCourseduringthethirdquarterof2018.In ordertokeepstudents'informationcon dential,eachparticipantwas assignedanumberfrom1to19,sothat,theiranswerscouldbequoted throughoutthereport. 3.2.Researchdesign:qualitativedatacollectioninstrument Theinterviewisatoolthatallowstoknowthephenomenaindepth. Amotivation  Extrinsic  Motivation  Intrinsic  Motivation  Non- Regulation External Regulation Introjected Regulation  Identified Regulation Integrated  Regulation Intrinsic  Regulation Less  Self-determination  More  Self-determination  Amotivation =   No MotivationControlled Motives  = Low- quality MotivationAutonomous  Motives  = High-quality  Motivation Fig.1. MotivationtypesandregulationinSDT.Adaptedfrom "Facilitatingoptimalmotivationandpsychologicalwell-beingacrosslife'sdomains ".InE.L.Deci, &R.M.Ryan,(2008),CanadianPsychology/Psychologiecanadienne,49(1),14 Œ23.F.G.EscobarFandi ~noetal. Heliyon5(2019)e02394 3 Thistoolwaschosenbecauseitmayeasilyprovidetheinformation concerningthequestion,theproblemandtheresearchobjectives. Therefore,asemi-structuredinterviewwasconductedwith19partici- pantstoobtaintheDataandtoanswertheresearchquestions( Día-z-Bravoetal.,2013 ;Flick,2004 ).Thisresearchtoolwasdesignedtaking intoaccounttheliteraturereviewed. Theinterviewwasstructuredintwoblocks:the rstonewasgeared towardscollectingsocio-demographicdata,andthesecondone,towards answeringtheresearchquestions;thislastgroupfocusedonobtaining informationonpersonalandcontextualfactorsassociatedwiththe motivationtolearnEnglishasaforeignlanguagesuchasthemeaningof learning( DeciandRyan,1985 ,2008;2014;Nu~nezandLe on,2015 ;Vallerandetal.,1993 ),thevirtuallearningenvironmentscharacteriza- tion( Burkle,2011 ;Contrerasetal.,2011 ;Rosario,2006 )andthe awarenessoftheirdisplayedlearningstrategies( Canning,2004 ;FarooqandJavid,2012 ;Kakotyetal.,2011 ;Levy,1997 ).3.3.Procedure:interview Tomaketheinterviews,amassmessagewasemailedfromtheEnglish AreaOf cetoUVDstudentspursuingoneofthe3levelsoftheEnglish Course.Allthoseinterestedinparticipatingreceivedspeci cinformation regardingtheobjectiveoftheresearchanditsdatacollectioninstrument. 19respondentswereselectedandthentheycompletedtheinformed consenttoguaranteeandprotecttheircon dentiality.The19interviews wereconductedinSpanishlanguage,fromOctober4 thtoNovember23 rd,2018,15ofwhichwereappliedonlineand4face-to-face.Inorderto minimizeanybiasesassociatedwithframingtheanswerstothequestions regardingwiththetutor,eachresearcherinterviewedstudentswhowere studyingalevelofEnglishdifferentfromtheoneheorshewasdirecting atthatmoment. 3.4.Resultsanalysis Theinterviewsweretranscribedtomakeacodingandacontentand discourseanalysis( Fernandez,2006 ;Rodríguezetal.,2005 ).Theanal- ysiswascarriedoutbyusingNVivosoftwareversion11andacodebook (seeTable2 ),whichwasdevelopedfromtheexistingscienti cliterature previouslypresented( Al-kahiry,2013 ;Baracketal.,2016 ;Davies,2008 ;DeciandRyan,1985 ,2008;2014;FarooqandJavid,2012 ;Hani,2014 ;Kazantsevaetal.,2016 ;Lee,2016 ;Levy,1997 ;Majidetal.,2012 ;Nu~nezandLe on,2015 ;Pourfeiz,2016 ;RyanandDeci,2000 ;Sergisetal.,2018 ;Shyan,2016 ;Vallerandetal.,1993 ;VermuntandDonche,2017 ).4.Resultsanddiscussion Resultsabouttheresearchquestionsandobjectivesarepresented below.Theseincludeadescriptivesectionrelatedtothefrequencyof wordsandananalytical-descriptivesectioncorrespondingtotheinter- viewdatacodingwhichwasdevelopedfromthereviewedliterature: Self-DeterminationTheory,E-Learning(CALLmodel)andlearningEn- glishasaforeignlanguage.Theseanalyticresultsaredividedinpersonal andcontextualfactors. 4.1.Wordsfrequency NVivosoftwareversion11wasusedtoreviewandorganizewords,as wellas,toproduceacloud(see Fig.2 ),whichexhibitsthefrequencyand the19UVDstudents'responsessemantictendency.Inthissection,ex- cerptsfromtheinterviewstranscriptionsareincludedparticularlythe onespertainingtothecategoriesofthecodebook:motivation,CALLand learning.Havinganalysedfactorsassociatedwiththemotivationtolearn Englishasaforeignlanguageinavirtuallearningenvironments,the researchersfoundahighfrequencyofwordssuchas "English","activ-ities","learning","exercises","platform","tutor"and"Benets".Thewordsfrequencyseemstorevealthatthemotivationtolearn Englishasaforeignlanguagecanbeaffectedbyseveralfactors,fromthe mostpersonalorconcerningthelearnertothemostexternalsuchas virtualenvironmentscharacterizationandthetutor'srole.Additionally, Table1 StatisticaldescriptionofUVDstudentsparticipatingintheresearch. Age:yearsaverage33 Gender:MaleFemale316Socialstratum(mode)3  Semester(mode)5  Undergraduateprograms(studentsnumber) ManagementBusinessandFinance OccupationalHealthAdministration CommunicationStudies AccountingBachelorDegreeinNaturalSciences Psychology12 1 2 4 1 8Cohabitation:Nuclearfamily19 ResidenceBogotaOthers145Occupation:EmployeeUnemployedOther(homemaker) 1711Table2 Codebook:StudyaboutmotivationtolearnEnglishinvirtualenvironments. Code1 LabelMotivation  DenitionCoreguideaboutwhatdrivesaparticularbehaviourorthought: 1.Extrinsic:external,introjected,identi ed,integrated 2.Intrinsic:autonomousbehaviour. DescriptionPsychologicalprocessthatanalyseswhatdrivesindividuals'particular behaviourorthoughtinaccordancetotheirenvironment.Human  motivationvariesaccordingtotheindividuals'regulationcapacityof  theirownbehaviour. Code2  LabelCALL  DenitionOnlinelanguageLearningEnvironments DescriptionComputer-assistedLanguageLearningenvironmentsthatinvolvea widerangeofmultimediatoolsandactivitiesfosteringlearners'active participationandinteractioninnaturalcontexts,therefore,motivation. Code3  LabelLearning  DenitionForeignLanguageLearning DescriptionPhenomenonthatexplainsforeignlanguagelearningbasedona multidimensional,multifactorialandintegratingperspective  concerningsubjectsandtheirdifferentstrategies,competenciesand  mentaloperations. Fig.2. Wordscloud. F.G.EscobarFandi ~noetal. Heliyon5(2019)e02394 4 whenguringoutwhatlearningEnglishmeans,theintervieweesshowed differenttendencies.Next,the19respondents'descriptionandanalysisis exhibitedinagroupedmanner. 4.2.Personalfactors Inthepersonalfactorscoexistintrinsicandextrinsicmotivationthat arerelatedtoindividual'spsychologicalprocessandhis/hersocialspe- cicdevelopmentframework.Inthecaseofinterviewedsomefactorsare impliedandoverlappedinthemeaningassignedtolearnEnglishsuchas theneedtointeract,tobecompetent,toachieveautonomyandthe learningstrategiesused( Alietal.,2016 ;ButzandStupnisky,2017 ;RyanandDeci,2000 ;Linetal.,2017 ;Santanaetal.,2016 ;Shyan,2016 ;VanLoonetal.,2012 ).4.2.1.MeaningoflearningEnglish Accordingtotheinformationgatheredthroughtheinterviewsandin regardstowhatmotivatestheparticipantstolearnEnglish,thereare threetrends:(1)itmayprovidepotentialshortandlong-termbene ts,(2)itmayboostpersonaldevelopmentandautonomy,and(3)itmay supplysocial/contextualneeds.Thesetendenciesareusuallypointedout inresearchonforeignlanguagelearningprocesses( Santanaetal.,2016 ;Stern,1991 ).Regardingthepotentialbene tandtheneedtointeract,someofthe respondentsindicated, "Itisanopportunitytocommunicate,tobroaden myhorizons,tolearnanewculture,toseetheworldwithdifferenteyes andtosearchforbetterthingslikeabetterjob.Itisatoolforempow- erment.Steptowardabetterfutureformeandmyfamily "(takenfrom participants1,2and7).Thistypeofresponsesisconsistentwithan autonomousmotivation,aslongasthereisaneedtorelatetoothers,to becompetentandtoachievegoals( DeciandRyan,2008 ;RyanandDeci, 2000)."StudyingEnglishisagatewaytomanypossibilities.Icouldmakean exchangeandmeetmanypeopleortoaccessbetteropportunitiessince I'vebeenabletotravelseveraltimes,speci callytotheUSAandIhaven't feltfree,itisauniversallanguage,itisakeytoolforthefutureandit seemsimportanttome "(takenfromtheparticipants5,6,13,16and17). Theseanswersarealsorelatedtoanexternalregulation;however,the participantsseemtohaveinternalizedcontingencyrelationshipsandsee themalmostastheirown,consequencesarisingfromtheenvironment. Thus,itwouldbeaformofintrojectedregulation( Al-kahiry,2013 ;Baracketal.,2016 ;DeciandRyan,2008 ,2014).Inthenextsection,there isanothertypeofmotivation. "LearningEnglishbringsbene ts.Ithinkthatifyouwanttoexpand yourprofessionalpro le;youshouldreadmanybooks,buttheyarein English"(takenfromparticipants3and6).Inthisapart,althoughthe responsesarealsorelatedtoexternalregulation,theseallowindividuals toaccept,toacertainextent,theimportanceoftheactivity,whichis consistentwithanidenti edregulation( DeciandRyan,1985 ,2008,2014).Inthisexcerpt,theintervieweesexhibitedthelasttypeofmotivation. "IstudyEnglishbecauseIlikethatlanguage,becauseitisalanguage whichiswidelyspoken,Icanmanagemyselfinanycountrywiththis "(apartfromparticipant9).Inthisapart,althoughdistinctlythe discoursestillreferstoextrinsicmotivation,itcanbeseenthatthe intervieweeisdrivenbyhis/herneeds,whichwouldcorrespondtoan integratedregulation( DeciandRyan,2008 ,2014;Huangetal.,2019 ).Withreferencetorespondents'learningstrategies,itisespecially noteworthythatmostofthemarefullyawareofhavingaclearde nedstrategy.Inaddition,students ndtheactivitiestricky,eventhoughthey arenotclearabouthowtheirlearningisbeingconsolidated.Mostofthe answersrevealthattheintervieweesarenotactivelyinvolvedinthe exercises,buttheyaddressthetopicswithoutbearinginmindthepre- viousconcepts( Davies,2008 ;FarooqandJavid,2012 ;Kakotyetal., 2011;Levy,1997 ).Importantexamplescon rmingthisinthefollowing excerpt:"Idonotfeelveryskilled.Idonotknowwhyorhowtodothings. Idothem(workshops)becausetheyareontheplatform "(takenfrom participants1,2,4,7,8,10,11and19). Ontheotherhand,theinterviewees'studyroutinesarenotperma- nent.Themajoritysaidtheydidnothaveaspeci cdayoraspeci ctime tocoverthecoursedemands,buttheylettheminstand-bydependingon otheroverdueresponsibilities.Thefollowingsectionislinkedwiththe foregoing:"It'samatteroftime.After9:30atnightandIstartdoingmy collegehomework.IdonotworkonEnglisheveryweekbutevery15 daysandIdoitwhenpossible.IstudiedeverytwoorthreedaysbutnowI onlydoitonSaturdaysorSundaysformyjob "(takenfromparticipants1, 3,6,8,9,10,11,12,13and15). Withregardtothedevelopmentandtoachieveautonomysomeofthe participantsconsiderthat: "Itisanevolution;anotherchallengetofacein mylife.Itallowsdevelopingnewskills,speakinginanotherlanguage, usingthemostwidelyspokenlanguage "(takenfromparticipants13and 18).Thiskindofresponseisrelatedtoatendencytowardmotivationthat isthemainrequirementforpositiveresults( DeciandRyan,2008 ;Kazantsevaetal.,2016 ;RyanandDeci,2000 ).Someanswerswerefoundtobeassociatedwiththepossibilityof supplyingasocial/contextualneed: "Asecondlanguageisamust.Ifeel thatitisnecessaryformyprofessionallife.Currentlyitissomething necessarytohavebetterjobopportunities "(takenfromparticipants2,12 and16).Theseconsiderationspointoutthatthemeaningoflearning greatlyreliesonhowmuchaneedimposedbytheindividualsthemselves ortheirenvironmentcanbemet,thatis,extrinsicmotivation( Baracketal.,2016 ;DeciandRyan,2008 ;RyanandDeci,2000 ).Apparently,themeaningascribedtolearningEnglishasaforeign languagevariesinaccordancewiththeindividuals'appraisalsofthisskill  andthecontingencyrelationshipswiththeirenvironment.Itisnote- worthythatalthoughmostoftheinterviewees'responsesrevealan awarenessofthepotentiallong-termbene t,itisalsotruethatthesetend tobeconsistentwithanextrinsicmotivationsincebehaviourisdrivenby stimuliintheenvironmentandnotbyautonomousdecisions. Insummary,alltheparticipants'responsesseemtobeframedinone ofthelevelsofextrinsicmotivation( DeciandRyan,2008 ).Theresults indicatethat,althoughsomeintervieweestendtobemoreautonomous thanothers,someofthemdonotknowwhytheylearnEnglish,itcanbe observedthattheregulationoftheirbehaviourislargelyexhibitedin responsetotheenvironmentwhichrevealsthatthecontroloftheir behaviourreliesonexternalvariablestotheindividual( DeciandRyan, 2014;Pourfeiz,2016 ).4.3.Contextualfactors Contextualfactorssuchasonlineenvironment(curriculum,re- sources,andteacher)arealsooverlappedandintrinsicandextrinsic motivationcoexistintheinterviewedresponses,aswell.Inspiteof,each studentconsidersmanydifferentbene tsfromlearningEnglish;these areusuallyrelatedtoexternalreinforcements.Therefore,learningEn- glishasforeignlanguageinOnlineModalitycandependonexternal aspects( Al-kahiry,2013 ;Baracketal.,2016 ;Hani,2014 ;Hammouriand Abu-Shanab,2018 ;Sergisetal.,2018 ).4.3.1.Onlineenvironmentcharacterization Differentrolesfromonlineenvironmentalsoplayakeyroleonthe motivationtolearnEnglish.Forexample,therequirementsofthecur- riculum.Inthisregard,thestudentssaid: ﬁWhydoyoustudyEnglish? Basically,becausethesubjectisinthecurriculum,becauseitismanda- toryinthemajor,Ihavetostudyitforthemajoror,truthfully,Iwould notdoit,mainlybecauseitisinmymajor,forprofessionalreasons, becauseitisarequirementofthefaculty,orbecausethelabour elddemandsit "(takenfromparticipants1,2,4,7,8,12,14and19).The answersaforementionedimplytheyaredirectlyrelatedtoanexternal regulationthatcorrespondstothe rstlevelofextrinsicmotivation( DeciandRyan,1985 ,2008).Theseexcerptssuggestthatthegoalisseparated fromtheactivityitself;thatis,extrinsicmotivatorsdeterminethe F.G.EscobarFandi ~noetal. Heliyon5(2019)e02394 5 possiblepro ts( DeciandRyan,2008 ,2014).Nevertheless,inthenext sectionitcanbeseenanothertypeofmotivation. Asenseofself-controlandahighdegreeofinteractionarebasic conditionstoenhancestudents'learningbyCALL( Hani,2014 ;Salgadoetal.,2015 ;SeguraandGreener,2014 ).Moststudentsperceiveasagreat advantagethe exibilityandself-controlthattheinstitution'splatform offersthem;giventhattheycanworkatitsownpacefromthefeedback oftheexerciseandthelearningenvironmenthasagreatvarietyofre- sourcesandactivities,ascanbeinferredfromthefollowingsection: "Thenyoushouldnotthinkthatyoufailand...;itisgoodbecauseyoucan seewhatwentwrongandyoucancorrectandsinceyouareatadistance youdonotfeelsomuchpressure "(takenfromparticipants4and9). Moststudents ndactivitiesinterestingandinteractivesincethey requirethelearner'sactiveparticipationinnaturalcontexts( Huangetal., 2019)."Itgenerateslearningifpeoplearededicated.Ithinktheplatform isveryinteractiveandtheactivitiesareverydynamic.Beingassessed throughvideos,audiosanddrawingsmakestheactivitiesentertaining "(takenfromparticipants5,6,10,12,15and16). Ontheotherhand,akeytosuccesswhenlearningaforeignlanguage largelyreliesonanef cienttutor'saccompanimentandanappropriate selectionofstrategiesthatpowerlearning( MutluandEr €oz-Tuga,2013 ;SeguraandGreener,2014 ).Moststudentssaidtheyfeltthetutor's accompanimentthroughhisorherconstantmailsdelivery,timelyand clearfeedbackandothercommunicativestrategiessuchasvideocon- ferencesimplementationandanaccessibleattitudeandguidingrole providingapositivemotivation.However,althoughmoststudentsstated  theyfeltpermanentlyaccompaniedbythetutor,someofthemrevealeda feelingoflonelinessworkingvirtually.Thatiswhy;theyconsiderthat Englishteachingshouldbeface-to-faceatUNIMINUTObecauseinthe onlineenvironmentthereisnoonetotellthemwhatiswrongimmedi- atelywhentheyfailatperformingatask.Additionally,theythinkthata realconversationwiththetutorcouldhelpthemtoimprovethelistening skill.Althoughseveraloftheintervieweessuggestedtheface-to-faceclas- ses,theyalsostatedthattheydidnotgotothelaboratorytutorialsnordid theyattendvideoconferencesbecausetheydidnothavetimeforitor becauseitwasnotmandatory. "ThetutoringonSaturdays,wellit'sgood, butlet'sjustsay,youdonothavetimetogothere.TutoringonSaturdays aregood,butI'veneverbeenabletoattendbecauseofwork "(takenfrom participants2and16). Insummary,amongtheintervieweesthereisaconsensusregarding theCALL exibilityandself-controlsenseaswellasagoodaccompani- mentthatthecourseandthetutorprovidesthem.However,noteveryone ndstheactivitiesentertaining,asforsomeofthem,theexercisesare evenboringandextensive,andthiscouldpossiblybebecausepartici- pantsmayhavedifferentlearningstyles.Apartfromthis,somestudents keepasenseofisolationandparticularunful lledexpectationsremain. Learningstrategiesandstudyroutinesreportedbytheintervieweesobey tonon-regularor uctuatingbehaviours,asthereisnoautonomousstudy apartfromtheplatform.Thestudentsapproachthecoursedependingon otheractivities,whichindicatesEnglishlearninginavirtualwayisnot particularlyhighpriority.Mostofparticipantsarenotclearabouthow theEnglishcoursecanmodifytheircognitiveschemes,howitrelatesto theircareer,orhownewskillsaredeveloped( FarooqandJavid,2012 ).Apparently,theintervieweesarenotactivelyinvolvedintheEnglish coursesincealltheparticipantscommentedthattheydidnotattendany ofthevirtualorface-to-facetutoringduringthethirdacademicperiodof 2018.5.Conclusions ThemotivationtolearnEnglishasaforeignlanguage,usingvirtual methodologies(CALL)canbeacomplexprocess( Davies,2008 ;Deciand Ryan,2014 ;Levy,1997 ;Saville-Troike,2006 ).Forthe19interviewees, althoughitistruethatrespondents'motivationalprocessesdifferinmany respects,itisalsotruethatallofthemshowcharacteristicsofextrinsic motivationsincetheirbehaviourisdrivenbyexternalreinforcements thatarederivedfromthecontingencyrelationshipsestablishedwiththe context( DeciandRyan,2014 ;VermuntandDonche,2017 ).Strongevidenceabouthowexternalfactorsin uenceEnglish learninghasbeenfoundandforthe19respondentswouldbethose concernedwiththevirtualmethodology(CALL),thecourseresources andtherelationshipwiththeteacher/tutor,asrecognizedbymostpar- ticipants( Al-kahiry,2013 ;Baracketal.,2016 ).However,severalfactors arebeyondtheteacherandtheinstitutionsscope,suchasthein- terviewee'ssocio-economicstability,otherresponsibilitiesalearnerhas todealwithorfeelcomfortablewithcourseactivitiescanaffectthe learner'smotivation( Kazantsevaetal.,2016 ;Huangetal.,2019 ).UsingCALLseemstocontributesigni cantlytoEnglishlearning; however,successgreatlydependsonthetypeofmotivationeachstudent has;onthequalityofthetutor'sassistanceandhoweffectiveand entertainingtheteachingstrategiesare( MutluandEr €oz-Tuga,2013 ;Firatetal.,2018 ).However,therearesomevariablesthatcanbeopti- mized,suchasbroadeningthetypeofactivities,sothattheycould challengeallthelearningstyles( SeguraandGreener,2014 ).6.Futurestudies Furtherresearchneedstobedonetoidentifylearningdif cultiesrelatedtoUVDstudents'motivationalprocessesandgiventhatUNIM-  INUTOispresentinseveralregionsofColombia,eachoneofthemwith itsparticularities.Itisfundamentaltoassumeacriticalpositionconsid- eringlocalperspectivesasconcomitantfactors.Likewise,itisnecessary toinvestigatehowtoimprovethetransactionaldistancetoensurethat thestudentfeelsaccompaniedbyateacherandtutor'sin uenceonline learningprocess. Ontheotherhand,thisstudypretendstobeconsideredaknowledge baseforfutureresearchaboutmotivationasapsychologicalprocessthat isinvolvedinlearningEnglishasaforeignlanguagethroughtheuseof onlinemodality.Inaddition,thisstudyexpectstocontributeandfollow uponfutureprojectsofqualitativeresearchmethodologytogetGroun- dedTheory,thus,itletstoimproveonlineeducation,aslongas,psy- chologyscience. Declarations Authorcontributionstatement F.G.Escobar-Fandi ~no:Performedtheexperiments.Analyzedand interpretedthedata.Wrotethepaper. L.D.Mu ~noz:Analyzedandinterpretedthedata.Wrotethepaper. A.J.Silva-Velandia:Performedtheexperiments.Wrotethepaper. Fundingstatement Thisresearchdidnotreceiveanyspeci cgrantfromfundingagencies inthepublic,commercial,ornot-for-pro tsectors. Competingintereststatement Theauthorsdeclarenocon ictofinterest. Additionalinformation Noadditionalinformationisavailableforthispaper. ReferencesAli,A.K.,Jafarizadegan,N.,Karampoor,F.,2016.Relationbetweensocioeconomicstatus andmotivationoflearnersinlearningEnglishasaforeignlanguage.TheoryPract. Lang.Stud.6(4),742 Œ750.Al-kahiry,M.H.,2013.Englishasaforeignlanguagelearning:demotivationalfactorsas perceivedbySaudiundergraduates.Eur.Sci.J.9(32),365 Œ382.F.G.EscobarFandi ~noetal. Heliyon5(2019)e02394 6 Barack,M.,Watted,A.,Haick,H.,2016.Motivationtolearninmassiveopenonline courses:examiningaspectsoflanguageandsocialengagement.Comput.Educ.94, 49Œ60.BaniHani,N.,2014.Bene tsandbarriersofcomputerassistedlanguagelearningand teachingintheArabWorld's:Jordanasamodel.TheoryPract.Lang.Stud.4(8), 1609Œ1615.Bhowmik,S.K.,2015.WorldEnglishesandEnglishlanguageteaching:apragmaticand humanisticapproach.Colomb.Appl.Ling.J.17(1),142 Œ157.Burkle,M.,2011.Elaprendizajeonline:oportunidadesyretoseninstituciones academicas.RevistaCientí cadeEducomunicaci on.Comunicar37(XIX),45 Œ53.Busse,V.,Walter,C.,2013.Foreignlanguagemotivationinhighereducation:a longitudinalstudyofmotivationalchangesandtheircauses.Mod.Lang.J.97(2), 435Œ456.Butz,N.T.,Stupnisky,R.H.,2017.Improvingstudentrelatednessthroughanonline discussionintervention:theapplicationofself-determinationtheoryinsynchronous hybridprograms.Comput.Educ.114,117 Œ138.Caceres,P.,2003.An alisiscualitativodecontenido:Unaalternativametodol ogicaalcanzable.Psicoperspectivas2(1),53 Œ82.Canning,J.,2004.DisabilityandResidenceAbroad.RetrievedonMay12,2017from. CentreforLanguages,LinguisticsandAreaStudiesGoodPracticeGuide. https://www.llas.ac.uk/resources/gpg/61 .Chapelle,C.,Jamieson,J.,2008.TipsforTeachingwithCALL:PracticalApproachesto Computer-AssistedLanguageLearning.PearsonLongman,NewYork .Contreras,L.,Gonz alez,K.,Fuentes,H.,2011.UsodelasTICyespecialmentedelBlended Learningenlaense ~nanzauniversitaria.RevistaEducaci onyDesarrolloSocial1, 151Œ160.Davies,G.,2008.CALL(ComputerAssistedLanguageLearning).RetrievedonJune16, 2017from.CentreforLanguages,LinguisticsandAreaStudiesGoodPracticeGuide. https://www.llas.ac.uk/resources/gpg/61 .Deci,E.L.,Ryan,R.M.,1985.IntrinsicMotivationandSelf-DeterminationinHuman Behavior.Plenum,NewYork .Deci,E.L.,Ryan,R.M.,2008.Facilitatingoptimalmotivationandpsychologicalwell- beingacrosslife ™sdomains.Can.Psychol./Psychologiecanadienne49(1),14 Œ23.Deci,E.L.,Ryan,R.M.,2014.Motivation,personality,anddevelopmentwithinembedded socialcontexts:anoverviewofself-determinationtheory.In:Ryan,R.(Ed.),The  OxfordHandbookofHumanMotivation(85 Œ107).OxfordUniversityPress,New York.Díaz-Bravo,L.,Torruco-García,U.,Martínez-Hern andez,M.,Varela-Ruíz,M.,2013.La entrevista,recursodin amicoy exible.Investigaci onenEducaci onM edica2(7), 162Œ167.Farooq,M.U.,Javid,C.,2012.NUM:attitudeofstudentstowardsE-learning:astudyof EnglishlearneratTaifuniversityEnglishlanguagecentre.J.Crit.Inq.10(2),17 Œ31.Fernandez,L.,2006.C omoanalizardatoscualitativos?ButilietíLaRecerca7,1 Œ13.Firat,M.,Kilinc,H.,Yuzer,T.,2018.Levelintrinsicmotivationofdistanceeducation studentsine-learningenvironments.J.Comput.Assist.Learn.34(1),63 Œ70.Flick,U.,2004.Introducci onalainvestigaci oncualitativa.EdicionesMorata,Madrid .Galishnikova,E.M.,2014.Languagelearningmotivation:alookattheadditional program.Procedia-Soc.Behav.Sci.152,1137 Œ1142.Gonzalez,D.,2008.Psicologíadelamotivaci on.EditorialCienciasM edicas,LaHabana .Hammouri,Q.,Abu-Shanab,E.,2018.Exploringfactorsaffectingusers'satisfaction towardE-learningsystems.Int.J.Inf.Commun.Technol.Educ.14(1),44 Œ57.Hani,N.A.,2014.Bene tsandbarriersofcomputerassistedlanguagelearningand teachingintheArabworld:Jordanasamodel.TheoryPract.Lang.Stud.4(8), 1609Œ1615.Huang,Y.,Backman,S.,Backman,K.,MacGuire,F.,Moore,D.,2019.Aninvestigationof motivationandexperienceinvirtuallearningenvironments:aself-determination theory.Educ.Inf.Technol.24,591 Œ611.InstitutoColombianoparalaEvaluaci ondelaEducaci onIcfes,2016.InformeNacional deResultadosSaberPro2012-2015.Icfes,Bogot a.James,M.A.,2012.Aninvestigationofmotivationtotransfersecondlanguagelearning. Mod.Lang.J.96(1),51 Œ69.Kakoty,S.,Lal,M.,Sarma,S.K.,2011.E-Learningasaresearcharea:ananalytical approach.Int.J.Adv.Comput.Sci.Appl.2(9),144 Œ148.Kazantseva,E.A.,Valiakhmetova,E.K.,Minisheva,L.V.,Anokhina,S.Z.,Latypova,E.M., 2016.Asurveybasedstudyofmotivationandattitudetolearningasecondlanguage atUFAStateUniversityofEconomicsandService.Glob.MediaJ.1 Œ9.Supl.Special IssueS2:YouthintheGlobalInformation .Kessler,G.,2018.Technologyandthefutureoflanguageteaching.ForeignLang.Ann.51, 205Œ218.Kitcharkarn,O.,2015.EFLlearners ™attitudestowardsusingcomputersasalearningtool inlanguagelearning.TurkishOnlineJ.Educ.Technol.14(2),52 Œ58.Krashen,S.D.,1989.PrinciplesandPracticeinSecondLanguageAcquisition.Prentice- HallInternational,EnglewoodCliffs,N.J. Lee,L.,2016.Autonomouslearningthroughtask-basedinstructioninfullyonline languagecourses.Lang.Learn.Technol.20(2),81 Œ97.Levy,M.,1997.CALL:ContextandConceptualisation.OxfordUniversityPress,Oxford .Lin,M.Y.,2008.Cambiosdeparadigmaenlaense ~nanzadelingl escomolengua extranjera:elcambiocríticoym asall a.RevistaEducaci onyPedagogía20(51), 11Œ23.Lin,C.H.,Zhang,Y.,Zheng,B.,2017.Therolesoflearningstrategiesandmotivationin onlinelanguagelearning:astructuralequationmodellinganalysis.Comput.Educ. 113,75 Œ85.Liu,J.,2013.E-learninginEnglishClassroom:InvestigatingFactorsImpactingonESL (EnglishasSecondLanguage)CollegeStudents'AcceptanceandUseoftheModular Object-OrientedDynamicLearningEnvironment(Moodle).GraduateThesesand Dissertations.Paper13256.Retrievedfrom. http://lib.dr.iastate.edu/cgi/viewconte nt.cgi?article¼4263&context¼etd.Liu,Y.,2014.Motivationandattitude:twoimportantnon-intelligencefactorstoArouse students™potentialitiesinlearningEnglish.Creativ.Educ.5,1249 Œ1253.Majid,F.,etal.,2012.Astudyontheon-linelanguagelearninganxietyamongadult learner.Int.J.e-Educ.,e-Bus.,e-Manag.ande-Learn.2(3),187 Œ192.Mantiri,O.,2015.KeytoLanguagelearningsuccess.J.ArtsHumanit.4,14 Œ18.Marshall,R.,2010.Motivaci onyEmoci on,5taed.McGrawHill,M exico.MinisteriodeEducaci onNacional,2004.ProgramaNacionaldeBilingüismo.Colombia 2004-2019.Ingl escomolenguaextranjera:Unaestrategiaparalacompetitividad. MEN,Bogot a.Motteram,G.,2013.InnovationsinLearningTechnologiesforEnglishLanguage Teaching.BritishCouncil,LondonUK .Mutlu,A.,Er €oz-Tuga,B.,2013.TheroleofComputerAssistedLanguageLearning(CALL) inpromotinglearnerautonomy.Eur.J.Educ.Res.51,107 Œ122.Nu~nez,J.,Le on,J.,2015.Autonomysupportintheclassroom.AreviewfromSelf- DeterminationTheory.Eur.Psychol.20(4),275 Œ283.Papaefthymiou-Lytra,S.,2014.L2lifelonglearninguseandnewmediapedagogies.Res. PaperLang.Teach.Learn.5(1),16 Œ33.Pourfeiz,J.,2016.Studyofrelationshipbetweenattitudestowardforeignlanguage learningandacademicmotivation.Procedia-Soc.Behav.Sci.232,668 Œ676.Quecedo,R.,Casta ~no,C.,2002.Introducci onalametodologíadeinvestigaci oncualitativa.RevistadePsicodid actica14,5 Œ39.Richards,J.,2014.ErrorAnalysis.PerceptionsonSecondLanguageAcquisition. Routledge,NewYork .Rodríguez,C.,Lorenzo,O.,Herrera,L.,2005.Teoríaypr acticadelan alisisdedatos cualitativos.Procesogeneralycriteriosdecalidad.RevistaInternacionaldeCiencias  SocialesyHumanidadesXV(2),133 Œ154.SOCIOTAM .Rosario,J.,2006.TIC:Suusocomoherramientaparaelfortalecimientoyeldesarrollode laeducaci onvirtual.Did actica,innovaci onymultimedia.Recuperadode. http://www.cibersociedad.net/archivo/articulo.php?art ¼221.Ryan,R.,Deci,E.,2000.Lateoríadelaautodeterminaci onylafacilitaci ondela motivacionintrínseca,eldesarrollosocial,yelbienestar.Am.Psychol.Assoc.55(1), 68Œ78.Salgado,N.,S anchez,H.,Rico,M.,2015.Evaluaci ondelametodologíaypr acticaseducativasmedianteelusodeplataformasvirtualesparaelaprendizajedelingl es.AnalisisdecasodelinstitutodeidiomasdelaUniversidadTecnol ogicaEquinoccial delaciudaddeQuito.RevistaTecnol ogicaESPOL ŒRTE28(5),8 Œ26.Sandoval,C.,2002.Investigaci onCualitativa.Icfes,Bogot a.Santana,J.,García,A.,Escalera,K.,2016.Variablesquein uyensobreelaprendizajedel inglescomosegundalengua.RevistaInternacionaldeLenguasExtranjeras5,79 Œ94.Segura,B.,Greener,S.,2014.TechnologyonCALL:improvingEnglishlanguagelearning inaSpanishcontext.In:Orngreen,Levinsen(Eds.),13thEuropeanConferenceone- Learning(464-469).CurranAssociates,Inc,Copenhagen,Denmark .Saville-Troike,M.,2006.IntroducingSecondLanguageAcquisition.Cambridge UniversityPress,NewYork .Sergis,S.,Sampson,D.,Pelliccione,L.,2018.InvestigatingtheimpactofFlipped Classroomonstudents'learningexperiences:aSelf-DeterminationTheoryapproach. Comput.Hum.Behav.78,368 Œ378.Shroff,R.H.,Vogel,D.R.,Coombes,J.,Lee,F.,2008.StudentE-learningintrinsic motivation:aqualitativeanalysis.Commun.Assoc.Inf.Syst.19,241 Œ260.Shyan,K.,2016.Exploringdifferencesinmotivationbetweenstudentswhoexcelledand underperformedinlearningtheEnglishlanguage.Eng.Teach45(1),1 Œ20.Stern,H.H.,1991.FundamentalConceptsofLanguageTeaching.OxfordUniversityPress, HongKong .Støen,B.,Haugan,G.,2016.Theacademicmotivationscale:dimensionality,reliability, andconstructvalidityamongvocationalstudents.NordicJ.Vocat.Educ.Train. NJVET6(2),17 Œ45.Tavangarian,D.,Leypold,M.E.,N €olting,K.,R €oser,M.,Voigt,D.,2004.Ise-Learningthe solutionforindividuallearning?Electron.J.eLearn.2(2),273 Œ280.Thomas,M.,Motteram,G.,2019.Editorialforthespecialeditioncommemoratingthe workofStephenBax.System83,1 Œ3.Vallerand,R.J.,Pelletier,L.G.,Blais,M.R.,Bri ere,N.M.,Sen ecal,C.,Valli eres,E.F.,1993. Ontheassessmentofintrinsic,extrinsic,andamotivationineducation:evidenceon theconcurrentandconstructvalidityoftheAcademicMotivationScale.Educ. Psychol.Meas.53(1),159 Œ172.VanLoon,A.,Ros,A.,Martens,R.,2012.Motivatedlearningwithdigitallearningtasks: whataboutautonomyandstructure?Educ.Technol.Res.Dev.60(6),1015 Œ1032.Vermunt,J.D.,Donche,V.,2017.Alearningpatternsperspectiveonstudentlearningin highereducation:stateoftheartandmovingforward.Educ.Psychol.Rev.29,269 Œ299.Wang,S.,V asquez,C.,2012.Web2.0andsecondlanguagelearning:whatdoesthe researchtellus?CalicoJ.29(3),412 Œ430.Woon,L.,Wang,J.,Ryan,R.(Eds.),2016.BuildingAutonomousLearners.Perspectives fromResearchandPracticeUsingSelf-DeterminationTheory.Springer,NewYork .Zarei,A.,Hashemipour,M.,2015.Theeffectofcomputer-AssistedLanguageinstruction onimprovingEFLlearners ™autonomyandmotivation.J.Appl.Linguist.1(1),40 Œ58.Zenots,V.,2012.Motivaci onenelaprendizajedelaslenguas.FilologíayDid acticadela Lengua12,75 Œ81.F.G.EscobarFandi ~noetal. Heliyon5(2019)e02394 7
Applied linguistics	Foreign languages	Language acquisition	Branches of linguistics
Discuss and elaborate on the following statement: "The brain is divided into two hemispheres"	The brain is divided into two hemispheres formally known as cerebral hemispheres: the left hemisphere and the right hemisphere. The left hemisphere is responsible for processing information in a focussed and analytical manner. The right hemisphere processes information in a broader and more insightful manner. The hemispheres do no operate independently from one another. Each hemisphere controls the opposing side of the body (EG: left hemisphere processes sensory information and controls motor functions of the right side of the body and visa versa).	What is the biggest difference between the two cerebral hemispheres?	Language	Processing speed|||Number of neurons|||Sound interpretation	Corpus Callosum clip||youtube||N/A
Psycholinguistics	Neurolinguistics	Neuroanatomy	Cerebral hemispheres	Applied linguistics
What are the four processes involved in speech production?	Respiration is the airflow from the lungs. Phonation is the production of sound. Resonation is the intensification and prolongation of sound. Articulation is the formation of distinct sounds in speech.	Which of the following is incorrect about the systems involved in speech production?	Preceeding phonation, airflow is resonated in the vocal cavities shaped by the jaw, soft palate, lips, and tongue.	Airflow from the lungs is the primary cause of movement of the vocal folds during phonation.|||During articulation, articulators in the face adjust the shape of the mouth, pharynx, and nasal cavaities to produce distince sounds.|||During inspiration of air, the diaphragm contracts and the lungs expand drawn by pleurae through surface tension and negative pressure.	Mechanisms of Speech Clip||youtube||N/A
Key terms in linguistics
What assumptions are made by a Naive Bayes classifier?	Two key assumptions are made by Naive Bayes:  1. Bag of words assumption––position doesn't matter. That is, seeing the word "lamp" at the beginning of a document or end of the document has no effect on the ultimate prediction. 2. Conditional independence––words are conditionally independent on each other. That is, the probabilities for "thank" and "you" are considered independently, even though there may be a strong correlation between the two words.	True or false: Naive Bayes classifiers consider that since documents that include the word "medieval" often also include the word "king", a higher probability should be used.	False	True	Naive Bayes presentation||slides||>?'$*.-8*('%0( 12"3(4")%5./"7"./".,"% K))?87$*-.) P(x1,x2,É,xn|c)¥3(A%-+%:-</)%())?87$*-. 5$F&&87#$G(&.+.(0$D(#&0n+$ 7%++#9 ¥&-./*$*-.('%5./"7"./".," 5$F&&87#$+"#$)#%+89#$ G9(L%L.-.+.#&$ 5i67m)8j$%9#$.0D#G#0D#0+$N.:#0$+"#$/-%&&$ )2P(x1,É,xn|c)=P(x1|c)¥P(x2|c)¥P(x3|c)¥...¥P(xn|c)|||Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||4 C HAPTER 4  N AIVE B AYESAND S ENTIMENT C LASSIFICATION WecanthensubstituteEq. 4.2 intoEq. 4.1 togetEq. 4.3 : ‹ c = argmax c 2 C P ( c j d )= argmax c 2 C P ( d j c ) P ( c ) P ( d ) (4.3) WecanconvenientlysimplifyEq. 4.3 bydroppingthedenominator P ( d ) .This ispossiblebecausewewillbecomputing P ( d j c ) P ( c ) P ( d ) foreachpossibleclass.But P ( d ) doesn'tchangeforeachclass;wearealwaysaskingaboutthemostlikelyclassfor thesamedocument d ,whichmusthavethesameprobability P ( d ) .Thus,wecan choosetheclassthatmaximizesthissimplerformula: ‹ c = argmax c 2 C P ( c j d )= argmax c 2 C P ( d j c ) P ( c ) (4.4) Wethuscomputethemostprobableclass‹ c givensomedocument d bychoosing theclasswhichhasthehighestproductoftwoprobabilities:the priorprobability prior probability oftheclass P ( c ) andthe likelihood ofthedocument P ( d j c ) : likelihood ‹ c = argmax c 2 C likelihood z }| { P ( d j c ) prior z }| { P ( c ) (4.5) Withoutlossofgeneralization,wecanrepresentadocument d asasetoffeatures f 1 ; f 2 ;:::; f n : ‹ c = argmax c 2 C likelihood z }| { P ( f 1 ; f 2 ;::::; f n j c ) prior z }| { P ( c ) (4.6) Unfortunately,Eq. 4.6 isstilltoohardtocomputedirectly:withoutsomesim- plifyingassumptions,estimatingtheprobabilityofeverypossiblecombinationof features(forexample,everypossiblesetofwordsandpositions)wouldrequirehuge numbersofparametersandimpossiblylargetrainingsets.NaiveBayes thereforemaketwosimplifyingassumptions. Theisthe bagofwords assumptiondiscussedintuitivelyabove:weassume positiondoesn'tmatter,andthatthewordﬁloveﬂhasthesameeffecton whetheritoccursasthe1st,20th,orlastwordinthedocument.Thusweassume thatthefeatures f 1 ; f 2 ;:::; f n onlyencodewordidentityandnotposition. Thesecondiscommonlycalledthe naiveBayesassumption :thisisthecondi- naiveBayes assumption tionalindependenceassumptionthattheprobabilities P ( f i j c ) areindependentgiven theclass c andhencecanbe`naively'multipliedasfollows: P ( f 1 ; f 2 ;::::; f n j c )= P ( f 1 j c )  P ( f 2 j c )  :::  P ( f n j c ) (4.7) TheequationfortheclasschosenbyanaiveBayesisthus: c NB = argmax c 2 C P ( c ) Y f 2 F P ( f j c ) (4.8) ToapplythenaiveBayestotext,weneedtoconsiderwordpositions,by simplywalkinganindexthrougheverywordpositioninthedocument: positions   allwordpositionsintestdocument c NB = argmax c 2 C P ( c ) Y i 2 positions P ( w i j c ) (4.9)
Naive Bayes	Bag-of-words	Conditional independence	Natural Language Processing (NLP)
What are illocutionary forces?	Illocutionary forces are part of J. L. Austin and John R. Searle’s Speech-Act Theory. They describe what linguistic utterances are intended to do, to accomplish, as actions in the world, instead of what content they express. For example, the illocutionary force of an apology is expressive (it expresses an attitude)  a command, directive (it directs someone to do something) and a promise, commissive (it commits the speaker to something).	Which of the following is NOT TRUE?	The illocutionary force is determined purely by the form of an utterance.	Different linguists believe in different illocutionary forces.|||The illocutionary force of a question is directive.|||The locutionary force of an utterance is its meaning.	Speech Act Theory Article||image||N/A
Speech-Act Theory	John R. Searle	J.L. Austin	Illocutionary force	Locutionary force	Pragmatics
According to the idea of semantic decomposition, what do the following verbs have in common? : kill, amuse, and pacify	According to the theory of semantic decomposition, all these verbs have the same general conceptual structure: Cause (X, Become (Y, Z)) – where, for ‘kill’, Z = Not(Alive), for ‘amuse,’ Z = Amused, and for ‘pacify,’ Z = Calm (or Peaceful). This practice could be valuable for natural language understanding, enabling an inference system to reason about the most important aspects of the events described in more detail, without needing tons of verb-specific knowledge, by having knowledge about the primitive concepts (e.g. Cause, Become, Not, etc.)	Follow-up question: According to a theory of semantic decomposition, which of the following verbs would not have the general conceptual structure: Cause (X, Go (Y, to(Z)))? (a) send (a message) (b) drive (a car) * (c) tell (a story)	ride (a train)	send (a message)|||tell (a story)|||expel (a student)	Chapter Section on Semantic Decomposition||publication||20.8  P RIMITIVE D ECOMPOSITIONOF P REDICATES 17 tocreate banana-door ).Thetaskofthesystemistoidentifywhichofthetwowords istheoriginalword.Toevaluateaselectionalpreferencemodel(forexampleonthe relationshipbetweenaverbandadirectobject)wetakeatestcorpusandselectall verbtokens.Foreachverbtoken(say drive )weselectthedirectobject(e.g., car ), concatenatedwithaconfounderwordthatisits nearestneighbor ,thenounwiththe frequencyclosesttotheoriginal(say house ),tomake car/house ).Wethenusethe selectionalpreferencemodeltochoosewhichof car and house aremorepreferred objectsof drive ,andcomputehowoftenthemodelchoosesthecorrectoriginalob- ject(e.g., car ) (ChambersandJurafsky,2010) . Anotherevaluationmetricistogethumanpreferencesforatestsetofverb- argumentpairs,andhavethemratetheirdegreeofplausibility.Thisisusuallydone byusingmagnitudeestimation,atechniquefrompsychophysics,inwhichsubjects ratetheplausibilityofanargumentproportionaltoamodulusitem.Aselectional preferencemodelcanthenbeevaluatedbyitscorrelationwiththehumanprefer- ences (KellerandLapata,2003) . 20.8PrimitiveDecompositionofPredicates Onewayofthinkingaboutthesemanticroleswehavediscussedthroughthechapter isthattheyhelpustherolesthatargumentsplayinadecompositionalway, basedonlistsofthematicroles(agent,patient,instrument,proto-agent,proto- patient,etc.).Thisideaofdecomposingmeaningintosetsofprimitivesemantics elementsorfeatures,called primitivedecomposition or componentialanalysis , componential analysis hasbeentakenevenfurther,andfocusedparticularlyonpredicates. Considertheseexamplesoftheverb kill : (20.41) Jimkilledhisphilodendron. (20.42) Jimdidsomethingtocausehisphilodendrontobecomenotalive. Thereisatruth-conditional(`propositionalsemantics')perspectivefromwhichthese twosentenceshavethesamemeaning.Assumingthisequivalence,wecouldrepre- sentthemeaningof kill as: (20.43) KILL (x,y) , CAUSE (x, BECOME ( NOT ( ALIVE (y)))) thususingsemanticprimitiveslike do , cause , becomenot ,and alive . Indeed,onesuchsetofpotentialsemanticprimitiveshasbeenusedtoaccountfor someoftheverbalalternationsdiscussedinSection 20.2 ( Lakoff1965 , Dowty1979 ). Considerthefollowingexamples. (20.44) Johnopenedthedoor. ) CAUSE (John, BECOME ( OPEN (door))) (20.45) Thedooropened. ) BECOME ( OPEN (door)) (20.46) Thedoorisopen. ) OPEN (door) Thedecompositionalapproachassertsthatasinglestate-likepredicateassoci- atedwith open underliesalloftheseexamples.Thedifferencesamongthemeanings oftheseexamplesarisesfromthecombinationofthissinglepredicatewiththeprim- itives CAUSE and BECOME . Whilethisapproachtoprimitivedecompositioncanexplainthesimilaritybe- tweenstatesandactionsorcausativeandnon-causativepredicates,itstillrelieson havingalargenumberofpredicateslike open .Moreradicalapproacheschooseto  18 C HAPTER 20  S EMANTIC R OLE L ABELING breakdownthesepredicatesaswell.Onesuchapproachtoverbalpredicatede- compositionthatplayedaroleinearlynaturallanguageunderstandingsystemsis conceptualdependency ( CD ),asetoftenprimitivepredicates,showninFig. 20.8 . conceptual dependency Primitive A TRANS Theabstracttransferofpossessionorcontrolfromoneentityto another P TRANS Thephysicaltransferofanobjectfromonelocationtoanother M TRANS Thetransferofmentalconceptsbetweenentitiesorwithinan entity M BUILD Thecreationofnewinformationwithinanentity P ROPEL Theapplicationofphysicalforcetomoveanobject M OVE Theintegralmovementofabodypartbyananimal I NGEST Thetakinginofasubstancebyananimal E XPEL Theexpulsionofsomethingfromananimal S PEAK Theactionofproducingasound A TTEND Theactionoffocusingasenseorgan Figure20.8 Asetofconceptualdependencyprimitives. Belowisanexamplesentencealongwithits CD representation.Theverb brought istranslatedintothetwoprimitives ATRANS and PTRANS toindicatethatthewaiter bothphysicallyconveyedthechecktoMaryandpassedcontrolofittoher.Note that CD alsoassociatesaedsetofthematicroleswitheachprimitivetorepresent thevariousparticipantsintheaction. (20.47) ThewaiterbroughtMarythecheck. 9 x ; yAtrans ( x ) ^ Actor ( x ; Waiter ) ^ Object ( x ; Check ) ^ To ( x ; Mary ) ^ Ptrans ( y ) ^ Actor ( y ; Waiter ) ^ Object ( y ; Check ) ^ To ( y ; Mary ) 20.9Summary  Semanticroles areabstractmodelsoftheroleanargumentplaysintheevent describedbythepredicate.  Thematicroles areamodelofsemanticrolesbasedonasinglelistof roles.Othersemanticrolemodelsincludeper-verbsemanticrolelistsand proto-agent / proto-patient ,bothofwhichareimplementedin PropBank , andper-framerolelists,implementedin FrameNet .  Semanticrolelabeling isthetaskofassigningsemanticrolelabelstothe constituentsofasentence.Thetaskisgenerallytreatedasasupervisedma- chinelearningtask,withmodelstrainedonPropBankorFrameNet.Algo- rithmsgenerallystartbyparsingasentenceandthenautomaticallytageach parsetreenodewithasemanticrole.Neuralmodelsmapstraightfromwords end-to-end.  Semantic selectionalrestrictions allowwords(particularlypredicates)topost constraintsonthesemanticpropertiesoftheirargumentwords. Selectional
Semantic decomposition	Semantic primitives
What are stopwords and how are they used in NLP?	Stopwords is a term in natural language processing, not traditional linguistics, but it refers more-or-less to what linguists call closed-class words or function words, including pronouns, determiners, numbers, and conjunctions. As opposed to open-class, or content words, which include nouns, verbs, adjectives, and adverbs.  The stopwords are singled out because they are considered noise for some NLP tasks, such as topic modeling, where perhaps only content words contribute useful information. More generally, a custom list of the most frequent stopwords in any data might be filtered out. However people may also choose to keep stopwords in, believing that they may carry some useful information. There is an extensive list of them in nltk.corpus that you can import and use.	Which of the following tasks would definitely not benefit from filtering out stopwords?	Machine learning rules of syntax 	 Sentiment analysis|||Topic modeling |||Word embeddings	NONE
Stopwords	Data preparation	Word classes	Natural Language Processing (NLP) algorithms
Describe the differences between deep roles, thematic roles, and proto-roles, and the main problems and advantages of each.	Deep semantic roles, such as “Breaker” and “EatenThing,”  are specific to relatively narrow event-types, potentially to single verbs. They are therefore potentially most accurate, but they do not reveal the generalizations that relate these roles to grammar—like the fact that subjects are typically causing, moving, or experiencing, or that the objects are typically being moved or altered.  These generalizations are captured by “thematic roles,” such as Agent, Patient, Theme, and others, but generally only a dozen or two, unlike the 100s of deep roles. They effectively capture the semantic sameness of sentences with syntactic “alternations” such as passive versus active voice and “diathesis” where the same set or thematic roles can correspond to two different syntactic forms—such as in, “She gave him a book / She gave a book to him.” However, the thematic roles are often difficult to assign using necessary and sufficient conditions; there seems to be nothing that is always true of an Agent or a Patient, only prototypical characteristics, such as (for Agents), volition and animacy. So, proto-roles were invented, such as proto-Agent and proto-Patient, which indicate that an argument is more Agent-like or more Patient-like.	Which of the following is not, and could not be, a thematic role? 	Feeling	 Instrument|||Location|||Beneficiary	Chapter section on Semantic Roles||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 20 SemanticRoleLabeling Sometimebetweenthe7thand4thcenturiesBCE,theIndiangrammarianP ¯ an . ini 1 wroteafamoustreatiseonSanskritgrammar,theAs . t . ¯ adhy ¯ ay ¯ (`8books'),atreatise thathasbeencalledﬁoneofthegreatestmonumentsof humanintelligenceﬂ 1933,11) .Thework describesthelinguisticsoftheSanskritlanguageinthe formof3959sutras,eachveryef(sinceithadto bememorized!)expressingpartofaformalrulesystem thatbrilliantlymodernmechanismsofformal languagetheory (PennandKiparsky,2012) .Onesetof rules,relevanttoourdiscussioninthischapter,describes the k ¯ arakas ,semanticrelationshipsbetweenaverband nounarguments,roleslike agent , instrument ,or destina- tion .P ¯ an . ini'sworkwastheearliestweknowofthattried tounderstandthelinguisticrealizationofeventsandtheirparticipants.Thistask ofunderstandinghowparticipantsrelatetoeventsŠbeingabletoanswertheques- tionﬁWhodidwhattowhomﬂ(andperhapsalsoﬁwhenandwhereﬂ)Šisacentral questionofnaturallanguageunderstanding. Let'smoveforward2.5millenniatothepresentandconsidertheverymundane goalofunderstandingtextaboutapurchaseofstockbyXYZCorporation.This purchasingeventanditsparticipantscanbedescribedbyawidevarietyofsurface forms.Theeventcanbedescribedbyaverb( sold,bought )oranoun( purchase ), andXYZCorpcanbethesyntacticsubject(of bought ),theindirectobject(of sold ), orinagenitiveornouncompoundrelation(withthenoun purchase )despitehaving notionallythesameroleinallofthem:  XYZcorporationboughtthestock.  TheysoldthestocktoXYZcorporation.  ThestockwasboughtbyXYZcorporation.  ThepurchaseofthestockbyXYZcorporation...  ThestockpurchasebyXYZcorporation... Inthischapterweintroducealevelofrepresentationthatcapturesthecommon- alitybetweenthesesentences:therewasapurchaseevent,theparticipantswere XYZCorpandsomestock,andXYZCorpwasthebuyer.Theseshallowsemantic representations, semanticroles ,expresstherolethatargumentsofapredicatetake intheevent,indatabaseslikePropBankandFrameNet.We'llintroduce semanticrolelabeling ,thetaskofassigningrolestospansinsentences,and selec- tionalrestrictions ,thepreferencesthatpredicatesexpressabouttheirarguments, suchasthefactthatthethemeof eat isgenerallysomethingedible. 1 FigureshowsabirchbarkmanuscriptfromKashmiroftheRupavatra,agrammaticaltextbookbased ontheSanskritgrammarofPanini.ImagefromtheWellcomeCollection.  2 C HAPTER 20  S EMANTIC R OLE L ABELING 20.1SemanticRoles ConsiderhowinChapter16werepresentedthemeaningofargumentsforsentences likethese: (20.1) Sashabrokethewindow. (20.2) Patopenedthedoor. Aneo-Davidsonianeventrepresentationofthesetwosentenceswouldbe 9 e ; x ; yBreaking ( e ) ^ Breaker ( e ; Sasha ) ^ BrokenThing ( e ; y ) ^ Window ( y ) 9 e ; x ; yOpening ( e ) ^ Opener ( e ; Pat ) ^ OpenedThing ( e ; y ) ^ Door ( y ) Inthisrepresentation,therolesofthesubjectsoftheverbs break and open are Breaker and Opener respectively.These deeproles aretoeachevent; Break- deeproles ing eventshave Breakers , Opening eventshave Openers ,andsoon. Ifwearegoingtobeabletoanswerquestions,performinferences,ordoany furtherkindsofnaturallanguageunderstandingoftheseevents,we'llneedtoknow alittlemoreaboutthesemanticsofthesearguments. Breakers and Openers have somethingincommon.Theyarebothvolitionalactors,oftenanimate,andtheyhave directcausalresponsibilityfortheirevents. Thematicroles areawaytocapturethissemanticcommonalitybetween Break- thematicroles ers and Eaters .Wesaythatthesubjectsofboththeseverbsare agents .Thus, AGENT agents isthethematicrolethatrepresentsanabstractideasuchasvolitionalcausation.Sim- ilarly,thedirectobjectsofboththeseverbs,the BrokenThing and OpenedThing ,are bothprototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction. Thesemanticrolefortheseparticipantsis theme . theme ThematicRole AGENT Thevolitionalcauserofanevent EXPERIENCER Theexperiencerofanevent FORCE Thenon-volitionalcauseroftheevent THEME Theparticipantmostdirectlyaffectedbyanevent RESULT Theendproductofanevent CONTENT Thepropositionorcontentofapropositionalevent INSTRUMENT Aninstrumentusedinanevent BENEFICIARY Theofanevent SOURCE Theoriginoftheobjectofatransferevent GOAL Thedestinationofanobjectofatransferevent Figure20.1 Somecommonlyusedthematicroleswiththeir Althoughthematicrolesareoneoftheoldestlinguisticmodels,aswesawabove, theirmodernformulationisdueto Fillmore(1968) and Gruber(1965) .Although thereisnouniversallyagreed-uponsetofroles,Figs. 20.1 and 20.2 listsomethe- maticrolesthathavebeenusedinvariouscomputationalpapers,togetherwithrough andexamples.Mostthematicrolesetshaveaboutadozenroles,butwe'll seesetswithsmallernumbersofroleswithevenmoreabstractmeanings,andsets withverylargenumbersofrolesthataretosituations.We'llusethegeneral term semanticroles forallsetsofroles,whethersmallorlarge. semanticroles  20.2  D IATHESIS A LTERNATIONS 3 ThematicRoleExample AGENT Thewaiter spilledthesoup. EXPERIENCER John hasaheadache. FORCE Thewind blowsdebrisfromthemallintoouryards. THEME OnlyafterBenjaminFranklinbroke theice ... RESULT Thecitybuilta regulation-sizebaseballdiamond ... CONTENT Monaasked ﬁYoumetMaryAnnatasupermarket?ﬂ INSTRUMENT Hepoachedstunningthem withashockingdevice ... BENEFICIARY WheneverAnnCallahanmakeshotelreservations forherboss ... SOURCE Iwin fromBoston . GOAL Idrove toPortland . Figure20.2 Someprototypicalexamplesofvariousthematicroles. 20.2DiathesisAlternations Themainreasoncomputationalsystemsusesemanticrolesistoactasashallow meaningrepresentationthatcanletusmakesimpleinferencesthataren'tpossible fromthepuresurfacestringofwords,orevenfromtheparsetree.Toextendthe earlierexamples,ifadocumentsaysthat CompanyAacquiredCompanyB ,we'd liketoknowthatthisanswersthequery WasCompanyBacquired? despitethefact thatthetwosentenceshaveverydifferentsurfacesyntax.Similarly,thisshallow semanticsmightactasausefulintermediatelanguageinmachinetranslation. Semanticrolesthushelpgeneralizeoverdifferentsurfacerealizationsofpred- icatearguments.Forexample,whilethe AGENT isoftenrealizedasthesubjectof thesentence,inothercasesthe THEME canbethesubject.Considerthesepossible realizationsofthethematicargumentsoftheverb break : (20.3) John AGENT brokethewindow. THEME (20.4) John AGENT brokethewindow THEME witharock. INSTRUMENT (20.5) Therock INSTRUMENT brokethewindow. THEME (20.6) Thewindow THEME broke. (20.7) Thewindow THEME wasbrokenbyJohn. AGENT Theseexamplessuggestthat break has(atleast)thepossiblearguments AGENT , THEME ,and INSTRUMENT .Thesetofthematicroleargumentstakenbyaverbis oftencalledthe thematicgrid , q -grid,or caseframe .Wecanseethatthereare thematicgrid caseframe (amongothers)thefollowingpossibilitiesfortherealizationoftheseargumentsof break : AGENT /Subject, THEME /Object AGENT /Subject, THEME /Object, INSTRUMENT /PP with INSTRUMENT /Subject, THEME /Object THEME /Subject Itturnsoutthatmanyverbsallowtheirthematicrolestoberealizedinvarious syntacticpositions.Forexample,verbslike give canrealizethe THEME and GOAL argumentsintwodifferentways:  4 C HAPTER 20  S EMANTIC R OLE L ABELING (20.8) a. Doris AGENT gavethebook THEME toCary. GOAL b. Doris AGENT gaveCary GOAL thebook. THEME Thesemultipleargumentstructurerealizations(thefactthat break cantake AGENT , INSTRUMENT ,or THEME assubject,and give canrealizeits THEME and GOAL in eitherorder)arecalled verbalternations or diathesisalternations .Thealternation verb alternation weshowedabovefor give ,the dativealternation ,seemstooccurwithparticularse- dative alternation manticclassesofverbs,includingﬁverbsoffuturehavingﬂ( advance , allocate , offer , owe ),ﬁsendverbsﬂ( forward , hand , mail ),ﬁverbsofthrowingﬂ( kick , pass , throw ), andsoon. Levin(1993) listsfor3100Englishverbsthesemanticclassestowhich theybelong(47high-levelclasses,dividedinto193moreclasses)andthe variousalternationsinwhichtheyparticipate.Theselistsofverbclasseshavebeen incorporatedintotheonlineresourceVerbNet (Kipperetal.,2000) ,whichlinkseach verbtobothWordNetandFrameNetentries. 20.3SemanticRoles:ProblemswithThematicRoles Representingmeaningatthethematicrolelevelseemslikeitshouldbeusefulin dealingwithcomplicationslikediathesisalternations.Yetithasprovedquitedif culttocomeupwithastandardsetofroles,andequallydiftoproduceaformal ofroleslike AGENT , THEME ,or INSTRUMENT . Forexample,researchersattemptingtorolesetsoftentheyneedto fragmentarolelike AGENT or THEME intomanyroles. LevinandRappa- portHovav(2005) summarizeanumberofsuchcases,suchasthefactthereseem tobeatleasttwokindsof INSTRUMENTS , intermediary instrumentsthatcanappear assubjectsand enabling instrumentsthatcannot: (20.9) a. Thecookopenedthejarwiththenewgadget. b. Thenewgadgetopenedthejar. (20.10) a. Shellyatetheslicedbananawithafork. b. *Theforkatetheslicedbanana. Inadditiontothefragmentationproblem,therearecasesinwhichwe'dliketo reasonaboutandgeneralizeacrosssemanticroles,butthediscretelistsofroles don'tletusdothis. Finally,ithasproveddiftoformallythethematicroles.Considerthe AGENT role;mostcasesof AGENTS areanimate,volitional,sentient,causal,butany individualnounphrasemightnotexhibitalloftheseproperties. Theseproblemshaveledtoalternative semanticrole modelsthatuseeither semanticrole manyfewerormanymoreroles. Theoftheseoptionsisto generalizedsemanticroles thatabstract overthethematicroles.Forexample, PROTO - AGENT and PROTO - PATIENT pr pr aregeneralizedrolesthatexpressroughlyagent-likeandroughlypatient-likemean- ings.Theserolesarenotbynecessaryandsufconditions,butrather byasetofheuristicfeaturesthataccompanymoreagent-likeormorepatient-like meanings.Thus,themoreanargumentdisplaysagent-likeproperties(beingvoli- tionallyinvolvedintheevent,causinganeventorachangeofstateinanotherpar- ticipant,beingsentientorintentionallyinvolved,moving)thegreaterthelikelihood
Semantic role labeling (SRL)	Thematic roles	Deep roles	Semantic roles	Proto-roles
Of the branches of linguistics, which one studies differences between male and female speech patterns?	Sociolinguistics studies how language uses vary based on geographic location, social classes, gender differences, political orientations, occupations, and more.	What is an example of a difference between male and female speech patterns?	Women tend to use filler words like "um", "just", "you know" more often than men.	School teachers speak more slowly than brokers.|||Women speak more softly than men.|||When women are excited about something, they speak louder than men.	Branches of Linguistics presentation||slides||BRANCHES OF  LINGUISTICS  LINGUISTICS Linguistics is the scientific study of natural language . Someone who engages in this study is called a linguist .  BRANCHES OF LINGUISTICS  Language in general and language in particular can be studied from different points of view  The field of linguistics as a whole can be divided into several subfields according to the point of view that is adopted  FIRST DISTINCTION GENERAL LINGUISTICS  Studying language in  general  Supplies the concepts  and categories in terms  of which particular  languages are to be  analysed DESCRIPTIVE LINGUISTICS  Studying particular  languages  Provides the data  which confirm or refute  the propositions and  theories put forward in  general linguistics  SECOND DISTINCTION Diachronic (Historical)  Linguistics  Traces the historical  development of the  language and records  the changes that have  taken place in it between  successive points in time:   to historical  Of particular interest to  linguists throughout the  nineteenth century Synchronic Linguistics  Non - historical: presents  an account of the  language as it is at  some particular point in  time  THIRD DISTINCTION Theoretical Linguistics  Studies language and  languages with a view to  constructing a theory of  their structure and functions  and without regard to any  practical applications that  the investigation of  language and languages  might have  Goal: formulation of a  satisfactory theory of the  structure of language in  general Applied Linguistics  Application of the  concepts and findings  of linguistics to a variety  of practical tasks ,  including language  teaching  Concerned with both  the general and  descriptive branches of  the subject  FOURTH DISTINCTION Micro linguistics  Adopts the narrower  view  Concerned solely with  the structures of the  language system in  itself and for itself Macro linguistics  Adopts the broader view  Concerned with the way  languages are acquired,  stored in the brain and  used for various functions;  interdependence of  language and culture;  physiological and  psychological  mechanisms involved in  language behaviour  FOURTH DISCTINCTION (CONTD.) Micro Linguistics  Phonetics  Phonology  Morphology  Syntax  Semantics  Pragmatics Macro linguistics  Psycholinguistics  Sociolinguistics  Neurolinguistics  Discourse Analysis  Computational  Linguistics  Applied Linguistics   MICROLINGUISTICS  Phonetics is the scientific study of speech sounds . It studies how speech sounds are articulated, transmitted, and received .  Phonology is the study of how speech sounds function in a language, it studies the ways speech sounds are organized . It can be seen as the functional phonetics of a particular language .  Morphology is the study of the formation of words . It is a branch of linguistics which breaks words into morphemes . It can be considered as the grammar of words as syntax is the grammar of sentences .  MICROLINGUISTICS  Syntax deals with the combination of words into phrases, clauses and sentences . It is the grammar of sentence construction .  Semantics is a branch of linguistics which is concerned with the study of meaning in all its formal aspects . Words have several types of meaning  Pragmatics can be defined as the study of language in use in context .  MACROLINGUISTICS  Sociolinguistics studies the relations between language and society : how social factors influence the structure and use of language .  Psycholinguistics is the study of language and mind : the mental structures and processes which are involved in the acquisition, comprehension and production of language .  Neurolinguistics is the study of language processing and language representation in the brain . It typically studies the disturbances of language comprehension and production caused by the damage of certain areas of the brain .  MACROLINGUISTICS  Discourse analysis, or text linguistics is the study of the relationship between language and the contexts in which language is used . It deals with how sentences in spoken and written language form larger meaningful units .  Computational linguistics is an approach to linguistics which employs mathematical techniques, often with the help of a computer .  Applied linguistics is primarily concerned with the application of linguistic theories, methods and findings to the elucidation of language problems which have arisen in other areas of experience  MACROLINGUISTICS  Forensic linguistics, legal linguistics, or language and the law, is the application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure . It is a branch of applied linguistics .  There are principally three areas of application for linguists working in forensic contexts :  understanding language of the written law,  understanding language use in forensic and judicial processes, and  the provision of linguistic evidence
Gender linguistics	Sociolinguistics	Branches of linguistics	Speech patterns
Define phoneme, morpheme, and grapheme.	A phoneme is the smallest distinctive sound, a morpheme is the smallest language unit that carries meaning, and a grapheme is a written symbol representing sound/speech.	On a microlinguistic level, what is the difference between prescriptive and descriptive grammar?	Both A and B	Descriptive grammar describes how grammar is used and prescriptive grammar describes how it should be used.|||Descriptive grammar studies the patterns and rules underlying the way we use words, phrases, and sentences, while prescriptive grammar presents rules on the correct use of these|||Neither A nor B	Branches of Linguistics presentation||slides||P HONOLOGY  Studies the sound system of languages .  Distinctive sounds within a language,  Nature of sound systems across the languages .  Phoneme (from the Greek :   , "a sound uttered") is the smallest segmental unit of sound employed to form meaningful contrasts between utterances . 9  M ORPHOLOGY  Studies the formation of words from smaller units  called  morphemes .  Morpheme : minimal meaningful language unit .  Phoneme(s) : smallest linguistically distinctive units of sound) in spoken language .  Grapheme(s) : written symbol to represent speech . 10
Phoneme	Morpheme	Grapheme	Linguistic terminology
Discuss the diversity of human languages in terms of phonemic inventories.	"Phonemic inventory" simply means the list of phonemes recognized by a language. English has 44, which is about average. Some Caucasian languages (from the Caucasus mountains, not "white") have very large inventories, of almost 100 consonants, and only a few vowels (some languages of southern Africa, such as Khoi-San also have this many). Polynesian languages tend to have few phonemes, such as Hawaiian, with only 15 total. Some major differences from English include languages with clicks or tones. Other phonemes alien to English include glottalized or retroflex consonants, and gutterals. There are many other unique consonants in the world but these are the biggest categories alien to English.	Which of the following is NOT a class of sounds characterizing some language?	Reflexes	Pharyngeals|||Flaps|||Implosives	Phonological Typology paragraph from Western Washington University||image||N/A
Phonology	Phonemic inventory	Phonetic features	Comparative linguistics	Typology
What is a hemispherectomy?	A hemispherectomy is a rare surgical procedure that involves removing one of the hemispheres from a patient. The deeper structures are left intact. This is often done when patients suffer from epileptic seizures that are localized to one of the brains hemispheres.	Which two lobes of the brain are often not removed?	Frontal and Parietal	Frontal and Temporal|||Occipital and Parietal|||Temporal and Parietal	Hemispherectomy Slideshow||slides||Hemispherectomy Overview A  hemispherectomy is the surgical removal of one half of the brain leaving  the deep structures intact.  Hemispherectomy is only considered for  patients (usually children) who have severe epilepsy with seizures arising  from only one side of the brain.  This procedure is a last resort in children with severe brain damage on one  side and seizures that do not respond to medication. It involves removing  the entire affected side of the brain. The remaining hemisphere develops  language and motor areas for both sides of the body. With intense  rehabilitation, many patients will lead functional lives.   During the procedure, the surgeon removes portions of the affected  hemisphere, often taking all of the temporal lobe but leaving the frontal  and parietal lobes. The surgeon also gently separates the hemispheres to  access and cut the corpus  callosum . After the tissue is removed, the  dura and bone are fixed back into place, and the scalp is closed using stitches  or staples. Most people who have a functional  hemispherectomy will be  able to return to their normal activities, including work or school in 6 to 8  weeks after surgery.
Psycholinguistics	Neurolinguistics	Neuroanatomy	Cerebral hemispheres	Applied linguistics	Hemispherectomy
Describe Jean Piaget's contributions to our modern understanding of cognitive development in children.	Jean Piaget is known for his work in "genetic epistemology"- the developmental theory of knowledge. He contributed a theory of cognitive development––"Piaget's theory of cognitive development"––that includes four different stages where children manage to develop their cognitive skills. Piaget believed that cognitive development was central to humans, and that learning language requires knowledge obtained through cognitive development. Piaget's impact has been significant in early education and has manifested itself in child-centered classrooms and open education (i.e., education open to all with no admissions requirements).	Which of the following is not one of Piaget's four stages of development?	Autonomy	Sensorimotor|||Preoperational|||Concrete Operational	Piaget's Theory of Cognitive Development||slides||N/A
Language acquisition
What is morphology?	Morphology is the study of morphemes, which form words. A morpheme is the smallest unit of language with meaning. Morphology is a subfield of linguistics dealing with the form and internal constituent structure of words.	What are the different types of morphemes?	Free and bound morphemes	Prefixes and suffixes|||Affixes and endings|||Derivational and inflectional morphemes	NONE
Morphemes	Morphology	Branches of linguistics	Units of sound
Define the following speech errors: Substitution, Deletion and Perseveration	Substitution is when a phoneme unit within a sentence is substituted with a different and incorrect phoneme Deletion is when a phone unit is accidentally deleted from a sentence Perseveration is when an earlier phoneme or segment reappears later in the sentence	Which type of speech error is the following sentence and example of? Saying 'the bighty mat' instead of 'the mighty bat'	Substitution	Deletion|||Perseveration	Speech Errors||slides||SPEECH  ERRORS :   SLIP OF  TOUNGE IN  PSYCHOLINGUISTIC       Serendre Shutter in 2004 stated that it is a which is happening entirely below the level of consciousness, so we're not aware of doing anything except when we hear ourselves saying something funny, and its all happening at such lighting speed that   Victoria Fromkin Said that  of the tongue are often the result of a sound being carried over from one word to the next  . Although the slips are mostly treated as errors of articulation, it has been suggested that they may result from  of the  as it tries to organize linguistic messages .   One of researcher named Yule and he come out his conception and he believes that when brain and tongue deny to work in accordance slips of tongue occur . Since our whole linguistics knowledge is stored in our mind he calls it the  SLIPS OF BRAIN  . He suggests that our  -  system is organized on the basis of some phonological information and that some words in the store are more easily retrieved than others .    The scientific study of speech  errors, c ommonly called Slips of the tongue or tongue - slips c an provided useful clues to the processes;  o f language production :  t hey can tell us where a speaker stop to  think.    Type Description Examples 1 Substitution A unit of the sentence contains  an intruder The queer old dean instead of ( the dear old queen) 2 Deletion A unit is omitted in the  utterance  was there) 3 Perseveration An    earlier    segment  reappears  in  a  latter  one Pulled a tantrum (pulled a pantrum) 4 Addition A new unit  is added The   optional   number   (the  moptiona l number)   Spooner had a nervous tendency to  sometimes transpose initial letters or half - syllables in speech. This tendency became  known, circa 1885,  as Spoonerism and the  sometimes hilarious transpositions became  known as Spoonerisms - Dr. Spooner's  occasional transpositions created a  reputation and started a fad. Students began  devising  transpositional puns, and  attributing them to him. His famous speech lapses are thought to  have resulted from the difficulty he may  have had reading. Spooner was an albino  and as such, suffered from defective  eyesight.  5 Swapping Two words are exchanged To  let  the  cat  out  of  the   bag (to   let   the  house out of the cat 6 Shifting A segment or unit is relocated  somewhere else in the  utterance She decides to hit it . ( she decide to hits it) . 7 Anticipation A later segment is used to  replace an earlier one Reading   list   (leading  list) 8 Blending Where  more  than  one  item  is  untended,  two  items       are       fused  together Person/people (perple) 9 Malapropism Inappropriate word  selection The  two  cars  collide  (collude) 10 Spoonerism Taken  from the Rev.  W.  Spooner  noted  for  puns and word plays Drink  is  the  curse  of  the    working    classes  (work  is  the  curse  of  drinking classes)  HOW TO OVERCOME ?   Practise and talk slow down Get enough sleep.
Psycholinguistics	Applied linguistics	Linguistic terminology	Speech errors
What is a phoneme?	Phonemes are the minimal units of a language recognized by the language as different sounds, and the building blocks of morphemes (or words). But phonemes do not have meanings. Phonemes are also not literally sounds; they are categories of sounds, or sound-concepts, recognized in a particular language, and thought of as single sounds by speakers of the language, although in reality, one phoneme can correspond to a variety of more specific sounds called allophones; for example the phoneme /t/ in English can be expressed as aspirated, flapped, or as a glottal stop.  Phonemes can be identified by comparing words with each other (within one language). If changing one sound in a word makes it into a different word, then those two sounds represent different phonemes (or allophones of different phonemes) in that language. These are called "minimal pairs" For example, this is how we can know that /p/ and /b/ are different phonemes in English: bit, pit, beet, peet, bay, pay, etc.	Which of the following statements is NOT true about phonemes?	The fact that "cats" and "dogs" are different words, shows that /s/ and /z/ are different phonemes in English.	Whether a difference in sound is phonemic or not, varies from language to language.|||Phonemes are differentiated by distinctive features.|||The differences between the "t"s in "time" and "matter" are non-distinctive in English.	Phonology: The Sound Patterns of Language Presentation||slides||Phonemes: The Phonological Units   of Language ¥!Phonemes  are the basic unit of sound  and are sensed in your mind rather than  spoken or heard  ¥!Each phoneme has one or more sounds  called  allophones associated with it,  which represent the actual sound being  produced in various environments   W'?#4$0"'(#1#2$+($>(:4+2"$ ¥!O"#($9')$7'$6"#2#$2)G2-6)-'(2$9')$,&#$*&#,-(:$1+(+1,4$ 0,+&2=$2)*"$,2$+($6"+2$4+263$ ¥!!"+2$4+26$7#1'(26&,6#2$6",6$6"+2$7+,4#*6$'.$>(:4+2"$",2$.')&6##($ 7+8#&#(6$H'?#4$0"'(#1#23$ /i  " e  #  ¾ u  $  o  %  a  &/ ,(7$ /a" /, /a $/$,(7 $PXYP 'Ð!J(7$,44$'.$6"#2#$0"'(#1#2$",2$,6$4#,26$6?'$,44'0"'(#2Z$ ¥!!"#$(,2,4$H#&2+'(=$?"+*"$'**)&2$G#.'&#$(,2,4$*'(2'(,(62$ ¥!!"#$'&,4$H#&2+'(=$?"+*"$'**)&2$#42#?"#&#$  Distinctive Features of Phonemes  ¥!For two  phones, or sounds, to contrast meaning  there must be some difference between them  Ð!For example, the phonetic feature of voicing  distinguishes [s] from [z]  ¥!When a feature distinguishes one phoneme from  another, it is a  distinctive feature  or a  phonemic feature   Feature Values  ¥!Features have two values: [+ feature] and     [-feature] to indicate the presence or  absence of that particular feature  Ð!For example, [b] is [+voiced] and [p] is [- voiced]  ¥!At least one feature    difference must distinguish    each phoneme of a language   Nondistinctive Features  ¥!When a feature is predictable by a rule for a certain  class of sounds, that feature is a  nondistinctive  (or  redundant or  predictable ) feature for that class  Ð!For example, nasalization is a redundant feature for  English vowels but is distinctive for English consonants  ¥!But in Akan and French nasalization is a distinctive feature for  vowels  Ð!Also, aspiration is a nondistinctive feature for voiceless  stops in English    Phonemic Patterns May Vary   Across Languages  ¥!The same phones may occur in two languages  but pattern differently because the phonologies  of the languages are different  ¥!While aspiration is not distinctive in English, it is  distinctive in Thai:
Phonology	Phonemes	Allophones	Distinctive features	Minimal pairs
What is orthography?	Orthography is a standard way of associating the sounds of a language with the symbols of an alphabet or another grapheme-based representation of sound.	Why do we need orthography?	None of the above	Without it, we would not be able to understand what is being said|||It explains grammar rules so we know how to speak and write correctly|||Both A and B	"What is orthography?" clip||youtube||N/A
Orthography	Language sounds
What is John Searle’s “Chinese Room” and what is it supposed to show?	John Searle’s “Chinese Room” is a thought-experiment concerning NLP and AI which is supposed to prove that even a computer which could pass the Turing Test would not understand language.  In the experiment, a person is locked in a room with a zillion file cabinets or a big book, or even a computer, full of symbolic rules that can be followed to produce intelligent responses to prompts submitted through a window in the door in Chinese. The person in the room does not know Chinese, but they are able to produce intelligent responses by blindly following the instructions in the room. The person-in-the-room is similar to a computer system and therefore shows that the Turing Test does not necessarily indicate real intelligence or understanding just the possibility of imitating it.	Which of the following is NOT an implication of Searle's Chinese Room?	Computers will never be as intelligent as humans. 	Understanding is not merely the ability to produce a meaningful response.|||The human mind does not work the way computers do.|||A computer that passes the Turing Test need not possess human-like intelligence or consciousness	NONE
John Searle	Chinese room	Functionalism	Turing test
What are two reasons English has a relatively deep orthography?	The Great Vowel Shift in the Middle Ages and the fact that there is no single, established formal academy of English. The Great Vowel Shift was a process where the sounds of vowels switched to every next vowel sound depending on the type of sound and how it was pronounced. A formal academy of English is used in the sense of there not being a single authority regulating English. Another reason is that English is spoken by a huge number of people all over the world and there are variations on pronunciation everywhere. A fourth reason is that English is like a melting pot of languages with vocabulary and roots from French, German, Latin, and more.	Which of the following branches of linguistics would not study English's deep orthography?	Syntax	Phonetics|||Diachronic linguistics	"Why is English spelling so weird?" clip||youtube||N/A
Orthography	English language	Great Vowel Shift	Branches of linguistics
What are some examples of text classification?	Text classification is the task of assigning labels to text. This can include determining the sentiment of text (sentiment analysis), which is whether a text has a positive, neutral or negative attitude; determining if an email is spam or not; determining the language of a text; assigning topics or a genre to a text; etc. Example labels for sentiment analysis are "positive," "neural," or "negative" and example labels for determining the language of a text are "English," "Thai," and "Spanish."	Which of the following is not a text classification task?	Taking an input of text from a textbook and outputting generated test questions based on the text.	Taking an input of text from a textbook and outputting its subject matter.|||Taking an input of text from a textbook and outputting a list of topics pertaining to the text.|||Taking an input of text from a textbook and outputting a prediction on who authored it.	Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 4 NaiveBayesandSentiment   liesattheheartofbothhumanandmachineintelligence.Deciding whatletter,word,orimagehasbeenpresentedtooursenses,recognizingfaces orvoices,sortingmail,assigninggradestohomeworks;theseareallexamplesof assigningacategorytoaninput.Thepotentialchallengesofthistaskarehighlighted bythefabulistJorgeLuisBorges(1964),whoimaginedclassifyinganimalsinto: (a)thosethatbelongtotheEmperor,(b)embalmedones,(c)thosethat aretrained,(d)sucklingpigs,(e)mermaids,(f)fabulousones,(g)stray dogs,(h)thosethatareincludedinthis(i)thosethat trembleasiftheyweremad,(j)innumerableones,(k)thosedrawnwith averycamel'shairbrush,(l)others,(m)thosethathavejustbroken avase,(n)thosethatresemblefromadistance. Manylanguageprocessingtasksinvolvealthoughluckilyourclasses aremucheasiertothanthoseofBorges.Inthischapterweintroducethenaive Bayesalgorithmandapplyitto textcategorization ,thetaskofassigningalabelor text categorization categorytoanentiretextordocument. Wefocusononecommontextcategorizationtask, sentimentanalysis ,theex- sentiment analysis tractionof sentiment ,thepositiveornegativeorientationthatawriterexpresses towardsomeobject.Areviewofamovie,book,orproductonthewebexpressesthe author'ssentimenttowardtheproduct,whileaneditorialorpoliticaltextexpresses sentimenttowardacandidateorpoliticalaction.Extractingconsumerorpublicsen- timentisthusrelevantforfrommarketingtopolitics. Thesimplestversionofsentimentanalysisisabinarytask,and thewordsofthereviewprovideexcellentcues.Consider,forexample,thefollow- ingphrasesextractedfrompositiveandnegativereviewsofmoviesandrestaurants. Wordslike great , richly , awesome ,and pathetic ,and awful and ridiculously arevery informativecues: + ...zanycharactersandrichlyappliedsatire,andsomegreatplottwists  Itwaspathetic.Theworstpartaboutitwastheboxingscenes... + ...awesomecaramelsauceandsweettoastyalmonds.Ilovethisplace!  ...awfulpizzaandridiculouslyoverpriced... Spamdetection isanotherimportantcommercialapplication,thebinaryclas- spamdetection taskofassigninganemailtooneofthetwoclasses spam or not-spam . ManylexicalandotherfeaturescanbeusedtoperformthisForex- ampleyoumightquitereasonablybesuspiciousofanemailcontainingphraseslike ﬁonlinepharmaceuticalﬂorﬁWITHOUTANYCOSTﬂorﬁDearWinnerﬂ. Anotherthingwemightwanttoknowaboutatextisthelanguageit'swritten in.Textsonsocialmedia,forexample,canbeinanynumberoflanguagesandwe'll needtoapplydifferentprocessing.Thetaskof languageid isthusthestep languageid inmostlanguageprocessingpipelines.Relatedtaskslikedeterminingatext'sau- thor,( authorshipattribution ),orauthorcharacteristicslikegender,age,andnative authorship attribution  2 C HAPTER 4  N AIVE B AYESAND S ENTIMENT C LASSIFICATION languagearetexttasksthatarealsorelevanttothedigitalhumanities, socialsciences,andforensiclinguistics. Finally,oneoftheoldesttasksintextisassigningalibrarysub- jectcategoryortopiclabeltoatext.Decidingwhetheraresearchpaperconcerns epidemiologyorinstead,perhaps,embryology,isanimportantcomponentofinfor- mationretrieval.Varioussetsofsubjectcategoriesexist,suchastheMeSH(Medical SubjectHeadings)thesaurus.Infact,aswewillsee,subjectcategory isthetaskforwhichthenaiveBayesalgorithmwasinventedin1961. isessentialfortasksbelowthelevelofthedocumentaswell. We'vealreadyseenperioddisambiguation(decidingifaperiodistheendofasen- tenceorpartofaword),andwordtokenization(decidingifacharactershouldbe awordboundary).Evenlanguagemodelingcanbeviewedaseach wordcanbethoughtofasaclass,andsopredictingthenextwordisclassifyingthe context-so-farintoaclassforeachnextword.Apart-of-speechtagger(Chapter8) eachoccurrenceofawordinasentenceas,e.g.,anounoraverb. Thegoalofistotakeasingleobservation,extractsomeuseful features,andthereby classify theobservationintooneofasetofdiscreteclasses. Onemethodforclassifyingtextistousehandwrittenrules.Therearemanyareasof languageprocessingwherehandwrittenrule-basedconstituteastate-of- the-artsystem,oratleastpartofit. Rulescanbefragile,however,assituationsordatachangeovertime,andfor sometaskshumansaren'tnecessarilygoodatcomingupwiththerules.Mostcases ofinlanguageprocessingareinsteaddonevia supervisedmachine learning ,andthiswillbethesubjectoftheremainderofthischapter.Insupervised supervised machine learning learning,wehaveadatasetofinputobservations,eachassociatedwithsomecorrect output(a`supervisionsignal').Thegoalofthealgorithmistolearnhowtomap fromanewobservationtoacorrectoutput. Formally,thetaskofsupervisedistotakeaninput x andaed setofoutputclasses Y = y 1 ; y 2 ;:::; y M andreturnapredictedclass y 2 Y .Fortext we'llsometimestalkabout c (forﬁclassﬂ)insteadof y asouroutput variable,and d (forﬁdocumentﬂ)insteadof x asourinputvariable.Inthesupervised situationwehaveatrainingsetof N documentsthathaveeachbeenhand-labeled withaclass: ( d 1 ; c 1 ) ;::::; ( d N ; c N ) .Ourgoalistolearnathatiscapableof mappingfromanewdocument d toitscorrectclass c 2 C .A probabilistic additionallywilltellustheprobabilityoftheobservationbeingintheclass.This fulldistributionovertheclassescanbeusefulinformationfordownstreamdecisions; avoidingmakingdiscretedecisionsearlyoncanbeusefulwhencombiningsystems. ManykindsofmachinelearningalgorithmsareusedtobuildThis chapterintroducesnaiveBayes;thefollowingoneintroduceslogisticregression. Theseexemplifytwowaysofdoing Generative likenaive Bayesbuildamodelofhowaclasscouldgeneratesomeinputdata.Givenanob- servation,theyreturntheclassmostlikelytohavegeneratedtheobservation. Dis- criminative likelogisticregressioninsteadlearnwhatfeaturesfromthe inputaremostusefultodiscriminatebetweenthedifferentpossibleclasses.While discriminativesystemsareoftenmoreaccurateandhencemorecommonlyused, generativestillhavearole.
Text classification	Natural Language Processing (NLP)	Sentiment analysis
What is Speech Recognition?	Speech Recognition is when computers are able to process and identify speech sounds, words and sentences. They can simultaneously recognize acoustic signals through their microphone applications.	How do computers capture and read acoustic signals?	With a waveform	With a digital sound net|||With a computerized ear structure	NLP Crash Course clip||youtube||N/A
Applied linguistics	Branches of linguistics	Computer science	Natural Language Processing (NLP)	Phonetics	Acoustic signals	Speech recognition
Phone, phoneme, and allophone are all examples of linguistic terminology. What is the difference between them?	When you change a phoneme, you change a word's meaning, but if you change an allophone, you don't change the meaning, only the sound of the word's realization. In linguistic terminology, a phoneme is the smallest unit of sound in speech, while a phone is an unanalyzed language sound. It is the smallest unit in speech that you can identify and transcribe with an IPA symbol. Its exact sound is not critical to the meaning of words.	What is the basic sound in speech?	Phoneme	Morpheme|||Allophone|||Phone	Allophone and Phoneme presentation||slides||By : Imas Suwangsih  Mala Purnamasari Rd. Zaenal    phoneme:   o allophone(s):   the phonetic variant(s) of a phoneme  So, each phoneme comprises a set of  allophone, and each allophone is particular  realisation of phoneme in a particular  linguistics environment    PHONEMES are the basic sounds  - the  significant , non - predictable ones.  The different ways the phonemes are  realised in various positions are called  ALLOPHONES  - predictable, and non - significant.  Different language can have the same sound and  yet organise them differently in their sound  system. For example : 1).  T he phoneme /p/ of English has two allophones.   English speakers treat them as the same sound,  but they are different:  the first the second is  unaspirated (plain). is  aspirated and  Plain [p] also occurs as the  p in  cap [ k  æp ],  and  the second  p in  paper [ p  e  .p  ]   One, (p h ), has a puff of air after the lips  open, and occurs at the beginnings of words  such as "pit". The other, (p), does not have a  puff of air, and occurs after  second word such  as  in "spit".   We can see from the other language  2). In  Chinese languages treat these two phones  differently;  for example  :  in Mandarin, [p] (written  b in  Pinyin ) and [ p  ]  (written  p ) .  English has there phonemes at the bilabial position.  For instance, the phone [p h]  and [b] occur in the  ninimal pair park [p h ark] versus bark [bak],  And the  bilabial nasal [m] in mark [mak].   They must, therefore, be allophone (variant) of  different phonemes. So we nkow that there are at least  there bilabial phoneme in english, and we have been  able to show that the differences between [p h b m]  contain significant information for speaker of that  language.  Usually, the different ALLOPHONES of  the same PHONEME are all  similar to  each other  - they form a FAMILY of  sounds . We can see this by the fact that the  same difference can be allophonic in one  language, and phonemic in another.   We say that allophones have  complementary distribution  In English, s and  sh are phonemes, and so  have  contrastive distribution .  PHONEMES ALLOPHONES Significant non - significant Unpredictable Predictable contrastive distribution complementary distribution broad transcription narrow transcription
Phonemes	Morphemes	Allophones	Phones	Units of sound	Phonology
How many Native American languages and families once lived, and how many survive?	It is believed that before European contact, there were at least 2,000 Native American languages in over 200 families. Now only about 200 languages survive. The most spoken languages are Quechua in Peru, Nahuatl in Mexico, and Mayan languages such as Yucatec and Kaqchikel in Mexico and Guatemala, each spoken by one to seven million. In North America the most spoken are Hopi and Navajo in the American Southwest and Cree, Eskimo, Crow, and Lakhota, in the northern US and Canada, each spoken by thousands to tens of thousands of people.	Which of the following is NOT true about the history of Native American languages?	It has been shown that they are related to Siberian languages.	Some Native American languages in the same region were as different as English and Chinese.|||Every habitable area of the Americas had its own local languages by 9,000 BC.|||Nobody actually knows how many American citizens speak native languages now.	Native American Languages from Western Washington University||document||Native American Languages Origins and pre-Columbian distribution              The earliest immigrants seem to have come through the Western  part of the present day United States.  Many probably settled there without  moving on:  more than half the language families in North America were spoken  on the Pacific coast, especially in California.  The northern and eastern regions  of the present day United States seemed to have been settled later and have  few families.  So the original Native American spread of population was the  mirror opposite of the spread of Europeans many centuries later.             California languages (Hokan, Penutian-Mayan)             Pacific Northwest             Uto-Aztecan (Mexica/Nahuatl/Hopi/Papago) and isolates of the SW             the Siouan tribes (Sioux = Lakhota, Dakhota)             the American South (Muskogean and isolates)             Iroquois (Houdenasanee) and Cherokee (Tsalagi)             Algonquin  (Lenni Lenape, Delaware,  Wallam Olam ) in California;   Beothuk, Yuchi             Na-Dene (Navajo, Apache, Athabascan)             Eskimo-Aleut (Inuit) South American languages--greatest diversity in the rain forest--refuge area  for the descendants of the first immigrants. Quechua, Aymara, Guarani, Chibchan Language isolates-in the Amazon Carib, Arawak, Taino       What accounts for the tremendous linguistic diversity of the aboriginal  Americas?  There were dozens of language families each the equivalent of the  Indo-European family.  If anything, your map presents an oversimplification  of this language diversity.  In California languages as different as English  and Chinese were spoken side by side.  Many linguists suspect that at least  some of these separate families date back to separate migrations of different  tribes from Asia who originally spoke unrelated languages.  Linguistic and archeological  data hint at more than one migration from Asia into the Americas, all of them  through Alaska.  These migrations began at least 14 thousand years ago.  By  9000 BC all habitable parts of the Americas from the Arctic to the tip of South  America seem to have been populated by groups of  hunter-gatherers.             250 spoken in North America north of Mexico             350 In Mexico and Central America (Mesoamerica)             1450 in South America                        No proven relation to Siberian languages (mention the Paleo-Siberian languages, legends in the Wallam Olam)  There is no evidence of any substantial contact by other routes until 1492.  The Vikings visited the north Atlantic  coast from the 10th century to the 14th century but left no lasting impact--linguistically  ordemographically. Native American languages after contact       Before the Europeans, the Aztecs and Incas had complex and powerful states  with many millions of subjects.  In 1492 the Aztec capital Tenochtitlan was  larger than London, and only the Chinese Empire was larger and more powerful  than the empire of the Incas (Tawatsinsusu).  In 1492 a substantial portion  of the world's languages were native American languages.  It has been estimated  that one out of every five human beings on earth spoke a Native American language  at that time.        This is clearly not the case today.  Many Native American languages have  become extint; of the ones that survive, only a small number are spoken by any  sizeable number of people;  no native American language has ever been used to  address the United Nations.  The Indians were so completely displaced by the  Europeans that in the early part of this century many people thought the Indians  and their languages were doomed to total extinction.       The most spoken native languages are south of the United States: Quechua--7 million Mayan languages --over 1 million. Nahuatl--over 1 million       Today, perhaps one in every 250 American citizens speak a native American language.  The last reliable figures for speakers of native American languages  were published in 1962.  Navaho--150thou; Cherokee--15thou; Papago 20, Hopi--several thousand Crow--8thou; Lakhota--4thou Various Eskimo lang's--several thousand, Cree--a few thousand.  Impact of Native American Languages on English     Let's now examine the impact of Native American languages on English.   Although the native languages as means of communication were pushed to oblivion  or at best into obscurity, these languages have had a greater effect on the  languages of the conquerors than is often realized.     The most obvious effect is in the large number of place names that derive  fromnative American languages:     Fully half the states of the United States have names associated with aboriginal America; only a minority of state names come from European langauges. Michigan (Alg) big water Minnesota (Souian) water that reflects the sky Missouri (Souian) water flowing along Ohio (Ir) good river Texas (Caddoan) Friends Nebraska (Omaha) broad river Kentucky (Ir) dark and bloody ground     Many states are named after tribes who once lived there:  Mass, Conn, Illinois, Dakota     Cities and Counties also:  Punksatony (bad air), Skagit, Snohomish, Okanogan.  . . .    Native names for rivers, bodies of water and mountains are even more common: Potomac, Allegheny, Monongahela, Lake Okeechobee, Okefenoke Swamp, Skagit, Samish,  Wabash, Washatch mts.      Ironically, Native Americans usually didn't give a single name to an entire  river or mountain.  Instead, they tended to give separate names to each individual  feature such as the mouth of a river or a particular bend of a river.  They  tended to name each peak or crag rather than the whole mountain.  Europeans  often misunderstood this technique and applied native names to entire geographic  units: Tennessee (Cherokee) named after a village Appalachian Mountains were named after a village in northern Florida,  Appalachen Canada (Ir) name of a small village applied by Jaques Cartier to the whole  land.     In keeping with the European practice of naming places in honor of individuals, Europeans often named places after notable Indians: Tammany, Pocatella (after  aBannock chief, Sacagewea (Shoshone  bird woman ), Whatcom, Seattle.    Ironically, the  Indians themselves rarely named anything after the name of an  individual.     European naming practices also changed:  they adopted Indian custom of  naming areas after animals rather than after individual people:  Buffalo, NY;  Turkey Island (near Jamestown); Turtle Lake, Michigan, Deer Creek.  Ancient  Europe had abandoned this practice after the acceptance of Christianity; as  a rule only pre-Christian European toponyms are animalistic:  Berlin, Bern.     In recent years there has been a push to rediscover and reuse old native  names:  Denali, Kalaallit Nunaat, Kulshan, Tahoma.     Thousands of native words also came into the general vocabulary of English.   This is no surprise, since the Americas contained such a tremendous array of  new plants, animals, lifeways and environments that the Europeans had to find  some way of expressing. bayou  (Choctaw);  savanna  (Taino);  pampas  ;  jerky   (Quechua);  potato  (Taino); hurricane  (Carib);  tomato, chocolate  (Aztec); blizzard, shark  are probably Native American --many Native American words came to English through the Jamestown and Massachusetts  Bay colonies, the first areas to witness extensive Indian-English contact:  squash  from 'asquatasquash =  eaten raw , succotash =  grain of corn '  Other Algonquin words include:   skunk, chipmunk, racoon, moose, opossum, persimmon, sassafras, hickory, wampum, toboggan, wigwam, tomahawk, papoose, squaw, powwow  (holy man), caucus .   Also  Yankee  from the Algonquin pronunciation  ofEnglish .Podunk --(Algonquian) an isolated part of land. muckamuck  --(Chinook) important chief honk--the sound of a goose, then of a car;  honky-tonk music--honky.     The effect of native languages on the English vocabulary goes deeper than  merely the borrowing of individual words, however.  The very technique by which  English makes words was enriched.  European languages are rich in nouns, but  weaker in verbs;  consequently, they borrowed mostly nouns from the Native Americans.   Butthe Native American nouns were often whole verb-like complexes, not very  different from an entire sentence.  Therefore, English speakers began to compound  nounstogether to try to capture the essence of a native term.  This resulted  in hundreds of new noun compounds:    rattlesnake, june bug, red cedar, bloodroot,  chokecherry, sugar maple, peanut, firewater, bullfrog, catfish .  Such types  of compounds involving strange or antithetical elements of meaning had been  rare in English until the colonial period.  The polysynthetic structure of native  American languages thus influenced modern English, which continued making such  semantically unusual compounds even after the initial influence of the Native  American languages wore off:   bootleg .     Many new phrases and sayings also came into English:   going on the warpath,  scalp  hunting, paleface, burying the hatchet, smoking the peace pipe, Great  Spirit, Happy Hunting Ground .    What is the future of Native American languages?     Some of them will definitely survive.  Greater efforts are being expended  to value them and preserve them.  Some extinct or nearly extinct languages may  beresurrected from the dead.  Lummis are involved in a language revitalization campaign.  Such an effort actually succeeded in resurrecting Hebrew in this  century.
Native American languages	Historical linguistics	Language death
Explain the ambiguity of this sentence in formal terms: "I shot an elephant in my pajamas."	"What he was doing in my pajamas, I don’t know..." A famous Groucho Marx joke. The semantic ambiguity is the question of who was in the speaker’s pajamas. The syntactic ambiguity concerns the placement of the prepositional phrase "in my pajamas" within the phrase structure. It could be within the noun phrase headed by "an elephant", modifying that noun phrase, in which case, it is an adjectival modifier and the elephant was wearing the pajamas. But it could also be part of the verb phrase headed by "shot" and modifying "shot" as an adverbial modifier, in which case the speaker was shooting in his, her, or their, pajamas.	Which of the following sentences does not possess a PP-attachment ambiguity?	We watched the eclipse with a telescope.	The spy watched the professor with a telescope.|||We wondered about his speech in the Volvo.|||Nobody told me they would reject my paper with harsh words.	PP Attachment and Argumenthood||publication||MIT Working Papers in Linguistics 26, 95Œ151Papers on Language Processing and Acquisition.  © 1995  Carson T. Schütze                PP Attachment and Argumenthood     *Carson T. Schütze, Massachusetts Institute of Technology1.              Introduction  One of the  best known kinds of syntactic  ambiguity in the  sentenceprocessing literature involves the possible  attachments of prepositional phrases(PPs) in V-NP-PP sequences, as exemplified by sentences like those in (1).(1)a.The spy saw the cop with the telescope.b.The spy saw the cop with the revolver.In sentence (1a), the PP  with the telescope can be  taken to modify  the act of seeing, describing the instrument the spy  used (a  VP-attached reading), or to modify the cop, describing what he or she was holding  (an NP-attached read-ing). In general, sentences of  this form are usually not ambiguous  once thewhole sentence has been processed; for example, in the  minimally contrastingsentence (1b) our knowledge of  the real world dictates that revolvers cannot be used for seeing, and so the  NP-attached reading is forced. But since prepositions like with can be used in  various ways, an incremental parser cannot determinewhich attachment of a  PP will be required until the disambiguating noun (e.g.,revolver) has been encountered. To the extent that the human  sentence process-ing mechanism attempts to assign a structure and an  interpretation to  incomingmaterial word-by-word as soon as it is encountered (Marslen-Wilson 1973, 1975;Tyler & Marslen-Wilson  1977; Swinney 1979; Shillcock 1982;  Garrod &Sanford 1985; Tanenhaus, Carlson & Seidenberg 1985; etc.), one can ask how it  decides which structure to  assign upon encountering a partial sentence like Thespy saw the cop  with: which attachment will it choose for the PP containing with. That is the question I explore in this paper.Many answers to  this question have already appeared in  the literature.The best known has  been developed by  Lyn Frazier and her colleagues (Frazier1978, 1987; Frazier & Fodor 1978; Frazier & Rayner 1982; etc.). Their claim is                                                 * This work was inspired by a suggestion from Ted Gibson, whom I  thank for his on- going collaboration on this project. I  would like to thank  the following for helpfuldiscussions and/or comments on earlier versions of this  work: Liz Cowper, Lyn Frazier, Alec Marantz, Martha McGinnis, David Pesetsky, Sylvie Ratte, Ron Smyth,and the RTG group. Special thanks to Ted Gibson and Colin Phillips for detailedcomments on a previous draft. This research was supported by a  SSHRCC Fellowshipand by  the Research Training Grant ﬁLanguage: Acquisition and Computationﬂawarded by  the National Science Foundation (US) to  the Massachusetts Institute of Technology (DIR 9113607).
Syntactic ambiguity	Phrase structure	Prepositional phrase attachment	Syntax	Natural Language Processing (NLP) basics
Annotate the following sentence with Penn Treebank POS tags: “All the king’s horses and all the king’s men could not put Humpty-Dumpty together again.”	All / PDT, the / DT, king / NN, ‘s / POS, horses / NNS, and / CC, all / PDT, the / DT, king / NN, ‘s / POS, men / NNS, could / MD, not / RB, put / VB, humpty-dumpty / NNP, together / RB, again RB, . / .	Which of the following is the most correct annotation for the phrase: “the most looked up words”?	the / DT, most / RB, looked / VBN, up / RP, words / NNS	the / DT, most / RB, looked / JJ, up / IN, words / NNS|||the / DT, most /JJ, looked / VBD, up / IN, words / NNS|||the / DT, most / RB, looked / VBD, up / RP, words / NNS	Chapter Sections on basic POS tags||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 8 Part-of-SpeechTagging DionysiusThraxofAlexandria( c. 100 B . C .),orperhapssomeoneelse(itwasalong timeago),wroteagrammaticalsketchofGreek(aﬁ techn ¯ e ﬂ)thatsummarizedthe linguisticknowledgeofhisday.Thisworkisthesourceofanastonishingproportion ofmodernlinguisticvocabulary,includingwordslike syntax , diphthong , clitic ,and analogy .Alsoincludedareadescriptionofeight partsofspeech :noun,verb, partsofspeech pronoun,preposition,adverb,conjunction,participle,andarticle.Althoughearlier scholars(includingAristotleaswellastheStoics)hadtheirownlistsofpartsof speech,itwasThrax'ssetofeightthatbecamethebasisforpracticallyallsubsequent part-of-speechdescriptionsofmostEuropeanlanguagesforthenext2000years. SchoolhouseRockwasaseriesofpopularanimatededucationaltelevisionclips fromthe1970s.ItsGrammarRocksequenceincludedsongsaboutexactly8parts ofspeech,includingthelategreatBobDorough's ConjunctionJunction : ConjunctionJunction,what'syourfunction? Hookingupwordsandphrasesandclauses... Althoughthelistof8wasslightlyfromThrax'soriginal,theastonishing durabilityofthepartsofspeechthroughtwomillenniaisanindicatorofboththe importanceandthetransparencyoftheirroleinhumanlanguage. 1 Partsofspeech(alsoknownas POS , wordclasses ,or syntacticcategories )are POS usefulbecausetheyrevealalotaboutawordanditsneighbors.Knowingwhether awordisa noun ora verb tellsusaboutlikelyneighboringwords(nounsarepre- cededbydeterminersandadjectives,verbsbynouns)andsyntacticstructure(nouns aregenerallypartofnounphrases),makingpart-of-speechtaggingakeyaspectof parsing(Chapter13).Partsofspeechareusefulfeaturesforlabeling namedentities likepeopleororganizationsin informationextraction (Chapter18),orforcorefer- enceresolution(Chapter22).Aword'spartofspeechcanevenplayaroleinspeech recognitionorsynthesis,e.g.,theword content ispronounced CONtent whenitisa nounand conTENT whenitisanadjective. Thischapterintroducespartsofspeech,andthenintroducestwoalgorithmsfor part-of-speechtagging ,thetaskofassigningpartsofspeechtowords.Oneis generativeŠHiddenMarkovModel(HMM)ŠandoneisdiscriminativeŠtheMax- imumEntropyMarkovModel(MEMM).Chapter9thenintroducesathirdalgorithm basedontherecurrentneuralnetwork(RNN).Allthreehaveroughlyequalperfor- mancebut,aswe'llsee,havedifferenttradeoffs. 8.1(Mostly)EnglishWordClasses Untilnowwehavebeenusingpart-of-speechtermslike noun and verb ratherfreely. Inthissectionwegiveamorecompleteoftheseandotherclasses.While wordclassesdohavesemantictendenciesŠadjectives,forexample,oftendescribe 1 Nonetheless,eightisn'tverymanyand,aswe'llsee,recenttagsetshavemore.  2 C HAPTER 8  P ART - OF -S PEECH T AGGING properties andnouns people Špartsofspeecharetraditionallyinsteadbased onsyntacticandmorphologicalfunction,groupingwordsthathavesimilarneighbor- ingwords(their distributional properties)ortakesimilarafes(theirmorpholog- icalproperties). Partsofspeechcanbedividedintotwobroadsupercategories: closedclass types closedclass and openclass types.Closedclassesarethosewithrelativelyedmembership, openclass suchasprepositionsŠnewprepositionsarerarelycoined.Bycontrast,nounsand verbsareopenclassesŠnewnounsandverbslike iPhone or tofax arecontinually beingcreatedorborrowed.Anygivenspeakerorcorpusmayhavedifferentopen classwords,butallspeakersofalanguage,andsuflargecorpora,likely sharethesetofclosedclasswords.Closedclasswordsaregenerally functionwords functionword like of , it , and ,or you ,whichtendtobeveryshort,occurfrequently,andoftenhave structuringusesingrammar. Fourmajoropenclassesoccurinthelanguagesoftheworld: nouns , verbs , adjectives ,and adverbs .Englishhasallfour,althoughnoteverylanguagedoes. Thesyntacticclass noun includesthewordsformostpeople,places,orthings,but noun othersaswell.Nounsincludeconcretetermslike ship and chair ,abstractionslike bandwidth and relationship ,andverb-liketermslike pacing asin Hispacingtoand frobecamequiteannoying .WhatanouninEnglish,then,arethingslikeits abilitytooccurwithdeterminers( agoat,itsbandwidth,Plato'sRepublic ),totake possessives( IBM'sannualrevenue ),andformostbutnotallnounstooccurinthe pluralform( goats,abaci ). Openclassnounsfallintotwoclasses. Propernouns ,like Regina , Colorado , propernoun and IBM ,arenamesofpersonsorentities.InEnglish,theygenerallyaren't precededbyarticles(e.g., thebookisupstairs ,but Reginaisupstairs ).Inwritten English,propernounsareusuallycapitalized.Theotherclass, commonnouns ,are commonnoun dividedinmanylanguages,includingEnglish,into countnouns and massnouns . countnoun massnoun Countnounsallowgrammaticalenumeration,occurringinboththesingularandplu- ral( goat/goats,relationship/relationships )andtheycanbecounted( onegoat,two goats ).Massnounsareusedwhensomethingisconceptualizedasahomogeneous group.Sowordslike snow,salt ,and communism arenotcounted(i.e., *twosnows or *twocommunisms ).Massnounscanalsoappearwithoutarticleswheresingular countnounscannot( Snowiswhite butnot *Goatiswhite ). Verbs refertoactionsandprocesses,includingmainverbslike draw , provide , verb and go .Englishverbshave(non-third-person-sg( eat ),third-person-sg ( eats ),progressive( eating ),pastparticiple( eaten )).Whilemanyresearchersbelieve thatallhumanlanguageshavethecategoriesofnounandverb,othershaveargued thatsomelanguages,suchasRiauIndonesianandTongan,don'tevenmakethis distinction( Broschart1997 ; Evans2000 ; Gil2000 ). ThethirdopenclassEnglishformis adjectives ,aclassthatincludesmanyterms adjective forpropertiesorqualities.Mostlanguageshaveadjectivesfortheconceptsofcolor ( white , black ),age( old , young ),andvalue( good , bad ),buttherearelanguages withoutadjectives.InKorean,forexample,thewordscorrespondingtoEnglish adjectivesactasasubclassofverbs,sowhatisinEnglishanadjectiveﬁbeautifulﬂ actsinKoreanlikeaverbmeaningﬁtobebeautifulﬂ. Theopenclassform, adverbs ,isratherahodge-podgeinbothformand adverb meaning.Inthefollowingalltheitalicizedwordsareadverbs: Actually ,Iran homeextremelyquicklyyesterday Whatcoherencetheclasshassemanticallymaybesolelythateachofthese wordscanbeviewedasmodifyingsomething(oftenverbs,hencethenameﬁad-  8.1  (M OSTLY )E NGLISH W ORD C LASSES 3 verbﬂ,butalsootheradverbsandentireverbphrases). Directionaladverbs or loca- tiveadverbs ( home , here , downhill )specifythedirectionorlocationofsomeaction; locative degreeadverbs ( extremely , very , somewhat )specifytheextentofsomeaction,pro- degree cess,orproperty; manneradverbs ( slowly , slinkily , delicately )describethemanner manner ofsomeactionorprocess;and temporaladverbs describethetimethatsomeac- temporal tionoreventtookplace( yesterday , Monday ).Becauseoftheheterogeneousnature ofthisclass,someadverbs(e.g.,temporaladverbslike Monday )aretaggedinsome taggingschemesasnouns. Theclosedclassesdiffermorefromlanguagetolanguagethandotheopen classes.SomeoftheimportantclosedclassesinEnglishinclude: prepositions: on,under,over,near,by,at,from,to,with particles: up,down,on,off,in,out,at,by determiners: a,an,the conjunctions: and,but,or,as,if,when pronouns: she,who,I,others auxiliaryverbs: can,may,should,are numerals: one,two,three,second,third Prepositions occurbeforenounphrases.Semanticallytheyoftenindicatespatial preposition ortemporalrelations,whetherliteral( onit , beforethen , bythehouse )ormetaphor- ical( ontime , withgusto , besideherself ),butoftenindicateotherrelationsaswell, likemarkingtheagentin Hamletwaswrittenby Shakespeare .A particle resembles particle aprepositionoranadverbandisusedincombinationwithaverb.Particlesoften haveextendedmeaningsthataren'tquitethesameastheprepositionstheyresemble, asintheparticle over in sheturnedthepaperover . Averbandaparticlethatactasasinglesyntacticand/orsemanticunitare calleda phrasalverb .Themeaningofphrasalverbsisoftenproblematically non- phrasalverb compositional Šnotpredictablefromthedistinctmeaningsoftheverbandthepar- ticle.Thus, turndown meanssomethinglike`reject', ruleout `eliminate', out `discover',and goon `continue'. Aclosedclassthatoccurswithnouns,oftenmarkingthebeginningofanoun phrase,isthe determiner .Onesmallsubtypeofdeterminersisthe article :English determiner article hasthreearticles: a , an ,and the .Otherdeterminersinclude this and that ( thischap- ter , thatpage ). A and an markanounphraseaswhile the canmarkit asisadiscourseproperty(Chapter23).Articlesarequitefre- quentinEnglish;indeed, the isthemostfrequentlyoccurringwordinmostcorpora ofwrittenEnglish,and a and an aregenerallyrightbehind. Conjunctions jointwophrases,clauses,orsentences.Coordinatingconjunc- conjunctions tionslike and , or ,and but jointwoelementsofequalstatus.Subordinatingconjunc- tionsareusedwhenoneoftheelementshassomeembeddedstatus.Forexample, that in ﬁIthoughtthatyoumightlikesomemilkﬂ isasubordinatingconjunction thatlinksthemainclause Ithought withthesubordinateclause youmightlikesome milk .Thisclauseiscalledsubordinatebecausethisentireclauseistheﬁcontentﬂof themainverb thought .Subordinatingconjunctionslike that whichlinkaverbtoits argumentinthiswayarealsocalled complementizers . complementizer Pronouns areformsthatoftenactasakindofshorthandforreferringtosome pronoun nounphraseorentityorevent. Personalpronouns refertopersonsorentities( you , personal she , I , it , me ,etc.). Possessivepronouns areformsofpersonalpronounsthatin- possessive dicateeitheractualpossessionormoreoftenjustanabstractrelationbetweenthe personandsomeobject( my,your,his,her,its,one's,our,their ). Wh-pronouns wh ( what,who,whom,whoever )areusedincertainquestionforms,ormayalsoactas  4 C HAPTER 8  P ART - OF -S PEECH T AGGING complementizers( Frida,whomarriedDiego... ). AclosedclasssubtypeofEnglishverbsarethe auxiliary verbs.Cross-linguist- auxiliary ically,auxiliariesmarksemanticfeaturesofamainverb:whetheranactiontakes placeinthepresent,past,orfuture(tense),whetheritiscompleted(aspect),whether itisnegated(polarity),andwhetheranactionisnecessary,possible,suggested,or desired(mood).Englishauxiliariesincludethe copula verb be ,thetwoverbs do and copula have ,alongwiththeirforms,aswellasaclassof modalverbs . Be iscalled modal acopulabecauseitconnectssubjectswithcertainkindsofpredicatenominalsand adjectives( Heis aduck ).Theverb have canmarktheperfecttenses( Ihave gone , I had gone ),and be isusedaspartofthepassive( Wewere robbed )orprogressive( We are leaving )constructions.Modalsareusedtomarkthemoodassociatedwiththe eventdepictedbythemainverb: can indicatesabilityorpossibility, may permission orpossibility, must necessity.Thereisalsoamodaluseof have (e.g., Ihave togo ). Englishalsohasmanywordsofmoreorlessuniquefunction,including inter- jections ( oh,hey,alas,uh,um ), negatives ( no,not ), politenessmarkers ( please, interjection negative thankyou ), greetings ( hello,goodbye ),andtheexistential there ( there aretwoon thetable )amongothers.Theseclassesmaybedistinguishedorlumpedtogetheras interjectionsoradverbsdependingonthepurposeofthelabeling. 8.2ThePennTreebankPart-of-SpeechTagset AnimportanttagsetforEnglishisthe45-tagPennTreebanktagset (Marcusetal., 1993) ,showninFig. 8.1 ,whichhasbeenusedtolabelmanycorpora.Insuch labelings,partsofspeecharegenerallyrepresentedbyplacingthetagaftereach word,delimitedbyaslash: TagDescriptionExampleTagDescriptionExampleTagDescriptionExample CC coordinating conjunction and,but,or PDT predeterminer all,both VBP verbnon-3sg present eat CD cardinalnumber one,two POS possessiveending 's VBZ verb3sgpres eats DT determiner a,the PRP personalpronoun I,you,he WDT wh-determ. which,that EX existential`there' there PRP$ possess.pronoun your,one's WP wh-pronoun what,who FW foreignword meaculpa RB adverb quickly WP$ wh-possess. whose IN preposition/ subordin-conj of,in,by RBR comparative adverb faster WRB wh-adverb how,where JJ adjective yellow RBS superlatv.adverb fastest $ dollarsign $ JJR comparativeadj bigger RP particle up,off # poundsign # JJS superlativeadj wildest SYM symbol + , % , & ﬁ leftquote `orﬁ LS listitemmarker 1,2,One TO ﬁtoﬂ to ﬂ rightquote 'orﬂ MD modal can,should UH interjection ah,oops ( leftparen [,(, f , < NN singormassnoun llama VB verbbaseform eat ) rightparen ],), g , > NNS noun,plural llamas VBD verbpasttense ate , comma , NNP propernoun,sing. IBM VBG verbgerund eating . sent-endpunc .!? NNPS propernoun,plu. Carolinas VBN verbpastpart. eaten : sent-midpunc :;...Œ- Figure8.1 PennTreebankpart-of-speechtags(includingpunctuation). (8.1) The/DTgrand/JJjury/NNcommented/VBDon/INa/DTnumber/NNof/IN other/JJtopics/NNS./. (8.2) There/EX are/VBP70/CDchildren/NNS there/RB
Penn Treebank	Part-of-speech (POS) tagging
What is semantic role labeling (SRL)?	Semantic role labeling is the process of assigning semantic roles to the constituents of a sentence—primarily to the arguments and adjuncts of verbs, but sometimes also to other constituents. The roles labeled are those called thematic roles by linguists, which describe the relational structures of events, for example who is acting (the agent) on whom (the patient) with what (the instrument).  Although the thematic roles agent and patient are similar to grammatical relations (subject, object, etc.) they are not the same. For example the subject of a verb can be a Patient or a Theme, not an Agent: The window (patient) broke when the rock (theme) hit it. Other typical thematic roles include recipient, location, goal, and experiencer. The roles are functions of the verbs’ meanings, or of whatever predicate the roles are arguments of. SRL could be used for better natural language understanding, such as in query answering or automatic summarization.	Thematic roles tell us what properties of events and the entities in them are considered most important by languages. Which of the following does NOT make a difference to the selection of thematic roles?	Dominance	Volition|||Direction|||Causation	Introduction to Semantic Role Labeling||slides||!"#$%&'()*+,") -$.",'%/  !"#$%&'()*+,") -$.",'%/ !"#$%&'(#)%"  !"#$%&'()*+,")-$.",'%/ !""#$%&'$()* +,-.*'$()+/+&)*0.1+*2*'.3* +!!!"#$++++++ 4$4+%#&'+'(+%#$(++++++&'+%#)*) 5++30 The police officer detained the suspect at the scene of the crime  ARG0  ARG2  AM-loc V *+,"# -.,/, 0$,&)(1#, 2%(1#)%"  0$%)1")2'/34")+3&)&5$&)&5"6")5$7")&5") 6$#")#"$%'%/8 3456 (%$7%$1#)%"6 .+3/5& #.,6 8#%(9: -.,;6 6+,9 #.,68#%(96#%63456 (%$7%$1#)%": -.,6 8#%(96<186 .+3/5& =;63456 (%$7%$1#)%": -.,6 :34(5$6" %>6#.,68#%(96=;63456(%$7%$1#)%":::6 -.,6 8#%(96 :34(5$6" =;63456(%$7%$1#)%":::6 ? ;)!5$,,+1)!"#$%&'()*":4"6"%&$&'+%<) !"#$%&'()*+,"6 0$,&)(1#,86@=%'+.#A68%B&A67'$(.18,C6$,7$,8,"#61"6 "7"%& 6"#$%&'()4+,"6) ,D7$,886 #.,61=8#$1(#6$%B,6#.1#61$+'/,"#86%>616 7$,&)(1#,6(1"6#19,6)"6#.,6 ,E,"# F.3="4 :4+&+ >$/"%& $/"%& G%$,687,()>)( G%$,6+,",$1B  !"#$%&'()*+,") -$.",'%/ H,/1"#)(6I%B,8  ?"&&'%/)&+)6"#$%&'()4+,"6 J,% KL1E)&8%")1" ,E,"#6$,7$,8,"#1#)%"M H18.16=$%9,6#.,6<)"&%< 01#6%7,",&6#.,6&%%$ H'=N,(#86%>6=$,1961"&6%7,"M6 @4"$A"4 1"&6 B:"%"4 C"":)4+,"6) 87,()>)(6#%6,1(.6,E,"#6@=$,19)"+A6%7,")"+C O1$&6#%6$,18%"61=%'#6#.,/6>%$6J2P6177B)(1#)%"86B)9,6Q* R2CHAPTER 22¥SEMANTIC ROLE LABELING ThematicRoleDeÞnition AGENT Thevolitionalcauserofanevent EXPERIENCER Theexperiencerofanevent FORCE Thenon-volitionalcauseroftheevent THEME Theparticipantmostdirectlyaffectedbyanevent RESULT Theendproductofanevent CONTENT Thepropositionorcontentofapropositionalevent INSTRUMENT Aninstrumentusedinanevent BENEFICIARY ThebeneÞciaryofanevent SOURCE Theoriginoftheobjectofatransferevent GOAL Thedestinationofanobjectofatransferevent Figure22.1 SomecommonlyusedthematicroleswiththeirdeÞnitions. (22.1)Sashabrokethewindow. (22.2)Patopenedthedoor. Aneo-Davidsonianeventrepresentationofthesetwosentenceswouldbe !e,x,yBreaking (e)"Breaker (e,Sasha )"BrokenThing (e,y)"Window (y)!e,x,yOpening (e)"Opener (e,Pat )"OpenedThing (e,y)"Door(y)Inthisrepresentation,therolesofthesubjectsoftheverbs break andopenareBreaker andOpenerrespectively.These deeproles arespeciÞctoeachevent; Break- deeproles ingeventshave Breakers ,Openingeventshave Openers ,andsoon. Ifwearegoingtobeabletoanswerquestions,performinferences,ordoany furtherkindsofnaturallanguageunderstandingoftheseevents,weÕllneedtoknow alittlemoreaboutthesemanticsofthesearguments. Breakers andOpeners have somethingincommon.Theyarebothvolitionalactors,oftenanimate,andtheyhave directcausalresponsibilityfortheirevents. Thematicroles areawaytocapturethissemanticcommonalitybetween Break- Thematicroles ers andEaters .Wesaythatthesubjectsofboththeseverbsare agents.Thus, AGENT isthe agentsthematicrolethatrepresentsanabstractideasuchasvolitionalcausation.Similarly, thedirectobjectsofboththeseverbs,the BrokenThing andOpenedThing,areboth prototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction.The semanticrolefortheseparticipantsis theme.themeThematicrolesareoneoftheoldestlinguisticmodels,proposedÞrstbythe IndiangrammarianPaninisometimebetweenthe7thand4thcenturiesBCE.Their modernformulationisdueto Fillmore(1968) andGruber(1965) .Althoughthereis nouniversallyagreed-uponsetofroles,Figs. 22.1and22.2listsomethematicroles thathavebeenusedinvariouscomputationalpapers,togetherwithroughdeÞnitions andexamples.Mostthematicrolesetshaveaboutadozenroles,butweÕllseesets withsmallernumbersofroleswithevenmoreabstractmeanings,andsetswithvery largenumbersofrolesthatarespeciÞctosituations.WeÕllusethegeneralterm semanticroles forallsetsofroles,whethersmallorlarge. semanticroles  D5"#$&'()4+,"6 ¥@4"$A"4 1"&6 B:"%"4 .1E,68%/,#.)"+6)"6(%//%"S ¥T%B)#)%"1B61(#%$8 ¥U>#,"61")/1#, ¥L)$,(#6 (1'81B6$,87%"8)=)B)#;6>%$6#.,)$6 ,E,"#8 ¥-.,/1#)(6$%B,861$,616<1;6#%6(17#'$,6#.)868,/1"#)(6(%//%"1B)#;6 =,#<,,"6 !"#$%#"&' 1"&6 ($)#"& :6¥-.,;61$,6=%#.6 *VWJ-H :6¥-.,6 !"*%#+,-.+/ 1"&6 01#+#2,-.+/ A61$,6 -OWGWH :¥7$%#%#;7)(1BB;6 )"1")/1#,6%=N,(#86 1>>,(#,&6 )"68%/,6<1;6=;6#.,6 1(#)%" X D5"#$&'()4+,"6 ¥U",6%>6 #.,6%B&,8#6B)"+')8#)(6 /%&,B8 ¥!"&)1"6 +$1//1$)1"601")")6 =,#<,,"6 #.,6 R#.61"&6 ?#.6(,"#'$),86YZW6 ¥G%&,$"6>%$/'B1#)%"6>$%/6[)BB/%$,6@ \]^^A\]^X CA6V$'=,$6@ \]^FC¥[)BB/%$,6)">B',"(,&6=;62'(),"6 -,8")_$,`8 @\]F]C63456#+)& 2#' 78+)$9# 7)":;):"$4# <'#.,6=%%96#.1#6)"#$%&'(,&6&,7,"&,"(;6+$1//1$ ¥[)BB/%$,6 >)$8#6$,>,$$,&6#%6 $%B,86 186 $;)$+)& @[)BB/%$,A6 \]^^C6='#6 8<)#(.,&6 #%6 #.,6#,$/6 ;$&# ] D5"#$&'()4+,"6 ¥*6#;7)(1B68,#M \a2CHAPTER 22¥SEMANTIC ROLE LABELING ThematicRoleDeÞnition AGENT Thevolitionalcauserofanevent EXPERIENCER Theexperiencerofanevent FORCE Thenon-volitionalcauseroftheevent THEME Theparticipantmostdirectlyaffectedbyanevent RESULT Theendproductofanevent CONTENT Thepropositionorcontentofapropositionalevent INSTRUMENT Aninstrumentusedinanevent BENEFICIARY ThebeneÞciaryofanevent SOURCE Theoriginoftheobjectofatransferevent GOAL Thedestinationofanobjectofatransferevent Figure22.1 SomecommonlyusedthematicroleswiththeirdeÞnitions. (22.1)Sashabrokethewindow. (22.2)Patopenedthedoor. Aneo-Davidsonianeventrepresentationofthesetwosentenceswouldbe !e,x,yBreaking (e)"Breaker (e,Sasha )"BrokenThing (e,y)"Window (y)!e,x,yOpening (e)"Opener (e,Pat )"OpenedThing (e,y)"Door(y)Inthisrepresentation,therolesofthesubjectsoftheverbs break andopenareBreaker andOpenerrespectively.These deeproles arespeciÞctoeachevent; Break- deeproles ingeventshave Breakers ,Openingeventshave Openers ,andsoon. Ifwearegoingtobeabletoanswerquestions,performinferences,ordoany furtherkindsofnaturallanguageunderstandingoftheseevents,weÕllneedtoknow alittlemoreaboutthesemanticsofthesearguments. Breakers andOpeners have somethingincommon.Theyarebothvolitionalactors,oftenanimate,andtheyhave directcausalresponsibilityfortheirevents. Thematicroles areawaytocapturethissemanticcommonalitybetween Break- Thematicroles ers andEaters .Wesaythatthesubjectsofboththeseverbsare agents.Thus, AGENT isthe agentsthematicrolethatrepresentsanabstractideasuchasvolitionalcausation.Similarly, thedirectobjectsofboththeseverbs,the BrokenThing andOpenedThing,areboth prototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction.The semanticrolefortheseparticipantsis theme.themeThematicrolesareoneoftheoldestlinguisticmodels,proposedÞrstbythe IndiangrammarianPaninisometimebetweenthe7thand4thcenturiesBCE.Their modernformulationisdueto Fillmore(1968) andGruber(1965) .Althoughthereis nouniversallyagreed-uponsetofroles,Figs. 22.1and22.2listsomethematicroles thathavebeenusedinvariouscomputationalpapers,togetherwithroughdeÞnitions andexamples.Mostthematicrolesetshaveaboutadozenroles,butweÕllseesets withsmallernumbersofroleswithevenmoreabstractmeanings,andsetswithvery largenumbersofrolesthatarespeciÞctosituations.WeÕllusethegeneralterm semanticroles forallsetsofroles,whethersmallorlarge. semanticroles 22.2¥DIATHESIS ALTERNATIONS 3ThematicRoleExample AGENT Thewaiter spilledthesoup. EXPERIENCER John hasaheadache. FORCE Thewind blowsdebrisfromthemallintoouryards. THEME OnlyafterBenjaminFranklinbroke theice ...RESULT Thecitybuilta regulation-sizebaseballdiamond ...CONTENT Monaasked ÒYoumetMaryAnnatasupermarket?Ó INSTRUMENT HepoachedcatÞsh,stunningthem withashockingdevice ...BENEFICIARY WheneverAnnCallahanmakeshotelreservations forherboss ...SOURCE Ißewin fromBoston .GOAL Idrove toPortland .Figure22.2 Someprototypicalexamplesofvariousthematicroles. 22.2DiathesisAlternations Themainreasoncomputationalsystemsusesemanticrolesistoactasashallow meaningrepresentationthatcanletusmakesimpleinferencesthatarenÕtpossible fromthepuresurfacestringofwords,orevenfromtheparsetree.Toextendthe earlierexamples,ifadocumentsaysthat CompanyAacquiredCompanyB ,weÕd liketoknowthatthisanswersthequery WasCompanyBacquired? despitethefact thatthetwosentenceshaveverydifferentsurfacesyntax.Similarly,thisshallow semanticsmightactasausefulintermediatelanguageinmachinetranslation. Semanticrolesthushelpgeneralizeoverdifferentsurfacerealizationsofpred- icatearguments.Forexample,whilethe AGENT isoftenrealizedasthesubjectof thesentence,inothercasesthe THEME canbethesubject.Considerthesepossible realizationsofthethematicargumentsoftheverb break :(22.3)John AGENT brokethewindow. THEME (22.4)John AGENT brokethewindow THEME witharock. INSTRUMENT (22.5)Therock INSTRUMENT brokethewindow. THEME (22.6)Thewindow THEME broke. (22.7)Thewindow THEME wasbrokenbyJohn. AGENT Theseexamplessuggestthat break has(atleast)thepossiblearguments AGENT ,THEME ,and INSTRUMENT .Thesetofthematicroleargumentstakenbyaverbis oftencalledthe thematicgrid ,!-grid,or caseframe .Wecanseethatthereare thematicgrid caseframe (amongothers)thefollowingpossibilitiesfortherealizationoftheseargumentsof break :AGENT /Subject,THEME /ObjectAGENT /Subject,THEME /Object,INSTRUMENT /PPwithINSTRUMENT /Subject,THEME /ObjectTHEME /SubjectItturnsoutthatmanyverbsallowtheirthematicrolestoberealizedinvarious syntacticpositions.Forexample,verbslike givecanrealizethe THEME andGOAL argumentsintwodifferentways:  D5"#$&'()/4'9E)($6")24$#"E) F>/4'9 22.2¥DIATHESIS ALTERNATIONS 3ThematicRoleExample AGENT Thewaiter spilledthesoup. EXPERIENCER John hasaheadache. FORCE Thewind blowsdebrisfromthemallintoouryards. THEME OnlyafterBenjaminFranklinbroke theice ...RESULT Thecitybuilta regulation-sizebaseballdiamond ...CONTENT Monaasked ÒYoumetMaryAnnatasupermarket?Ó INSTRUMENT HepoachedcatÞsh,stunningthem withashockingdevice ...BENEFICIARY WheneverAnnCallahanmakeshotelreservations forherboss ...SOURCE Ißewin fromBoston .GOAL Idrove toPortland .Figure22.2 Someprototypicalexamplesofvariousthematicroles. 22.2DiathesisAlternations Themainreasoncomputationalsystemsusesemanticrolesistoactasashallow meaningrepresentationthatcanletusmakesimpleinferencesthatarenÕtpossible fromthepuresurfacestringofwords,orevenfromtheparsetree.Toextendthe earlierexamples,ifadocumentsaysthat CompanyAacquiredCompanyB ,weÕd liketoknowthatthisanswersthequery WasCompanyBacquired? despitethefact thatthetwosentenceshaveverydifferentsurfacesyntax.Similarly,thisshallow semanticsmightactasausefulintermediatelanguageinmachinetranslation. Semanticrolesthushelpgeneralizeoverdifferentsurfacerealizationsofpred- icatearguments.Forexample,whilethe AGENT isoftenrealizedasthesubjectof thesentence,inothercasesthe THEME canbethesubject.Considerthesepossible realizationsofthethematicargumentsoftheverb break :(22.3)John AGENT brokethewindow. THEME (22.4)John AGENT brokethewindow THEME witharock. INSTRUMENT (22.5)Therock INSTRUMENT brokethewindow. THEME (22.6)Thewindow THEME broke. (22.7)Thewindow THEME wasbrokenbyJohn. AGENT Theseexamplessuggestthat break has(atleast)thepossiblearguments AGENT ,THEME ,and INSTRUMENT .Thesetofthematicroleargumentstakenbyaverbis oftencalledthe thematicgrid ,!-grid,or caseframe .Wecanseethatthereare thematicgrid caseframe (amongothers)thefollowingpossibilitiesfortherealizationoftheseargumentsof break :AGENT /Subject,THEME /ObjectAGENT /Subject,THEME /Object,INSTRUMENT /PPwithINSTRUMENT /Subject,THEME /ObjectTHEME /SubjectItturnsoutthatmanyverbsallowtheirthematicrolestoberealizedinvarious syntacticpositions.Forexample,verbslike givecanrealizethe THEME andGOAL argumentsintwodifferentways: \\thematic grid,  case frame,  F-grid Break: AGENT , THEME,  INSTRUMENT . 22.2¥DIATHESIS ALTERNATIONS 3ThematicRoleExample AGENT Thewaiter spilledthesoup. EXPERIENCER John hasaheadache. FORCE Thewind blowsdebrisfromthemallintoouryards. THEME OnlyafterBenjaminFranklinbroke theice ...RESULT Thecitybuilta regulation-sizebaseballdiamond ...CONTENT Monaasked ÒYoumetMaryAnnatasupermarket?Ó INSTRUMENT HepoachedcatÞsh,stunningthem withashockingdevice ...BENEFICIARY WheneverAnnCallahanmakeshotelreservations forherboss ...SOURCE Ißewin fromBoston .GOAL Idrove toPortland .Figure22.2 Someprototypicalexamplesofvariousthematicroles. 22.2DiathesisAlternations Themainreasoncomputationalsystemsusesemanticrolesistoactasashallow meaningrepresentationthatcanletusmakesimpleinferencesthatarenÕtpossible fromthepuresurfacestringofwords,orevenfromtheparsetree.Toextendthe earlierexamples,ifadocumentsaysthat CompanyAacquiredCompanyB ,weÕd liketoknowthatthisanswersthequery WasCompanyBacquired? despitethefact thatthetwosentenceshaveverydifferentsurfacesyntax.Similarly,thisshallow semanticsmightactasausefulintermediatelanguageinmachinetranslation. Semanticrolesthushelpgeneralizeoverdifferentsurfacerealizationsofpred- icatearguments.Forexample,whilethe AGENT isoftenrealizedasthesubjectof thesentence,inothercasesthe THEME canbethesubject.Considerthesepossible realizationsofthethematicargumentsoftheverb break :(22.3)John AGENT brokethewindow. THEME (22.4)John AGENT brokethewindow THEME witharock. INSTRUMENT (22.5)Therock INSTRUMENT brokethewindow. THEME (22.6)Thewindow THEME broke. (22.7)Thewindow THEME wasbrokenbyJohn. AGENT Theseexamplessuggestthat break has(atleast)thepossiblearguments AGENT ,THEME ,and INSTRUMENT .Thesetofthematicroleargumentstakenbyaverbis oftencalledthe thematicgrid ,!-grid,or caseframe .Wecanseethatthereare thematicgrid caseframe (amongothers)thefollowingpossibilitiesfortherealizationoftheseargumentsof break :AGENT /Subject,THEME /ObjectAGENT /Subject,THEME /Object,INSTRUMENT /PPwithINSTRUMENT /Subject,THEME /ObjectTHEME /SubjectItturnsoutthatmanyverbsallowtheirthematicrolestoberealizedinvarious syntacticpositions.Forexample,verbslike givecanrealizethe THEME andGOAL argumentsintwodifferentways: WD1/7B,6'81+,86%>6b=$,19c H%/,6$,1B)d1#)%"8M  C'$&5"6'6)$,&"4%$&'+%6)G+4)7"4.)$,&"4%$&'+%H C$&'7")$,&"4%$&'+% M671$#)('B1$68,/1"#)(6 (B188,86%>6E,$=8A6 bE,$=86%>6>'#'$,6.1E)"+c6 @$2=$+;# A6$44*;$)# A6*>>#" A6*?# CA6b8,"&6E,$=8c6@ >*"?$"2 A6-$+2A66$.4 CA6bE,$=86%>6 #.$%<)"+c6@ %.;% A61$&&A6)-"*? CA6,#(: -"7'%)G IJJKH<) ?R68,/1"#)(6(B188,86@b -"7'%)(,$66"6 cC6>%$6e\aa6W"+B)8.6E,$=861"&6 1B#,$"1#)%"8:6!"6%"B)",6$,8%'$(,6 T,$=J,# :\f4CHAPTER 22¥SEMANTIC ROLE LABELING (22.8)a.DorisAGENT gavethebook THEME toCary. GOAL b.DorisAGENT gaveCary GOAL thebook. THEME Thesemultipleargumentstructurerealizations(thefactthat break cantake AGENT ,INSTRUMENT ,or THEME assubject,and givecanrealizeits THEME andGOAL ineitherorder)arecalled verbalternations ordiathesisalternations .Thealternation verb alternation weshowedabovefor give,the dativealternation ,seemstooccurwithparticularse- dative alternation manticclassesofverbs,includingÒverbsoffuturehavingÓ( advance,allocate,offer ,owe),ÒsendverbsÓ( forward ,hand,mail),ÒverbsofthrowingÓ( kick ,pass,throw ),andsoon. Levin(1993) listsfor3100Englishverbsthesemanticclassestowhich theybelong(47high-levelclasses,dividedinto193morespeciÞcclasses)andthe variousalternationsinwhichtheyparticipate.Theselistsofverbclasseshavebeen incorporatedintotheonlineresourceVerbNet (Kipperetal.,2000) ,whichlinkseach verbtobothWordNetandFrameNetentries. 22.3SemanticRoles:ProblemswithThematicRoles Representingmeaningatthethematicrolelevelseemslikeitshouldbeusefulin dealingwithcomplicationslikediathesisalternations.YetithasprovedquitedifÞ- culttocomeupwithastandardsetofroles,andequallydifÞculttoproduceaformal deÞnitionofroleslike AGENT ,THEME ,or INSTRUMENT .Forexample,researchersattemptingtodeÞnerolesetsoftenÞndtheyneedto fragmentarolelike AGENT orTHEME intomanyspeciÞcroles. LevinandRappa- portHovav(2005) summarizeanumberofsuchcases,suchasthefactthereseem tobeatleasttwokindsof INSTRUMENTS ,intermediaryinstrumentsthatcanappear assubjectsand enablinginstrumentsthatcannot: (22.9)a.Thecookopenedthejarwiththenewgadget. b.Thenewgadgetopenedthejar. (22.10)a.Shellyatetheslicedbananawithafork. b.*Theforkatetheslicedbanana. Inadditiontothefragmentationproblem,therearecasesinwhichweÕdliketo reasonaboutandgeneralizeacrosssemanticroles,buttheÞnitediscretelistsofroles donÕtletusdothis. Finally,ithasproveddifÞculttoformallydeÞnethethematicroles.Considerthe AGENT role;mostcasesof AGENTS areanimate,volitional,sentient,causal,butany individualnounphrasemightnotexhibitalloftheseproperties. Theseproblemshaveledtoalternative semanticrole modelsthatuseeither semanticrole manyfewerormanymoreroles. TheÞrstoftheseoptionsistodeÞne generalizedsemanticroles thatabstract overthespeciÞcthematicroles.Forexample, PROTO -AGENT andPROTO -PATIENT proto-agent proto-patient aregeneralizedrolesthatexpressroughlyagent-likeandroughlypatient-likemean- ings.TheserolesaredeÞned,notbynecessaryandsufÞcientconditions,butrather byasetofheuristicfeaturesthataccompanymoreagent-likeormorepatient-like meanings.Thus,themoreanargumentdisplaysagent-likeproperties(beingvoli- tionallyinvolvedintheevent,causinganeventorachangeofstateinanotherpar- ticipant,beingsentientorintentionallyinvolved,moving)thegreaterthelikelihood Break:  AGENT , INSTRUMENT, or THEME as  subject Give:   THEME  and GOAL  in either order  L4+.,"#6)1'&5)D5"#$&'()*+,"6 O1$&6#%6($,1#,68#1"&1$&6 8,#6%>6 $%B,86%$6>%$/1BB;6&,>)",6#.,/ U>#,"6$%B,86",,&6#%6=,6>$1+/,"#,&6#%6=,6&,>)",&: 2,E)"61"&6I17717%$#6 O%E1E @fa\FCM6#<%69)"&86%>6 !JH-IPGWJ-H '%&"4#"9'$4= '%6&43#"%&6) #.1#6(1"6177,1$61868'=N,(#86 -.,6(%%96%7,",&6#.,6N1$6<)#.6#.,6",<6+1&+,#:6 -.,6",<6+1&+,#6%7,",&6#.,6N1$:6 "%$.,'%/) '%6&43#"%&6) #.1#6 (1""%# H.,BB;6 1#,6#.,68B)(,&6=1"1"16<)#.616>%$9:6 g-.,6 >%$961#,6#.,68B)(,&6=1"1"1:6 \e ;,&"4%$&'7"6)&+)&5"#$&'()4+,"6 IMN"1"4)4+,"6 M6+,",$1B)d,&6 8,/1"#)(6 $%B,8A6&,>)",&6186 7$%#%#;7,86@ L%<#; \]]\C0IU-U K*VWJ-6 0IU-U K0*-!WJ-6 OMP+4")4+,"6 M6L,>)",6$%B,8687,()>)(6#%616+$%'76%>67$,&)(1#,8 \?N4$#"Q"& L4+:@$%A  !"#$%&'()*+,") -$.",'%/ -.,60$%7%8)#)%"6Y1"96 @0$%7Y1"9 C L4+:@$%A ¥01B/,$A6G1$#.1A6L1"),B6 V)B&,1 A61"&601'B6h)"+8='$;:6faaF:6-.,6 0$%7%8)#)%"6Y1"9M6*"6*""%#1#,&6Z%$7'86%>6H,/1"#)(6I%B,8:6 @*61:)$).*+$4'A.+/:.&).;& A6e\@\CMR\ i\a^6\^ L4+:@$%A *+,"6 0$%#% K*+,"# ¥T%B)#)%"1B6)"E%BE,/,"#6)"6,E,"#6%$68#1#, ¥H,"#),"(,6 @1"&j%$6 7,$(,7#)%"C ¥Z1'8,86 1"6,E,"#6%$6(.1"+,6%>68#1#,6)"61"%#.,$671$#)()71"#6 ¥G%E,/,"#6 @$,B1#)E,6#%67%8)#)%"6%>61"%#.,$671$#)()71"# C0$%#% K01#),"# ¥P"&,$+%,86(.1"+,6%>6 8#1#, ¥Z1'81BB;6 1>>,(#,&6=;61"%#.,$6 71$#)()71"# ¥H#1#)%"1$;6 $,B1#)E,6#%6/%E,/,"#6%>61"%#.,$6 71$#)()71"# \RFollowing  Dowty 1991 L4+:@$%A *+,"6 ¥[%BB%<)"+6 L%<#; \]]\¥I%B,6&,>)")#)%"86 &,#,$/)",&6 E,$=6=;6E,$=A6 <)#.6 $,87,(#6#%6#.,6%#.,$6$%B,86 ¥H,/1"#)(6 $%B,86)"6 0$%7Y1"9 1$,6 #.'86E,$= K8,"8,687,()>)(: ¥W1(.6E,$=68,"8,6.186"'/=,$,&61$+'/,"#M6 *$+aA6*$+\A6 *$+fAk *$+aM60IU-U K*VWJ- *$+\ M0IU-U K0*-!WJ- *$+fM6'8'1BB;M6 =,",>1(#)E, A6)"8#$'/,"#A61##$)='#,A6%$6,"&6 8#1#, *$+eM6'8'1BB;M68#1$#6 7%)"#A6 =,",>1(#)E, A6)"8#$'/,"#A6%$6 1##$)='#, *$+?6 #.,6,"&6 7%)"# BC"/D EC"/F'$"#'+*)'"#$448')-$)';*+&.&)#+)<';$:&#&'$'1"*G4#6'>*"'4$G#4.+/H \X L4+:@$%A N4$#")N',"6 \]22.4¥THEPROPOSITION BANK 5thattheargumentcanbelabeleda PROTO -AGENT .Themorepatient-liketheproper- ties(undergoingchangeofstate,causallyaffectedbyanotherparticipant,stationary relativetootherparticipants,etc.),thegreaterthelikelihoodthattheargumentcan belabeleda PROTO -PATIENT .TheseconddirectionisinsteadtodeÞnesemanticrolesthatarespeciÞctoa particularverboraparticulargroupofsemanticallyrelatedverbsornouns. Inthenexttwosectionswedescribetwocommonlyusedlexicalresourcesthat makeuseofthesealternativeversionsofsemanticroles. PropBank usesbothproto- rolesandverb-speciÞcsemanticroles. FrameNetusessemanticrolesthatarespe- ciÞctoageneralsemanticideacalleda frame .22.4ThePropositionBank ThePropositionBank ,generallyreferredtoas PropBank ,isaresourceofsen- PropBank tencesannotatedwithsemanticroles.TheEnglishPropBanklabelsallthesentences inthePennTreeBank;theChinesePropBanklabelssentencesinthePennChinese TreeBank.BecauseofthedifÞcultyofdeÞningauniversalsetofthematicroles, thesemanticrolesinPropBankaredeÞnedwithrespecttoanindividualverbsense. EachsenseofeachverbthushasaspeciÞcsetofroles,whicharegivenonlynumbers ratherthannames: Arg0 ,Arg1 ,Arg2 ,andsoon.Ingeneral, Arg0 representsthe PROTO -AGENT ,and Arg1 ,the PROTO -PATIENT .Thesemanticsoftheotherroles arelessconsistent,oftenbeingdeÞnedspeciÞcallyforeachverb.Nonethelessthere aresomegeneralization;the Arg2 isoftenthebenefactive,instrument,attribute,or endstate,the Arg3 thestartpoint,benefactive,instrument,orattribute,andthe Arg4 theendpoint. HerearesomeslightlysimpliÞedPropBankentriesforonesenseeachofthe verbs agree andfall.SuchPropBankentriesarecalled frameÞles ;notethatthe deÞnitionsintheframeÞleforeachrole(ÒOtherentityagreeingÓ,ÒExtent,amount fallenÓ)areinformalglossesintendedtobereadbyhumans,ratherthanbeingformal deÞnitions.(22.11)agree.01 Arg0:Agreer Arg1:Proposition Arg2:Otherentityagreeing Ex1:[ Arg0 Thegroup] agreed [Arg1 itwouldnÕtmakeanoffer]. Ex2:[ ArgM-TMP Usually][ Arg0 John]agrees [Arg2 withMary] [Arg1 oneverything]. (22.12)fall.01Arg1:Logicalsubject,patient,thingfalling Arg2:Extent,amountfallen Arg3:startpoint Arg4:endpoint,endstateofarg1 Ex1:[ Arg1 Sales]fell[Arg4 to$25million][ Arg3 from$27million]. Ex2:[ Arg1 Theaveragejunkbond] fell[Arg2 by4.2%]. NotethatthereisnoArg0rolefor fall,becausethenormalsubjectof fallisa PROTO -PATIENT .22.4¥THEPROPOSITION BANK 5thattheargumentcanbelabeleda PROTO -AGENT .Themorepatient-liketheproper- ties(undergoingchangeofstate,causallyaffectedbyanotherparticipant,stationary relativetootherparticipants,etc.),thegreaterthelikelihoodthattheargumentcan belabeleda PROTO -PATIENT .TheseconddirectionisinsteadtodeÞnesemanticrolesthatarespeciÞctoa particularverboraparticulargroupofsemanticallyrelatedverbsornouns. Inthenexttwosectionswedescribetwocommonlyusedlexicalresourcesthat makeuseofthesealternativeversionsofsemanticroles. PropBank usesbothproto- rolesandverb-speciÞcsemanticroles. FrameNetusessemanticrolesthatarespe- ciÞctoageneralsemanticideacalleda frame .22.4ThePropositionBank ThePropositionBank ,generallyreferredtoas PropBank ,isaresourceofsen- PropBank tencesannotatedwithsemanticroles.TheEnglishPropBanklabelsallthesentences inthePennTreeBank;theChinesePropBanklabelssentencesinthePennChinese TreeBank.BecauseofthedifÞcultyofdeÞningauniversalsetofthematicroles, thesemanticrolesinPropBankaredeÞnedwithrespecttoanindividualverbsense. EachsenseofeachverbthushasaspeciÞcsetofroles,whicharegivenonlynumbers ratherthannames: Arg0 ,Arg1 ,Arg2 ,andsoon.Ingeneral, Arg0 representsthe PROTO -AGENT ,and Arg1 ,the PROTO -PATIENT .Thesemanticsoftheotherroles arelessconsistent,oftenbeingdeÞnedspeciÞcallyforeachverb.Nonethelessthere aresomegeneralization;the Arg2 isoftenthebenefactive,instrument,attribute,or endstate,the Arg3 thestartpoint,benefactive,instrument,orattribute,andthe Arg4 theendpoint. HerearesomeslightlysimpliÞedPropBankentriesforonesenseeachofthe verbs agree andfall.SuchPropBankentriesarecalled frameÞles ;notethatthe deÞnitionsintheframeÞleforeachrole(ÒOtherentityagreeingÓ,ÒExtent,amount fallenÓ)areinformalglossesintendedtobereadbyhumans,ratherthanbeingformal deÞnitions.(22.11)agree.01 Arg0:Agreer Arg1:Proposition Arg2:Otherentityagreeing Ex1:[ Arg0 Thegroup] agreed [Arg1 itwouldnÕtmakeanoffer]. Ex2:[ ArgM-TMP Usually][ Arg0 John]agrees [Arg2 withMary] [Arg1 oneverything]. (22.12)fall.01Arg1:Logicalsubject,patient,thingfalling Arg2:Extent,amountfallen Arg3:startpoint Arg4:endpoint,endstateofarg1 Ex1:[ Arg1 Sales]fell[Arg4 to$25million][ Arg3 from$27million]. Ex2:[ Arg1 Theaveragejunkbond] fell[Arg2 by4.2%]. NotethatthereisnoArg0rolefor fall,becausethenormalsubjectof fallisa PROTO -PATIENT . ;97$%&$/")+2)$) L4+.@$%A -$.",'%/ 6CHAPTER 22¥SEMANTIC ROLE LABELING ThePropBanksemanticrolescanbeusefulinrecoveringshallowsemanticin- formationaboutverbalarguments.Considertheverb increase :(22.13)increase.01 ÒgoupincrementallyÓ Arg0:causerofincrease Arg1:thingincreasing Arg2:amountincreasedby,EXT,orMNR Arg3:startpoint Arg4:endpoint APropBanksemanticrolelabelingwouldallowustoinferthecommonalityin theeventstructuresofthefollowingthreeexamples,thatis,thatineachcase BigFruitCo. isthe AGENT andthepriceofbananas isthe THEME ,despitethediffering surfaceforms. (22.14)[Arg0 BigFruitCo.]increased[ Arg1 thepriceofbananas]. (22.15)[Arg1 Thepriceofbananas]wasincreasedagain[ Arg0 byBigFruitCo.] (22.16)[Arg1 Thepriceofbananas]increased[ Arg2 5%].PropBankalsohasanumberofnon-numberedargumentscalled ArgMs ,(ArgM- TMP,ArgM-LOC,etc)whichrepresentmodiÞcationoradjunctmeanings.Theseare relativelystableacrosspredicates,soarenÕtlistedwitheachframeÞle.Datalabeled withthesemodiÞerscanbehelpfulintrainingsystemstodetecttemporal,location, ordirectionalmodiÞcationacrosspredicates.SomeoftheArgMÕsinclude: TMPwhen?yesterdayevening,now LOCwhere?atthemuseum,inSanFrancisco DIRwhereto/from?down,toBangkok MNRhow?clearly,withmuchenthusiasm PRP/CAU why?because...,inresponsetotheruling RECthemselves,eachother ADV miscellaneousPRDsecondarypredication...atethemeatraw WhilePropBankfocusesonverbs,arelatedproject,NomBank (Meyersetal., 2004)addsannotationstonounpredicates.Forexamplethenoun agreement inAppleÕsagreementwithIBM wouldbelabeledwithAppleastheArg0andIBMas theArg2.Thisallowssemanticrolelabelerstoassignlabelstoargumentsofboth verbalandnominalpredicates. 22.5FrameNet Whilemakinginferencesaboutthesemanticcommonalitiesacrossdifferentsen- tenceswith increase isuseful,itwouldbeevenmoreusefulifwecouldmakesuch inferencesinmanymoresituations,acrossdifferentverbs,andalsobetweenverbs andnouns.Forexample,weÕdliketoextractthesimilarityamongthesethreesen- tences:(22.17)[Arg1 Thepriceofbananas]increased[ Arg2 5%].(22.18)[Arg1 Thepriceofbananas]rose[ Arg2 5%].(22.19)Therehasbeena[ Arg2 5%]rise[ Arg1 inthepriceofbananas]. Notethatthesecondexampleusesthedifferentverb rise,andthethirdexample usesthenounratherthantheverb rise.WeÕdlikeasystemtorecognizethat thefa6CHAPTER 22¥SEMANTIC ROLE LABELING ThePropBanksemanticrolescanbeusefulinrecoveringshallowsemanticin- formationaboutverbalarguments.Considertheverb increase :(22.13)increase.01 ÒgoupincrementallyÓ Arg0:causerofincrease Arg1:thingincreasing Arg2:amountincreasedby,EXT,orMNR Arg3:startpoint Arg4:endpoint APropBanksemanticrolelabelingwouldallowustoinferthecommonalityin theeventstructuresofthefollowingthreeexamples,thatis,thatineachcase BigFruitCo. isthe AGENT andthepriceofbananas isthe THEME ,despitethediffering surfaceforms. (22.14)[Arg0 BigFruitCo.]increased[ Arg1 thepriceofbananas]. (22.15)[Arg1 Thepriceofbananas]wasincreasedagain[ Arg0 byBigFruitCo.] (22.16)[Arg1 Thepriceofbananas]increased[ Arg2 5%].PropBankalsohasanumberofnon-numberedargumentscalled ArgMs ,(ArgM- TMP,ArgM-LOC,etc)whichrepresentmodiÞcationoradjunctmeanings.Theseare relativelystableacrosspredicates,soarenÕtlistedwitheachframeÞle.Datalabeled withthesemodiÞerscanbehelpfulintrainingsystemstodetecttemporal,location, ordirectionalmodiÞcationacrosspredicates.SomeoftheArgMÕsinclude: TMPwhen?yesterdayevening,now LOCwhere?atthemuseum,inSanFrancisco DIRwhereto/from?down,toBangkok MNRhow?clearly,withmuchenthusiasm PRP/CAU why?because...,inresponsetotheruling RECthemselves,eachother ADV miscellaneousPRDsecondarypredication...atethemeatraw WhilePropBankfocusesonverbs,arelatedproject,NomBank (Meyersetal., 2004)addsannotationstonounpredicates.Forexamplethenoun agreement inAppleÕsagreementwithIBM wouldbelabeledwithAppleastheArg0andIBMas theArg2.Thisallowssemanticrolelabelerstoassignlabelstoargumentsofboth verbalandnominalpredicates. 22.5FrameNet Whilemakinginferencesaboutthesemanticcommonalitiesacrossdifferentsen- tenceswith increase isuseful,itwouldbeevenmoreusefulifwecouldmakesuch inferencesinmanymoresituations,acrossdifferentverbs,andalsobetweenverbs andnouns.Forexample,weÕdliketoextractthesimilarityamongthesethreesen- tences:(22.17)[Arg1 Thepriceofbananas]increased[ Arg2 5%].(22.18)[Arg1 Thepriceofbananas]rose[ Arg2 5%].(22.19)Therehasbeena[ Arg2 5%]rise[ Arg1 inthepriceofbananas]. Notethatthesecondexampleusesthedifferentverb rise,andthethirdexample usesthenounratherthantheverb rise.WeÕdlikeasystemtorecognizethat the-.)86<%'B&61BB%<6'86#%68,,6#.,6(%//%"1B)#),86)"6#.,8,6e68,"#,"(,8M  P+9'2'"46)+4)$9R3%(&6)+2)&5"):4"9'($&"<) ;4/ >P6CHAPTER 22¥SEMANTIC ROLE LABELING ThePropBanksemanticrolescanbeusefulinrecoveringshallowsemanticin- formationaboutverbalarguments.Considertheverb increase :(22.13)increase.01 ÒgoupincrementallyÓ Arg0:causerofincrease Arg1:thingincreasing Arg2:amountincreasedby,EXT,orMNR Arg3:startpoint Arg4:endpoint APropBanksemanticrolelabelingwouldallowustoinferthecommonalityin theeventstructuresofthefollowingthreeexamples,thatis,thatineachcase BigFruitCo. isthe AGENT andthepriceofbananas isthe THEME ,despitethediffering surfaceforms. (22.14)[Arg0 BigFruitCo.]increased[ Arg1 thepriceofbananas]. (22.15)[Arg1 Thepriceofbananas]wasincreasedagain[ Arg0 byBigFruitCo.] (22.16)[Arg1 Thepriceofbananas]increased[ Arg2 5%].PropBankalsohasanumberofnon-numberedargumentscalled ArgMs ,(ArgM- TMP,ArgM-LOC,etc)whichrepresentmodiÞcationoradjunctmeanings.Theseare relativelystableacrosspredicates,soarenÕtlistedwitheachframeÞle.Datalabeled withthesemodiÞerscanbehelpfulintrainingsystemstodetecttemporal,location, ordirectionalmodiÞcationacrosspredicates.SomeoftheArgMÕsinclude: TMPwhen?yesterdayevening,now LOCwhere?atthemuseum,inSanFrancisco DIRwhereto/from?down,toBangkok MNRhow?clearly,withmuchenthusiasm PRP/CAU why?because...,inresponsetotheruling RECthemselves,eachother ADV miscellaneousPRDsecondarypredication...atethemeatraw WhilePropBankfocusesonverbs,arelatedproject,NomBank (Meyersetal., 2004)addsannotationstonounpredicates.Forexamplethenoun agreement inAppleÕsagreementwithIBM wouldbelabeledwithAppleastheArg0andIBMas theArg2.Thisallowssemanticrolelabelerstoassignlabelstoargumentsofboth verbalandnominalpredicates. 22.5FrameNet Whilemakinginferencesaboutthesemanticcommonalitiesacrossdifferentsen- tenceswith increase isuseful,itwouldbeevenmoreusefulifwecouldmakesuch inferencesinmanymoresituations,acrossdifferentverbs,andalsobetweenverbs andnouns.Forexample,weÕdliketoextractthesimilarityamongthesethreesen- tences:(22.17)[Arg1 Thepriceofbananas]increased[ Arg2 5%].(22.18)[Arg1 Thepriceofbananas]rose[ Arg2 5%].(22.19)Therehasbeena[ Arg2 5%]rise[ Arg1 inthepriceofbananas]. Notethatthesecondexampleusesthedifferentverb rise,andthethirdexample usesthenounratherthantheverb rise.WeÕdlikeasystemtorecognizethat thef\ArgM - L4+:@$%A'%/ $)!"%&"%(" PropBank - A  TreeBanked  Sentence  Analysts  S NP-SBJ  VP  have  VP  been  VP  expecting  NP  a GM-Jaguar  pact  NP  that  SBAR  WHNP-1 *T*-1  S NP-SBJ  VP  would  VP  give  the US car  maker  NP  NP  an eventual  30% stake  NP  the British  company  NP  PP-LOC  in  (S (NP-SBJ  Analysts )      (VP  have           (VP  been              (VP  expecting             (NP (NP  a GM-Jaguar pact )                    (SBAR (WHNP -1 that)                  (S (NP-SBJ  *T*-1 )                             (VP  would               (VP  give                                     (NP  the U.S. car maker )                  (NP (NP  an eventual  (ADJP  30 %) stake )              (PP-LOC  in (NP  the British company )))))))))))) Analysts have been expecting a GM-Jaguar   pact that  would give the U.S. car maker an   eventual 30% stake in the British company.  ffG1$#.1601B/,$6fa\e A sample  parse  tree  D5") 6$#"):$46")&4"") L4+:@$%A"9 The same sentence, PropBanked  Analysts  have been expecting  a GM-Jaguar  pact  Arg0  Arg1  (S Arg0  (NP-SBJ  Analysts )      (VP  have           (VP  been              (VP  expecting             Arg1  (NP (NP  a GM-Jaguar pact )                    (SBAR (WHNP -1 that)                        (S  Arg0  (NP-SBJ  *T*-1 )                             (VP  would                     (VP  give                                           Arg2  (NP  the U.S. car maker )                     Arg1  (NP (NP  an eventual  (ADJP  30 % ) stake)               (PP-LOC  in (NP  the British  company )))))))))))) that would give  *T*-1  the US car  maker  an eventual 30% stake in the  British company   Arg0  Arg2  Arg1  expect(Analysts, GM-J pact)  give(GM-J pact, US car maker, 30% stake)  feG1$#.1601B/,$6fa\e  ;%%+&$&"9) L4+:@$%A C$&$ ¥0,""6W"+B)8.6 -$,,Y1"9 A6U"#%J%#,8 F:a:6 ¥-%#1B6lf6/)BB)%"6<%$&8 ¥0,""6Z.)",8,6 -$,,Y1"9 ¥O)"&)jP$&'6 0$%7Y1"9 ¥*$1=)(6 0$%7Y1"9 f?Verb Frames Coverage By Language Ð   Current Count of Senses (lexical units)  Language Final Count  Estimated Coverage  in Running Text  English    10,615*  99% Chinese  24, 642  98% Arabic      7,015  99%  ¥!Only 111 English adjectives  54 fa\e6T,$=6 [$1/,86Z%E,$1+,6 Z%'"#6%>6<%$&68,"8,6@B,D)(1B6'")#8C [$%/6G1$#.1601B/,$6fa\e6-'#%$)1B  L,36)%+3%6)$%9),'/5&)7"4.6 English Noun and LVC annotation  !!Example Noun:  Decision  !!Roleset:  Arg0: decider , Arg1: decision É !!ÒÉ[ your ARG0 ] [ decision REL]       [to say look I don't want to go through this anymore ARG1 ]Ó  !!Example within an LVC:  Make a decision  !!ÒÉ[ the President ARG0 ] [ made REL-LVB ]        the [ fundamentally  correct ARGM -ADJ ]       [decision REL]  [ to get on offense ARG1 ]Ó  57 fFHB)&,6 >$%/601B/,$6fa\e  !"#$%&'()*+,") -$.",'%/ [$1/,J,#  0$:&34'%/)9"6(4':&'+%6)+2)&5")6$#")"7"%&) .=)9'22"4"%&)%+3%6S7"4.6 6CHAPTER 22¥SEMANTIC ROLE LABELING ThePropBanksemanticrolescanbeusefulinrecoveringshallowsemanticin- formationaboutverbalarguments.Considertheverb increase :(22.13)increase.01 ÒgoupincrementallyÓ Arg0:causerofincrease Arg1:thingincreasing Arg2:amountincreasedby,EXT,orMNR Arg3:startpoint Arg4:endpoint APropBanksemanticrolelabelingwouldallowustoinferthecommonalityin theeventstructuresofthefollowingthreeexamples,thatis,thatineachcase BigFruitCo. isthe AGENT andthepriceofbananas isthe THEME ,despitethediffering surfaceforms. (22.14)[Arg0 BigFruitCo.]increased[ Arg1 thepriceofbananas]. (22.15)[Arg1 Thepriceofbananas]wasincreasedagain[ Arg0 byBigFruitCo.] (22.16)[Arg1 Thepriceofbananas]increased[ Arg2 5%].PropBankalsohasanumberofnon-numberedargumentscalled ArgMs ,(ArgM- TMP,ArgM-LOC,etc)whichrepresentmodiÞcationoradjunctmeanings.Theseare relativelystableacrosspredicates,soarenÕtlistedwitheachframeÞle.Datalabeled withthesemodiÞerscanbehelpfulintrainingsystemstodetecttemporal,location, ordirectionalmodiÞcationacrosspredicates.SomeoftheArgMÕsinclude: TMPwhen?yesterdayevening,now LOCwhere?atthemuseum,inSanFrancisco DIRwhereto/from?down,toBangkok MNRhow?clearly,withmuchenthusiasm PRP/CAU why?because...,inresponsetotheruling RECthemselves,eachother ADV miscellaneousPRDsecondarypredication...atethemeatraw WhilePropBankfocusesonverbs,arelatedproject,NomBank (Meyersetal., 2004)addsannotationstonounpredicates.Forexamplethenoun agreement inAppleÕsagreementwithIBM wouldbelabeledwithAppleastheArg0andIBMas theArg2.Thisallowssemanticrolelabelerstoassignlabelstoargumentsofboth verbalandnominalpredicates. 22.5FrameNet Whilemakinginferencesaboutthesemanticcommonalitiesacrossdifferentsen- tenceswith increase isuseful,itwouldbeevenmoreusefulifwecouldmakesuch inferencesinmanymoresituations,acrossdifferentverbs,andalsobetweenverbs andnouns.Forexample,weÕdliketoextractthesimilarityamongthesethreesen- tences:(22.17)[Arg1 Thepriceofbananas]increased[ Arg2 5%].(22.18)[Arg1 Thepriceofbananas]rose[ Arg2 5%].(22.19)Therehasbeena[ Arg2 5%]rise[ Arg1 inthepriceofbananas]. Notethatthesecondexampleusesthedifferentverb rise,andthethirdexample usesthenounratherthantheverb rise.WeÕdlikeasystemtorecognizethat thefR N4$#"Q"& ¥Y19,$6 ,#61B:6\]]XA6[)BB/%$,6,#61B:6faaeA6[)BB/%$,61"&6Y19,$6faa]A6 I'77,".%>,$ ,#61B:6faa^6 ¥I%B,86)"6 0$%7Y1"9 1$,687,()>)(6#%616E,$= ¥I%B,6)"6 [$1/,J,# 1$,687,()>)(6#%616 24$#"<)$) =1(9+$%'"&6 9"%<B,&+,68#$'(#'$,6#.1#6&,>)",86168,#6%>6>$1/, K87,()>)(6 8,/1"#)(6$%B,8A6(1BB,& 24$#")","#"%&6 A6¥)"(B'&,86 168,#6%>6 7$,& (1#,86#.1#6'8,6#.,8,6 $%B,8 ¥,1(.6 <%$&6,E%9,8616>$1/,61"&67$%>)B,868%/,6187,(#6%>6#.,6 >$1/, fX D5")T05$%/"):+6'&'+%)+%)$)6($,"U)N4$#" -.)86 >$1/,6(%"8)8#86%>6<%$&86#.1#6)"&)(1#,6#.,6(.1"+,6%>61"6 !-WG `86 7%8)#)%"6 %"6168(1B,6@#.,6 *--I!YP-W C6>$%/6168#1$#)"+67%)"#6@ !J!-!*2 T*2PW C6#%61"6,"&67%)"#6@ [!J*2 T*2PW Cf]22.5¥FRAMENET7priceofbananas iswhatwentup,andthat 5%istheamountitwentup,nomatter whetherthe 5%appearsastheobjectoftheverb increased orasanominalmodiÞer ofthenoun rise.TheFrameNetprojectisanothersemantic-role-labelingprojectthatattempts FrameNettoaddressjustthesekindsofproblems( Bakeretal.1998 ,Fillmoreetal.2003 ,FillmoreandBaker2009 ,Ruppenhoferetal.2006 ).WhereasrolesinthePropBank projectarespeciÞctoanindividualverb,rolesintheFrameNetprojectarespeciÞc toa frame.Whatisaframe?Considerthefollowingsetofwords: reservation,ßight,travel,buy,price,cost,fare,rates,meal,plane Therearemanyindividuallexicalrelationsofhyponymy,synonymy,andsoon betweenmanyofthewordsinthislist.Theresultingsetofrelationsdoesnot, however,adduptoacompleteaccountofhowthesewordsarerelated.Theyare clearlyalldeÞnedwithrespecttoacoherentchunkofcommon-sensebackground informationconcerningairtravel. Wecalltheholisticbackgroundknowledgethatunitesthesewordsa frame(Fill-framemore,1985) .TheideathatgroupsofwordsaredeÞnedwithrespecttosomeback- groundinformationiswidespreadinartiÞcialintelligenceandcognitivescience, wherebesides frameweseerelatedworkslikea model(Johnson-Laird,1983) ,or modeleven script(SchankandAbelson,1977) .scriptAframeinFrameNetisabackgroundknowledgestructurethatdeÞnesasetof frame-speciÞcsemanticroles,called frameelements ,andincludesasetofpredi- frameelements catesthatusetheseroles.EachwordevokesaframeandproÞlessomeaspectofthe frameanditselements.TheFrameNetdatasetincludesasetofframesandframe elements,thelexicalunitsassociatedwitheachframe,andasetoflabeledexample sentences.Forexample,the changepositiononascaleframeisdeÞnedasfollows: ThisframeconsistsofwordsthatindicatethechangeofanItemÕsposi- tiononascale(theAttribute)fromastartingpoint(Initial value)toan endpoint(Final value). Someofthesemanticroles(frameelements)intheframearedeÞnedasin Fig.22.3.Notethattheseareseparatedinto coreroles ,whichareframespeciÞc,and Coreroles non-coreroles ,whicharemoreliketheArg-MargumentsinPropBank,expressed Non-coreroles moregeneralpropertiesoftime,location,andsoon. Herearesomeexamplesentences: (22.20)[ITEM Oil]rose [ATTRIBUTE inprice][ DIFFERENCE by2%]. (22.21)[ITEM It]has increased [FINAL STATE tohavingthem1dayamonth]. (22.22)[ITEM Microsoftshares] fell[FINAL VALUE to75/8]. (22.23)[ITEM Coloncancerincidence] fell[DIFFERENCE by50%][ GROUP amongmen].(22.24)asteady increase [INITIAL VALUE from9.5][ FINAL VALUE to14.3][ ITEM individends] (22.25)a[DIFFERENCE 5%][ ITEM dividend] increase ...Notefromtheseexamplesentencesthattheframeincludestargetwordslike rise,fall,and increase .Infact,thecompleteframeconsistsofthefollowingwords:  D5")T05$%/"):+6'&'+%)+%)$)6($,"U)N4$#" 8CHAPTER 22¥SEMANTIC ROLE LABELING CoreRoles ATTRIBUTE TheA TTRIBUTE isascalarpropertythattheI TEM possesses.DIFFERENCE ThedistancebywhichanI TEM changesitspositiononthescale. FINAL STATE AdescriptionthatpresentstheI TEM ÕsstateafterthechangeintheA TTRIBUTE Õsvalueasanindependentpredication. FINAL VALUE ThepositiononthescalewheretheI TEM endsup. INITIAL STATE AdescriptionthatpresentstheI TEM ÕsstatebeforethechangeintheA T-TRIBUTE Õsvalueasanindependentpredication. INITIAL VALUE TheinitialpositiononthescalefromwhichtheI TEM movesaway. ITEM Theentitythathasapositiononthescale. VALUE RANGE Aportionofthescale,typicallyidentiÞedbyitsendpoints,alongwhichthe valuesoftheA TTRIBUTE ßuctuate.SomeNon-CoreRoles DURATION Thelengthoftimeoverwhichthechangetakesplace. SPEED TherateofchangeoftheV ALUE .GROUP TheG ROUP inwhichanI TEM changesthevalueofan ATTRIBUTE inaspeciÞedway. Figure22.3 Theframeelementsinthe changepositiononascaleframefromtheFrameNetLabelers Guide(Ruppenhoferetal.,2006) .VERBS:dwindlemove soarescalationshiftadvance edgemushroomswellexplosion tumbleclimbexplode plummetswingfall declinefall reachtripleßuctuationADVERBS: decreaseßuctuaterisetumblegain increasinglydiminishgain rocket growth dipgrow shiftNOUNS:hike doubleincreaseskyrocket declineincreasedropjumpslidedecreaseriseFrameNetalsocodesrelationshipsbetweenframes,allowingframestoinherit fromeachother,orrepresentingrelationsbetweenframeslikecausation(andgen- eralizationsamongframeelementsindifferentframescanberepresentingbyinher- itanceaswell).Thus,thereisa Causechangeofpositiononascaleframethatis linkedtothe Changeofpositiononascaleframebythe causerelation,butthat addsanA GENT roleandisusedforcausativeexamplessuchasthefollowing: (22.26)[AGENT They] raised [ITEM thepriceoftheirsoda][ DIFFERENCE by2%]. Together,thesetwoframeswouldallowanunderstandingsystemtoextractthe commoneventsemanticsofalltheverbalandnominalcausativeandnon-causative usages.FrameNetshavealsobeendevelopedformanyotherlanguagesincludingSpan- ish,German,Japanese,Portuguese,Italian,andChinese. 22.6SemanticRoleLabeling Semanticrolelabeling (sometimesshortenedasSRL)isthetaskofautomatically semanticrole labelingÞndingthe semanticroles ofeachargumentofeachpredicateinasentence.Cur- rentapproachestosemanticrolelabelingarebasedonsupervisedmachinelearning, oftenusingtheFrameNetandPropBankresourcestospecifywhatcountsasapred- icate,deÞnethesetofrolesusedinthetask,andprovidetrainingandtestsets. ea 8CHAPTER 22¥SEMANTIC ROLE LABELING CoreRoles ATTRIBUTE TheA TTRIBUTE isascalarpropertythattheI TEM possesses.DIFFERENCE ThedistancebywhichanI TEM changesitspositiononthescale. FINAL STATE AdescriptionthatpresentstheI TEM ÕsstateafterthechangeintheA TTRIBUTE Õsvalueasanindependentpredication. FINAL VALUE ThepositiononthescalewheretheI TEM endsup. INITIAL STATE AdescriptionthatpresentstheI TEM ÕsstatebeforethechangeintheA T-TRIBUTE Õsvalueasanindependentpredication. INITIAL VALUE TheinitialpositiononthescalefromwhichtheI TEM movesaway. ITEM Theentitythathasapositiononthescale. VALUE RANGE Aportionofthescale,typicallyidentiÞedbyitsendpoints,alongwhichthe valuesoftheA TTRIBUTE ßuctuate.SomeNon-CoreRoles DURATION Thelengthoftimeoverwhichthechangetakesplace. SPEED TherateofchangeoftheV ALUE .GROUP TheG ROUP inwhichanI TEM changesthevalueofan ATTRIBUTE inaspeciÞedway. Figure22.3 Theframeelementsinthe changepositiononascaleframefromtheFrameNetLabelers Guide(Ruppenhoferetal.,2006) .VERBS:dwindlemove soarescalationshiftadvance edgemushroomswellexplosion tumbleclimbexplode plummetswingfall declinefall reachtripleßuctuationADVERBS: decreaseßuctuaterisetumblegain increasinglydiminishgain rocket growth dipgrow shiftNOUNS:hike doubleincreaseskyrocket declineincreasedropjumpslidedecreaseriseFrameNetalsocodesrelationshipsbetweenframes,allowingframestoinherit fromeachother,orrepresentingrelationsbetweenframeslikecausation(andgen- eralizationsamongframeelementsindifferentframescanberepresentingbyinher- itanceaswell).Thus,thereisa Causechangeofpositiononascaleframethatis linkedtothe Changeofpositiononascaleframebythe causerelation,butthat addsanA GENT roleandisusedforcausativeexamplessuchasthefollowing: (22.26)[AGENT They] raised [ITEM thepriceoftheirsoda][ DIFFERENCE by2%]. Together,thesetwoframeswouldallowanunderstandingsystemtoextractthe commoneventsemanticsofalltheverbalandnominalcausativeandnon-causative usages.FrameNetshavealsobeendevelopedformanyotherlanguagesincludingSpan- ish,German,Japanese,Portuguese,Italian,andChinese. 22.6SemanticRoleLabeling Semanticrolelabeling (sometimesshortenedasSRL)isthetaskofautomatically semanticrole labelingÞndingthe semanticroles ofeachargumentofeachpredicateinasentence.Cur- rentapproachestosemanticrolelabelingarebasedonsupervisedmachinelearning, oftenusingtheFrameNetandPropBankresourcestospecifywhatcountsasapred- icate,deÞnethesetofrolesusedinthetask,andprovidetrainingandtestsets. e\D5")T05$%/"):+6'&'+%)+%)$)6($,"U)N4$#"  *",$&'+%)."&1""%)24$#"6 !".,$)#86 >$%/M6 !86 !".,$)#,&6=; M0,$87,(#)E,6%"M6 !86 0,$87,(#)E)d,& )"M6 P8,8M6 !86P8,&6=;M6 H'=>$1/, %>M6 O186 H'=>$1/, @8CM6 0$,(,&,8M6 !860$,(,&,&6=;M6 !86!"(.%1#)E,6%>M6 !86Z1'81#)E,6%> Mef *",$&'+%)."&1""%)24$#"6 b(1'8,6(.1"+,67%8)#)%"6%"6168(1B,c !86Z1'81#)E,6%>M6 Z.1"+,m7%8)#)%"m%"m1m8(1B, *&&861"61+,"#6I%B, ¥$22I= <';"$+%I= <';:")$.4I= <';:)I+ <';:)I= <'2#;"#$&#I= <'2#=#4*16#+)I+ <'2.6.+.&-I= <'2*:G4#I= <'2"*1I= <'#+-$+;#I= <'/"*?)-I+ <'.+;"#$&#I= <'%+*;%' 2*?+I= <'4*?#"I= <'6*=#I= <'1"*6*)#I= <'1:&-I+ <'1:&-I= <'"$.&#I= <'"#2:;#I= <'"#2:;).*+I+ <'&4$&-I= <'&)#1' :1I= <'&?#44I= ee8CHAPTER 22¥SEMANTIC ROLE LABELING CoreRoles ATTRIBUTE TheA TTRIBUTE isascalarpropertythattheI TEM possesses.DIFFERENCE ThedistancebywhichanI TEM changesitspositiononthescale. FINAL STATE AdescriptionthatpresentstheI TEM ÕsstateafterthechangeintheA TTRIBUTE Õsvalueasanindependentpredication. FINAL VALUE ThepositiononthescalewheretheI TEM endsup. INITIAL STATE AdescriptionthatpresentstheI TEM ÕsstatebeforethechangeintheA T-TRIBUTE Õsvalueasanindependentpredication. INITIAL VALUE TheinitialpositiononthescalefromwhichtheI TEM movesaway. ITEM Theentitythathasapositiononthescale. VALUE RANGE Aportionofthescale,typicallyidentiÞedbyitsendpoints,alongwhichthe valuesoftheA TTRIBUTE ßuctuate.SomeNon-CoreRoles DURATION Thelengthoftimeoverwhichthechangetakesplace. SPEED TherateofchangeoftheV ALUE .GROUP TheG ROUP inwhichanI TEM changesthevalueofan ATTRIBUTE inaspeciÞedway. Figure22.3 Theframeelementsinthe changepositiononascaleframefromtheFrameNetLabelers Guide(Ruppenhoferetal.,2006) .VERBS:dwindlemove soarescalationshiftadvance edgemushroomswellexplosion tumbleclimbexplode plummetswingfall declinefall reachtripleßuctuationADVERBS: decreaseßuctuaterisetumblegain increasinglydiminishgain rocket growth dipgrow shiftNOUNS:hike doubleincreaseskyrocket declineincreasedropjumpslidedecreaseriseFrameNetalsocodesrelationshipsbetweenframes,allowingframestoinherit fromeachother,orrepresentingrelationsbetweenframeslikecausation(andgen- eralizationsamongframeelementsindifferentframescanberepresentingbyinher- itanceaswell).Thus,thereisa Causechangeofpositiononascaleframethatis linkedtothe Changeofpositiononascaleframebythe causerelation,butthat addsanA GENT roleandisusedforcausativeexamplessuchasthefollowing: (22.26)[AGENT They] raised [ITEM thepriceoftheirsoda][ DIFFERENCE by2%]. Together,thesetwoframeswouldallowanunderstandingsystemtoextractthe commoneventsemanticsofalltheverbalandnominalcausativeandnon-causative usages.FrameNetshavealsobeendevelopedformanyotherlanguagesincludingSpan- ish,German,Japanese,Portuguese,Italian,andChinese. 22.6SemanticRoleLabeling Semanticrolelabeling (sometimesshortenedasSRL)isthetaskofautomatically semanticrole labelingÞndingthe semanticroles ofeachargumentofeachpredicateinasentence.Cur- rentapproachestosemanticrolelabelingarebasedonsupervisedmachinelearning, oftenusingtheFrameNetandPropBankresourcestospecifywhatcountsasapred- icate,deÞnethesetofrolesusedinthetask,andprovidetrainingandtestsets.  *",$&'+%6)."&1""%)24$#"6 EVENTPlaceTimeEventTRANSITIVE_ACTIONAgentPatientEventCausePlaceTimeOBJECTIVE_INFLUENCEDependent_entityInßuencing_situationPlaceTimeInßuencing_entityCAUSE_TO_MAKE_NOISEAgentSound_makerCausePlaceTimeMAKE_NOISENoisy_eventSoundSound_sourcePlaceTimecough.v, gobble.v, hiss.v, ring.v, yodel.v, ...blare.v, honk.v, play.v, ring.v, toot.v, ...Ñaffect.v, effect.n, impact.n, impact.v, ...event.n, happen.v, occur.v, take place.v, ...Inheritance relationCausative_of relationExcludes relationPurposeFigure2:Partialillustrationofframes,roles,andLUsrelatedtothe CAUSE TOMAKE NOISE frame,fromtheFrameNetlexicon.ÒCoreÓrolesareÞlled ovals.Non-coreroles(suchas PlaceandTime)asunÞlledovals.NoparticularsigniÞ- canceisascribedtotheorderingofaframeÕsrolesinitslexiconentry(theselectionand orderingofrolesaboveisforillustativeconvenience). CAUSE TOMAKENOISE deÞnesa totalof14roles,manyofthemnotshownhere. datathatdoesnotcorrespondtoanLUfortheframeitevokes.EachframedeÞnition alsoincludesasetofframeelements,or roles,correspondingtodifferentaspectsofthe conceptrepresentedbytheframe,suchasparticipants,props,andattributes.Weuse theterm argumenttorefertoasequenceofwordtokensannotatedasÞllingaframe role.Fig. 1showsanexamplesentencefromthetrainingdatawithannotatedtargets, LUs,frames,androle-argumentpairs.TheFrameNetlexiconalsoprovidesinformation aboutrelationsbetweenframesandbetweenroles(e.g., INHERITANCE ).Fig. 2showsa subsetoftherelationsbetweenthreeframesandtheirroles. AccompanyingmostframedeÞnitionsintheFrameNetlexiconisasetoflexico- graphicexemplarsentences (primarilyfromtheBritishNationalCorpus)annotatedfor thatframe.Typicallychosentoillustratevariationinargumentrealizationpatternsfor theframeinquestion,thesesentencesonlycontainannotationsforasingleframe.We foundthatusingexemplarsentencesdirectlytotrainourmodelshurtperformanceas evaluatedonSemEvalÕ07data,eventhoughthenumberofexemplarsentencesisanor- derofmagnitudelargerthanthenumberofsentencesinourtrainingset( ¤2.2).Thisis presumablybecausetheexemplarsareneitherrepresentativeasasamplenorsimilarto thetestdata.Instead,wemakeuseoftheseexemplarsinfeatures( ¤4.2).2.2Data Ourtraining,development,andtestsetsconsistofdocumentsannotatedwithframe- semanticstructuresfortheSemEvalÕ07task,whichwerefertocollectivelyasthe SemEvalÕ07data .3Forthemostpart,theframesandrolesusedinannotatingthese documentsweredeÞnedintheFrameNetlexicon,buttherearesomeexceptionsfor whichtheannotatorsdeÞnedsupplementaryframesandroles;theseareincludedinthe 3Thefull-textannotationsandotherresourcesforthe2007taskareavailableat http://framenet.icsi.berkeley.edu/semeval/FSSE.html.4e?[)+'$,6>$%/6L186,#61B6fa\a  !(5"#$&'()+2)N4$#")!"#$%&'(6 ComputationalLinguisticsVolume40,Number1 1.Introduction FrameNet(Fillmore,Johnson,andPetruck2003)isalinguisticresourcestoringconsider- ableinformationaboutlexicalandpredicate-argumentsemanticsinEnglish.Grounded inthetheoryofframesemantics(Fillmore1982),itsuggestsÑbutdoesnotformally deÞneÑasemanticrepresentationthatblendsrepresentationsfamiliarfromword-sense disambiguation(IdeandV «eronis1998)andsemanticrolelabeling(SRL;Gildeaand Jurafsky2002).Giventhelimitedsizeofavailableresources,accuratelyproducing richlystructuredframe-semanticstructureswithhighcoveragewillrequiredata-driven techniquesbeyondsimplesupervisedclassiÞcation,suchaslatentvariablemodeling, semi-supervisedlearning,andjointinference. Inthisarticle,wepresentacomputationalandstatisticalmodelforframe-semantic parsing,theproblemofextractingfromtextsemanticpredicate-argumentstructures suchasthoseshowninFigure1.Weaimtopredictaframe-semanticrepresentation withtwostatisticalmodelsratherthanacollectionoflocalclassiÞers,unlikeearlierap- proaches(Baker,Ellsworth,andErk2007).Weuseaprobabilisticframeworkthatcleanly integratestheFrameNetlexiconandlimitedavailabletrainingdata.Theprobabilistic frameworkweadoptishighlyamenabletofutureextensionthroughnewfeatures,more relaxedindependenceassumptions,andadditionalsemi-supervisedmodels. CarefullyconstructedlexicalresourcesandannotateddatasetsfromFrameNet, detailedinSection3,formthebasisoftheframestructurepredictiontask.Wede- composethistaskintothreesubproblems: targetidentiÞcation (Section4),inwhich frame-evokingpredicatesaremarkedinthesentence; frameidentiÞcation (Section5), inwhichtheevokedframeisselectedforeachpredicate;and argumentidentiÞcation (Section6),inwhichargumentstoeachframeareidentiÞedandlabeledwitharolefrom thatframe.Experimentsdemonstratingfavorableperformancetothepreviousstateof theartonSemEval2007andFrameNetdatasetsaredescribedineachsection.Some novelaspectsofourapproachincludealatent-variablemodel(Section5.2)andasemi- supervisedextensionofthepredicatelexicon(Section5.5)tofacilitatedisambiguationof wordsnotintheFrameNetlexicon;auniÞedmodelforÞndingandlabelingarguments Figure1 AnexamplesentencefromtheannotationsreleasedaspartofFrameNet1.5withthreetargets markedin bold.Notethatthisannotationispartialbecausenotallpotentialtargetshavebeen annotatedwithpredicate-argumentstructures.Eachtargethasitsevokedsemanticframe markedaboveit,enclosedinadistinctshapeorborderstyle.Foreachframe,itssemanticroles areshownenclosedwithinthesameshapeorborderstyle,andthespansfulÞllingtherolesare connectedtothelatterusingdottedlines.Forexample, mannerevokesthe CONDUCTframe,and hasthe AGENT andMANNERrolesfulÞlledby Austriaandmostun-Viennese ,respectively. 10eF[)+'$,6>$%/6L186,#61B6@fa\?C  N4$#"Q"& 0+#:,"V'&= 1Introduction bell.nring.v there be.v enough.aLUNOISE _MAKERS SUFFICIENCY Frame EXISTENCE CAUSE _TO_MAKE _NOISE .bells  N_mmore than six of the eight Sound_maker Enabled_situation ring toringers ItemenoughEntity Agent n'tare still there But Figure1:AsentencefromPropBankandtheSemEvalÕ07trainingdata,andapartial depictionofgoldFrameNetannotations.Eachframeisarowbelowthesentence(or- deredforreadability).Thicklinesindicatetargetsthatevokeframes;thinsolid/dotted lineswithlabelsindicatearguments.ÒN mÓunder bellsisshortforthe Noisemaker roleofthe NOISEMAKERS frameÑitisa denotedframeelement becauseitisalsothe target.Thelastrowindicatesthat there...are isadiscontinuoustarget.InPropBank,the verbringistheonlyannotatedpredicateforthissentence,anditisnotrelatedtoother predicateswithsimilarmeanings. FrameNet( Fillmoreetal. ,2003)isarichlinguisticresourcecontainingconsiderable informationaboutlexicalandpredicate-argumentsemanticsinEnglish.Groundedinthe theoryofframesemantics( Fillmore ,1982),itsuggestsÑbutdoesnotformallydeÞneÑa semanticrepresentationthatblendsword-sensedisambiguationandsemanticrolelabel- ing.Inthisreport,wepresentacomputationalandstatisticalmodelforframe-semantic parsing,theproblemofextractingfromtextsemanticpredicate-argumentstructures suchasthoseshowninFig. 1.Weaimtopredictaframe-semanticrepresentationas astructure ,notasapipelineofclassiÞers.Weuseaprobabilisticframeworkthatcleanly integratestheFrameNetlexiconand(currentlyverylimited)availabletrainingdata.Al- thoughourmodelsofteninvolvestrongindependenceassumptions,theprobabilistic frameworkweadoptishighlyamenabletofutureextensionthroughnewfeatures,re- laxedindependenceassumptions,andsemisupervisedlearning.Somenovelaspectsof ourcurrentapproachincludealatent-variablemodelthatpermitsdisambiguationof wordsnotintheFrameNetlexicon,auniÞedmodelforÞndingandlabelingarguments, andaprecision-boostingconstraintthatforbidsargumentsofthesamepredicatetoover- lap.Ourparser,namedSEMAFOR, 1achievesthebestpublishedresultstodateonthe SemEvalÕ07FrameNettask( Bakeretal. ,2007).2ResourcesandTask Weconsiderframe-semanticparsingresources. 2.1FrameNetLexicon TheFrameNetlexiconisataxonomyofmanuallyidentiÞedgeneral-purpose framesforEnglish.2Listedinthelexiconwitheachframeareseverallemmas(withpartofspeech) thatcandenotetheframeorsomeaspectofitÑthesearecalled lexicalunits (LUs).In asentence,wordorphrasetokensthatevokeaframeareknownas targets.Thesetof LUslistedforaframeinFrameNetmaynotbeexhaustive;wemayseeatargetinnew 1SemanticAnalyzerofFrameRepresentations 2LiketheSemEvalÕ07participants,weusedFrameNetv.1.3( http://framenet.icsi.berkeley.edu).3e^[$%/6L186,#61B:6fa\a  N4$#"Q"& $%9) L4+:@$%A 4":4"6"%&$&'+%6 ComputationalLinguisticsVolume40,Number1 (a)(b)Figure2 (a)Aphrase-structuretreetakenfromthePennTreebankandannotatedwithPropBank predicate-argumentstructures.Theverbs created andpushedserveaspredicatesinthis sentence.Dottedarrowsconnecteachpredicatetoitssemanticarguments(bracketedphrases). (b)Apartialdepictionofframe-semanticstructuresforthesamesentence.Thewordsinbold are targets,whichinstantiatea(lemmatizedandpart-of-speechÐtagged) lexicalunit andevoke asemanticframe.Everyframeannotationisshownenclosedinadistintshapeorborderstyle, anditsargumentlabelsareshowntogetheronthesameverticaltierbelowthesentence. Seetextforexplanationofabbreviations. phrase-structuresyntaxtreesfromthe WallStreetJournal sectionofthePennTreebank (Marcus,Marcinkiewicz,andSantorini1993)annotatedwithpredicate-argument structuresforverbs.InFigure2(a),thesyntaxtreeforthesentenceismarkedwith varioussemanticroles.Thetwomainverbsinthesentence, created andpushed,are thepredicates.Fortheformer,theconstituent morethan1.2millionjobs servesasthe semanticrole ARG1andtheconstituent Inthattime servesastherole ARGM-TMP.Similarly forthelatterverb,roles ARG1,ARG2,ARGM-DIR,and ARGM-TMPareshownintheÞgure. PropBankdeÞnes coreroles ARG0through ARG5,whichreceivedifferentinterpretations fordifferentpredicates.Additional modiÞerroles ARGM-*includeARGM-TMP(temporal)andARGM-DIR(directional),asshowninFigure2(a).ThePropBankrepresentation thereforehasasmallnumberofroles,andthetrainingdatasetcomprisessome 40,000sentences,thusmakingthesemanticrolelabelingtaskanattractiveonefromthe perspectiveofmachinelearning. Therearemanyinstancesofinßuentialworkonsemanticrolelabelingusing PropBankconventions.Pradhanetal.(2004)presentasystemthatusessupportvector machines(SVMs)toidentifytheargumentsinasyntaxtreethatcanserveassemantic roles,followedbyclassiÞcationoftheidentiÞedargumentstorolenamesviaacollection ofbinarySVMs.Punyakanoketal.(2004)describeasemanticrolelabelerthatusesinte- gerlinearprogrammingforinferenceandusesseveralglobalconstraintstoÞndthebest 12eR !"#$%&'()*+,") -$.",'%/ H,/1"#)(6I%B,621=,B)"+6 *B+%$)#./  !"#$%&'()4+,"),$.",'%/)G!*-H) ¥-.,6#1896 %>6 >)"&)"+6 #.,68,/1"#)(6$%B,86%>6,1(.61$+'/,"#6%>6,1(.6 7$,&)(1#,6)"616 8,"#,"(,: ¥[$1/,J,# E,$8'86 0$%7Y1"9 Me]22.6¥SEMANTIC ROLE LABELING 9Recallthatthedifferencebetweenthesetwomodelsofsemanticrolesisthat FrameNet( 22.27)employsmanyframe-speciÞcframeelementsasroles,whileProp- Bank( 22.28)usesasmallernumberofnumberedargumentlabelsthatcanbeinter- pretedasverb-speciÞclabels,alongwiththemoregeneralARGMlabels.Some examples: (22.27)[You]canÕt[blame][theprogram][forbeingunabletoidentifyit] COGNIZERTARGETEVALUEEREASON (22.28)[TheSanFranciscoExaminer]issued[aspecialedition][yesterday] ARG 0TARGETARG 1ARGM -TMP AsimpliÞedsemanticrolelabelingalgorithmissketchedinFig. 22.4.While therearealargenumberofalgorithms,manyofthemusesomeversionofthesteps inthisalgorithm. Mostalgorithms,beginningwiththeveryearliestsemanticroleanalyzers (Sim-mons,1973) ,beginbyparsing,usingbroad-coverageparserstoassignaparsetothe inputstring.Figure 22.5showsaparseof( 22.28)above.Theparseisthentraversed toÞndallwordsthatarepredicates. Foreachofthesepredicates,thealgorithmexamineseachnodeintheparsetree anddecidesthesemanticrole(ifany)itplaysforthispredicate. ThisisgenerallydonebysupervisedclassiÞcation.Givenalabeledtrainingset suchasPropBankorFrameNet,afeaturevectorisextractedforeachnode,using featuretemplatesdescribedinthenextsubsection. A1-of-NclassiÞeristhentrainedtopredictasemanticroleforeachconstituent giventhesefeatures,whereNisthenumberofpotentialsemanticrolesplusan extraNONErolefornon-roleconstituents.MoststandardclassiÞcationalgorithms havebeenused(logisticregression,SVM,etc).Finally,foreachtestsentencetobe labeled,theclassiÞerisrunoneachrelevantconstituent.Wegivemoredetailsof thealgorithmafterwediscussfeatures. functionSEMANTIC ROLE LABEL (words )returns labeledtree parse!PARSE (words )foreach predicate inparse doforeach nodeinparse dofeaturevector !EXTRACT FEATURES (node,predicate ,parse )CLASSIFY NODE (node,featurevector ,parse )Figure22.4 Agenericsemantic-role-labelingalgorithm.C LASSIFY NODE isa1-of- Nclas-siÞerthatassignsasemanticrole(orNONEfornon-roleconstituents),trainedonlabeleddata suchasFrameNetorPropBank. FeaturesforSemanticRoleLabeling Awidevarietyoffeaturescanbeusedforsemanticrolelabeling.Mostsystemsuse somegeneralizationofthecoresetoffeaturesintroducedby GildeaandJurafsky (2000).Atypicalsetofbasicfeaturesarebasedonthefollowingfeaturetemplates (demonstratedonthe NP-SBJconstituentTheSanFranciscoExaminer inFig. 22.5):¥Thegoverning predicate ,inthiscasetheverb issued.Thepredicateisacru- cialfeaturesincelabelsaredeÞnedonlywithrespecttoaparticularpredicate. ¥Thephrasetype oftheconstituent,inthiscase, NP(orNP-SBJ).Somese- manticrolestendtoappearas NPs,othersas SorPP,andsoon.  W'6&+4= ¥H,/1"#)(6$%B,8618616)"#,$/,&)1#,68,/1"#)(8A6'8,&6,1$B;6)" ¥/1(.)",6#$1"8B1#)%"6 @n)B98 A6\]Re C¥o',8#)%" K1"8<,$)"+6 @O,"&$)D6,#61B:A6\]Re C¥87%9," KB1"+'1+,6 '"&,$8#1"&)"+6@J18. Kn,==,$A6 \]RFC¥&)1B%+',6 8;8#,/86@ Y%=$%< ,#61B:A6\]RR C¥W1$B;6HI268;8#,/8 H)//%"86\]ReA6G1$('86\]XaM6 ¥71$8,$6>%BB%<,&6=;6.1"& K<$)##,"6$'B,86>%$6,1(.6E,$= ¥&)(#)%"1$),86 <)#.6E,$= K87,()>)(6(18,6>$1/,86@2,E)"6 \]RRC6 ?a X5=)!"#$%&'()*+,")-$.",'%/ ¥*6'8,>'B68.1BB%<68,/1"#)(6$,7$,8,"#1#)%" ¥!/7$%E,86J206#18986B)9,M ¥o',8#)%"6 1"8<,$)"+6 H.,"6 1"&6 2171#1 faaRA6 H'$&,1"' ,#61B:6 fa\\¥/1(.)",6 #$1"8B1#)%"6 2)'6 1"&6 V)B&,1 fa\aA62%6,#61B:6 fa\e?\ ;)6'#:,")#+9"4%) $,/+4'&5# 22.6¥SEMANTIC ROLE LABELING 9Recallthatthedifferencebetweenthesetwomodelsofsemanticrolesisthat FrameNet( 22.27)employsmanyframe-speciÞcframeelementsasroles,whileProp- Bank( 22.28)usesasmallernumberofnumberedargumentlabelsthatcanbeinter- pretedasverb-speciÞclabels,alongwiththemoregeneralARGMlabels.Some examples: (22.27)[You]canÕt[blame][theprogram][forbeingunabletoidentifyit] COGNIZERTARGETEVALUEEREASON (22.28)[TheSanFranciscoExaminer]issued[aspecialedition][yesterday] ARG 0TARGETARG 1ARGM -TMP AsimpliÞedsemanticrolelabelingalgorithmissketchedinFig. 22.4.While therearealargenumberofalgorithms,manyofthemusesomeversionofthesteps inthisalgorithm. Mostalgorithms,beginningwiththeveryearliestsemanticroleanalyzers (Sim-mons,1973) ,beginbyparsing,usingbroad-coverageparserstoassignaparsetothe inputstring.Figure 22.5showsaparseof( 22.28)above.Theparseisthentraversed toÞndallwordsthatarepredicates. Foreachofthesepredicates,thealgorithmexamineseachnodeintheparsetree anddecidesthesemanticrole(ifany)itplaysforthispredicate. ThisisgenerallydonebysupervisedclassiÞcation.Givenalabeledtrainingset suchasPropBankorFrameNet,afeaturevectorisextractedforeachnode,using featuretemplatesdescribedinthenextsubsection. A1-of-NclassiÞeristhentrainedtopredictasemanticroleforeachconstituent giventhesefeatures,whereNisthenumberofpotentialsemanticrolesplusan extraNONErolefornon-roleconstituents.MoststandardclassiÞcationalgorithms havebeenused(logisticregression,SVM,etc).Finally,foreachtestsentencetobe labeled,theclassiÞerisrunoneachrelevantconstituent.Wegivemoredetailsof thealgorithmafterwediscussfeatures. functionSEMANTIC ROLE LABEL (words )returns labeledtree parse!PARSE (words )foreach predicate inparse doforeach nodeinparse dofeaturevector !EXTRACT FEATURES (node,predicate ,parse )CLASSIFY NODE (node,featurevector ,parse )Figure22.4 Agenericsemantic-role-labelingalgorithm.C LASSIFY NODE isa1-of- Nclas-siÞerthatassignsasemanticrole(orNONEfornon-roleconstituents),trainedonlabeleddata suchasFrameNetorPropBank. FeaturesforSemanticRoleLabeling Awidevarietyoffeaturescanbeusedforsemanticrolelabeling.Mostsystemsuse somegeneralizationofthecoresetoffeaturesintroducedby GildeaandJurafsky (2000).Atypicalsetofbasicfeaturesarebasedonthefollowingfeaturetemplates (demonstratedonthe NP-SBJconstituentTheSanFranciscoExaminer inFig. 22.5):¥Thegoverning predicate ,inthiscasetheverb issued.Thepredicateisacru- cialfeaturesincelabelsaredeÞnedonlywithrespecttoaparticularpredicate. ¥Thephrasetype oftheconstituent,inthiscase, NP(orNP-SBJ).Somese- manticrolestendtoappearas NPs,othersas SorPP,andsoon. ?f W+1)9+)1")9"('9")15$&)'6)$):4"9'($&" ¥!>6<,`$,6N'8#6&%)"+6 0$%7Y1"9 E,$=8 ¥Z.%%8,61BB6E,$=8 ¥0%88)=B;6$,/%E)"+6B)+.#6E,$=86@>$%/616B)8#C ¥!>6<,`$,6&%)"+6 [$1/,J,# @E,$=8A6"%'"8A61&N,(#)E,8C ¥Z.%%8,6,E,$;6<%$&6#.1#6<186B1=,B,&618616#1$+,#6)"6#$1)")"+6&1#1 ?e !"#$%&'()*+,")-$.",'%/ 10CHAPTER 22¥SEMANTIC ROLE LABELING SNP-SBJ =ARG0 VPDTNNPNNPNNP TheSanFranciscoExaminer VBD =TARGET NP=ARG1 PP-TMP =ARGM-TMP issuedDTJJNNINNP aspecialeditionaroundNNNP-TMP noonyesterday Figure22.5 ParsetreeforaPropBanksentence,showingthePropBankargumentlabels.Thedottedline showsthe pathfeatureNP !S"VP"VBDforARG0,theNP-SBJconstituent TheSanFranciscoExaminer. ¥Theheadword oftheconstituent, Examiner.Theheadwordofaconstituent canbecomputedwithstandardheadrules,suchasthosegiveninChapter11 inFig. ??.Certainheadwords(e.g.,pronouns)placestrongconstraintsonthe possiblesemanticrolestheyarelikelytoÞll. ¥Theheadwordpartofspeech oftheconstituent, NNP.¥Thepathintheparsetreefromtheconstituenttothepredicate.Thispathis markedbythedottedlineinFig. 22.5.Following GildeaandJurafsky(2000) ,wecanuseasimplelinearrepresentationofthepath,NP !S"VP"VBD.!and"representupwardanddownwardmovementinthetree,respectively.The pathisveryusefulasacompactrepresentationofmanykindsofgrammatical functionrelationshipsbetweentheconstituentandthepredicate. ¥Thevoice oftheclauseinwhichtheconstituentappears,inthiscase, active (ascontrastedwith passive ).Passivesentencestendtohavestronglydifferent linkingsofsemanticrolestosurfaceformthandoactiveones. ¥Thebinary linearposition oftheconstituentwithrespecttothepredicate, eitherbefore orafter.¥Thesubcategorizationofthepredicate,thesetofexpectedargumentsthat appearintheverbphrase.Wecanextractthisinformationbyusingthephrase- structurerulethatexpandstheimmediateparentofthepredicate;VP #VBDNPPPforthepredicateinFig. 22.5.¥Thenamedentitytypeoftheconstituent. ¥TheÞrstwordsandthelastwordoftheconstituent. ThefollowingfeaturevectorthusrepresentstheÞrstNPinourexample(recall thatmostobservationswillhavethevalueNONEratherthan,forexample, ARG0,sincemostconstituentsintheparsetreewillnotbearasemanticrole): ARG 0:[issued,NP,Examiner,NNP,NP !S"VP"VBD,active,before,VP #NPPP, ORG,The,Examiner] Otherfeaturesareoftenusedinaddition,suchassetsofn-gramsinsidethe constituent,ormorecomplexversionsofthepathfeatures(theupwardordownward halves,orwhetherparticularnodesoccurinthepath). ItÕsalsopossibletousedependencyparsesinsteadofconstituencyparsesasthe basisoffeatures,forexampleusingdependencyparsepathsinsteadofconstituency paths.?? N"$&34"6 O,1&<%$&6%>6(%"8#)#',"# WD1/)",$ O,1&<%$&60UH JJ0T%)(,6%>6#.,6(B1'8, *(#)E, H'=(1#,+%$)d1#)%" %>6 7$,& T06 Kp6TYL6J0600 ?F10CHAPTER 22¥SEMANTIC ROLE LABELING SNP-SBJ =ARG0 VPDTNNPNNPNNP TheSanFranciscoExaminer VBD =TARGET NP=ARG1 PP-TMP =ARGM-TMP issuedDTJJNNINNP aspecialeditionaroundNNNP-TMP noonyesterday Figure22.5 ParsetreeforaPropBanksentence,showingthePropBankargumentlabels.Thedottedline showsthe pathfeatureNP !S"VP"VBDforARG0,theNP-SBJconstituent TheSanFranciscoExaminer. ¥Theheadword oftheconstituent, Examiner.Theheadwordofaconstituent canbecomputedwithstandardheadrules,suchasthosegiveninChapter11 inFig. ??.Certainheadwords(e.g.,pronouns)placestrongconstraintsonthe possiblesemanticrolestheyarelikelytoÞll. ¥Theheadwordpartofspeech oftheconstituent, NNP.¥Thepathintheparsetreefromtheconstituenttothepredicate.Thispathis markedbythedottedlineinFig. 22.5.Following GildeaandJurafsky(2000) ,wecanuseasimplelinearrepresentationofthepath,NP !S"VP"VBD.!and"representupwardanddownwardmovementinthetree,respectively.The pathisveryusefulasacompactrepresentationofmanykindsofgrammatical functionrelationshipsbetweentheconstituentandthepredicate. ¥Thevoice oftheclauseinwhichtheconstituentappears,inthiscase, active (ascontrastedwith passive ).Passivesentencestendtohavestronglydifferent linkingsofsemanticrolestosurfaceformthandoactiveones. ¥Thebinary linearposition oftheconstituentwithrespecttothepredicate, eitherbefore orafter.¥Thesubcategorizationofthepredicate,thesetofexpectedargumentsthat appearintheverbphrase.Wecanextractthisinformationbyusingthephrase- structurerulethatexpandstheimmediateparentofthepredicate;VP #VBDNPPPforthepredicateinFig. 22.5.¥Thenamedentitytypeoftheconstituent. ¥TheÞrstwordsandthelastwordoftheconstituent. ThefollowingfeaturevectorthusrepresentstheÞrstNPinourexample(recall thatmostobservationswillhavethevalueNONEratherthan,forexample, ARG0,sincemostconstituentsintheparsetreewillnotbearasemanticrole): ARG 0:[issued,NP,Examiner,NNP,NP !S"VP"VBD,active,before,VP #NPPP, ORG,The,Examiner] Otherfeaturesareoftenusedinaddition,suchassetsofn-gramsinsidethe constituent,ormorecomplexversionsofthepathfeatures(theupwardordownward halves,orwhetherparticularnodesoccurinthepath). ItÕsalsopossibletousedependencyparsesinsteadofconstituencyparsesasthe basisoffeatures,forexampleusingdependencyparsepathsinsteadofconstituency paths.J1/,&6W"#)#;6#;7,6%>6 (%"8#)# UIV*J!5*-!UJ [)$8#61"&6B18#6<%$&86%>6 (%"8#)# -.,A6WD1/)",$ 2)",1$6 7%8)#)%"A(B1'8, $,M67$,&)(1#, =,>%$,  L$&5)N"$&34"6 L$&5 )"6 #.,671$8,6#$,,6>$%/6#.,6(%"8#)#',"#6#%6#.,67$,&)(1#,6 ?^10CHAPTER 22¥SEMANTIC ROLE LABELING SNP-SBJ =ARG0 VPDTNNPNNPNNP TheSanFranciscoExaminer VBD =TARGET NP=ARG1 PP-TMP =ARGM-TMP issuedDTJJNNINNP aspecialeditionaroundNNNP-TMP noonyesterday Figure22.5 ParsetreeforaPropBanksentence,showingthePropBankargumentlabels.Thedottedline showsthe pathfeatureNP !S"VP"VBDforARG0,theNP-SBJconstituent TheSanFranciscoExaminer. ¥Theheadword oftheconstituent, Examiner.Theheadwordofaconstituent canbecomputedwithstandardheadrules,suchasthosegiveninChapter11 inFig. ??.Certainheadwords(e.g.,pronouns)placestrongconstraintsonthe possiblesemanticrolestheyarelikelytoÞll. ¥Theheadwordpartofspeech oftheconstituent, NNP.¥Thepathintheparsetreefromtheconstituenttothepredicate.Thispathis markedbythedottedlineinFig. 22.5.Following GildeaandJurafsky(2000) ,wecanuseasimplelinearrepresentationofthepath,NP !S"VP"VBD.!and"representupwardanddownwardmovementinthetree,respectively.The pathisveryusefulasacompactrepresentationofmanykindsofgrammatical functionrelationshipsbetweentheconstituentandthepredicate. ¥Thevoice oftheclauseinwhichtheconstituentappears,inthiscase, active (ascontrastedwith passive ).Passivesentencestendtohavestronglydifferent linkingsofsemanticrolestosurfaceformthandoactiveones. ¥Thebinary linearposition oftheconstituentwithrespecttothepredicate, eitherbefore orafter.¥Thesubcategorizationofthepredicate,thesetofexpectedargumentsthat appearintheverbphrase.Wecanextractthisinformationbyusingthephrase- structurerulethatexpandstheimmediateparentofthepredicate;VP #VBDNPPPforthepredicateinFig. 22.5.¥Thenamedentitytypeoftheconstituent. ¥TheÞrstwordsandthelastwordoftheconstituent. ThefollowingfeaturevectorthusrepresentstheÞrstNPinourexample(recall thatmostobservationswillhavethevalueNONEratherthan,forexample, ARG0,sincemostconstituentsintheparsetreewillnotbearasemanticrole): ARG 0:[issued,NP,Examiner,NNP,NP !S"VP"VBD,active,before,VP #NPPP, ORG,The,Examiner] Otherfeaturesareoftenusedinaddition,suchassetsofn-gramsinsidethe constituent,ormorecomplexversionsofthepathfeatures(theupwardordownward halves,orwhetherparticularnodesoccurinthepath). ItÕsalsopossibletousedependencyparsesinsteadofconstituencyparsesasthe basisoffeatures,forexampleusingdependencyparsepathsinsteadofconstituency paths.10CHAPTER 22¥SEMANTIC ROLE LABELING SNP-SBJ =ARG0 VPDTNNPNNPNNP TheSanFranciscoExaminer VBD =TARGET NP=ARG1 PP-TMP =ARGM-TMP issuedDTJJNNINNP aspecialeditionaroundNNNP-TMP noonyesterday Figure22.5 ParsetreeforaPropBanksentence,showingthePropBankargumentlabels.Thedottedline showsthe pathfeatureNP !S"VP"VBDforARG0,theNP-SBJconstituent TheSanFranciscoExaminer. ¥Theheadword oftheconstituent, Examiner.Theheadwordofaconstituent canbecomputedwithstandardheadrules,suchasthosegiveninChapter11 inFig. ??.Certainheadwords(e.g.,pronouns)placestrongconstraintsonthe possiblesemanticrolestheyarelikelytoÞll. ¥Theheadwordpartofspeech oftheconstituent, NNP.¥Thepathintheparsetreefromtheconstituenttothepredicate.Thispathis markedbythedottedlineinFig. 22.5.Following GildeaandJurafsky(2000) ,wecanuseasimplelinearrepresentationofthepath,NP !S"VP"VBD.!and"representupwardanddownwardmovementinthetree,respectively.The pathisveryusefulasacompactrepresentationofmanykindsofgrammatical functionrelationshipsbetweentheconstituentandthepredicate. ¥Thevoice oftheclauseinwhichtheconstituentappears,inthiscase, active (ascontrastedwith passive ).Passivesentencestendtohavestronglydifferent linkingsofsemanticrolestosurfaceformthandoactiveones. ¥Thebinary linearposition oftheconstituentwithrespecttothepredicate, eitherbefore orafter.¥Thesubcategorizationofthepredicate,thesetofexpectedargumentsthat appearintheverbphrase.Wecanextractthisinformationbyusingthephrase- structurerulethatexpandstheimmediateparentofthepredicate;VP #VBDNPPPforthepredicateinFig. 22.5.¥Thenamedentitytypeoftheconstituent. ¥TheÞrstwordsandthelastwordoftheconstituent. ThefollowingfeaturevectorthusrepresentstheÞrstNPinourexample(recall thatmostobservationswillhavethevalueNONEratherthan,forexample, ARG0,sincemostconstituentsintheparsetreewillnotbearasemanticrole): ARG 0:[issued,NP,Examiner,NNP,NP !S"VP"VBD,active,before,VP #NPPP, ORG,The,Examiner] Otherfeaturesareoftenusedinaddition,suchassetsofn-gramsinsidethe constituent,ormorecomplexversionsofthepathfeatures(theupwardordownward halves,orwhetherparticularnodesoccurinthepath). ItÕsalsopossibletousedependencyparsesinsteadofconstituencyparsesasthe basisoffeatures,forexampleusingdependencyparsepathsinsteadofconstituency paths. N4"Y3"%&):$&5)2"$&34"6 383.MACHINELEARNINGFORSEMANTICROLELABELING SNPNNHousing NNSlobbiesVPVBD persuaded NP1NNPCongress SNP*1VPTOtoVPVBraise NPDTtheNNceilingPPto$124,875 Figure3.4: Treebankannotationofequiconstructions.Anemptycategoryisindicatedby*,andco- indexingbysuperscript 1.Themostcommonvaluesofthepathfeature,alongwithinterpretations,areshowninTa- ble3.1.Table3.1: Mostfrequentvaluesof pathfeatureinthetrainingdata. Frequency Path Description 14.2%VB!VP"PPPPargument/adjunct 11.8VB!VP!S"NPsubject10.1VB!VP"NPobject7.9VB!VP!VP!S"NPsubject(embeddedVP) 4.1VB!VP"ADVP adverbialadjunct 3.0NN!NP!NP"PPprepositionalcomplementofnoun 1.7VB!VP"PRT adverbialparticle 1.6VB!VP!VP!VP!S"NPsubject(embeddedVP) 14.2nomatchingparseconstituent 31.4Other Forthepurposesofchoosingaframeelementlabelforaconstituent,thepathfeatureissimilar tothegoverningcategoryfeaturedeÞnedabove.Becausethepathcapturesmoreinformation,itmay bemoresusceptibletoparsererrorsanddatasparseness.Asanindicationofthis,thepathfeature ?R[$%/601B/,$A6 V)B&,1 A63', fa\a N'%$,)2"$&34")7"(&+4 ¥[%$6b-.,6H1"6[$1"()8(%6WD1/)",$cA6 ¥*$+aA6q)88',& A6J0A6WD1/)",$A6 JJ0A6 1(#)E,A6=,>%$,A6 T0!J06 00A6 UIVA6-.,A6 WD1/)",$A6666666666666666666666666r ¥U#.,$6>,1#'$,86(%'B&6=,6'8,&6186<,BB ¥8,#86 %>6" K+$1/86)"8)&,6#.,6 (%"8#)#',"# ¥%#.,$671#.6>,1#'$,8 ¥#.,6 '7<1$&6%$6&%<"<1$&6 .1BE,8 ¥<.,#.,$6 71$#)('B1$6"%&,86%(('$6)"6#.,6 71#.6 ?X10CHAPTER 22¥SEMANTIC ROLE LABELING SNP-SBJ =ARG0 VPDTNNPNNPNNP TheSanFranciscoExaminer VBD =TARGET NP=ARG1 PP-TMP =ARGM-TMP issuedDTJJNNINNP aspecialeditionaroundNNNP-TMP noonyesterday Figure22.5 ParsetreeforaPropBanksentence,showingthePropBankargumentlabels.Thedottedline showsthe pathfeatureNP !S"VP"VBDforARG0,theNP-SBJconstituent TheSanFranciscoExaminer. ¥Theheadword oftheconstituent, Examiner.Theheadwordofaconstituent canbecomputedwithstandardheadrules,suchasthosegiveninChapter11 inFig. ??.Certainheadwords(e.g.,pronouns)placestrongconstraintsonthe possiblesemanticrolestheyarelikelytoÞll. ¥Theheadwordpartofspeech oftheconstituent, NNP.¥Thepathintheparsetreefromtheconstituenttothepredicate.Thispathis markedbythedottedlineinFig. 22.5.Following GildeaandJurafsky(2000) ,wecanuseasimplelinearrepresentationofthepath,NP !S"VP"VBD.!and"representupwardanddownwardmovementinthetree,respectively.The pathisveryusefulasacompactrepresentationofmanykindsofgrammatical functionrelationshipsbetweentheconstituentandthepredicate. ¥Thevoice oftheclauseinwhichtheconstituentappears,inthiscase, active (ascontrastedwith passive ).Passivesentencestendtohavestronglydifferent linkingsofsemanticrolestosurfaceformthandoactiveones. ¥Thebinary linearposition oftheconstituentwithrespecttothepredicate, eitherbefore orafter.¥Thesubcategorizationofthepredicate,thesetofexpectedargumentsthat appearintheverbphrase.Wecanextractthisinformationbyusingthephrase- structurerulethatexpandstheimmediateparentofthepredicate;VP #VBDNPPPforthepredicateinFig. 22.5.¥Thenamedentitytypeoftheconstituent. ¥TheÞrstwordsandthelastwordoftheconstituent. ThefollowingfeaturevectorthusrepresentstheÞrstNPinourexample(recall thatmostobservationswillhavethevalueNONEratherthan,forexample, ARG0,sincemostconstituentsintheparsetreewillnotbearasemanticrole): ARG 0:[issued,NP,Examiner,NNP,NP !S"VP"VBD,active,before,VP #NPPP, ORG,The,Examiner] Otherfeaturesareoftenusedinaddition,suchassetsofn-gramsinsidethe constituent,ormorecomplexversionsofthepathfeatures(theupwardordownward halves,orwhetherparticularnodesoccurinthepath). ItÕsalsopossibletousedependencyparsesinsteadofconstituencyparsesasthe basisoffeatures,forexampleusingdependencyparsepathsinsteadofconstituency paths. K>6&":)7"46'+%)+2)!*-)$,/+4'&5# IML43%'%/ M6'8,6 8)/7B,6.,'$)8#)(86#%67$'",6'"B)9,B;6(%"8#)#',"#8:6 OMZ9"%&'2'($&'+% M616=)"1$;6(B188)>)(1#)%"6%>6,1(.6"%&,61861"6 1$+'/,"#6#%6=,6 B1=,B,&6 %$616JUJW:6 KM0,$66'2'($&'+% M616\ K%>KJ'(B188)>)(1#)%"6%>61BB6#.,6(%"8#)#',"#86#.1#6 <,$,6B1=,B,&61861$+'/,"#86=;6#.,67$,E)%'868#1+,6 ?] X5=)$99)L43%'%/)$%9)Z9"%&'2'($&'+%)6&":68 ¥*B+%$)#./6)86B%%9)"+61#6%",67$,&)(1#,61#616#)/, ¥T,$;6>,<6%>6#.,6"%&,86)"6#.,6#$,,6(%'B&67%88)=B,6=,61$+'/,"#86 %>6#.1#6%",67$,&)(1#, ¥!/=1B1"(,6=,#<,,"6 ¥7%8)#)E,681/7B,86@(%"8#)#',"#86#.1#61$,61$+'/,"#86%>67$,&)(1#,C ¥",+1#)E,681/7B,86@(%"8#)#',"#86#.1#61$,6"%#61$+'/,"#86%>67$,&)(1#,C ¥!/=1B1"(,&6&1#16(1"6=,6.1$&6>%$6/1";6(B188)>),$8 ¥H%6<,67$'",6#.,6 7"4= '"B)9,B;6(%"8#)#',"#86>)$8#A61"&6#.,"6'8,616 (B188)>),$6#%6+,#6$)&6%>6#.,6$,8#: Fa L43%'%/)5"34'6&'(6) [\3" $%9)L$,#"4)GO]]^H ¥*&&68)8#,$86%>6#.,67$,&)(1#,A6#.,"61'"#8A6#.,"6+$,1# K1'"#8A6 ,#( ¥Y'#6)+"%$)"+61";#.)"+6)"616(%%$&)"1#)%"68#$'(#'$, F\323.MACHINELEARNINGFORSEMANTICROLELABELING tree.Inaddition,sinceitisnotuncommonforaconstituenttobeassignedmultiplesemanticroles bydifferentpredicates(generallyapredicatecanonlyassignonesemanticroletoaconstituent), thesemanticrolelabelingsystemcanonlylookatonepredicateatatime,tryingtoÞndallthe argumentsforthisparticularpredicateinthetree.Thetreewillbetraversedasmanytimesasthere arepredicatesinthetree.Thismeansthereisanevenhigherproportionofconstituentsintheparse treethatarenotargumentsforthepredicatethesemanticrolelabelingsystemiscurrentlylooking atanygivenpoint.Thereisthusaseriousimbalancebetween positivesamples (constituentsthatare argumentstoaparticularpredicate)and negativesamples (constituentsthatarenotargumentstothis particularpredicate).Machinelearningalgorithmsgenerallydonothandleextremelyunbalanced dataverywell. Forthesereasons,manysystemsdividethesemanticrolelabelingtaskintotwosteps, identiÞ-cation,inwhichabinarydecisionismadeastowhetheraconstituentcarriesasemanticroleforagiven predicate,and classiÞcationinwhichthespeciÞcsemanticroleis chosen.Separatemachinelearning classiÞersaretrainedforthesetwotasks,oftenwithmanyofthesamefeatures( GildeaandJurafsky ,2002;Pradhanetal. ,2005).Anotherapproachistouseasetofheuristicstopruneoutthemajorityofthenegativesamples, asapredicateÕsrolesaregenerallyfoundinalimitednumberofsyntacticrelationstothepredicate itself.Somesemanticlabelingsystemsuseacombinationofbothapproaches:heuristicsareÞrst appliedtopruneouttheconstituentsthatareobviouslynotanargumentforacertainpredicate, andthenabinaryclassiÞeristrainedtofurtherseparatethepositivesamplesfromthenegative samples.ThegoalofthisÞlteringprocessisjusttodecidewhetheraconstituentisanargumentor not.Thenamulti-classclassiÞeristrainedtodecidethespeciÞcsemanticroleforthisargument. IntheÞlteringstage,itisgenerallyagoodideatobeconservativeanderronthesideofkeeping toomanyconstituentsratherthanbeingtooaggressiveandÞlteringouttruearguments.Thiscan beachievedbyloweringthethresholdforpositivesamples,orconversely,raisingthethresholdfor negativesamples. (20)SSCCSNPVPandNP VPStrikes andmismanagementVBD VPPremier Ryzhkov VBD PPwereVBD warnedoftoughmeasures cited ;)(+##+%)2'%$,)6&$/"<)R+'%&)'%2"4"%(" ¥-.,61B+%$)#./68%6>1$6(B188)>),86,E,$;#.)"+6 ,+($,,=) [,1(.6&,()8)%"6 1=%'#616(%"8#)#',"#6)86/1&,6)"&,7,"&,"#B;6%>61BB6%#.,$8 ¥Y'#6#.)86(1"`#6=,6$)+.#M62%#86%>6 /,+.$,) %$R+'%& )"#,$1(#)%"86 =,#<,,"6 1$+'/,"#8 ¥Z%"8#)#',"#86 )"6 [$1/,J,# 1"&6 0$%7Y1"9 /'8#6=,6 "%" K%E,$B177)"+:6 ¥*6B%(1B68;8#,/6/1;6 )"(%$$,(#B;6B1=,B6#<%6%E,$B177)"+6 (%"8#)#',"#86 186 1$+'/,"#86 ¥0$%7Y1"9 &%,86"%#61BB%<6/'B#)7B,6)&,"#)(1B6 1$+'/,"#8 ¥B1=,B)"+6 %",6(%"8#)#',"#6 *IVa6 ¥-.'868.%'B&6)"($,18,6 #.,67$%=1=)B)#;6%>61"%#.,$6 =,)"+6*IV\6 Ff W+1)&+)9+)R+'%&)'%2"4"%(" ¥I,$1"9)"+ ¥-.,6>)$8#68#1+,6HI268;8#,/67$%&'(,86/'B#)7B,6 7%88)=B,6B1=,B86>%$6,1(.6(%"8#)#',"# ¥-.,68,(%"&68#1+,6(B188)>),$6#.,6=,8#6 /,+.$, B1=,B6>%$6 1BB6(%"8#)#',"#8 ¥U>#,"616(B188)>),$6#.1#6#19,861BB6#.,6)"7'#861B%"+6<)#.6 %#.,$6>,1#'$,86@8,o',"(,86%>6B1=,B8C Fe P+4")(+#:,'($&'+%6<) N4$#"Q"& n,6",,&61"6,D#$168#,76#%6>)"&6#.,6>$1/, F?22.6¥SEMANTIC ROLE LABELING 9Recallthatthedifferencebetweenthesetwomodelsofsemanticrolesisthat FrameNet( 22.27)employsmanyframe-speciÞcframeelementsasroles,whileProp- Bank( 22.28)usesasmallernumberofnumberedargumentlabelsthatcanbeinter- pretedasverb-speciÞclabels,alongwiththemoregeneralARGMlabels.Some examples: (22.27)[You]canÕt[blame][theprogram][forbeingunabletoidentifyit] COGNIZERTARGETEVALUEEREASON (22.28)[TheSanFranciscoExaminer]issued[aspecialedition][yesterday] ARG 0TARGETARG 1ARGM -TMP AsimpliÞedsemanticrolelabelingalgorithmissketchedinFig. 22.4.While therearealargenumberofalgorithms,manyofthemusesomeversionofthesteps inthisalgorithm. Mostalgorithms,beginningwiththeveryearliestsemanticroleanalyzers (Sim-mons,1973) ,beginbyparsing,usingbroad-coverageparserstoassignaparsetothe inputstring.Figure 22.5showsaparseof( 22.28)above.Theparseisthentraversed toÞndallwordsthatarepredicates. Foreachofthesepredicates,thealgorithmexamineseachnodeintheparsetree anddecidesthesemanticrole(ifany)itplaysforthispredicate. ThisisgenerallydonebysupervisedclassiÞcation.Givenalabeledtrainingset suchasPropBankorFrameNet,afeaturevectorisextractedforeachnode,using featuretemplatesdescribedinthenextsubsection. A1-of-NclassiÞeristhentrainedtopredictasemanticroleforeachconstituent giventhesefeatures,whereNisthenumberofpotentialsemanticrolesplusan extraNONErolefornon-roleconstituents.MoststandardclassiÞcationalgorithms havebeenused(logisticregression,SVM,etc).Finally,foreachtestsentencetobe labeled,theclassiÞerisrunoneachrelevantconstituent.Wegivemoredetailsof thealgorithmafterwediscussfeatures. functionSEMANTIC ROLE LABEL (words )returns labeledtree parse!PARSE (words )foreach predicate inparse doforeach nodeinparse dofeaturevector !EXTRACT FEATURES (node,predicate ,parse )CLASSIFY NODE (node,featurevector ,parse )Figure22.4 Agenericsemantic-role-labelingalgorithm.C LASSIFY NODE isa1-of- Nclas-siÞerthatassignsasemanticrole(orNONEfornon-roleconstituents),trainedonlabeleddata suchasFrameNetorPropBank. FeaturesforSemanticRoleLabeling Awidevarietyoffeaturescanbeusedforsemanticrolelabeling.Mostsystemsuse somegeneralizationofthecoresetoffeaturesintroducedby GildeaandJurafsky (2000).Atypicalsetofbasicfeaturesarebasedonthefollowingfeaturetemplates (demonstratedonthe NP-SBJconstituentTheSanFranciscoExaminer inFig. 22.5):¥Thegoverning predicate ,inthiscasetheverb issued.Thepredicateisacru- cialfeaturesincelabelsaredeÞnedonlywithrespecttoaparticularpredicate. ¥Thephrasetype oftheconstituent,inthiscase, NP(orNP-SBJ).Somese- manticrolestendtoappearas NPs,othersas SorPP,andsoon. 22.6¥SEMANTIC ROLE LABELING 9Recallthatthedifferencebetweenthesetwomodelsofsemanticrolesisthat FrameNet( 22.27)employsmanyframe-speciÞcframeelementsasroles,whileProp- Bank( 22.28)usesasmallernumberofnumberedargumentlabelsthatcanbeinter- pretedasverb-speciÞclabels,alongwiththemoregeneralARGMlabels.Some examples: (22.27)[You]canÕt[blame][theprogram][forbeingunabletoidentifyit] COGNIZERTARGETEVALUEEREASON (22.28)[TheSanFranciscoExaminer]issued[aspecialedition][yesterday] ARG 0TARGETARG 1ARGM -TMP AsimpliÞedsemanticrolelabelingalgorithmissketchedinFig. 22.4.While therearealargenumberofalgorithms,manyofthemusesomeversionofthesteps inthisalgorithm. Mostalgorithms,beginningwiththeveryearliestsemanticroleanalyzers (Sim-mons,1973) ,beginbyparsing,usingbroad-coverageparserstoassignaparsetothe inputstring.Figure 22.5showsaparseof( 22.28)above.Theparseisthentraversed toÞndallwordsthatarepredicates. Foreachofthesepredicates,thealgorithmexamineseachnodeintheparsetree anddecidesthesemanticrole(ifany)itplaysforthispredicate. ThisisgenerallydonebysupervisedclassiÞcation.Givenalabeledtrainingset suchasPropBankorFrameNet,afeaturevectorisextractedforeachnode,using featuretemplatesdescribedinthenextsubsection. A1-of-NclassiÞeristhentrainedtopredictasemanticroleforeachconstituent giventhesefeatures,whereNisthenumberofpotentialsemanticrolesplusan extraNONErolefornon-roleconstituents.MoststandardclassiÞcationalgorithms havebeenused(logisticregression,SVM,etc).Finally,foreachtestsentencetobe labeled,theclassiÞerisrunoneachrelevantconstituent.Wegivemoredetailsof thealgorithmafterwediscussfeatures. functionSEMANTIC ROLE LABEL (words )returns labeledtree parse!PARSE (words )foreach predicate inparse doforeach nodeinparse dofeaturevector !EXTRACT FEATURES (node,predicate ,parse )CLASSIFY NODE (node,featurevector ,parse )Figure22.4 Agenericsemantic-role-labelingalgorithm.C LASSIFY NODE isa1-of- Nclas-siÞerthatassignsasemanticrole(orNONEfornon-roleconstituents),trainedonlabeleddata suchasFrameNetorPropBank. FeaturesforSemanticRoleLabeling Awidevarietyoffeaturescanbeusedforsemanticrolelabeling.Mostsystemsuse somegeneralizationofthecoresetoffeaturesintroducedby GildeaandJurafsky (2000).Atypicalsetofbasicfeaturesarebasedonthefollowingfeaturetemplates (demonstratedonthe NP-SBJconstituentTheSanFranciscoExaminer inFig. 22.5):¥Thegoverning predicate ,inthiscasetheverb issued.Thepredicateisacru- cialfeaturesincelabelsaredeÞnedonlywithrespecttoaparticularpredicate. ¥Thephrasetype oftheconstituent,inthiscase, NP(orNP-SBJ).Somese- manticrolestendtoappearas NPs,othersas SorPP,andsoon. 22.6¥SEMANTIC ROLE LABELING 9Recallthatthedifferencebetweenthesetwomodelsofsemanticrolesisthat FrameNet( 22.27)employsmanyframe-speciÞcframeelementsasroles,whileProp- Bank( 22.28)usesasmallernumberofnumberedargumentlabelsthatcanbeinter- pretedasverb-speciÞclabels,alongwiththemoregeneralARGMlabels.Some examples: (22.27)[You]canÕt[blame][theprogram][forbeingunabletoidentifyit] COGNIZERTARGETEVALUEEREASON (22.28)[TheSanFranciscoExaminer]issued[aspecialedition][yesterday] ARG 0TARGETARG 1ARGM -TMP AsimpliÞedsemanticrolelabelingalgorithmissketchedinFig. 22.4.While therearealargenumberofalgorithms,manyofthemusesomeversionofthesteps inthisalgorithm. Mostalgorithms,beginningwiththeveryearliestsemanticroleanalyzers (Sim-mons,1973) ,beginbyparsing,usingbroad-coverageparserstoassignaparsetothe inputstring.Figure 22.5showsaparseof( 22.28)above.Theparseisthentraversed toÞndallwordsthatarepredicates. Foreachofthesepredicates,thealgorithmexamineseachnodeintheparsetree anddecidesthesemanticrole(ifany)itplaysforthispredicate. ThisisgenerallydonebysupervisedclassiÞcation.Givenalabeledtrainingset suchasPropBankorFrameNet,afeaturevectorisextractedforeachnode,using featuretemplatesdescribedinthenextsubsection. A1-of-NclassiÞeristhentrainedtopredictasemanticroleforeachconstituent giventhesefeatures,whereNisthenumberofpotentialsemanticrolesplusan extraNONErolefornon-roleconstituents.MoststandardclassiÞcationalgorithms havebeenused(logisticregression,SVM,etc).Finally,foreachtestsentencetobe labeled,theclassiÞerisrunoneachrelevantconstituent.Wegivemoredetailsof thealgorithmafterwediscussfeatures. functionSEMANTIC ROLE LABEL (words )returns labeledtree parse!PARSE (words )foreach predicate inparse doforeach nodeinparse dofeaturevector !EXTRACT FEATURES (node,predicate ,parse )CLASSIFY NODE (node,featurevector ,parse )Figure22.4 Agenericsemantic-role-labelingalgorithm.C LASSIFY NODE isa1-of- Nclas-siÞerthatassignsasemanticrole(orNONEfornon-roleconstituents),trainedonlabeleddata suchasFrameNetorPropBank. FeaturesforSemanticRoleLabeling Awidevarietyoffeaturescanbeusedforsemanticrolelabeling.Mostsystemsuse somegeneralizationofthecoresetoffeaturesintroducedby GildeaandJurafsky (2000).Atypicalsetofbasicfeaturesarebasedonthefollowingfeaturetemplates (demonstratedonthe NP-SBJconstituentTheSanFranciscoExaminer inFig. 22.5):¥Thegoverning predicate ,inthiscasetheverb issued.Thepredicateisacru- cialfeaturesincelabelsaredeÞnedonlywithrespecttoaparticularpredicate. ¥Thephrasetype oftheconstituent,inthiscase, NP(orNP-SBJ).Somese- manticrolestendtoappearas NPs,othersas SorPP,andsoon. 0$,&)(1#,E,(#%$ "WD#$1(#[$1/,[,1#'$,8 @7$,&)(1#,A71$8, C[$1/,6 "ZB188)>;[$1/, @7$,&)(1#,A7$,&)(1#,E,(#%$ CA6[$1/,C  N"$&34"6)2+4)N4$#")Z9"%&'2'($&'+% ComputationalLinguisticsVolume40,Number1 Table4 FeaturesusedforframeidentiÞcation(Equation(2)).Allalsoincorporate f,theframebeing scored. !=!w!,"!"consistsofthewordsandPOStags 20ofatargetseeninanexemplaror trainingsentenceasevoking f.ThefeatureswithstarredbulletswerealsousedbyJohansson andNugues(2007). ¥thePOSoftheparentoftheheadwordof ti¥!thesetofsyntacticdependenciesoftheheadword 21ofti¥!iftheheadwordof tiisaverb,thenthesetofdependencylabelsofitschildren ¥thedependencylabelontheedgeconnectingtheheadof tianditsparent ¥thesequenceofwordsintheprototype,w !¥thelemmatizedsequenceofwordsintheprototype ¥thelemmatizedsequenceofwordsintheprototypeandtheirpart-of-speechtags "!¥WordNetrelation 22#holdsbetween !andti¥WordNetrelation 22#holdsbetween !andti,andtheprototypeis !¥WordNetrelation 22#holdsbetween !andti,thePOStagsequenceof !is"!,andthePOS tagsequenceof tiis"texemplarsentences.Notethatthismodelmakesanindependenceassumption:Each frameispredictedindependentlyofallothersinthedocument.Inthiswaythemodel issimilartoJ&NÕ07.However,oursisasingleconditionalmodelthatsharesfeatures andweightsacrossalltargets,frames,andprototypes,whereastheapproachofJ&NÕ07 consistsofmanyseparatelytrainedmodels.Moreover,ourmodelisuniqueinthatit usesalatentvariabletosmoothoverframesforunknownorambiguousLUs. FrameidentiÞcationfeaturesdependonthepreprocessedsentencex,theprototype !anditsWordNetlexical-semanticrelationshipwiththetarget ti,andofcoursethe framef.Ourmodelusesbinaryfeatures,whicharedetailedinTable4. 5.3ParameterEstimation Givenatrainingdataset(eitherSemEval2007datasetortheFrameNet1.5fulltext annotations),whichisoftheform !!x(j),t(j),f(j),A(j)""Nj=1,wediscriminativelytrainthe frameidentiÞcationmodelbymaximizingthetrainingdatalog-likelihood: 23max"N!j=1mj!i=1log!!"Lf(j)ip"(f(j)i,!|t(j)i,x(j))(3) InEquation(3), mjdenotesthenumberofframesinasentenceindexedby j.Note thatthetrainingproblemisnon-convexbecauseofthesummed-outprototypelatent 20POStagsarefoundautomaticallyduringpreprocessing. 21Ifthetargetisnotasubtreeintheparse,weconsiderthewordsthathaveparentsoutsidethespan, andapplythreeheuristicrulestoselectthehead:(1)choosetheÞrstwordifitisaverb;(2)choosethe lastwordiftheÞrstwordisanadjective;(3)ifthetargetcontainstheword of,andtheÞrstwordisa noun,wechooseit.Ifnoneofthesehold,choosethelastwordwithanexternalparenttobethehead. 22Theseare:IDENTICAL-WORD,SYNONYM,ANTONYM(includingextendedandindirectantonyms), HYPERNYM,HYPONYM,DERIVEDFORM,MORPHOLOGICALVARIANT(e.g.,pluralform),VERB GROUP,ENTAILMENT,ENTAILED-BY,SEE-ALSO,CAUSALRELATION,andNORELATION. 23WefoundnobeneÞtoneitherdevelopmentdatasetfromusingan L2regularizer(zero-mean Gaussianprior). 24FFL186,#61B6@fa\?C  Q+&)R36&)_%/,'65 4.3.LANGUAGE-(IN)DEPENDENTSEMANTICROLELABELING63 constituentsinthatchainareassignedthesamesemanticrole.Theotherscenarioiswhenthereisa discontinuousargumentwheremultipleconstituentsjointlyplayarolewithrespecttoapredicate. Aconstituentinaparsetreereceivesmultiplesemanticroleswhenthereis argumentsharing where thisconstituentplaysaroleformultiplepredicates.Thiscanhappeninacoordinationstructure whenmultiplepredicatesareconjoinedandshareasubject.Thiscanalsohappeninsubjectcontrol orobjectcontrolstructureswhentwoverbsshareasubjectoranobject. (22)Arg0 ArgM-TMP ArgM-MNR !!  Rel Arg1 !! "! !! !"!! 4.3.2SEMANTICROLELABELINGFORVERBS Commonalities LikeEnglishsemanticrolelabeling,Chinesesemanticrolelabelingcanbeformu- latedasaclassiÞcationtaskwiththreedistinctstages:pruning,argumentidentiÞcation,andargument classiÞcation.ThepruningalgorithmdescribedinChapter3turnsouttobestraightforwardtoim- plementforChinesedata,anditinvolvesminorchangesinthephraselabels.Forexample, IPintheChineseTreebankcorrespondsroughlyto SinthePennTreebank,and CPcorrespondsroughly toSBAR.Example 23illustrateshowthepruningalgorithmworksforChinese.Assumingthe predicateofinterestis !!(ÒinvestigateÓ),thealgorithmÞrstaddstheNP( !"ÒaccidentÓ !!ÒcauseÓ)tothelistofcandidates.ThenitmovesupalevelandaddsthetwoADVPs( !!ÒnowÓ and"!ÒthoroughlyÓ)tothelistofcandidates.Atthenextlevel,thetwoVPsformacoordination structureandthusnocandidateisadded.Finally,atthenextlevel,theNP( !!ÒpoliceÓ)isadded tothelistofcandidates.Obviously,thepruningalgorithmworksbetterwhentheparsetreesthat aretheinputtothesemanticrolelabelingsystemarecorrect.Inarealisticscenario,theparsetrees aregeneratedbyasyntacticparserandarenotexpectedtobeperfect.However,experimentalresults F^ Q+&)R36&)7"4.6<) Q+#@$%A S!!!!!!""""""NP(AR G0) !!!"""NNPBen NNPBernank eVP!!!!!"""""VBD wasVP!!!!!"""""VBN (Supp ort) nominated PP!!!!!"""""INasNP!!!!""""NP(AR G1) ###$$$Greenspan ÕsNNpredicate replacemen tFigure1:Asamplesentenceanditsparsetreela- beledinthestyleofNomBank PropBankSRLanddiscussespossiblefuturere- searchdirections. 2OverviewofNomBank TheNomBank(Meyersetal.,2004c;Meyers etal.,2004b)annotationprojectoriginatedfrom theNOMLEX(Macleodetal.,1997;Macleodet al.,1998)nominalizationlexicondevelopedunder theNewYorkUniversityProteusProject.NOM- LEXlists1,000nominalizationsandthecorre- spondencesbetweentheirargumentsandthear- gumentsoftheirverbcounterparts.NomBank framescombinevariouslexicalresources(Meyers etal.,2004a),includinganextendedNOMLEX andPropBankframes,andformthebasisforanno- tatingtheargumentstructuresofcommonnouns. SimilartoPropBank,NomBankannotationis madeonthePennTreeBankII(PTBII)corpus. ForeachcommonnouninPTBIIthattakesargu- ments,itscoreargumentsarelabeledwithARG0, ARG1,etc,andmodifyingargumentsarelabeled withARGM-LOCtodenotelocation,ARGM- MNRtodenotemanner,etc.Annotationsare madeonPTBIIparsetreenodes,andargument boundariesalignwiththespanofparsetreenodes. Asamplesentenceanditsparsetreelabeled inthestyleofNomBankisshowninFigure1. ForthenominalpredicateÒreplacementÓ,ÒBen BernankeÓislabeledasARG0andÒGreenspan ÕsÓislabeledasARG1.Thereisalsothespecial labelÒSupportÓonÒnominatedÓwhichintroduces ÒBenBernankeÓasanargumentofÒreplacementÓ. Thesupportconstructwillbeexplainedindetailin Section4.2.3. WearenotawareofanyNomBank-basedauto- maticSRLsystems.Theworkin(Pradhanetal., 2004)experimentedwithanautomaticSRLsys- temdevelopedusingarelativelysmallsetofman- uallyselectednominalizationsfromFrameNetand PennChineseTreeBank.TheSRLaccuracyof theirsystemisnotdirectlycomparabletoours. 3Modeltrainingandtesting WetreattheNomBank-basedSRLtaskasaclas- siÞcationproblemanddivideitintotwophases: argumentidentiÞcationandargumentclassiÞca- tion.DuringtheargumentidentiÞcationphase, eachparsetreenodeismarkedaseitherargument ornon-argument.Eachnodemarkedasargument isthenlabeledwithaspeciÞcclassduringthe argumentclassiÞcationphase.TheidentiÞcation modelisabinaryclassiÞer,whiletheclassiÞca- tionmodelisamulti-classclassiÞer. Opennlpmaxent 1,animplementationofMaxi- mumEntropy(ME)modeling,isusedastheclas- siÞcationtool.SinceitsintroductiontotheNatural LanguageProcessing(NLP)community(Berger etal.,1996),ME-basedclassiÞershavebeen showntobeeffectiveinvariousNLPtasks.ME modelingisbasedontheinsightthatthebest modelisconsistentwiththesetofconstraintsim- posedandotherwiseasuniformaspossible.ME modelstheprobabilityoflabel lgiveninput xasinEquation1. fi(l,x )isafeaturefunctionthat mapslabel landinput xtoeither0or1,whilethe summationisoverall nfeaturefunctionsandwith !iastheweightparameterforeachfeaturefunc- tionfi(l,x ).Zxisanormalizationfactor.Inthe identiÞcationmodel,label lcorrespondstoeither ÒargumentÓorÒnon-argumentÓ,andintheclassi- Þcationmodel,label lcorrespondstooneofthe speciÞcNomBankargumentclasses.TheclassiÞ- cationoutputisthelabel lwiththehighestcondi- tionalprobability p(l|x).p(l|x)=exp(!ni=1!ifi(l,x ))Zx(1)TotraintheME-basedidentiÞcationmodel, trainingdataisgatheredbytreatingeachparsetree nodethatisanargumentasapositiveexampleand therestasnegativeexamples.ClassiÞcationtrain- ingdataisgeneratedfromargumentnodesonly. Duringtesting,thealgorithmofenforcingnon- overlappingargumentsby(Toutanovaetal.,2005) isused.Thealgorithmmaximizesthelog- probabilityoftheentireNomBanklabeledparse 1http://maxent.sourceforge.net/ 139FRG,;,$86,#61B:6faa? [)+'$,6>$%/6s)1"+6 1"&6J+6faa^  ;99'&'+%$,)Z663"6)2+4)%+3%6 ¥[,1#'$,8M ¥J%/)"1B)d1#)%"6B,D)(%"6@,/7B%;/,"# !,/7B%;C ¥G%$7.%B%+)(1B68#,/ ¥O,1B#.(1$,A6G,&)(1#,6 !(1$, ¥L)>>,$,"#67%8)#)%"8 ¥G%8#61$+'/,"#86%>6"%/)"1B67$,&)(1#,86%(('$6)"8)&,6#.,6J0 ¥U#.,$861$,6)"#$%&'(,&6=;68'77%$#6E,$=8 ¥W87,()1BB;6B)+.#6E,$=866b36/1&,61"61$+'/,"#cA6b46#%%9616"17c FX !"#$%&'()*+,") -$.",'%/ Z%"(B'8)%"  !"#$%&'()*+,")-$.",'%/ ¥*6B,E,B6%>68.1BB%<68,/1"#)(86>%$6$,7$,8,"#)"+6,E,"#861"&6#.,)$6 71$#)()71"#8 ¥!"#,$/,&)1#,6=,#<,,"671$8,861"&6>'BB68,/1"#)(8 ¥-<%6(%//%"61$(.)#,(#'$,8A6>%$6E1$)%'86B1"+'1+,8 ¥[$1/,J,# M6>$1/, K87,()>)(6$%B,8 ¥0$%7Y1"9 M60$%#% K$%B,8 ¥Z'$$,"#68;8#,/86,D#$1(#6=;6 ¥71$8)"+68,"#,"(, ¥[)"&)"+67$,&)(1#,86)"6#.,68,"#,"(, ¥[%$6,1(.6%",A6(B188)>;6,1(.671$8,6#$,,6(%"8#)#',"# ^a
Thematic roles	Semantics	Lexical semantics	Semantic syntax	Generative semantics	Semantic role labeling (SRL)
Identify all of the traces or ellipses in this sentence: “I like to eat the food you cook more than you.” (under the interpretation where cannibalism is not under consideration).	The first missing relationship is that “I” is also the subject of eat. The second one is that “food” is also the object of “cook.” The third one is that the phrase “eat the food” is also the object of “more than” but with the subject as the last “you” in the sentence, which refers to the same person as the first “you.” One might also say the word “that” after “food” is elided, but this is not important.	Which of the following sentences is not missing anything needed to complete its logic / grammar?	Bob believes eating animal flesh is wrong. 	That is incorrect. |||Order enough for everyone.|||John plays guitar, and Mary violin. 	NONE
Ellipsis	Parsing	Semantics	Grammar
Describe a semantic model for the following sentences by defining the domain, the properties, and the relations.	The model is: 	Alice, Bob, Chuck, Daryl = a, b, c, d 	D = { a,b,c,d } Properties: 	linguists = {a, b} 	computer scientists = { c,d } Relations: 	Work-for = {(a,d), (b,d)} 	Likes = {(a,b), (b,a), (d,a), (d,b)}	Which of the following describes extensional meaning in this model?	The properties	The denotations|||The relations|||The model as a whole	Chapter section on model-theoretic semantics||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 16 LogicalRepresentationsof SentenceMeaning I SHMAEL : Surelyallthisisnotwithoutmeaning. HermanMelville, MobyDick Inthischapterweintroducetheideathatthemeaningoflinguisticexpressionscan becapturedinformalstructurescalled meaningrepresentations .Considertasks meaning representations thatrequiresomeformofsemanticprocessing,likelearningtouseanewpieceof softwarebyreadingthemanual,decidingwhattoorderatarestaurantbyreading amenu,orfollowingarecipe.Accomplishingthesetasksrequiresrepresentations thatlinkthelinguisticelementstothenecessarynon-linguistic knowledgeofthe world .Readingamenuanddecidingwhattoorder,givingadviceaboutwhereto gotodinner,followingarecipe,andgeneratingnewrecipesallrequireknowledge aboutfoodanditspreparation,whatpeopleliketoeat,andwhatrestaurantsarelike. Learningtouseapieceofsoftwarebyreadingamanual,orgivingadviceonusing software,requiresknowledgeaboutthesoftwareandsimilarapps,computers,and usersingeneral. Inthischapter,weassumethatlinguisticexpressionshavemeaningrepresenta- tionsthataremadeupofthe samekindofstuff thatisusedtorepresentthiskindof everydaycommon-senseknowledgeoftheworld.Theprocesswherebysuchrepre- sentationsarecreatedandassignedtolinguisticinputsiscalled semanticparsing or semantic parsing semanticanalysis ,andtheentireenterpriseofdesigningmeaningrepresentations andassociatedsemanticparsersisreferredtoas computationalsemantics . computational semantics 9 e ; yHaving ( e ) ^ Haver ( e ; Speaker ) ^ HadThing ( e ; y ) ^ Car ( y ) Figure16.1 Alistofsymbols,twodirectedgraphs,andarecordstructure:asamplerof meaningrepresentationsfor Ihaveacar. ConsiderFig. 16.1 ,whichshowsexamplemeaningrepresentationsforthesen- tence Ihaveacar usingfourcommonlyusedmeaningrepresentationlanguages. Thetoprowillustratesasentencein First-OrderLogic ,coveredindetailinSec- tion 16.3 ;thedirectedgraphanditscorrespondingtextualformisanexampleof an AbstractMeaningRepresentation(AMR) form (Banarescuetal.,2013) ,and ontherightisa frame-based or  representation,discussedinSection 16.5 andagaininChapter18.  2 C HAPTER 16  L OGICAL R EPRESENTATIONSOF S ENTENCE M EANING Whiletherearenon-trivialdifferencesamongtheseapproaches,theyallshare thenotionthatameaningrepresentationconsistsofstructurescomposedfroma setofsymbols,orrepresentationalvocabulary.Whenappropriatelyarranged,these symbolstructuresaretakento correspond toobjects,propertiesofobjects,andrela- tionsamongobjectsinsomestateofaffairsbeingrepresentedorreasonedabout.In thiscase,allfourrepresentationsmakeuseofsymbolscorrespondingtothespeaker, acar,andarelationdenotingthepossessionofonebytheother. Importantly,theserepresentationscanbeviewedfromatleasttwodistinctper- spectivesinalloftheseapproaches:asrepresentationsofthemeaningofthepar- ticularlinguisticinput Ihaveacar ,andasrepresentationsofthestateofaffairsin someworld.Itisthisdualperspectivethatallowstheserepresentationstobeused tolinklinguisticinputstotheworldandtoourknowledgeofit. Inthenextsectionswegivesomebackground:ourdesiderataforameaning representationlanguageandsomeguaranteesthattheserepresentationswillactually dowhatweneedthemtodoŠprovideacorrespondencetothestateofaffairsbeing represented.InSection 16.3 weintroduceFirst-OrderLogic,historicallytheprimary techniqueforinvestigatingnaturallanguagesemantics,andseeinSection 16.4 how itcanbeusedtocapturethesemanticsofeventsandstatesinEnglish.Chapter17 thenintroducestechniquesfor semanticparsing :generatingtheseformalmeaning representationsgivenlinguisticinputs. 16.1ComputationalDesiderataforRepresentations Let'sconsiderwhymeaningrepresentationsareneededandwhattheyshoulddofor us.Tofocusthisdiscussion,let'sconsiderasystemthatgivesrestaurantadviceto touristsbasedonaknowledgebase. V Considerthefollowingsimplequestion: (16.1) DoesMaharaniservevegetarianfood? Toanswerthisquestion,wehavetoknowwhatit'sasking,andknowwhetherwhat it'saskingistrueofMahariniornot. v isasystem'sabilitytocompare v thestateofaffairsdescribedbyarepresentationtothestateofaffairsinsomeworld asmodeledinaknowledgebase.Forexamplewe'llneedsomesortofrepresentation like Serves ( Maharani ; VegetarianFood ) ,whichasystemcancanmatchagainstits knowledgebaseoffactsaboutparticularrestaurants,andifitarepresentation matchingthisproposition,itcanansweryes.Otherwise,itmusteithersay No ifits knowledgeoflocalrestaurantsiscomplete,orsaythatitdoesn'tknowifitknows itsknowledgeisincomplete. UnambiguousRepresentations Semantics,likealltheotherdomainswehavestudied,issubjecttoambiguity.Words andsentenceshavedifferentmeaningrepresentationsindifferentcontexts.Consider thefollowingexample: (16.2) Iwannaeatsomeplacethat'scloseto ICSI . Thissentencecaneithermeanthatthespeakerwantstoeat at somenearbylocation, orunderaGodzilla-as-speakerinterpretation,thespeakermaywanttodevoursome  16.1  C OMPUTATIONAL D ESIDERATAFOR R EPRESENTATIONS 3 nearbylocation.Thesentenceisambiguous;asinglelinguisticexpressioncanhave oneoftwomeanings.Butour meaningrepresentations itselfcannotbeambiguous. Therepresentationofaninput'smeaningshouldbefreefromanyambiguity,sothat thethesystemcanreasonoverarepresentationthatmeanseitheronethingorthe otherinordertodecidehowtoanswer. Aconceptcloselyrelatedtoambiguityis vagueness :inwhichameaningrepre- vagueness sentationleavessomepartsofthemeaningVaguenessdoesnotgive risetomultiplerepresentations.Considerthefollowingrequest: (16.3) IwanttoeatItalianfood. While Italianfood mayprovideenoughinformationtoproviderecommendations,it isnevertheless vague astowhattheuserreallywantstoeat.Avaguerepresentation ofthemeaningofthisphrasemaybeappropriateforsomepurposes,whileamore representationmaybeneededforotherpurposes. CanonicalForm Thedoctrineof canonicalform saysthatdistinctinputsthatmeanthesamething canonicalform shouldhavethesamemeaningrepresentation.Thisapproachgreatlyrea- soning,sincesystemsneedonlydealwithasinglemeaningrepresentationfora potentiallywiderangeofexpressions. Considerthefollowingalternativewaysofexpressing( 16.1 ): (16.4) DoesMaharanihavevegetariandishes? (16.5) DotheyhavevegetarianfoodatMaharani? (16.6) ArevegetariandishesservedatMaharani? (16.7) DoesMaharaniservevegetarianfare? Despitethefactthesealternativesusedifferentwordsandsyntax,wewantthem tomaptoasinglecanonicalmeaningrepresentations.Iftheywerealldifferent, assumingthesystem'sknowledgebasecontainsonlyasinglerepresentationofthis fact,mostoftherepresentationswouldn'tmatch.Wecould,ofcourse,storeall possiblealternativerepresentationsofthesamefactintheknowledgebase,butdoing sowouldleadtoenormousdifinkeepingtheknowledgebaseconsistent. Canonicalformdoescomplicatethetaskofsemanticparsing.Oursystemmust concludethat vegetarianfare , vegetariandishes ,and vegetarianfood refertothe samething,that having and serving areequivalenthere,andthatalltheseparse structuresstillleadtothesamemeaningrepresentation.Orconsiderthispairof examples: (16.8) Maharaniservesvegetariandishes. (16.9) VegetariandishesareservedbyMaharani. Despitethedifferentplacementoftheargumentsto serve ,asystemmuststillassign Maharani and vegetariandishes tothesamerolesinthetwoexamplesbydraw- ingongrammaticalknowledge,suchastherelationshipbetweenactiveandpassive sentenceconstructions. InferenceandVariables Whataboutmorecomplexrequestssuchas: (16.10) CanvegetarianseatatMaharani? Thisrequestresultsinthesameanswerastheothersnotbecausetheymeanthesame thing,butbecausethereisacommon-senseconnectionbetweenwhatvegetarianseat
Model-theoretic semantics	Denotation	Logical semantics
What are thematic roles?	Thematic roles describe certain roles that each constituent of a sentence can play in relation to the state or action described by the verb. Some of the most common thematic roles are Agent, Patient, Theme, and Beneficiary. Agents, which are usually subjects, perform an action. Patients, which are usually objects, are changed by actions. "Billy ate the pie" is an Agent-verb-Patient sentence. But subjects and objects can also be other things. A Theme is something being located or moved as in "The ball (Theme) rolled down the hill." And, "Bob got the message" shows a Recipient-subject. Thematic roles describe the semantics-syntax interface, translating the grammatical relations imposed by verbs into semantic relations.	Which of the following is NOT true?	Person and Organization could be thematic roles.	Setting and Bystander could be thematic roles.|||Thematic roles are assigned to arguments by verbs.|||The subject of a verb could be an Instrument.	Thematic Roles Presentation||slides||1LING 222: Thematic Roles1Thematic RolesSaeed: Chapter 6.1-6.6LING 222: Thematic Roles2List of Basic Thematic Roles¥AGENT: the initiator of some action, capable ofacting with volition.ÐJack ate the beans.¥PATIENT: the entity undergoing the effect of someaction, often undergoing some change of state.ÐSue mowed the lawn.¥THEME: the entity which is moved by an action, orwhose location is described.ÐFred threw the rock.¥EXPERIENCER: the entity which is aware of theaction or state described by the predicate but whichis not in control of the action or state.ÐKim saw the deer. 2LING 222: Thematic Roles3¥BENEFICIARY: the entity for whose benefit theaction was performed.ÐMary studied hard for her mother.¥INSTRUMENT: the means by which an action isperformed or something comes about.ÐFred opened the lock with a paper clip.¥LOCATION: the place in which something is situatedor takes place.ÐThe picture hangs above the fireplace.¥GOAL: the entity towards which something moves,either literally or metaphorically.ÐLee walked to school.¥SOURCE: the entity from which something moves,either literally or metaphorically.ÐSue ran from the policeman.LING 222: Thematic Roles4Additional Thematic Roles¥ACTOR:  the entity which performs, effects,instigates, or controls the situation denoted by thepredicate (supertype of AGENT):ÐThe bus hit a pedestrian.¥RECIPIENT: a subtype of GOAL involved in actionsdescribing changes of possession.ÐBill sold the car to Mary¥PERCEPT/STIMULUS: the entity which is perceivedor experienced.ÐMary fears thunder. 3LING 222: Thematic Roles5Tests for Thematic RolesÐFred mowed the lawn.¥ACTORÐWhat X did wasÉ¥What Fred did was mow the lawn.¥PATIENTÐWhat happened to Y wasÉ¥What happened to the lawn was that Fred mowed it.ÐWhat X did to Y wasÉ¥What Fred did to the lawn was mow it.LING 222: Thematic Roles6How many Thematic Roles can an NP have?¥ChomskyÕs 1981 Theta CriterionÐThere must be a one-to-one correspondence between nounphrases and thematic roles.¥Jackendoff 1990: two-tier approachÐSue   hit Fred.Theme     Goal(thematic tier)Actor        Patient(action tier)ÐPete threw the ball.Source         Theme(thematic tier)Actor   Patient(action tier)ÐBill entered the room.Theme          Goal(thematic tier)Actor(action tier)ÐBill received a letter.Goal               Theme(thematic teir)(action tier)
Thematic roles	Thematic relations	Semantics	Syntax-Semantics Interface	Grammatical relations	Syntax	Noun phrases	Verbs
What strategies exist to improve the performance of weak classifiers, such as decision trees?	Different "ensemble methods" exist to combine multiple weak classifiers into one model or to have multiple weak classifiers "vote" in a democratic system and predict the "winning" label. Random Forest and AdaBoost are common algorithms that utilize this strategy.	Which of the following is not a strategy to improve the performance of decision trees?	Removing irrelevant features.	Having multiple decision trees "vote" on a label.|||Combining multiple decision trees via weights.|||Adding more features to a decision tree.	Natural Language Processing (NLP) Presentation||slides||2BCK+K(H#1)BB+ !#-.%-;$(0$<-;l&($j+-;,%(&;$,($/>@;;%0< !GR:$-]@)*>-E$:I#$,@??%&? !K00%/%-&, C$A+,$*.(&-$,($ (H-.0%,,%&? !T-@O$/>@;;%0%-.$3A-,,-.$,=@&$.@&B()><$?+-;;%&?8 !U)*.(H-)-&,;E$ 2B@!((;, (.$K&;-)A>-$)-,=(B;|||Ensemble Learning clip||youtube||N/A
Decision Trees	Supervised learning	Ensemble methods	Boosting	AdaBoost	Random Forest
What's a Decision Tree?	A decision tree is a machine learning model that follows a series of binary (yes or no) "decisions" to form a prediction. For example, a decision tree may be formed to predict a person's favorite app as such:If the user is a male younger than 15 years old, predict Pokemon.If the user is a male at least 15 years years old, predict Snapchat.If the user is a female younger than 15 years old, predict Disney.If the user is a female older than 15 years old, predict TikTok.Decision trees are relatively easy to implement, but are prone to overfit data (in otherwords, they do not generalize well from a training set to testing set). They are considered "weak classifiers," which means they make predictions at a better rate than random guessing, but only slightly better. However, "Boosting" methods exist which combine multiple weak classifiers to form a strong classifier. For example, 100 different decision trees that aim to predict a person's favorite app could each "vote" on a label prediction, and the label with the most votes would serve as the prediction. This is known as an ensemble algorithm, and ensemble algorithms often perform better than any individual part because they will receive stronger weights for more relevant features.	What is a decision tree?	A series of yes or no questions used to form a prediction	An unsupervised learning model|||A strong classifier|||All of the above|||None of the above	Natural Language Processing (NLP) Presentation||slides||2BCK+K(H#1)BB+ !#-.%-;$(0$<-;l&($j+-;,%(&;$,($/>@;;%0< !GR:$-]@)*>-E$:I#$,@??%&? !K00%/%-&, C$A+,$*.(&-$,($ (H-.0%,,%&? !T-@O$/>@;;%0%-.$3A-,,-.$,=@&$.@&B()><$?+-;;%&?8 !U)*.(H-)-&,;E$ 2B@!((;, (.$K&;-)A>-$)-,=(B;
Statistical Natural Language Processing (NLP)	Natural Language Processing (NLP) algorithms	Natural Language Processing (NLP)	Decision Trees	Supervised learning	Weak classifiers
How should features be selected for Logistic Regression?	Often, features for Logistic Regression will be set by hand, with the implementer relying on linguistic intuition to select features that could be significant in determining a class. For instance, in the task of sentiment analysis, it seems logical to include count of positive lexicons as a feature and count of negative lexicons as a feature. Features can be adjusted when constructing a model, as an implementer could see features with weights close to 0 to potentially remove as noise and add or test different ideas for features.  Unsupervised learning could also be used to analyze a training corpus to automatically determine features.	What does a weight of \(w_i = 0\) imply about its associated feature in a logistic regression model? Assume a bias of 0.	The feature may be irrelevant for determining class.	The feature does not exist in any training examples.|||The feature may or may not have a strong positive or negative impact on determining the class.|||Both A and B.	Speech and Language Processing: Logistic Regression||publication||5.1  C LASSIFICATION : THESIGMOID 5 Figure5.2 Asampleminitestdocumentshowingtheextractedfeaturesinthevector x . Giventhese6featuresandtheinputreview x , P (+ j x ) and P ( j x ) canbecom- putedusingEq. 5.5 : p (+ j x )= P ( Y = 1 j x )= s ( w  x + b ) = s ([ 2 : 5 ;  5 : 0 ;  1 : 2 ; 0 : 5 ; 2 : 0 ; 0 : 7 ]  [ 3 ; 2 ; 1 ; 3 ; 0 ; 4 : 19 ]+ 0 : 1 ) = s ( : 833 ) = 0 : 70 (5.6) p ( j x )= P ( Y = 0 j x )= 1  s ( w  x + b ) = 0 : 30 LogisticregressioniscommonlyappliedtoallsortsofNLPtasks,andanyproperty oftheinputcanbeafeature.Considerthetaskof perioddisambiguation :deciding ifaperiodistheendofasentenceorpartofaword,byclassifyingeachperiod intooneoftwoclassesEOS(end-of-sentence)andnot-EOS.Wemightusefeatures like x 1 belowexpressingthatthecurrentwordislowercaseandtheclassisEOS (perhapswithapositiveweight),orthatthecurrentwordisinourabbreviations dictionary(ﬁProf.ﬂ)andtheclassisEOS(perhapswithanegativeweight).Afeature canalsoexpressaquitecomplexcombinationofproperties.Forexampleaperiod followinganuppercasewordislikelytobeanEOS,butiftheworditselfis St. and thepreviouswordiscapitalized,thentheperiodislikelypartofashorteningofthe word street . x 1 = ˆ 1ifﬁ Case ( w i )= Lowerﬂ 0otherwise x 2 = ˆ 1ifﬁ w i 2 AcronymDictﬂ 0otherwise x 3 = ˆ 1ifﬁ w i = St.& Case ( w i  1 )= Capﬂ 0otherwise Designingfeatures: Featuresaregenerallydesignedbyexaminingthetraining setwithaneyetolinguisticintuitionsandthelinguisticliteratureonthedomain.A carefulerroranalysisonthetrainingsetordevsetofanearlyversionofasystem oftenprovidesinsightsintofeatures. Forsometasksitisespeciallyhelpfultobuildcomplexfeaturesthatarecombi- nationsofmoreprimitivefeatures.Wesawsuchafeatureforperioddisambiguation above,whereaperiodontheword St. waslesslikelytobetheendofthesentence ifthepreviouswordwascapitalized.ForlogisticregressionandnaiveBayesthese combinationfeaturesor featureinteractions havetobedesignedbyhand. feature interactions  6 C HAPTER 5  L OGISTIC R EGRESSION Formanytasks(especiallywhenfeaturevaluescanreferencewords) we'llneedlargenumbersoffeatures.Oftenthesearecreatedautomaticallyvia fea- turetemplates ,abstractoffeatures.Forexampleabigramtemplate feature templates forperioddisambiguationmightcreateafeatureforeverypairofwordsthatoccurs beforeaperiodinthetrainingset.Thusthefeaturespaceissparse,sinceweonly havetocreateafeatureifthatn-gramexistsinthatpositioninthetrainingset.The featureisgenerallycreatedasahashfromthestringdescriptions.Auserdescription ofafeatureas,ﬁbigram(Americanbreakfast)ﬂishashedintoauniqueinteger i that becomesthefeaturenumber f i . Inordertoavoidtheextensivehumaneffortoffeaturedesign,recentresearchin NLPhasfocusedon representationlearning :waystolearnfeaturesautomatically inanunsupervisedwayfromtheinput.We'llintroducemethodsforrepresentation learninginChapter6andChapter7. Choosinga Logisticregressionhasanumberofadvantagesovernaive Bayes.NaiveBayeshasoverlystrongconditionalindependenceassumptions.Con- sidertwofeatureswhicharestronglycorrelated;infact,imaginethatwejustaddthe samefeature f 1 twice.NaiveBayeswilltreatbothcopiesof f 1 asiftheyweresep- arate,multiplyingthembothin,overestimatingtheevidence.Bycontrast,logistic regressionismuchmorerobusttocorrelatedfeatures;iftwofeatures f 1 and f 2 are perfectlycorrelated,regressionwillsimplyassignpartoftheweightto w 1 andpart to w 2 .Thuswhentherearemanycorrelatedfeatures,logisticregressionwillassign amoreaccurateprobabilitythannaiveBayes.Sologisticregressiongenerallyworks betteronlargerdocumentsordatasetsandisacommondefault. Despitethelessaccurateprobabilities,naiveBayesstilloftenmakesthecorrect decision.Furthermore,naiveBayescanworkextremelywell(some- timesevenbetterthanlogisticregression)onverysmalldatasets (NgandJordan, 2002) orshortdocuments (WangandManning,2012) .Furthermore,naiveBayesis easytoimplementandveryfasttotrain(there'snooptimizationstep).Soit'sstilla reasonableapproachtouseinsomesituations. 5.2LearninginLogisticRegression Howaretheparametersofthemodel,theweights w andbias b ,learned?Logistic regressionisaninstanceofsupervisedcationinwhichweknowthecorrect label y (either0or1)foreachobservation x .WhatthesystemproducesviaEq. 5.5 is‹ y ,thesystem'sestimateofthetrue y .Wewanttolearnparameters(meaning w and b )thatmake‹ y foreachtrainingobservationascloseaspossibletothetrue y . Thisrequires2componentsthatweforeshadowedintheintroductiontothe chapter.Theisametricforhowclosethecurrentlabel(‹ y )istothetruegold label y .Ratherthanmeasuresimilarity,weusuallytalkabouttheoppositeofthis: the distance betweenthesystemoutputandthegoldoutput,andwecallthisdistance the loss functionorthe costfunction .Inthenextsectionwe'llintroducetheloss loss functionthatiscommonlyusedforlogisticregressionandalsoforneuralnetworks, the cross-entropyloss . Thesecondthingweneedisanoptimizationalgorithmforiterativelyupdating theweightssoastominimizethislossfunction.Thestandardalgorithmforthisis gradientdescent ;we'llintroducethe stochasticgradientdescent algorithminthe followingsection.
Logistic regression	Natural Language Processing (NLP)
Of the branches of linguistics, which one deals with gender articles?	Syntax deals with gender articles because they are elements of word order.	The "What's in a pronoun? Why gender-fair language matters" study showed that Russian and Spanish-speaking participants displayed more sexism on a social attitude scale than their English-speaking counterparts. What is one possible explanation for this? 	Gender articles affect cognition (how we think)	People perceive the world in terms of gender as proposed by Skinner's Behavioral Theory|||Top-down processing	"What’s in a pronoun? Why gender-fair language matters" paper||publication||What•s in a pronoun? Why gender-fair language matters Chelsea A. Harris, MD1, Natale Biencowe, MBBS 2, and Dana A. Telem, MD, MPH 1,3,**1Department of Surgery, University of Michigan 2Centre for Surgical Research, University of Bristol, Bristol, UK 3Center for Healthcare Outcomes and Policy, University of Michigan Keywords Gender bias; gendered language; linguistics; gender-fair language; gender-neutral language;  surgery Introduction •The influence of language on thought is obligatory or at least habitual: thought is  always, or under most circumstances, guided by language† (Malt et al, 2003)As the surgical workforce diversifies, the hierarchies and gender norms that have long  characterized the profession are being challenged. This culture change has created a climate where overt discrimination is no longer tolerated and overall rates are declining 1,2. However,  while these gains are commendable, discrimination hasn‡t disappeared…it has become  subtler. Implicit biases, which are the automatic and often unconscious beliefs each of us  hold, are a key example and may contribute to the well-recognized gender achievement gap  in surgery. 3,4Implicit biases can manifest in many ways, from decisions regarding who should be on an  expert panel to the posture we assume when speaking with a colleague, but one of the most  powerful ways implicit bias can act is through language. Currently, many in surgery lack a  robust understanding of how their language can perpetuate gender or other stereotypes.  Often, when it comes to terminology reform, male and female skeptics alike shrug off a need  for change, dismissing any linguistic modernization as mere political-correctness. This  approach is neither helpful nor appropriate. Here we review the science detailing the ways  language reinforces gender inequality and offer strategies to decrease linguistic bias. Why You Should Care: The Impact of Gender Bias in Language Linguistic relativity, or the idea that language directs thought, has been shown to operate in  multiple contexts. 5,6 In a 2014 study, investigators demonstrated that objects‡ grammatical  **Corresponding Author:  Dana A. Telem MD, MPH, Associate Professor of Surgery, Department of Surgery, University of  Michigan, 1500 E. Medical Center Drive, Ann Arbor, MI 48109, (P) 734-936-5792, dtelem@med.umich.edu.  HHS Public Access Author manuscriptAnn Surg . Author manuscript; available in PMC 2018 January 19. Published in final edited form as: Ann Surg . 2017 December ; 266(6): 932—933. doi:10.1097/SLA.0000000000002505.Author ManuscriptAuthor ManuscriptAuthor ManuscriptAuthor Manuscript  gender strongly predicted whether Spanish/Russian (gendered language) speakers classified  objects as feminine or masculine.7 In other words, their language guided their decisions. In  this case the result seems inconsequential…few would be deeply invested in whether a table  is referred to in masculine or feminine terms; however, linguistic relativity becomes  problematic when language reinforces inequality. There is growing evidence that societies with gendered language consistently display deeper  gender inequality than societies with neutral language.8,9 For instance, in the  aforementioned study, Russian/Spanish-speaking participants displayed more sexism on the  study‡s social attitude scale than their English-speaking counterparts. Similarly, using the  Global Gender Gap Index which •benchmarks national gender gaps on economic, political,  education- and health-based criteria†, Prewitt-Freilino et. al demonstrated that countries  where >70% of the population spoke a gendered language scored lower on both the overall  index and on economic subscales. 10 In this context, it appears that language not only reflects  and defines culture, but actually shapes cultural norms. Language appears to play a particularly important role in molding individuals‡ attitudes  toward gender and occupation. Bem and Bem‡s landmark study reported that women were  significantly less likely to apply for jobs with masculine suffixes (- man versus — person).11 In a simulated hiring experiment, German-speaking business students rated standardized  female applicants as less suitable for high-power positions when the job description used  male rather than paired forms (e.g. Gesch–ftsfƒhrer versus  Gesch–ftsfƒhrerin/Gesch–ftsfƒhrer).12 Moreover, research suggests that language-induced stereotyping can be  difficult to overcome. Even when explicitly told that masculine generics (•he/him†) are  meant to include all genders, using male pronouns causes readers to imagine men. For  instance, college students who were told to complete sentences about professionals using the  gender-neutral  they  reported picturing fewer men than those who completed sentences using  he/him, even though both groups were clearly informed that pronouns referred to men and  women. 13Perhaps most critically, elements of linguistic bias appear to start early. According to  Gottfredson‡s theory on career development, by age six, children begin eliminating  occupations that contrast with their gender self-concept.14 Therefore, language surrounding gender and professional potential may be highly influential during this time. In experimental  settings, female schoolchildren deemed women to less successful in stereotypically male  professions (e.g. engineer, astronaut) when teachers described the occupation using  masculine rather than gender-neutral terminology. 15 More broadly, numerous studies  demonstrate that children‡s general linguistic environment also skews male. Analyses of  children‡s literature indicate that not only do male storybook characters vastly outnumber  females, but male characters enjoy heroic roles whereas female characters are relegated to  dependency themes. Furthermore, reading stereotypically masculine stores has been shown  to immediately narrow the scope of play that girls accept as appropriate for their gender. Taken together these results lend credence to the conclusion that gendered language is not  benign. Language bias has real and measurable consequences for individuals and society. Harris et al.Page 2 Ann Surg . Author manuscript; available in PMC 2018 January 19. Author ManuscriptAuthor ManuscriptAuthor ManuscriptAuthor Manuscript  Gendered Language in Medicine and Surgery Medicine is not immune to language bias. While studies are limited, their conclusions mirror findings from other disciplines Gender bias has been repeatedly documented in the language  used for federal funding award reviews, letters of recommendation, and tenure promotion  evaluations. 16—20 Formal recommendations frequently praise female physicians for •being  part of a team†, highlight women‡s teaching abilities, and refer to ⁄her training‡. In contrast,  male physicians‡ evaluations commend their •decisiveness†, often reference their research,  and refer to ⁄his career.‡ Differences extend to verbal language as well. As reported by Files  et al., women introduced by men in professional settings were significantly less likely to be  referred to by their title than men introduced by men.21This linguistic bias, although often implicit and unintentional, reinforces gender norms and perpetuates stereotyping. By using a different set of descriptors to depict valued male  (assertive, ambitious) and female (warm, communal) behaviors, we predispose female  surgeons to face conflicts between their gender roles and professional advancement.  Language patterns that diminish women‡s standings by omitting their titles and downplaying  their individual contributions make it more difficult for women to be seen as leaders. The  accumulation of these and other microinvalidations, potentiate constructs where women  don‡t have an equal presence at the table…operating or boardroom. As a result, female  physicians continue to grapple with slower advancement, lower pay, and higher attrition.  Moreover, because many of these biases operate at the subconscious level, they are difficult  to identify and address. Thus, mindful strategies to combat biases are needed. What can you do: Strategies to Avoid Gender Bias in Language Gender-fair language has been proposed to reduce both discrimination and gender  stereotyping. This can be accomplished through a variety of approaches, a few of which are  highlighted here:1. Language Neutralization Here, gender-neutral forms replace masculine forms or are removed all together. For  example, in lieu of gender-specific words such as •chairman† or •policeman†, the  corresponding •chairperson† or •police officer† is used. In cases where the gender is  unknown or indeterminate, or in languages where the traditional norm is to use a male  pronoun to refer to all genders, a neutral pronoun should be used instead.22 ⁄They† is now  widely accepted to have both singular and plural usages. The use of ⁄(s)he‡ or ⁄him/her‡ is  better than exclusively defaulting to a male generic, although this terminology reinforces  gender binaries and may still be problematic for transgender individuals. Completely new  pronouns such as ⁄ze‡ also exist, but have not been widely adopted. 232) Language Feminization Feminization is another approach; it relies on the proper use of feminine forms to increase the visibility of women in traditionally male fields. For example, masculine generic terms  are replaced with feminine-masculine word pairs. So instead of ⁄professor‡, one would  specify ⁄woman professor‡. 22 This strategy is somewhat more controversial. Although some  Harris et al.Page 3 Ann Surg . Author manuscript; available in PMC 2018 January 19. Author ManuscriptAuthor ManuscriptAuthor ManuscriptAuthor Manuscript  data suggest that feminine-masculine word pairs increase female mental imagery (meaning  more individuals who read the term picture women), others suggest that this approach also  reinforces gender binaries.24,25 Additionally, some argue that this feminization undervalues  female versions by tacitly confirming that terms such as ⁄professor‡ are implicitly male.  Attempts to feminize words using gender-specific stems (e.g. stewardess, comedienne) face  similar problems, as their longer more complex forms ensure female versions are never the  default. 3) Self-awareness and objectivity Finally, and perhaps most importantly, authors must be self-reflective and objective about  their own biases. Although this discussion focuses on gender bias, similar issues exist in the  language used to describe many populations. When writing publicly about any group,  authors should refer to members by their preferred terms, whether based on race/ethnicity,  gender, sexuality, or any other identity parameter. Authors should seek out diverse feedback  to mitigate the chance that their language will reinforce disparity, and strive for a people-first  approach.How we can all evolve Ultimately, in language, as in medicine, taking the position that our current approach is  justified because ⁄it has always been our approach‡ is not tenable. Much like the adoption of  any new technology or technique, evolving our terminology will almost certainly cause  growing pains. However, surgical workforce demographics have changed and are going to  keep changing. Thus, we must be rigorous in establishing a nomenclature that promotes not  only gender-inclusive language, but processes that represent the broad racial, social, and  sexual identities of our colleagues. In the end, achieving linguistic perfection may not be  possible, but we should strive for the same standards applied to all surgical trainees: make a  good faith effort, seek consultation when you are unsure, and admit humbly and openly  when you have erred. The onus is on all of us to challenge our biases and do better. References 1. Jacobs CD, Bergen MR, Korn D. Impact of a program to diminish gender insensitivity and sexual  harassment at a medical school. Academic medicine : journal of the Association of American Medical Colleges. 2000; 75(5):464—469. [PubMed: 10824771]  2. Best CL, Smith DW, Raymond JR Sr, Greenberg RS, Crouch RK. Preventing and responding to  complaints of sexual harassment in an academic health center: a 10-year review from the Medical  University of South Carolina. Academic medicine : journal of the Association of American Medical  Colleges. 2010; 85(4):721—727. [PubMed: 20354396]  3. Chapman EN, Kaatz A, Carnes M. Physicians and implicit bias: how doctors may unwittingly  perpetuate health care disparities. Journal of general internal medicine. 2013; 28(11):1504—1510. [PubMed: 23576243] 4. Girod S, Fassiotto M, Grewal D, et al. Reducing Implicit Gender Leadership Bias in Academic  Medicine With an Educational Intervention. Academic medicine : journal of the Association of  American Medical Colleges. 2016; 91(8):1143—1150. [PubMed: 26826068]  5. Wolff P, Holmes KJ. Linguistic relativity. Wiley interdisciplinary reviews. Cognitive science. 2011;  2(3):253—265. [PubMed: 26302074] Harris et al.Page 4 Ann Surg . Author manuscript; available in PMC 2018 January 19. Author ManuscriptAuthor ManuscriptAuthor ManuscriptAuthor Manuscript
Syntax	Gender linguistics	Branches of linguistics
How do tonic syllables relate to intonation?	Tonic syllables are the syllables in a word that receive the most stress in speech. There is one tonic stress in an intonation unit.	Why are tonic syllables the most important syllables in a tone unit?	Because they are the syllables that are the main stress of the respective tone unit..	Because they're the basic building blocks of the word.|||Because they carry meaning.|||Because they change the meaning of tone units.	Tonic syllables and weak forms clip||youtube||N/A
Morphemes	Tonic syllables	Intonation	Weak forms	Morphology
What is meant by "bag of words" in the context of text classification?	Bag of words refers to using a count of the words in a document to represent that document. For instance, consider the following sentence: "Today is a great day to have a great day." A standard bag of words may represent this sentence as follows:  Today: 1 is: 1 a: 2 great: 2 day: 2 to: 1 have: 1  Before forming such models, preprocessing can be done such as normalization (including case folding to lowercase all terms and/or lemmatization to convert all words to a root word) and removing stop words (removing common words such as 'and' or 'there' that are common words with little-to-no bearing on a prediction label). Additionally, other variations of bag of words could include stopping counts at 1, and thus having a bag of words representing if a word is included in a document or not.	Which of the following is NOT an appropriate bag of words for the following sentence: "It is not my fault, it is her fault."	it: 2 is: 2 my: 1 fault: 2 her: 1	It: 1 is: 2 not: 1 my: 1 fault: 2 it: 1 her: 1|||it: 2 is: 2 not: 1 my: 1 fault: 2 her: 1|||it: 1 is: 1 not: 1 my: 1 fault: 1 her: 1	Bag of words clip||youtube||N/A|||Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||4.1  N AIVE B AYES C LASSIFIERS 3 4.1NaiveBayes Inthissectionweintroducethe multinomialnaiveBayes ,socalledbe- naiveBayes  causeitisaBayesianthatmakesasimplifying(naive)assumptionabout howthefeaturesinteract. TheintuitionoftheisshowninFig. 4.1 .Werepresentatextdocument asifitwerea bag-of-words ,thatis,anunorderedsetofwordswiththeirposition ords ignored,keepingonlytheirfrequencyinthedocument.Intheexampleinthe insteadofrepresentingthewordorderinallthephraseslikeﬁIlovethismovieﬂand ﬁIwouldrecommenditﬂ,wesimplynotethattheword I occurred5timesinthe entireexcerpt,theword it 6times,thewords love , recommend ,and movie once,and soon. Figure4.1 IntuitionofthemultinomialnaiveBayesappliedtoamoviereview.Thepositionofthe wordsisignored(the bagofwords assumption)andwemakeuseofthefrequencyofeachword. NaiveBayesisaprobabilistic,meaningthatforadocument d ,outof allclasses c 2 C theclassireturnstheclass‹ c whichhasthemaximumposterior probabilitygiventhedocument.InEq. 4.1 weusethehatnotation ‹ tomeanﬁour ‹ estimateofthecorrectclassﬂ. ‹ c = argmax c 2 C P ( c j d ) (4.1) Thisideaof Bayesianinference hasbeenknownsincetheworkof Bayes(1763) , Bayesian inference andwasappliedtotextby MostellerandWallace(1964) .The intuitionofBayesianistouseBayes'ruletotransformEq. 4.1 into otherprobabilitiesthathavesomeusefulproperties.Bayes'ruleispresentedin Eq. 4.2 ;itgivesusawaytobreakdownanyconditionalprobability P ( x j y ) into threeotherprobabilities: P ( x j y )= P ( y j x ) P ( x ) P ( y ) (4.2)
Bag-of-words	Text classification	Word vectors	Lemmatization	Case folding
What is the difference between statistical and symbolic NLP?	Symbolic NLP is the older approach to NLP, which has been used since the 1960s at least. Symbolic means that linguistic classifications and transformations are made according to symbolic rules like those used in traditional non-computational syntax, morphology, and phonology. A rule like NP → AdjP + N’ would be a typical element of symbolic NLP.  Statistical or “stochastic” NLP models language in terms of the statistical associations between elements of the language (such as words) in natural data. The models must be derived automatically, through machine learning, normally using large amounts of labeled data. And the models are implemented in algorithms that find the most probable classification or transformation according to these models, rather than according to precise rules. An example is modeling words in terms of vectors defined by the words’ associations with other words in data. Or choosing the next word with “auto-correct”; the most probable word is chosen regardless of traditional syntactic rules.	Which of the following is NOT true about statistical versus symbolic NLP?	Statistical grammars never use symbolic rules.	Symbolic NLP gives higher quality results, when it applies.|||A complete grammar of English would consist of 1,000s of rules or more.|||Statistical NLP is more reliable for unpredictable data.	Statistical versus Symbolic NLP||publication||˝˛    ˘ ˇ˝˛ ˇˆ" ˘ ˘˘ˆ˘   !&' ˚ ( !*+˚ , !- ˘˘. ˆ-+//// ˚˘1"##)!-+ 3 ˚˜˛˘˘˘ˆ˛ˆ˘ ˆˆˆ˘"& '˜˛˘˘˛ˆ˘& (˜˘ˆˆ˛ˆ˘ˆˆ )*ˆˆ*˘ ˛%˘# ˆ,ˆ"!ˆˆ-ˆ˘ˆ˛#ˆ#ˆ˜# ˆ%%˘ˆˆ*ˆ˘˛˘*ˆˆ˛ˆ ˆˆˆ0*ˆˆˆ*ˆˆˆ ˆ˛12ˆ˘*ˆˆˆ˘˛#ˆ˘ ˛˘#˘#ˆ,ˆ%˘ˆ#ˆˆˆˆ˘ ˛˘ˆ˛ˆ˘˘ˆˆˆˆ˛ˆ˛ˆ $ˆˆˆˆ˛ˆ˜2ˆ˘ˆ˘ˆ# ˘ˆ˘ˆˆˆˆ˛˛ˆ## ˛ˆ 2˛*ˆˆˆ˘%˘ˆ ˆˆ˘ˆ#ˆˆˆˆ˘**ˆ ˘ˆ˛ˆˆ ˆ$ˆˆˆ˘ˆˆ˜)ˆ ˆˆˆ˛ˆ˘˘ˆˆˆ˛ˆˆ  ˆˆ#*ˆ˛˛%˘˛# ˆˆˆˆ˛ˆˆ˛ˆˆˆˆ ˆ˛#ˆ*˘˘ˆˆˆˆˆ˘˛%˘˘ ˆ#˛˜ˆ#˘˛ˆ˘ ˛˘˘˘˘˘ˆ˘#$ˆ%˘ˆ 3˛%˘˛˛˘#ˆ˛ˆˆ ˘ˆˆˆˆˆ˜ ˛˛#ˆˆˆ˘˘ˆ%ˆ%˘ˆ˘ˆ ˘%˘ˆ˘ˆ˛˛˘ˆ ˘ˆ#ˆˆ"$52˘˘#˛%˘#ˆ' ˆ˘ˆ˛˛˛˘˘ˆ˛ˆ˘ˆˆ˛# ˘˘˛ˆ˘6778977˛˛#97:ˆ ˜ˆ#ˆˆ˛˛˘ˆ˛ˆ'777 8'6777˛˛$ˆ*˛ˆˆ*ˆ ˘˝˚;;=&˝>˘ˆˆ ˆ˘˘ ?#ˆˆ )ˆ˘ˆˆˆˆˆ*#˜ˆ#ˆ˛%˘˛˘ ˘ˆ%ˆ*ˆˆ*#ˆˆ˘˘ˆˆˆ%˘˛ˆˆ˜>ˆ  ˘ˆˆ#ˆˆ˛* ˆ˘˛˛ˆ˘˘˜2˘ˆ˛ˆˆ ˆˆˆˆ˛ˆ˘ ˆˆˆˆˆ  ˆ˜2ˆˆˆˆ#ˆ˛ *˘ˆ˘ˆ#ˆ˛˘ ˛#˘˘ˆ˘ ˜2˛%˘ˆ˘ˆ*˘ˆ ˘ˆ˛0ˆˆ˝ˆˆˆˆˆˆ"ˆ ˆˆ˘˘"ˆˆˆˆ˘ˆ ˆ&ˆˆ˘ˆ˛?˝ˆ˘ˆˆˆˆˆˆˆ ˆ˘˜ 2*˘ˆˆ˛˘ˆˆ˛ˆ*ˆˆ* 0ˆˆˆˆ˘˘˘ 1ˆˆˆˆ˘ ˘ˆ˛˘ˆ1)ˆ*˘ˆˆˆ˘ #˘)˘˘>˘>ˆ#*˘˘ˆˆ$˘*ˆ˘ˆˆ˘ ˆ˛1
Statistical Natural Language Processing (NLP)	Symbolic Natural Language Processing (NLP)	Linguistic rules
What is Language Mixing?	Language Mixing is when a person switches between their different languages in order to communicate effectively while they are speaking. This often happens when a speaker knows the word in one language but not in their other language, so they switch between them. This can be done through code switching (inserting a complete word) or code mixing (blending phrases or rules from two different languages).	Can a person switch between more than two languages?	Yes	No	Bilingualism||slides||ˇˆˆ ˆˇ     ˛ˇ  ˘     ˛˚ ˚ ˛!ˇ   #˛ ˆ!$ ˛ ˆˆ     ˚$ˆ ˆ!%˚ %" "&$ ˆ!%˚ %" ˛  &$ˆ  ˆ ˆˆ %!%   ˜ !" )*$  ˆ+*$,˚-(˚-.-/ %0ˆ 12ˆ ˆ˛ 3$2 ˆˆ ˛& ˆ  ˘ ,4 ˆ/ ˆ  ˆ˙    5˛   & ˙ & ˇˇ    ˆ˙ $ˆ ˆˇ   ˆ  ˛ $˛  $   ˆ˙ ˆˇ   ˛  !  '! ˆˇ ˆˇˆ  ˛    !    ˆ˙ 3ˆ & + %˛˛'   ) (+& ˘ ) /01,  ˆ˙ ˘7 ' 7 'ˆ  ˆ    ˚˙ ˘ˇˇ  ˘ˇˇ ˛$˛&86ˆ ˚-9./  &˛ˆˇ ˆ ,  . + !ˆˆ ˆ ,:$ˆ% :; ˆˆˇ ,$;  ˙ ˘ ˇˆ  ˛ %ˆ˚-9 ˘ˆ ˇˆ   ˆ7   ˆˆˆˇ ˛ ˆˇ ˆ ˆ     ˆ ˆ ˘ ˆˆ  6ˆ ˆˇ ˆˆˇ ˇ  ˆˆ ˆ$   ˛ˇˇ      2$'& ˇ ˇˆ ˆ @ ,1 ˆ"CC./  ˆ ,"CC(/ ˆ ˛ˆ &˛˛ˇ˛ ˆ˛ &,$ˆ ;"CC4/        ˙ ˘! ˆ 1 D2ˆ ˆ     ˇ445 ˛˚ ˚˘!˘ ˇ   ,   /˘ˆˆ ˆ   " ˘!    ,   /!         6      6       6      6     6      ˛  E<  &;1:,˚--.5+$ˇ ˇ$ˇ2ˇ  ˇ  *,˚--H/˘ˆ&ˇ% ,"=+ˆ3˜ ˆ<,˚--˚ˇ6'$ˇ ˜ˇ + 3˜ :,"CC45& I&0    >,˚-94˜ %ˆ   ˝ 1 % 1˘"CC9/ˆ * 1 --H/ 1 ˚ ˆ J F%&ˆ2 #$; J˚-9./˙     J&˚-94/ˇˇ˘% % ˇ ˇ˝˛ F<%ˇˆ 2 ˆJ,˚-94/'=ˇ˘= %ˆ ˛  F<%ˇ&ˆˇ ˜˛=Jˆ5,˚-99/ ' =˘ˆˇ&  !"ˇˇ ˜J:˛ˇˆ% ,˚-9./ %˙ˇ=ˇ#    ˝ $ˇ*%0%==,˚-9./ˇ %˙ˇ2ˇˇ 5#ˇ ˜   ˇ 0,"C˚"/%˘ #˛<˚H"C˚"ˆ ˇˇ'E"C˚"CHEˇ' '''' ˝˛˚˙ 2Fˇ˘ ˚-H4'"CCC ,J˜"CC-/  ˆˆ&)0˜ˇˇ' #Jˇ,J#$˘ "CC./  %˜ˇ,˘ˆ*ˆK *CCH/ ˘#*0,0>/ ˘ ˆ,˚--C/  ˚ˆˆ!ˆˆ ˆˆˇ * ,ˆ/ 'ˆ$  ˇ ˜$ ˚C/ % 1ˆ* % #%2ˆ/  2ˇ˝ D,F Jˇ%$ˆˆ
Psycholinguistics	Bilingualism	Multilingualism	Applied linguistics	Linguistic terminology	Language mixing
How can a bag of words handle seeing words in testing that were not seen during training?	One strategy to handle seeing an unknown word is to include a feature with your bag of words to represent unknown words. In training, this may include counting words that appear less than 2 or 3 times in the training data as one "unknown token" feature. From that, whenever an unknown token is seen in testing, the model would use the probabilities or counts from this feature when making its evaluation.  For example, consider training data for a spam detector. "cheery" and "amphibiously" each appear once in non-spam emails, and "mAiLerzz" appears once in a spam email. Our model would count unknown tokens as non-spam with a count of 2 and spam with a count of 1. When we run into an unknown word 'razorback' in testing, this word would would be weighted using the unknown token weights.  Unknown token probabilities may be able to be improved even further by using certain information about the unknown tokens. For instance, perhaps an unknown token includes the suffix -ing, in which case it could consider weights from words found in testing ending in -ing that may be able to more accurately depict the unknown token.  Another solution is to simply ignore unknown tokens.  Note: Dealing with unknown tokens is another reason why normalization, ignoring stop words, and other techniques to minimize the language are beneficial. If all words are normalized, there is less of a chance to have unknown tokens.	Consider a model built from the following two training data sentences and their corresponding labels, where words seen less than twice are considered unknown tokens and included in the model as such:  1. "good good good bad chair" - NOT SPAM 2. "bad bad bad good couch bear" - SPAM  You come across the following sentence in testing:  "good good bad bad tricky"  Should this sentence be labeled as NOT SPAM or SPAM?	SPAM	NOT SPAM|||Not enough information to determine.	Speech and Language Processing: N-Gram Language Models||publication||12 C HAPTER 3  N- GRAM L ANGUAGE M ODELS deniedtheoffer deniedtheloan Ourmodelwillincorrectlyestimatethatthe P ( offer j deniedthe ) is0! These zeros Šthingsthatdon'teveroccurinthetrainingsetbutdooccurin zeros thetestsetŠareaproblemfortworeasons.First,theirpresencemeansweare underestimatingtheprobabilityofallsortsofwordsthatmightoccur,whichwill hurttheperformanceofanyapplicationwewanttorunonthisdata. Second,iftheprobabilityofanywordinthetestsetis0,theentireprobability ofthetestsetis0.Byperplexityisbasedontheinverseprobabilityofthe testset.Thusifsomewordshavezeroprobability,wecan'tcomputeperplexityat all,sincewecan'tdivideby0! 3.3.1UnknownWords Theprevioussectiondiscussedtheproblemofwordswhosebigramprobabilityis zero.Butwhataboutwordswesimplyhaveneverseenbefore? Sometimeswehavealanguagetaskinwhichthiscan'thappenbecauseweknow allthewordsthatcanoccur.Insucha closedvocabulary systemthetestsetcan closed vocabulary onlycontainwordsfromthislexicon,andtherewillbenounknownwords.Thisis areasonableassumptioninsomedomains,suchasspeechrecognitionormachine translation,wherewehaveapronunciationdictionaryoraphrasetablethatareed inadvance,andsothelanguagemodelcanonlyusethewordsinthatdictionaryor phrasetable. Inothercaseswehavetodealwithwordswehaven'tseenbefore,whichwe'll call unknown words,or outofvocabulary ( OOV )words.ThepercentageofOOV OOV wordsthatappearinthetestsetiscalledthe OOVrate .An openvocabulary system open vocabulary isoneinwhichwemodelthesepotentialunknownwordsinthetestsetbyaddinga pseudo-wordcalled <UNK> . Therearetwocommonwaystotraintheprobabilitiesoftheunknownword model <UNK> .Theoneistoturntheproblembackintoaclosedvocabularyone bychoosingaedvocabularyinadvance: 1. Chooseavocabulary (wordlist)thatisedinadvance. 2. Convert inthetrainingsetanywordthatisnotinthisset(anyOOVword)to theunknownwordtoken <UNK> inatextnormalizationstep. 3. Estimate theprobabilitiesfor <UNK> fromitscountsjustlikeanyotherregular wordinthetrainingset. Thesecondalternative,insituationswherewedon'thaveapriorvocabularyinad- vance,istocreatesuchavocabularyimplicitly,replacingwordsinthetrainingdata by <UNK> basedontheirfrequency.Forexamplewecanreplaceby <UNK> allwords thatoccurfewerthan n timesinthetrainingset,where n issomesmallnumber,or equivalentlyselectavocabularysizeVinadvance(say50,000)andchoosethetop VwordsbyfrequencyandreplacetherestbyUNK.Ineithercasewethenproceed totrainthelanguagemodelasbefore,treating <UNK> likearegularword. Theexactchoiceof <UNK> modeldoeshaveaneffectonmetricslikeperplexity. Alanguagemodelcanachievelowperplexitybychoosingasmallvocabularyand assigningtheunknownwordahighprobability.Forthisreason,perplexitiesshould onlybecomparedacrosslanguagemodelswiththesamevocabularies (Bucketal., 2014) .|||Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||6 C HAPTER 4  N AIVE B AYESAND S ENTIMENT C LASSIFICATION algorithmsinlanguagemodeling,itiscommonlyusedinnaiveBayestextcatego- rization: ‹ P ( w i j c )= count ( w i ; c )+ 1 P w 2 V ( count ( w ; c )+ 1 ) = count ( w i ; c )+ 1  P w 2 V count ( w ; c )  + j V j (4.14) NoteonceagainthatitiscrucialthatthevocabularyVconsistsoftheunionofallthe wordtypesinallclasses,notjustthewordsinoneclass c (trytoconvinceyourself whythismustbetrue;seetheexerciseattheendofthechapter). Whatdowedoaboutwordsthatoccurinourtestdatabutarenotinourvocab- ularyatallbecausetheydidnotoccurinanytrainingdocumentinanyclass?The solutionforsuch unknownwords istoignorethemŠremovethemfromthetest unknownword documentandnotincludeanyprobabilityforthematall. Finally,somesystemschoosetocompletelyignoreanotherclassofwords: stop words ,veryfrequentwordslike the and a .Thiscanbedonebysortingthevocabu- stopwords larybyfrequencyinthetrainingset,andthetop10Œ100vocabularyentries asstopwords,oralternativelybyusingoneofthemanystopwordlist availableonline.Theneveryinstanceofthesestopwordsaresimplyremovedfrom bothtrainingandtestdocumentsasiftheyhadneveroccurred.Inmosttextclassi- applications,however,usingastopwordlistdoesn'timproveperformance, andsoitismorecommontomakeuseoftheentirevocabularyandnotuseastop wordlist. Fig. 4.2 showsthealgorithm. function T RAIN N AIVE B AYES (D,C) returns log P ( c ) andlog P ( w j c ) foreach class c 2 C #Calculate P ( c ) terms N doc =numberofdocumentsinD N c =numberofdocumentsfromDinclassc logprior [c]   log N c N doc V   vocabularyofD bigdoc [ c ]   append (d) for d 2 D with class c foreach word w inV#Calculate P ( w j c ) terms count(w,c)   #ofoccurrencesof w in bigdoc [ c ] loglikelihood [w,c]   log count ( w ; c )+ 1 P w 0 inV ( count ( w 0 ; c )+ 1 ) return logprior , loglikelihood , V function T EST N AIVE B AYES ( testdoc , logprior , loglikelihood ,C,V) returns best c foreach class c 2 C sum [ c ]   logprior [ c ] foreach position i in testdoc word   testdoc[i] if word 2 V sum [ c ]   sum [ c ]+ loglikelihood [ word , c ] return argmax c sum [ c ] Figure4.2 ThenaiveBayesalgorithm,usingadd-1smoothing.Touseadd- a smoothing instead,changethe + 1to + a forloglikelihoodcountsintraining.|||Speech and Language Processing: Regular Expressions, Text Normalization, Edit Distance||publication||16 C HAPTER 2  R EGULAR E XPRESSIONS ,T EXT N ORMALIZATION ,E DIT D ISTANCE >>>text='ThatU.S.A.poster-printcosts$12.40...' >>>pattern=r'''(?x)#setflagtoallowverboseregexps ...([A-Z]\.)+#abbreviations,e.g.U.S.A. ...|\w+(-\w+)*#wordswithoptionalinternalhyphens ...|\$?\d+(\.\d+)?%?#currencyandpercentages,e.g.$12.40,82% ...|\.\.\.#ellipsis ...|[][.,;"'?():-_`]#theseareseparatetokens;includes],[ ...''' >>>nltk.regexp_tokenize(text,pattern) ['That','U.S.A.','poster-print','costs','$12.40','...'] Figure2.11 ApythontraceofregularexpressiontokenizationintheNLTK (Birdetal., 2009) Python-basednaturallanguageprocessingtoolkit,commentedforreadability;the (?x) verbosetellsPythontostripcommentsandwhitespace.FigurefromChapter3of Bird etal.(2009) . As Chenetal.(2017) pointout,thiscouldbetreatedas3words(`ChineseTreebank' segmentation): (2.5) Ú  YaoMing Û e reaches ; ³ [  oras5words(`PekingUniversity'segmentation): (2.6) Ú Yao  Ming Û e reaches ; overall ³ [  Finally,itispossibleinChinesesimplytoignorewordsaltogetherandusecharacters asthebasicelements,treatingthesentenceasaseriesof7characters: (2.7) Ú Yao  Ming Û enter e enter ; overall ³ decision [ game Infact,formostChineseNLPtasksitturnsouttoworkbettertotakecharacters ratherthanwordsasinput,sincecharactersareatareasonablesemanticlevelfor mostapplications,andsincemostwordstandardsresultinahugevocabularywith largenumbersofveryrarewords (Lietal.,2019) . However,forJapaneseandThaithecharacteristoosmallaunit,andsoalgo- rithmsfor wordsegmentation arerequired.ThesecanalsobeusefulforChinese word segmentation intheraresituationswherewordratherthancharacterboundariesarerequired.The standardsegmentationalgorithmsfortheselanguagesuseneural sequencemod- els trainedviasupervisedmachinelearningonhand-segmentedtrainingsets;we'll introducesequencemodelsinChapter8. 2.4.3Byte-PairEncodingforTokenization Thereisathirdoptiontotokenizingtextinput.Insteadoftokensaswords byspacesinorthographiesthathavespaces,ormorecomplexalgorithms), orascharacters(asinChinese),wecanuseourdatatoautomaticallytelluswhatsize tokensshouldbe.Perhapssometimeswemightwanttokensthatarespace-delimited words(like spinach )othertimesit'susefultohavetokensthatarelargerthanwords (like NewYorkTimes ),andsometimessmallerthanwords(likethemorphemes -est or -er .Amorphemeisthesmallestmeaning-bearingunitofalanguage;forexample theword unlikeliest hasthemorphemes un- , likely ,and -est ;we'llreturntothison page 20 . Onereasonit'shelpfultohave subword tokensistodealwithunknownwords. subword  2.4  T EXT N ORMALIZATION 17 Unknownwordsareparticularlyrelevantformachinelearningsystems.Aswewill seeinthenextchapter,machinelearningsystemsoftenlearnsomefactsaboutwords inonecorpus(a training corpus)andthenusethesefactstomakedecisionsabout aseparate test corpusanditswords.Thusifourtrainingcorpuscontains,saythe words low ,and lowest ,butnot lower ,butthentheword lower appearsinourtest corpus,oursystemwillnotknowwhattodowithit. Asolutiontothisproblemistouseakindoftokenizationinwhichmosttokens arewords,butsometokensarefrequentmorphemesorothersubwordslike -er ,so thatanunseenwordcanberepresentedbycombiningtheparts. Thesimplestsuchalgorithmis byte-pairencoding ,or BPE (Sennrichetal., BPE 2016) .Byte-pairencodingisbasedonamethodfortextcompression (Gage,1994) , buthereweuseitfortokenizationinstead.Theintuitionofthealgorithmisto iterativelymergefrequentpairsofcharacters, Thealgorithmbeginswiththesetofsymbolsequaltothesetofcharacters.Each wordisrepresentedasasequenceofcharactersplusaspecialend-of-wordsymbol .Ateachstepofthealgorithm,wecountthenumberofsymbolpairs,the mostfrequentpair(`A',`B'),andreplaceitwiththenewmergedsymbol(`AB').We continuetocountandmerge,creatingnewlongerandlongercharacterstrings,until we'vedone k merges; k isaparameterofthealgorithm.Theresultingsymbolset willconsistoftheoriginalsetofcharactersplus k newsymbols. Thealgorithmisruninsidewords(wedon'tmergeacrosswordboundaries). Forthisreason,thealgorithmcantakeasinputadictionaryofwordstogetherwith counts.Considerthefollowingtinyinputdictionarywithcountsforeachword, whichwouldhavethestartingvocabularyof11letters: dictionaryvocabulary 5 low ,d,e,i,l,n,o,r,s,t,w 2 lowest 6 newer 3 wider 2 new Wecountallpairsofsymbols:themostfrequentisthepair r because itoccursin newer (frequencyof6)and wider (frequencyof3)foratotalof9oc- currences.Wethenmergethesesymbols,treating r asonesymbol,andcount again: dictionaryvocabulary 5 low ,d,e,i,l,n,o,r,s,t,w,r 2 lowest 6 newer 3 wider 2 new Nowthemostfrequentpairis er ,whichwemerge;oursystemhaslearned thatthereshouldbeatokenforw er ,representedas er : dictionaryvocabulary 5 low , d , e , i , l , n , o , r , s , t , w , r , er 2 lowest 6 newer 3 wider 2 new Next ew (totalcountof8)getmergedto ew :
Text classification	Word vectors	Bag-of-words	Tokens	Unknown tokens	Out of vocabulary (OOV)
Name three different types of grammar and explain what they are.	Reference grammar, generative grammar, and comparative grammar (there are more than ten types of grammar, so there are other options too). Reference grammar is a description of grammar with clarifications of the principles governing the structure of sentences, clauses, phrases, and words. Generative grammar describes the rules behind the interpretation and structure of sentences that speakers accept as being part of the language. Comparative grammar involves the comparison and analysis of related languages' grammatical structures.	What is the most accurate description of mental grammar?	Grammar that allows a speaker to produce language that can be understood by other speakers of this language	The study of the main elements of language|||A grammar that explains linguistic constructions by phrase structure and linguistic transformations|||Grammar that analyzes acquisition of a foreign / second language	"Role of Grammar" clip||youtube||N/A
Noam Chomsky	Grammar	Reference grammar	Generative grammar	Comparative grammar
Name and discuss three types of Aphasia?	Broca’s Aphasia; This type of aphasia inhibits a patients ability to fluently produce speech, however their comprehension abilities are intact. Wernicke’s Aphasia; this type of aphasia is also known as "fluent aphasia". The speech is produced fluently however there is a severe lack of comprehension and the speech represents nonsense. Anomic Aphasia; this type of aphasia inhibits the patient from accessing the names for certain nouns and verbs. This causes many circumlocutions (round-about way of saying something).	Which of the following examples is NOT a type of Aphasia?	Sylvian Aphasia	Global Aphasia|||Anomic Aphasia|||Wernicke's Aphasia	Aphasia Definitions||image||N/A
Language and the brain	Aphasia	Speech pathology	Neurological disorders	Psycholinguistics	Speech impairment
What is a dependency parse versus a phrase structure (constituency) parse?	Parses are symbolic representations of the grammatical structures of sentences. A constituency parse portrays phrase structure as a tree with nodes showing a head-dependent structure for phrases, which have "part-of-speech" (lexical) categories. Dependency parses do not show phrase structure, only links from each word to the word it "depends" on, labeled with its grammatical relation to that word, such as subject or object. Phrase structure parses have the advantage of showing what the phrases are, and their lexical categories. But phrase structure parses are redundant, with many more nodes and links than words, and then the grammatical relations ("dependency relations") still need to be derived for most applications. Dependency parses are more elegant and skip directly to the production of the dependency relations, which are sufficient for many applications.	Which of the following is NOT a reason for the previous popularity of constituency parsing?	It is possible to derive dependency relations from constituency parses.	A corpus called the Penn Treebank was used to train a lot of parsers.|||Chomsky-influenced grammars defined rules in terms of phrase structure.|||Dependency parsers are not as accurate as the best constituency parsers.	Generating Typed Dependency Parses from Phrase Structure Parses||publication||GeneratingTypedDependencyParsesfromPhraseStructureParses Marie-CatherinedeMarneffe,  BillMacCartney,  and ChristopherD.Manning  y DepartmentofComputingScience,Universit ´ ecatholiquedeLouvain B-1340Louvain-la-Neuve,Belgium  ComputerScienceDepartment,StanfordUniversity Stanford,CA94305,USA f mcdm,wcmac,manning g @stanford.edu Abstract ThispaperdescribesasystemforextractingtypeddependencyparsesofEnglishsentencesfromphrasestructureparses.Inorderto captureinherentrelationsoccurringincorpustextsthatcanbecriticalinreal-worldapplications,manyNPrelationsareincludedinthe setofgrammaticalrelationsused.WeprovideacomparisonofoursystemwithMiniparandtheLinkparser.Thetypeddependency extractionfacilitydescribedhereisintegratedintheStanfordParser,availablefordownload. 1.Introduction Wedescribeasystemforautomaticallyextractingtypedde- pendencyparsesofEnglishsentencesfromphrasestruc- tureparses.Typeddependenciesandphrasestructures aredifferentwaysofrepresentingthestructureofsen- tences:whileaphrasestructureparserepresentsnest- ingofmulti-wordconstituents,adependencyparserep- resentsdependenciesbetweenindividualwords.A typed dependencyparseadditionallylabelsdependencieswith grammaticalrelations,suchas subject or indirectobject . Therehasbeenmuchlinguisticdiscussionofthetwofor- malisms.Thereareformalisomorphismsbetweencertain structures,suchasbetweendependencygrammarsandone bar-level,headedphrasestructuregrammars(Miller,2000). Inmorecomplextheoriesthereissignidebate:dom- inantChomskyantheories(Chomsky,1981)have grammaticalrelationsasonsatphrasestructure, whileothertheoriessuchasLexical-FunctionalGrammar hasrejectedtheadequacyofsuchanapproach(Bresnan, 2001).Ourgoalsherearemorepractical,thoughinessence wearefollowinganapproachwherestructural tionsareusedtogrammaticalroles. Recentyearshaveseentheintroductionofanumberof treebank-trainedstatisticalparsers[Collins(Collins,1999), Charniak(Charniak,2000),Stanford(KleinandManning, 2003)]capableofgeneratingparseswithhighaccuracy. Theoriginaltreebanks,inparticularthePennTreebank, wereforEnglish,andprovidedonlyphrasestructuretrees, andhencethisisthenativeoutputformatoftheseparsers. Atthesametime,therehasbeenincreasinginterestinusing dependencyparsesforarangeofNLPtasks,frommachine translationtoquestionanswering.Suchapplicationsbene- particularlyfromhavingaccesstodependenciesbetween wordstypedwithgrammaticalrelations,sincethesepro- videinformationaboutpredicate-argumentstructurewhich arenotreadilyavailablefromphrasestructureparses.Per- hapspartlyasaconsequenceofthis,severalmorerecent treebankshaveadopteddependencyrepresentationastheir primaryannotationformat,evenifaconversiontoaphrase structuretreeformisalsoprovided(e.g.,theDutchAlpino corpus(vanderBeeketal.,2002)andtheDanishDepen- dencyTreebank(Kromann,2003)).However,existingde- pendencyparsersforEnglishsuchasMinipar(Lin,1998) andtheLinkParser(SleatorandTemperley,1993)arenot asrobustandaccurateasphrase-structureparserstrained onverylargecorpora.Thepresentworkremediesthisre- sourcegapbyfacilitatingtherapidextractionofgrammat- icalrelationsfromphrasestructureparses.Theextraction usesrulesonthephrasestructureparses. 2.Grammaticalrelations Thissectionpresentsthegrammaticalrelationsoutputby oursystem. Theselectionofgrammaticalrelationstoincludeinour schemawasmotivatedbypracticalratherthantheoretical concerns.Weusedasastartingpointthesetofgrammat- icalrelationsin(Carrolletal.,1999)and(Kinget al.,2003).Thegrammaticalrelationsarearrangedinahi- erarchy,rootedwiththemostgenericrelation, dependent . Whentherelationbetweenaheadanditsdependentcanbe moreprecisely,relationsfurtherdowninthehier- archycanbeused.Forexample,the dependent relationcan bespecializedto aux (auxiliary), arg (argument),or mod The arg relationisfurtherdividedintothe subj (subject)relationandthe comp (complement)relation,and soon.Thewholehierarchyofourgrammaticalrelationsis giveninFigure2. Altogether,thehierarchycontains48grammaticalrela- tions.Whilethebackboneofthehierarchyisquitesimi- lartothatin(Carrolletal.,1999),overtimewehavein- troducedanumberofextensionsandtofacil- itateuseinapplications.ManyNP-internalrelationsplay averyminorroleintheoreticallymotivatedframeworks, butareaninherentpartofcorpustextsandcanbecriti- calinreal-worldapplications.Therefore,besidesthecom- monestgrammaticalrelationsforNPs( amod -adjective , rcmod -relativeclause, det -determiner, partmod -participial, infmod -val, prep -prepositionalourhierarchyincludesthe followinggrammaticalrelations: appos (appositivemodi-  nn (nouncompound), num (numeric num- ber (elementofcompoundnumber)and abbrev (abbrevi- ation).TheexamplesentenceﬁBillsonportsandimmi- grationweresubmittedbySenatorBrownback,Republican
Constituency parse	Dependency parse	Grammatical relations	Phrase structure	Parsing	Penn Treebank	Dependency relations	Phrasal categories	Syntax trees
What is a 'chatbot' and what two important NLP processes are needed for them to function as they do?	A chatbot is a computer program that has the ability to chat or converse with you.  The two NLP processes required to create a chatbot are 'parsing' and 'generating text.'	How was data initially processed throughout the early stages of chatbot technology?	Experts would encode 100's of different sentences to predict what could be said	Experts would insert one grammar rule and the computer generated multiple outcomes|||A real life person was always sitting behind the facade of a chatbot answering the questions that users asked	NLP Crash Course clip||youtube||N/A
Natural Language Processing (NLP)	Chatbot	Computer science	Applied linguistics	Data processing
What are semantic selection restrictions and how (roughly) could they be assigned using WordNet?	Semantic selection restrictions are limitations on what semantic types of entities can play the roles of a verb’s various arguments. For example, the object of “eat” should normally be limited to “edible” entities, and only a living thing can be the subject of “love” (putting aside metaphors). These selection restrictions might be found automatically using the WordNet hierarchy of semantic categories (called ‘synsets’). One can find the relevant synsets in WordNet by looking at the hypernyms of words (the more general categories to which each word belongs). The needed synset could be selected by finding the hypernyms common to all the words that play a particular role with a particular verb. For example, one would probably discover, that “edible” is the lowest common hypernym among all objects of “eat.” One could also assign these categories as semantic features to words, to help with semantic role classification, word-sense disambiguation, and verb frame classification.	What problem in assigning selection restrictions is demonstrated by the book title “To Serve Man”  (from an old Twilight Zone episode)?	 Verbs can have multiple different senses with the same arguments	It will be difficult to assign a role-set without a subject|||There are not enough features of the constituents to assign roles|||Infinitives cannot select arguments	Chapter Section on Semantic Selection Restrictions||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 20 SemanticRoleLabeling Sometimebetweenthe7thand4thcenturiesBCE,theIndiangrammarianP ¯ an . ini 1 wroteafamoustreatiseonSanskritgrammar,theAs . t . ¯ adhy ¯ ay ¯ (`8books'),atreatise thathasbeencalledﬁoneofthegreatestmonumentsof humanintelligenceﬂ 1933,11) .Thework describesthelinguisticsoftheSanskritlanguageinthe formof3959sutras,eachveryef(sinceithadto bememorized!)expressingpartofaformalrulesystem thatbrilliantlymodernmechanismsofformal languagetheory (PennandKiparsky,2012) .Onesetof rules,relevanttoourdiscussioninthischapter,describes the k ¯ arakas ,semanticrelationshipsbetweenaverband nounarguments,roleslike agent , instrument ,or destina- tion .P ¯ an . ini'sworkwastheearliestweknowofthattried tounderstandthelinguisticrealizationofeventsandtheirparticipants.Thistask ofunderstandinghowparticipantsrelatetoeventsŠbeingabletoanswertheques- tionﬁWhodidwhattowhomﬂ(andperhapsalsoﬁwhenandwhereﬂ)Šisacentral questionofnaturallanguageunderstanding. Let'smoveforward2.5millenniatothepresentandconsidertheverymundane goalofunderstandingtextaboutapurchaseofstockbyXYZCorporation.This purchasingeventanditsparticipantscanbedescribedbyawidevarietyofsurface forms.Theeventcanbedescribedbyaverb( sold,bought )oranoun( purchase ), andXYZCorpcanbethesyntacticsubject(of bought ),theindirectobject(of sold ), orinagenitiveornouncompoundrelation(withthenoun purchase )despitehaving notionallythesameroleinallofthem:  XYZcorporationboughtthestock.  TheysoldthestocktoXYZcorporation.  ThestockwasboughtbyXYZcorporation.  ThepurchaseofthestockbyXYZcorporation...  ThestockpurchasebyXYZcorporation... Inthischapterweintroducealevelofrepresentationthatcapturesthecommon- alitybetweenthesesentences:therewasapurchaseevent,theparticipantswere XYZCorpandsomestock,andXYZCorpwasthebuyer.Theseshallowsemantic representations, semanticroles ,expresstherolethatargumentsofapredicatetake intheevent,indatabaseslikePropBankandFrameNet.We'llintroduce semanticrolelabeling ,thetaskofassigningrolestospansinsentences,and selec- tionalrestrictions ,thepreferencesthatpredicatesexpressabouttheirarguments, suchasthefactthatthethemeof eat isgenerallysomethingedible. 1 FigureshowsabirchbarkmanuscriptfromKashmiroftheRupavatra,agrammaticaltextbookbased ontheSanskritgrammarofPanini.ImagefromtheWellcomeCollection.  2 C HAPTER 20  S EMANTIC R OLE L ABELING 20.1SemanticRoles ConsiderhowinChapter16werepresentedthemeaningofargumentsforsentences likethese: (20.1) Sashabrokethewindow. (20.2) Patopenedthedoor. Aneo-Davidsonianeventrepresentationofthesetwosentenceswouldbe 9 e ; x ; yBreaking ( e ) ^ Breaker ( e ; Sasha ) ^ BrokenThing ( e ; y ) ^ Window ( y ) 9 e ; x ; yOpening ( e ) ^ Opener ( e ; Pat ) ^ OpenedThing ( e ; y ) ^ Door ( y ) Inthisrepresentation,therolesofthesubjectsoftheverbs break and open are Breaker and Opener respectively.These deeproles aretoeachevent; Break- deeproles ing eventshave Breakers , Opening eventshave Openers ,andsoon. Ifwearegoingtobeabletoanswerquestions,performinferences,ordoany furtherkindsofnaturallanguageunderstandingoftheseevents,we'llneedtoknow alittlemoreaboutthesemanticsofthesearguments. Breakers and Openers have somethingincommon.Theyarebothvolitionalactors,oftenanimate,andtheyhave directcausalresponsibilityfortheirevents. Thematicroles areawaytocapturethissemanticcommonalitybetween Break- thematicroles ers and Eaters .Wesaythatthesubjectsofboththeseverbsare agents .Thus, AGENT agents isthethematicrolethatrepresentsanabstractideasuchasvolitionalcausation.Sim- ilarly,thedirectobjectsofboththeseverbs,the BrokenThing and OpenedThing ,are bothprototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction. Thesemanticrolefortheseparticipantsis theme . theme ThematicRole AGENT Thevolitionalcauserofanevent EXPERIENCER Theexperiencerofanevent FORCE Thenon-volitionalcauseroftheevent THEME Theparticipantmostdirectlyaffectedbyanevent RESULT Theendproductofanevent CONTENT Thepropositionorcontentofapropositionalevent INSTRUMENT Aninstrumentusedinanevent BENEFICIARY Theofanevent SOURCE Theoriginoftheobjectofatransferevent GOAL Thedestinationofanobjectofatransferevent Figure20.1 Somecommonlyusedthematicroleswiththeir Althoughthematicrolesareoneoftheoldestlinguisticmodels,aswesawabove, theirmodernformulationisdueto Fillmore(1968) and Gruber(1965) .Although thereisnouniversallyagreed-uponsetofroles,Figs. 20.1 and 20.2 listsomethe- maticrolesthathavebeenusedinvariouscomputationalpapers,togetherwithrough andexamples.Mostthematicrolesetshaveaboutadozenroles,butwe'll seesetswithsmallernumbersofroleswithevenmoreabstractmeanings,andsets withverylargenumbersofrolesthataretosituations.We'llusethegeneral term semanticroles forallsetsofroles,whethersmallorlarge. semanticroles  20.2  D IATHESIS A LTERNATIONS 3 ThematicRoleExample AGENT Thewaiter spilledthesoup. EXPERIENCER John hasaheadache. FORCE Thewind blowsdebrisfromthemallintoouryards. THEME OnlyafterBenjaminFranklinbroke theice ... RESULT Thecitybuilta regulation-sizebaseballdiamond ... CONTENT Monaasked ﬁYoumetMaryAnnatasupermarket?ﬂ INSTRUMENT Hepoachedstunningthem withashockingdevice ... BENEFICIARY WheneverAnnCallahanmakeshotelreservations forherboss ... SOURCE Iwin fromBoston . GOAL Idrove toPortland . Figure20.2 Someprototypicalexamplesofvariousthematicroles. 20.2DiathesisAlternations Themainreasoncomputationalsystemsusesemanticrolesistoactasashallow meaningrepresentationthatcanletusmakesimpleinferencesthataren'tpossible fromthepuresurfacestringofwords,orevenfromtheparsetree.Toextendthe earlierexamples,ifadocumentsaysthat CompanyAacquiredCompanyB ,we'd liketoknowthatthisanswersthequery WasCompanyBacquired? despitethefact thatthetwosentenceshaveverydifferentsurfacesyntax.Similarly,thisshallow semanticsmightactasausefulintermediatelanguageinmachinetranslation. Semanticrolesthushelpgeneralizeoverdifferentsurfacerealizationsofpred- icatearguments.Forexample,whilethe AGENT isoftenrealizedasthesubjectof thesentence,inothercasesthe THEME canbethesubject.Considerthesepossible realizationsofthethematicargumentsoftheverb break : (20.3) John AGENT brokethewindow. THEME (20.4) John AGENT brokethewindow THEME witharock. INSTRUMENT (20.5) Therock INSTRUMENT brokethewindow. THEME (20.6) Thewindow THEME broke. (20.7) Thewindow THEME wasbrokenbyJohn. AGENT Theseexamplessuggestthat break has(atleast)thepossiblearguments AGENT , THEME ,and INSTRUMENT .Thesetofthematicroleargumentstakenbyaverbis oftencalledthe thematicgrid , q -grid,or caseframe .Wecanseethatthereare thematicgrid caseframe (amongothers)thefollowingpossibilitiesfortherealizationoftheseargumentsof break : AGENT /Subject, THEME /Object AGENT /Subject, THEME /Object, INSTRUMENT /PP with INSTRUMENT /Subject, THEME /Object THEME /Subject Itturnsoutthatmanyverbsallowtheirthematicrolestoberealizedinvarious syntacticpositions.Forexample,verbslike give canrealizethe THEME and GOAL argumentsintwodifferentways:
Selection restrictions	Wordnet	Semantic features
Train a binary Naive Bayes and a standard Naive Bayes, both with add-one smoothing, using the document counts from Figure A. How would each classify the following sentence: "The movie was funny and hilarious, starring funny actors, and the three and a half hours went by extremely fast."	Counts for the Naive Bayes and Binary Naive Bayes are shown in Solution Figure A. The sentence can be reduced to s=“funny hilarious funny fast”, as these are the only key words our model will consider.  Using the counts for the Naive Bayes:  $$P(“Funny”, comedy) =  {11+1 \over 21+4 } = {12 \over 25}$$ $$P(“Funny”, action) = {1+1 \over 13+4} = {2 \over 17}$$ $$P(“Hilarious”, comedy) =  {6+1 \over 21+4 } = {7 \over 25}$$ $$P(“Hilarious”, action) = {0+1 \over 13+4} = {1 \over 17}$$ $$P(“Fast”, comedy) =  {3+1 \over 21+4 } = {4 \over 25}$$ $$P(“Fast”, action) = {6+1 \over 13+4} = {7 \over 17}$$  Our model would determine probabilities for each class as follows:  $$P(comedy)P(s|comedy) = {3 \over 5} \times {12 \over 25} \times {12 \over 25} \times {7 \over 25} \times {4 \over 25} = 0.00619$$ $$P(action)P(s|action) = {2 \over 5} \times {2 \over 17} \times {2 \over 17} \times {1 \over 17} \times {7 \over 17} = 0.000134$$  Thus, our Naive Bayes classifier would predict this sentence to be a comedy.  Using the counts for the binary Naive Bayes:  $$P(“Funny”, comedy) =  {3+1 \over 9+4 } = {4 \over 13}$$ $$P(“Funny”, action) = {1+1 \over 5+4} = {2 \over 9}$$ $$P(“Hilarious”, comedy) =  {3+1 \over 9+4 } = {4 \over 13}$$ $$P(“Hilarious”, action) = {0+1 \over 5+4} = {1 \over 9}$$ $$P(“Fast”, comedy) =  {2+1 \over 9+4 } = {3 \over 13}$$ $$P(“Fast”, action) = {2+1 \over 5+4} = {3 \over 9}$$  Our model would determine probabilities for each class as follows:  $$P(comedy)P(s|comedy) = {3 \over 5} \times {4 \over 13}  \times {4 \over 13} \times {3 \over 13} = 0.0131$$ $$P(action)P(s|action) = {2 \over 5} \times {2 \over 9} \times {1 \over 9} \times {3 \over 9} = 0.00329$$  Our binary Naive Bayes classifier would also predict this sentence to be a comedy.	Which of the following statements is true about binary Naive Bayes classifiers compared to Naive Bayes classifiers?	Binary Naive Bayes classifiers tend to perform better than Naive Bayes classifiers on sentiment analysis tasks.	Binary Naive Bayes classifiers always output the same results as Naive Bayes classifiers.|||Binary Naive Bayes classifiers are linear classifiers while Naive Bayes classifiers are nonlinear classifiers.|||Binary Naive Bayes classifiers imply that a word appearing multiple times in a document should be weighted more strongly towards the document's class, compared to Naive Bayes classifiers.	Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||4.3  W ORKEDEXAMPLE 7 4.3Workedexample Let'swalkthroughanexampleoftrainingandtestingnaiveBayeswithadd-one smoothing.We'lluseasentimentanalysisdomainwiththetwoclassespositive (+)andnegative(-),andtakethefollowingminiaturetrainingandtestdocuments fromactualmoviereviews. Cat Documents Training - justplainboring - entirelypredictableandlacksenergy - nosurprisesandveryfewlaughs + verypowerful + themostfunofthesummer Test ? predictablewithnofun Theprior P ( c ) forthetwoclassesiscomputedviaEq. 4.11 as N c N doc : P (  )= 3 5 P (+)= 2 5 Theword with doesn'toccurinthetrainingset,sowedropitcompletely(as mentionedabove,wedon'tuseunknownwordmodelsfornaiveBayes).Thelike- lihoodsfromthetrainingsetfortheremainingthreewordsﬁpredictableﬂ,ﬁnoﬂ,and ﬁfunﬂ,areasfollows,fromEq. 4.14 (computingtheprobabilitiesfortheremainder ofthewordsinthetrainingsetisleftasanexerciseforthereader): P ( ﬁpredictableﬂ j )= 1 + 1 14 + 20 P ( ﬁpredictableﬂ j +)= 0 + 1 9 + 20 P ( ﬁnoﬂ j )= 1 + 1 14 + 20 P ( ﬁnoﬂ j +)= 0 + 1 9 + 20 P ( ﬁfunﬂ j )= 0 + 1 14 + 20 P ( ﬁfunﬂ j +)= 1 + 1 9 + 20 ForthetestsentenceS=ﬁpredictablewithnofunﬂ,afterremovingtheword`with', thechosenclass,viaEq. 4.9 ,isthereforecomputedasfollows: P (  ) P ( S j )= 3 5  2  2  1 34 3 = 6 : 1  10  5 P (+) P ( S j +)= 2 5  1  1  2 29 3 = 3 : 2  10  5 Themodelthuspredictstheclass negative forthetestsentence. 4.4OptimizingforSentimentAnalysis WhilestandardnaiveBayestextcanworkwellforsentimentanalysis, somesmallchangesaregenerallyemployedthatimproveperformance. First,forsentimenttionandanumberofothertexttasks, whetherawordoccursornotseemstomattermorethanitsfrequency.Thusit oftenimprovesperformancetoclipthewordcountsineachdocumentat1(see theendofthechapterforpointerstotheseresults).Thisvariantiscalled binary  8 C HAPTER 4  N AIVE B AYESAND S ENTIMENT C LASSIFICATION multinomialnaiveBayes or binaryNB .ThevariantusesthesameEq. 4.10 except binaryNB thatforeachdocumentweremoveallduplicatewordsbeforeconcatenatingthem intothesinglebigdocument.Fig. 4.3 showsanexampleinwhichasetoffour documents(shortenedandtext-normalizedforthisexample)areremappedtobinary, withthecountsshowninthetableontheright.Theexampleisworked withoutadd-1smoothingtomakethedifferencesclearer.Notethattheresultscounts neednotbe1;theword great hasacountof2evenforBinaryNB,becauseitappears inmultipledocuments. Fouroriginaldocuments:  itwaspathetictheworstpartwasthe boxingscenes  noplottwistsorgreatscenes + andsatireandgreatplottwists + greatscenesgreat Afterper-documentbinarization:  itwaspathetictheworstpartboxing scenes  noplottwistsorgreatscenes + andsatiregreatplottwists + greatscenes NBBinary CountsCounts +  +  and 2 0 1 0 boxing0101 1010 great 3 1 2 1 it0101 no0101 or0101 part0101 pathetic0101 plot1111 satire1010 scenes1212 the 0 2 0 1 twists1111 was 0 2 0 1 worst0101 Figure4.3 AnexampleofbinarizationforthebinarynaiveBayesalgorithm. Asecondimportantadditioncommonlymadewhendoingtextfor sentimentistodealwithnegation.Considerthedifferencebetween Ireallylikethis movie (positive)and Ididn'tlikethismovie (negative).Thenegationexpressedby didn't completelyalterstheinferenceswedrawfromthepredicate like .Similarly, negationcanmodifyanegativewordtoproduceapositivereview( don'tdismissthis  , doesn'tletusgetbored ). Averysimplebaselinethatiscommonlyusedinsentimentanalysistodealwith negationisthefollowing:duringtextnormalization,prependthe NOT to everywordafteratokenoflogicalnegation( n't,not,no,never )untilthenextpunc- tuationmark.Thusthephrase didntlikethismovie,butI becomes didntNOT_likeNOT_thisNOT_movie,butI Newlyformed`words'like NOT like , NOT recommend willthusoccurmoreof- teninnegativedocumentandactascuesfornegativesentiment,whilewordslike NOT bored , NOT dismiss willacquirepositiveassociations.WewillreturninChap- ter17totheuseofparsingtodealmoreaccuratelywiththescoperelationshipbe- tweenthesenegationwordsandthepredicatestheymodify,butthissimplebaseline worksquitewellinpractice. Finally,insomesituationswemighthaveinsuflabeledtrainingdatato trainaccuratenaiveBayesusingallwordsinthetrainingsettoestimate positiveandnegativesentiment.Insuchcaseswecaninsteadderivethepositive
Naive Bayes	Natural Language Processing (NLP)	Text classification	Binary Naive Bayes (NB)	Add-one (Laplace)
Which of the branches of linguistics is the following definition most suitable for: The study of the relationship between language users and linguistic forms?	Pragmatics, because this branch studies the use of language in context i.e. by the speakers themselves.	According to the Hay Levels YouTube clip about pragmatics, what is the most important branch of microlinguistics and why?	Pragmatics, because it answers the question "why" - why certain words and expressions are used and not others.	Phonology, because we have to know how to pronounce words.|||Semantics, because we need to know the meaning of words to understand speech.|||Syntax, because we couldn't make a grammatically correct sentence if we didn't know grammar rules.	Pragmatics clip||youtube||N/A
Pragmatics	Branches of linguistics
Provide a complete context free grammar for the sentence “The sleeper wakes.”	S → NP VP NP → DT NN VP → Vi NN → sleeper DT → the Vi → wakes variables = {S, NP, VP, NN, Vi, DT} terminals = {the, sleeper, wakes}	How many production rules would need to be added to the grammar above to make it able also to generate the sentence, "The sleeper awakes the sleeper."	2	0|||1|||3	On parsing with cfgs (slides 12-15)||slides||Parsing,andContext-FreeGrammars MichaelCollins,ColumbiaUniversity  Overview I Anintroductiontotheparsingproblem I Contextfreegrammars I Abrief(!)sketchofthesyntaxofEnglish I Examplesofambiguousstructures  Parsing(SyntacticStructure) INPUT: BoeingislocatedinSeattle. OUTPUT: S NP N Boeing VP V is VP V located PP P in NP N Seattle  SyntacticFormalisms I WorkinformalsyntaxgoesbacktoChomsky'sPhDthesisin the1950s I Examplesofcurrentformalisms:minimalism,lexical functionalgrammar(LFG),head-drivenphrase-structure grammar(HPSG),treeadjoininggrammars(TAG),categorial grammars  DataforParsingExperiments I PennWSJTreebank=50,000sentenceswithassociatedtrees I Usualset-up:40,000trainingsentences,2400testsentences Anexampletree: CanadianUtilitieshad1988revenueofC$1.16billion, mainlyfromitsnaturalgasandelectricutilitybusinessesin Alberta,wherethecompanyservesabout800,000 customers.  TheInformationConveyedbyParseTrees (1)Partofspeechforeachword (N=noun,V=verb,DT=determiner) S NP DT the N burglar VP V robbed NP DT the N apartment  TheInformationConveyedbyParseTrees(continued) (2)Phrases S NP DT the N burglar VP V robbed NP DT the N apartment NounPhrases(NP):\theburglar",\theapartment" VerbPhrases(VP):\robbedtheapartment" Sentences(S):\theburglarrobbedtheapartment"  TheInformationConveyedbyParseTrees(continued) (3)UsefulRelationships S NP subject VP V verb S NP DT the N burglar VP V robbed NP DT the N apartment ) \theburglar"isthesubjectof\robbed"  AnExampleApplication:MachineTranslation I Englishwordorderis subject{verb{object I Japanesewordorderis subject{object{verb English:IBMboughtLotus Japanese: IBMLotusbought English:SourcessaidthatIBMboughtLotusyesterday Japanese: SourcesyesterdayIBMLotusboughtthatsaid  S NP-A Sources VP , SBAR-A , S NP yesterday NP-A IBM VP , NP-A Lotus VB bought COMP that VB said  Overview I Anintroductiontotheparsingproblem I Contextfreegrammars I Abrief(!)sketchofthesyntaxofEnglish I Examplesofambiguousstructures  Context-FreeGrammars HopcroftandUllman,1979 Acontextfreegrammar G =( N;  ;R;S ) where: I N isasetofnon-terminalsymbols I  isasetofterminalsymbols I R isasetofrulesoftheform X ! Y 1 Y 2 :::Y n for n  0 , X 2 N , Y i 2 ( N [  I S 2 N isadistinguishedstartsymbol  AContext-FreeGrammarforEnglish N = f S,NP,VP,PP,DT,Vi,Vt,NN,IN g S =S  = f sleeps,saw,man,woman,telescope,the,with,in g R = S ! NPVP VP ! Vi VP ! VtNP VP ! VPPP NP ! DTNN NP ! NPPP PP ! INNP Vi ! sleeps Vt ! saw NN ! man NN ! woman NN ! telescope DT ! the IN ! with IN ! in Note:S=sentence,VP=verbphrase,NP=nounphrase, PP=prepositionalphrase,DT=determiner,Vi=intransitiveverb, Vt=transitiveverb,NN=noun,IN=preposition  Left-MostDerivations Aleft-mostderivationisasequenceofstrings s 1 :::s n ,where I s 1 = S ,thestartsymbol I s n 2   ,i.e. s n ismadeupofterminalsymbolsonly I Each s i for i =2 :::n isderivedfrom s i  1 bypickingthe left-mostnon-terminal X in s i  1 andreplacingitbysome  where X !  isarulein R Forexample: [S],[NPVP],[DNVP],[theNVP],[themanVP], [themanVi],[themansleeps] Representationofaderivationasatree: S NP D the N man VP Vi sleeps  AnExample DERIVATION RULESUSED S S ! NPVP NPVP NP ! DTN DTNVP DT ! the theNVP N ! dog thedogVP VP ! VB thedogVB VB ! laughs thedoglaughs S NP DT the N dog VP VB laughs  AnExample DERIVATION RULESUSED S S ! NPVP NPVP NP ! DTN DTNVP DT ! the theNVP N ! dog thedogVP VP ! VB thedogVB VB ! laughs thedoglaughs S NP DT the N dog VP VB laughs  AnExample DERIVATION RULESUSED S S ! NPVP NPVP NP ! DTN DTNVP DT ! the theNVP N ! dog thedogVP VP ! VB thedogVB VB ! laughs thedoglaughs S NP DT the N dog VP VB laughs  AnExample DERIVATION RULESUSED S S ! NPVP NPVP NP ! DTN DTNVP DT ! the theNVP N ! dog thedogVP VP ! VB thedogVB VB ! laughs thedoglaughs S NP DT the N dog VP VB laughs  AnExample DERIVATION RULESUSED S S ! NPVP NPVP NP ! DTN DTNVP DT ! the theNVP N ! dog thedogVP VP ! VB thedogVB VB ! laughs thedoglaughs S NP DT the N dog VP VB laughs  AnExample DERIVATION RULESUSED S S ! NPVP NPVP NP ! DTN DTNVP DT ! the theNVP N ! dog thedogVP VP ! VB thedogVB VB ! laughs thedoglaughs S NP DT the N dog VP VB laughs  AnExample DERIVATION RULESUSED S S ! NPVP NPVP NP ! DTN DTNVP DT ! the theNVP N ! dog thedogVP VP ! VB thedogVB VB ! laughs thedoglaughs S NP DT the N dog VP VB laughs  AnExample DERIVATION RULESUSED S S ! NPVP NPVP NP ! DTN DTNVP DT ! the theNVP N ! dog thedogVP VP ! VB thedogVB VB ! laughs thedoglaughs S NP DT the N dog VP VB laughs  PropertiesofCFGs I ACFGasetofpossiblederivations I Astring s 2   isinthe language bythe CFGifthereisatleastonederivationthatyields s I EachstringinthelanguagegeneratedbytheCFG mayhavemorethanonederivation(\ambiguity")  AnExampleofAmbiguity S NP he VP VP VB drove PP IN down NP DT the NN street PP IN in NP DT the NN car  AnExampleofAmbiguity(continued) S NP he VP VB drove PP IN down NP NP DT the NN street PP IN in NP DT the NN car  TheProblemwithParsing: Ambiguity INPUT: Sheannouncedaprogramtopromotesafetyintrucksandvans + POSSIBLEOUTPUTS: Andtherearemore...  Overview I Anintroductiontotheparsingproblem I Contextfreegrammars I Abrief(!)sketchofthesyntaxofEnglish I Examplesofambiguousstructures  ProductDetails(fromAmazon) Hardcover:1779pages Publisher:Longman;2ndRevisededition Language:English ISBN-10:0582517346 ISBN-13:978-0582517349 ProductDimensions:8.4x2.4x10inches ShippingWeight:4.6pounds  ABriefOverviewofEnglishSyntax PartsofSpeech(tagsfromtheBrowncorpus): I Nouns NN=singularnoun e.g.,man,dog,park NNS=pluralnoun e.g.,telescopes,houses,buildings NNP=propernoun e.g.,Smith,Gates,IBM I Determiners DT=determiner e.g.,the,a,some,every I Adjectives JJ=adjective e.g.,red,green,large,idealistic  AFragmentofaNounPhraseGrammar  N ) NN  N ) NN  N  N ) JJ  N  N )  N  N NP ) DT  N NN ) box NN ) car NN ) mechanic NN ) pigeon DT ) the DT ) a JJ ) fast JJ ) metal JJ ) idealistic JJ ) clay  Prepositions,andPrepositionalPhrases I Prepositions IN=preposition e.g.,of,in,out,beside,as  AnExtendedGrammar  N ) NN  N ) NN  N  N ) JJ  N  N )  N  N NP ) DT  N PP ) INNP  N )  NPP NN ) box NN ) car NN ) mechanic NN ) pigeon DT ) the DT ) a JJ ) fast JJ ) metal JJ ) idealistic JJ ) clay IN ) in IN ) under IN ) of IN ) on IN ) with IN ) as Generates: inabox,underthebox,thefastcarmechanicunderthepigeonin thebox, :::  AnExtendedGrammar  N ) NN  N ) NN  N  N ) JJ  N  N )  N  N NP ) DT  N PP ) INNP  N )  NPP  Verbs,VerbPhrases,andSentences I BasicVerbTypes Vi=Intransitiveverb e.g.,sleeps,walks,laughs Vt=Transitiveverb e.g.,sees,saw,likes Vd=Ditransitiveverb e.g.,gave I BasicVPRules VP ! Vi VP ! VtNP VP ! VdNPNP I BasicSRule S ! NPVP ExamplesofVP: sleeps,walks,likesthemechanic,gavethemechanicthefastcar ExamplesofS: themansleeps,thedogwalks,thedoggavethemechanicthefastcar  PPsModifyingVerbPhrases Anewrule: VP ! VPPP NewexamplesofVP: sleepsinthecar,walkslikethemechanic,gavethemechanicthe fastcaronTuesday, :::  Complementizers,andSBARs I Complementizers COMP=complementizer e.g.,that I SBAR SBAR ! COMPS Examples: thatthemansleeps,thatthemechanicsawthedog :::  MoreVerbs I NewVerbTypes V[5] e.g.,said,reported V[6] e.g.,told,informed V[7] e.g.,bet I NewVPRules VP ! V[5]SBAR VP ! V[6]NPSBAR VP ! V[7]NPNPSBAR ExamplesofNewVPs: saidthatthemansleeps toldthedogthatthemechaniclikesthepigeon betthepigeon$50thatthemechanicownsafastcar  Coordination I ANewPart-of-Speech: CC=Coordinator e.g.,and,or,but I NewRules NP ! NPCCNP  N !  NCC  N VP ! VPCCVP S ! SCCS SBAR ! SBARCCSBAR  We'veOnlyScratchedtheSurface... I Agreement Thedogslaugh vs. Thedoglaughs I Wh-movement Thedogthatthecatliked I Activevs.passive Thedogsawthecat vs. Thecatwasseenbythedog I Ifyou'reinterestedinreadingmore: SyntacticTheory:AFormalIntroduction,2nd Edition.IvanA.Sag,ThomasWasow,andEmily M.Bender.  Overview I Anintroductiontotheparsingproblem I Contextfreegrammars I Abrief(!)sketchofthesyntaxofEnglish I Examplesofambiguousstructures  SourcesofAmbiguity I Part-of-Speechambiguity NN ! duck Vi ! duck VP VP Vt saw NP PRP her NN duck PP IN with NP thetelescope VP VP V saw S NP her VP Vi duck PP IN with NP thetelescope  S NP I VP VP Vi drove PP IN down NP DT the NN road PP IN in NP DT the NN car  S NP I VP Vi drove PP IN down NP NP DT the NN road PP IN in NP DT the NN car  Twoanalysesfor: JohnwasbelievedtohavebeenshotbyBill  SourcesofAmbiguity:NounPremo I Nounpremo NP D the  N JJ fast  N NN car  N NN mechanic NP D the  N  N JJ fast  N NN car  N NN mechanic
Parsing	Phrase structure	Production rules	Context-free grammars
Give a brief overview of the structural hierarchy of language.	Language begins with phones, groups of which are the phonemes of the language. While phonemes don’t have meaning in and of themselves, they can cause a change of meaning within a language. Phonemes make up morphemes or minimal word segments that have meaning. These, in turn, form words. Words are grouped into phrases, such as verb phrases, noun phrases, prepositional phrases, etc. These express complete thoughts.	What is the smallest language unit?	Phonemes	Nouns|||Letters|||Sentences	"The emergence of hierarchical structure in human language" paper||publication||PERSPECTIVEARTICLE published:20February2013 doi:10.3389/fpsyg.2013.00071 Theemergenceofhierarchicalstructureinhumanlanguage ShigeruMiyagawa 1*,RobertC.Berwick 2andKazuoOkanoya 31DepartmentofLinguisticsandPhilosophy,MassachusettsInstituteofTechnology,Cambridge,MA,USA 2DepartmentofElectricalEngineeringandComputerScienceandLaboratoryforInformationandDecisionSystems,MIT,CambridgeMA,USA 3DepartmentofLifeSciences,TheUniversityofTokyo,Tokyo,Japan Editedby: MarcelaPena,CatholicUniversityof Chile,Chile Reviewedby: AlanLangus,SISSA,Italy FranciscoAboitiz,P UniversidadCatolicadeChile,Chile *Correspondence: ShigeruMiyagawa,Departmentof LinguisticsandPhilosophy, MassachusettsInstituteof Technology,32D-886,Cambridge,MA 02139,USA. e-mail:miyagawa@mit.edu Weproposeanovelaccountfortheemergenceofhumanlanguagesyntax.Likemany evolutionaryinnovations,languagearosefromtheadventitiouscombinationoftwopre- existing,simplersystemsthathadbeenevolvedforotherfunctionaltasks.Thesystem, TypeE(xpression),isfoundinbirdsong,wherethesamesongmarksterritory,matingavail- ability,andsimilarﬁexpressiveﬂfunctions.Thesecondsystem,TypeL(exical),hasbeen suggestivelyfoundinnon-humanprimatecallsandinhoneybeewaggledances,whereit demarcatespredicateswithoneormoreﬁarguments,ﬂsuchascombinationsofcallsin  monkeysorcompassheadingssettosunpositioninhoneybees.Weshowthathuman  languagesyntaxiscomposedoftwolayersthatparallelthesetwoindependentlyevolved  systems:anﬁEﬂlayerresemblingtheTypeEsystemofbirdsongandanﬁLﬂlayerproviding words.TheexistenceoftheﬁEﬂandﬁLﬂlayerscanbeusingstandardlinguistic methodology.Eachlayer,EandL,whenconsidered separately,ischaracterizableasa statesystem,asobservedinseveralnon-humanspecies.Whenthetwosystemsareput togethertheyinteract,yieldingtheunbounded,state,hierarchicalstructurethat  servesasthehallmarkofhumanlanguagesyntax.Inthisway,weaccountfor theappearanceofanovelfunction,language,withinaconventionalDarwinianframework, alongwithitsapparentlyuniqueemergenceinasinglespecies. Keywords:humanlanguage,birdsong,honeybee,monkeycommunication,hierarchy INTRODUCTIONHumanlanguageappearstobearecentevolutionarydevelop-  ment,arisingwithinthepast100,000years,andhasnotevolvedin  anysigwaysinceourancestorsleftAfrica,about50,000Œ  80,000yearsago( Tattersall,2009 ).Ifso,thehumanlanguage facultyemergedrelativelysuddenlyinevolutionarytimeandhas notevolvedsince. Howdidthiscomeabout?Whilespeculationaboutevolution withoutdirectdataremainschallenging,itmaystillbepossible  toprovideanaccountbroadlycompatiblewithwhatweknow  abouthumanlanguagesyntax,alongwiththeapparentlyrapid emergenceoflanguage.Contemporaryhumanlanguagesyntax cannotbecharacterizedbyanyestategrammar( Chomsky,  1956).Suchsimplesystemscannotproperlyrepresenttheambi-  guityfoundinhumanlanguage,evenforthesimplestwordstrings  suchas deepbluesky .Finitestatesystemscannotrepresentsuch ambiguitybecausebydeiontheyare syntacticmonoids :alge- braically,theymustobeyassociativity,sotheycannotassigntwo  distinctrepresentationstotheconcatenation deepbluesky .One needsacompositionaloperatorthatcantaketwolexicalitems,  oringeneralanytwosyntacticobjects,andassemblethemintoa  single,newlylabeledwhole,beyondthepowerofanyestate  grammar. Inthispaperweadvanceanovelaccountfortheemergence ofthisspecies-spelanguageproperty.Likemanyevolutionary innovations,weproposethatlanguagearosefromtheadventi- tiouscombinationoftwopre-existing,simplersystemsevolved  forothertasks.Thestsystem,whichwewillcallTypeE,for expressive ,canbefound,forexample,inbirdsong( Berwicketal.,  2011),wherethesamesongservestomarkterritory,matingavail-  ability,andotherﬁexpressiveﬂfunctions.Thesecondsystem,which  wewillcallTypeL,for lexical ,hasbeensuggestivelyobservedin honeybees,whereitdemarcateapredicatewithoneormoreﬁargu-  mentsﬂŒhere,elementsofthehoneybees'dancecorresponding tocompassheadingsandhtpaths( Rileyetal.,2005 ).Some- whatcontroversialexamplesofTypeLaremonkeyalarmcalls thatreferentiallyconveytypesofpredators( Seyfarthetal.,1980 )andshowcombinatorialemergenceofnewsemantics( Arnoldand Zuberbuhler,2006 ).Humanlanguagesyntaxintegratesthesetwo systemsintosingle,composite,hierarchicallystructuredwhole bylinkingelementsfromtheLexicalsystemwiththoseofthe  Expressivesystem. ASIMPLEEXAMPLESHOWSHOWTOLINKTHEEANDL SYSTEMSAsimplequestionsuchas(1)illustrateshowtheExpressiveand Lexicalsystemsarelinked. (1)WhatidoyouthinkthatMarysaidJohnbought___ i?Thephrase what occursattheheadofasentenceinan expressive ﬁquestionpositionﬂbutitisalsosemanticallyassociatedasaverbal argumentina lexical positionafter buy whereitisgivenmean- ingful,interpretablecontent.Thenextsectiondemonstratesthat allsuchﬁlinkingrelationsﬂbridgethelexicalandexpressivelayers,  integratingthetwosystems. www.frontiersin.org February2013|Volume4|Article71| 1                                                                                                                   Miyagawaetal. Emergenceofhierarchyinlanguage HUMANLANGUAGESENTENCESCONTAINTWOLAYERSOF MEANING Allhumanlanguagesentencesarecomposedoftwomeaninglayers (e.g., Chomsky , 1995 ; Miyagawa , 2010 ).Consider(2). (2) DidJohneatpasta? Thecorelexicalmeaningof(2)isformedfromthewords, John, eat, and pasta .Regardlessofsyntacticform,the lexicalmeaning edbythesewordsremainsintact:e.g.,onecanaddmodalityor tense,asin, John may eatpasta ; John will eatpasta .Separatefrom the lexicalstructure ,sentence(2)containstheword did ,which hastwofunctions.Thestexpressestense( John did eatpasta ); thesecondexpressesaquestion( did Johneatpasta ?).Inthisway, startingwith lexicalstructure, Tense,andQuestion-formation outputanexpressionthatcanbeusedinconversation. Did indi- catesapastevent,anditformsaquestionaboutthisevent.This so-calledﬁdualityofsemanticsﬂ( Chomsky , 2000 )isrepresentedas ahierarchicalstructure( HaleandKeyser , 1993 ; Chomsky , 2005 ; Miyagawa , 2010 ). (3) Dualityofsemantics( Chomsky , 1995 , 2000 ; Miyagawa , 2010 ) lexicalstructure iscomposedfromapotentiallyopen-ended setoflexicalitemsthatoccurindependently( John,eat,pasta ).In contrast, expressionstructure iscomposedoflimitednum- berofelementstypicallycharacterizedasﬁfunctionalelementsﬂ thatlackindependentstatus,e.g.,thepasttense Œed inEnglish (e.g., HaleandKeyser , 1993 ).Asshownin(3),sentencesarecon- structedwithanﬁouterlayerﬂof expressionstructure andan ﬁinnerlayerﬂof lexicalstructure. ANTECEDENTSFORLEXICALSTRUCTUREINNON-HUMAN ANIMALS Tomakethecaseforanevolutionaryprecursorforlexicalstruc- ture,oneshouldlocateinanotheranimalspeciestheabilityto grouptwoorthreeelementstogether,withoutsyntax,arrivingat anamalgamatedﬁmeaning.ﬂInthehoneybeewaggledance,the dancemeaningmaybedecomposedintotwoparts,withoutsyn- tax:dancedirectionconveyscompassbearingforfoodlocation; dancespeedconveysinformationregardingdistancetoafood source( Rileyetal. , 2005 ). Thereisalargebodyofliteratureonthecallsofmonkeys andapes( SeedandTomasello , 2010 ).Earlierstudiesconcluded thatKenyanVervetmonkeys( Seyfarthetal. , 1980 )possessalarm callsforpythons,eagles,andleopards.Inasense,thisisthesim- plestlexicallybasedsystemwhereanutteredobjectcorrelateswith aparticularreal-worldstateofaffairs.Morerecently,therehas beenmuchdebateastowhethernon-humanprimatespossess theabilitytoconstrueobjectswithinanabstractevent( Tomasello andCall , 1997 ).Thesestudiessuggestthatnon-humanprimate callsmaybeconstruedaslexical.Forexample,anumberofstud- ieshavesuggestedthattheseprimatesperformreasonablywell onPiagetianobjectpermanenceuptoState4or5( Seedand Tomasello , 2010 );theyperceiveobjectsevenwhentheyareno longerintheiroriginallocation.Thereareevensomerecent studiesinvariousprimatespeciessuggestingthattheseanimals mightusemultiplecallstocomposeanovelmeaning( Dessalles , 2007 ; ArnoldandZuberbuhler , 2008 ; TallermanandGibson , 2011 ). BIRDSONGANDEXPRESSIONSTRUCTURE Linksbetweenbirdsongandhumanlanguagehavelongbeen noted( Darwin , 1871 ; Jespersen , 1922 ; Marler , 1970 ; Notte- bohm , 1975 ; DoupeandKuhl , 1999 ; Okanoya , 2002 ; Bolhuis etal. , 2010 ; Berwicketal. , 2012 ).Therearestrikingparal- lelsbetweenbirdsongandhumanlanguageacquisition:aneed forexternalinput;sensitivedevelopmentalperiodsendingat sexualmaturity;hemisphericlateralization;andmotor-auditory rehearsalsystems( Bolhuisetal. , 2010 ).Despitethesesimilari- ties,whatisstrikingabouteveryvarietyofbirdsongthathas beenstudiedisthatlexicalitemsinthesenseofhumanlan- guagesremainabsent( Berwicketal. , 2011 ).Nordoesbirdsong containtherichhierarchicalstructurecharacteristicofhuman language( Berwicketal. , 2012 ).Atypicalcaseinpointisthe songofthezebrah( Figure1 ),whichhasarestrictedsetof ﬁnotesﬂthatcombinetoformsequenceofsyllables,syllablesinto motifs,andmotifsintocompletesongﬁboutsﬂ( Berwicketal. , 2011 ). OthervocallearningbirdspeciessuchastheBengaleseh admitmorecomplexpatternsinvolvingbranches,loops,and repetitions. Asshownin Figure2 ,Bengalesehsongcanlooptoa precedingsongpositionatvariousstates,admittingconsiderable variation.Nightingaleshaveanevenmorecomplexsongstructure, withpossiblebranchesatmanymoreadditionalpositions,witha singlenightingale'srepertoirecontaining100Œ200distinctsongs ( Kipperetal. , 2006 ).Nevertheless,allknownbirdsongexamples canbedescribedasaparticularconstrainedkindofestate automaton( Berwicketal. , 2011 ). Therearetwosensesinwhichbirdsongslacklexicalitems,or ﬁwords.ﬂFirst,songelementsarenevercombinedtoyieldnew ﬁmeanings.ﬂThisisunlikeprimatecallsmentionedabove( Arnold andZuberbuhler , 2008 ).Second,regardlessofvariety,birdsong conveysonlyalimited,holisticrangeofintentions,primarily relatedwithreproduction.Inthissense,birdsongsconveymes- sages,notmeanings( TallermanandGibson , 2011 ).Wewillrefer tothistypeoflanguagesystemas TypeE ,for E ( xpression ),without meaning. BIRDSONGANDHUMANLANGUAGE Twoitemsthat Berwicketal. ( 2012 ) pointoutthathuman languagehasbutbirdsongdoesnotare:(i)phrasesﬁlabeledﬂ byelementfeatures(seebelow);(ii)hierarchicalstructureof phrases.Thesedistinctionsarisefromthefactthathuman FrontiersinPsychology |LanguageSciences February2013|Volume4|Article71 | 2                                                                                                                    Miyagawaetal. Emergenceofhierarchyinlanguage FIGURE1|Zebrahsong. FIGURE2|Bengalesehsong. languagepossesseslexicalitemswhilebirdsongdoesnot. Thus,birdsongsyntaxissometimesreferredtoasphono- logicalsyntax,emphasizingthelackofalexicon( Marler , 2000 ). Additionally,birdsongapparentlylacksanyﬁrecursion,ﬂinthe sensethatoneboutcanbehierarchicallycontainedwithinanother. Wearguethatjustsuchalimitationisalsoimposedonhuman language,butonlyinthedomainof expressionstructure , therebydrawingonekeyconnectionbetweenbirdsongandhuman language.Hence,theconnectionbetweenbirdsongandhuman languageisnotbetweensongandlanguageinitsentirety;rather, theconnectionisbetweenbirdsongandthe expressionstruc- ture componentofhumanlanguagesyntax.Whileithasbeen sometimessuggestedthatcertainbirdspeciescanacquirerecursive syntacticstructureseitherthroughconditioning( Gentneretal. , 2006 )orspontaneously( AbeandWatanabe , 2011 ),thisresult remainscontroversialand,asnotedin Beckersetal. ( 2012 ) ,sofar uncomed.Nonetheless,itseemsplausiblethatsomeabilities forprocessingtemporallyorderedacousticstreamsaresharedby bothavianandhumanvocallearners.Bynecessity,soundstreams mustbeparsedintobeginningandendingﬁchunksﬂŒwordsand wordcomponentsinthecaseofhumans,orsyllablechunksfor songbirds.Withoutwordboundariesandwordpatternrecog- nition,humanlanguageacquisitionbecomesimpossible;thisis clearlyrequiredforearlyvocallearning.Thesameholdsforbirds andsyllablechunks( Takahasietal. , 2010 ). (4) Humanlanguageandthenon-humanlanguage-liketypes LEXICALSTRUCTURE $ [ BEES/PRIMATES ] TYPEL EXPRESSIONSTRUCTURE $ [ BIRDSONG ] TYPEE LABELING Aseconduniquefeatureofhumanlanguageisﬁlabelingﬂ( Chom- sky , 1995 ).Givenaword,itscategory(Noun,Verb,etc.)formsthe labelofthelargerphrasethatcontainsit.Forinstance,giventhe pair eat and theapples ,theverb eat labelsthelargerphrase, eatthe apples (conventionally,aVerbPhrase). (5) Labeling Inthisway,phrasesinhumanlanguagehavethesamepropertyas theoriginallexicalitemthatprovidedthelabel( Chomsky , 1995 , 2008 ; Hornstein , 2009 ).Thisgiveshumanlanguageitsunique abilitytoformhierarchicalstructures( Chomsky , 1995 , 2008 ; Hornstein , 2009 ),aswenowdetail. EXPRESSIONSTRUCTURE:LIMITEDHIERARCHYANDLABELING Thelabelingphenomenonaboveappearstobeuniquelyhuman. Itoccurswithallkindsofphrases(e.g.,Noun,Verb,Preposition) suchthathumansyntacticstructurehasthepropertyofﬁdiscrete yﬂ( Chomsky , 2000 )throughrecursivelymergingandlabel- ingstructures.However,oncloseexamination,thereisasevere limitationonthedepthofthehierarchyforonecomponentof humanlanguage.Recallthat expressionstructure cancontain anitemwithpropertyTense;thereisaseconditem,convention- allylabeledﬁC(omplementizer)ﬂthathostsarangeofexpressive phrasessuchasQ(uestion),F(ocus)(e.g., Starlings,Ilike ),andso forth,asshownin(6). www.frontiersin.org February2013|Volume4|Article71 | 3                                                                                                                    Miyagawaetal. Emergenceofhierarchyinlanguage (6) ExpressionStructure Thesearethetwomostfrequentlycitedﬁlabelsﬂwithinthe expres- sionstructure .Strikingly,theselabelscannotbeassembled ashierarchicalstructuresofarbitrarydepth.Rather,theCP-TP hierarchicalstructurecanbeonlyonelayerdeep,asin(6).Pre- dictably,onedoesnothumanlanguagehierarchicalstructures suchasthefollowing(theasteriskmarksanimpossibleform). Suchunattestedstructureswouldcorrespondtohavingsucces- sivelayersofQuestionorFocusphrases,forexample( Arseni- jevicandHinzen ( 2012 ) makeasimilarobservationbasedon meaning). (7) Animpossiblesyntacticstructure Thisissimilartotherestrictionnotedearlierforbirdsong:here wealsoadepth-onehierarchicalstructure,aswiththeBen- galesehandNightingalesongs.Thissuggeststhat expression structure itselfisofTypeE,closelyrectingbirdsongstruc- ture.Importantlythen,thereisnorecursionthroughtheCP-TP expressionstructure. TherearetheoriesofES( Rizzi , 1997 ) thatpositamulti-layerwithinExpressionStructuretodealwith suchphenomenaastopicalizationandfocus.However,thereare alternativesthatdonotassumesuchamulti-layer(e.g., Miyagawa , 2010 ).Phenomenasuchasprosodymapontothesehierarchical structures. LEXICALSTRUCTURE Unlike expressionstructure,lexicalstructure elements cannotdirectlycombinewitheachothertoformpurelyLS hierarchicalstructures: (8) Examplesofimpossiblelexicalstructures Tomake(8a)possible,forexample,onemustinsert s ( John 'S book );this s isD(eterminer; Abney , 1987 ),amemberofthe expressionstructure (seebelow).ThefactthatLdoesnotallow hierarchymatchestheconstraintson typel languagesfoundin non-humanprimatesandbees. THESOURCEOFDISCRETEINFINITY IfESonlyadmitsonelayerofhierarchicalstructureandLSdoes notadmitanyhierarchicalstructure,whatisthesourceofhuman syntax'sunboundedhierarchicalstructure?Theanswerliesinthe wayunboundedhierarchicalstructuresareassembled,typically combinationsthatinterweavetheEandLlevels: (9) E/Lhierarchicalstructure(ﬁD(eterminer)ﬂispartoftheES fornounphrases) Inthisfragmentofasentence,weseeanalternationofLandE structure.Onecanabbreviatethisintheformofconventional context-freere-writeruleasfollows,whereEPistheﬁlabelﬂfor acategoryofthetypeEandﬁLPﬂistheﬁlabelﬂforanL-type structure. . 10 / (i)EP ! ELP (ii)LP ! LEP Rule(i)statesthattheEcategorycancombinewithLPtoforman E-levelstructure.Rule(ii)statesthattheLcategorycancombine withanE-levelstructuretoformanL-levelstructure.Together, thesetworulessetoyieldarbitrarilydeephierarchicalstruc- tures.Ifweexpandtheleft-handsideofRule(i),EP,weobtainthe twoitemsontheright-handsideoftherule,ELP.Wemayenclose thesewithsquarebrackets,toindicatethattheyformacomplete EPphrase[ELP].NowwecanapplyRule(ii)toLP,expandingitas, LEP.Againusingbracketnotation,weobtaintheform[E[LEP]]. WecanoncemoreapplyRule(i)totheEPunitthatisnowembed- dedwithinthebrackets,obtaining[E[L[ELP]]],andcontinue this adum toyieldarbitrarilynestedhierarchicalstructure. Allcurrentempiricallyadequatelinguistictheoriescontainsome FrontiersinPsychology |LanguageSciences February2013|Volume4|Article71 | 4                                                                                                                    Miyagawaetal. Emergenceofhierarchyinlanguage meanslikethistobuildsuchkindsofstructures.Arbitrarilydeep hierarchicalstructureisthustheby-productofE-andL-structures combiningalternately.Eachcomponentbyitselfisdescribable byaestategrammar.However,whencombined,theyinter- acttoyieldthefamiliararbitrarilydeephierarchicalstructurewe associatewithhumanlanguage. . 11 / ES:estate LS:estate E/LINTEGRATIONHYPOTHESIS Giventhedifferencebetween expressionstructure and lexical structure ,weproposethathumanlanguagearoseby integrat- ing thesetwodistinctsystems, typel(lexical) and typee ( expression ): (12) IntegrationofEandL Howdoesthisintegrationwork?Twopropertiesfoundinhuman languagesthathavebeenthefocusofintensivestudyinlinguis- tics,displacementandagreement,haveincommontheproperty thattheylinkanitemfromonelayerwithanitemfromtheother layer( Miyagawa , 2010 ),therebyunitingthetwolayers.Wediscuss displacementbelow. FROMLEXICALSTRUCTURETOEXPRESSIONSTRUCTURE Thedisplacementoflabeledphrasesalwaysoccursfrom lexi- calstructure to expressionstructure .InformingEnglish questions,somequestionwordthatstoccursinthe lexical structure isdisplacedtotheCpositioninthe expression structure . (13) Whatdidyoueat___? InChinese,toindicatethetopicofasentence,asimilardisplace- mentoccurs (14) Zhebenshu Zhangsanmai-le___. thisbookZhangsnabuy-ASPECT `Thisbook,Zhangsanbought.' Wecanpicturethisdisplacementasfollows: (15) DisplacementfromLtoE Bydisplacinganitemfromthe lexicalstructure tothe expres- sionstructure ,thesetwolayersoflanguagearethenlinked.We thereforepositthefollowingprinciple(16). (16) DisplacementexiststointegratetheExpressionandLexical structuresofhumanlanguage. CONCLUSIONANDDIRECTIONSFORFUTUREINQUIRY Ourproposalpartitionslanguagesyntaxintotwosystems,EandL, locatingsuggestiveantecedentsforeachinnon-humananimals. Wehaveoutlinedhowthesetwosystemscouldbeintegratedto yieldthediscreteyofhumanlanguage.HowdidtheEandL systemscometobelinkedinmodernhumans?Whileanswers tothisquestionmustnecessarilyremainspeculative,onecan advanceatleasttwopossibleroutes.Oneinvolvessharedhuman intentionality( Tomaselloetal. , 2005 ).Althoughthereislimited evidencethatalarmcallsinmonkeysareunderintentionalcon- trol( SeyfarthandCheney , 2010 ),thisabilityappearsfull-blown inhumans.Sharedintentionalityaddsanexpressivecomponent tothelexicalsystem,inthiswayfunctionallyinterleavingtheE andLsystems.Asecondpossibilityistheonenotedby Darwin ( 1871 ) inhis DescentofMan :humanlanguagestemergedas ﬁsongsﬂŒprosodiccontoursandsyllablestructureslikebirdsongŒ whichwerethengraftedontoaseparatewordsystem.Inthisarticle wehaveattemptedtoadvanceDarwin'shypothesis.(Othershave embracedDarwin'sproposal,thoughwithoutourdivisionintoE andLsystems;see,e.g., Fitch ( 2010 ) .)Additionally,theabilityto ﬁchunkﬂacousticstreamsintolinearsegments,alongwithprosody ormetricalstructureŒthepatternofstrongandlightﬁbeatsﬂin asongŒrhythmicentrainment,andvocallearning,areshared amongvocallearningavianspeciesaswellashumans.Whileneu- robiologypointstoright-brainedlocalizationforhumanprosodic processing,itiswellknownthatsyntacticprocessingislocalized toleft-brainareasinhumans,whileﬁnamingﬂinvolvesbothdorsal andventralstreams( Friederici , 2012 ).Takentogether,onemight speculate,following Berwick ( 2011 ) ,thatthepurelyesystem formetricalstructureŒaright-brainactivityŒwasjoinedwiththe ﬁnamingﬂabilityofearlyhumans(orpossiblyotherprimates)to yieldthecombinationE-Lsystemandsofullyhumanlanguage. ACKNOWLEDGMENTS Theauthorswishtothankthetwoanonymousreviewersfortheir suggestionsthatsubstantiallyimprovedthecontentofthepaper. KazuoOkanoya'sportionoftheworkforthisarticlewassupported inpartbyGrant-in-aid#23240033fromMEXT,Japan. www.frontiersin.org February2013|Volume4|Article71 | 5                                                                                                                    Miyagawaetal. Emergenceofhierarchyinlanguage REFERENCES Abe,K.,andWatanabe,D.(2011).Song- birdspossessthespontaneousability todiscriminatesyntacticrules. Nat. Neurosci. 14,1067Œ1074. Abney,S.P.(1987). TheEnglishNoun PhraseinItsSententialAspect .Ph.D. thesis,MassachusettsInstituteof Technology,Cambridge. Arnold,K.,andZuberbuhler,K.(2006). Languageevolution:semanticcom- binationsinprimatecalls. Nature 441,303. Arnold,K.,andZuberbuhler,K.(2008). Meaningfulcallcombinationsina non-humanprimate. Curr.Biol. 18, R202ŒR203. Arsenijevic,B.,andHinzen,W.(2012). OntheabsenceofX-within-Xrecur- sioninhumangrammar. Linguist. Inq. 43,423Œ440. Beckers,G.J.L.,Bolhuis,J.J., Okanoya,K.,andBerwick,R. C.(2012).Birdsongneurolinguis- tics:songbirdcontext-freegrammar claimispremature. Neuroreport 23, 139Œ145. Berwick,R.(2011).ﬁAllyouneedis merge:biology,computation,and languagefromthebottom-up,ﬂin TheBiolinguisticEnterprise ,edsA. M.DiSciulloandC.Boeckx (Cambridge,MA:TheMITPress), 706Œ825. Berwick,R.C.,Beckers,G.J.L., Okanoya,K.,andBolhuis,J. J.(2012).Abird'seyeview ofhumanlanguageevolu- tion. Front.Evol.Neurosci. 4:5. doi: 10.3389/fnevo.2012.00005 Berwick,R.C.,Okanoya,K.,Beckers, G.J.L.,andBolhuis,J.J.(2011). Songstosyntax:thelinguisticsof birdsong. TrendsCogn.Sci.(Regul. Ed.) 16,113Œ121. Bolhuis,J.J.,Okanoya,K.,andScharff, C.(2010).Twitterevolution:con- vergingmechanismsinbirdsongand humanspeech. Nat.Rev.Neurosci. 11,747Œ759. Chomsky,N.(1956).Threemodelsfor thedescriptionoflanguage. IEEE Trans.Inf.Theory 2,113Œ124. Chomsky,N.(1995). TheMinimalist Program .Cambridge,MA:TheMIT Press. Chomsky,N.(2000). NewHorizonsin theStudyofLanguageandMind . Cambrdige,MA:CambridgeUniver- sityPress. Chomsky,N.(2005).Threefactorsin languagedesign. Linguist.Inq. 36, 1Œ22. Chomsky,N.(2008).ﬁOnphases,ﬂin FoundationalIssuesinLinguisticThe- ory ,edsR.Freidin,C.P.Otero,and M.L.Zubizarreta(Cambridge,MA: TheMITPress),133Œ166. Darwin,C.(1871). TheDescentofMan inRelationtoSex, Vol.179.London: Murray,182. Dessalles,J.L.(2007). WhyWeTalk:The EvolutionaryOriginsofLanguage. Oxford:OxfordUniversityPress. Doupe,A.,andKuhl,P.K.(1999).Bird- songandhumanspeech:common themesandmechanisms. Annu.Rev. Neurosci. 22,567Œ631. Fitch,W.T.(2010). TheEvolutionof Language. Cambridge:Cambridge UniversityPress. Friederici,A.D.(2012).Thecortical languagecircuit:fromauditoryper- ceptiontosentencecomprehension. TrendsCogn.Sci. 16:262Œ268. Gentner,T.Q.,Fenn,K.M.,Mar- goliash,D.,andNusbaum,H.C. (2006).Recursivesyntacticpattern learningbysongbirds. Nature 440, 1204Œ1207. Hale,K.,andKeyser,S.J.(1993). ﬁOntheargumentstructureandthe lexicalexpressionofgrammatical relations,ﬂin TheViewfromBuld- ing20:EssaysinHonorofSylvain Bromberger ,edsK.H.HaleandK. J.Keyser(Cambridge,MA:TheMIT Press),53Œ110. Hornstein,N.(2009). ATheoryofSyn- tax:MinimalOperationsandUni- versalGrammar. Cambridge:Cam- bridgeUniversityPress Jespersen,O.(1922). Language,Its Nature,Development,andOrigin. NewYork:HenryHoltandCom- pany. Kipper,S.,Mundry,R.,Sommer,C., Hultsch,H.,andTodt,D.(2006). Songrepertoiresizeiscorrelated withbodymeasuresandarrival dateincommonnightingales,Lus- ciniamegarhynchos. Anim.Behav. 71,211Œ217. Marler,P.(1970).Birdsongandspeech development:couldtherebeparal- lels? Am.Sci. 58,669Œ673. Marler,P.(2000).ﬁOriginsofmusic andspeech:insightsfromanimals,ﬂ in TheOriginsofMusic ,edsN. Wallin,B.Merker,andS.Brown. (Cambridge:TheMITPress), 31Œ48. Miyagawa,S.(2010). WhyAgree?Why Move?:UnifyingAgreement-Based andDiscourse-CationalLan- guages .Cambrdige,MA:TheMIT Press. Nottebohm,F.(1975).Continental patternsofsongvariabilityin Zonotrichiacapensis:somepossible ecologicalcorrelates. Am.Nat. 109, 605Œ624. Okanoya,K.(2002).ﬁSexualdisplayas asyntacticalvehicle:theevolution ofsyntaxinbirdsongandhuman languagethroughsexualselection,ﬂ in TheTransitiontoLanguage ,ed. A.Wray(Oxford:OxfordUniversity Press),46Œ63. Riley,J.,Greggers,U.,Smith,A., Reynolds,D.,andMenzel,R.(2005). Thehtpathsofhoneybees recruitedbythewaggledance. Nature 435,205Œ207. Rizzi,L.(1997).ﬁThestructure oftheleftperiphery,ﬂin Elements ofGrammar:HandbookofGenera- tiveSyntax ,ed.I.Haegeman.(Dor- drecht:Kluwer),281Œ337. Seed,A.,andTomasello,M.(2010).Pri- matecognition. TopicsCogn.Sci. 2, 407Œ419. Seyfarth,R.M.,andCheney,D.L. (2010).Production,usage,andcom- prehensioninanimalvocalizations. BrainLang. 115,92Œ100. Seyfarth,R.M.,Cheney,D.L.,and Marler,P.(1980).Monkeyresponses tothreedifferentalarmcalls: evidenceofpredatorcion andsemanticcommunication. Sci- ence 210,801. Takahasi,M.,Yamada,H.,andOkanoya, K.(2010).StatisticalandProsodic CuesforSongSegmentationLearn- ingbyBengaleseFinches(Lonchura striatavar.domestica). Ethology 116, 481Œ489. Tallerman,M.,andGibson,K.R.(2011). TheOxfordHandbookofLanguage Evolution .Oxford:OxfordUniver- sityPress. Tattersall,I.(2009).Languageandthe originofsymbolicthought. Cogn. Archaeol.Hum.Evol. 109Œ116. Tomasello,M.,andCall,J.(1997). PrimateCognition .Oxford:Oxford UniversityPress. Tomasello,M.,Carpenter,M.,Call, J.,Behne,T.,andMoll,H.(2005). Understandingandsharinginten- tions:theoriginsofcultural cognition. Behav.BrainSci. 28, 675Œ690. CoofInterestStatement: The authorsdeclarethattheresearchwas conductedintheabsenceofanycom- mercialorrelationshipsthat couldbeconstruedasapotentialcon- tofinterest. Received:20November2012;accepted: 02February2013;publishedonline:20 February2013. Citation:MiyagawaS,BerwickRC andOkanoyaK(2013)Theemer- genceofhierarchicalstructureinhuman language.Front.Psychol. 4 :71.doi: 10.3389/fpsyg.2013.00071 ThisarticlewassubmittedtoFrontiersin LanguageSciences,aspecialtyofFrontiers inPsychology. Copyright©2013Miyagawa,Berwick andOkanoya.Thisisanopen-accessarti- cledistributedunderthetermsofthe CreativeCommonsAttributionLicense , whichpermitsuse,distributionand reproductioninotherforums,provided theoriginalauthorsandsourcearecred- itedandsubjecttoanycopyrightnotices concerninganythird-partygraphicsetc. FrontiersinPsychology |LanguageSciences February2013|Volume4|Article71 | 6
Units of sound	Phonemes	Phonology	Structural hierarchy of language
Name the most common categories or parts of speech	Nouns, Verbs, Adjectives, Pronouns, Prepositions, Adverbs, Conjunctions, Articles, Interjections	Which subcategories can nouns be further categorized into?	Singular and Plural	Superlative and Unsuperlative|||Possessive and Non-Possessive 	NLP Crash Course clip||youtube||N/A
Natural Language Processing (NLP)	Branches of linguistics	Computer science	Linguistic Theory	Part-of-speech (POS) tagging
What are some example tasks of sentiment analysis?	- Determining if a movie review is positive, neutral or negative - Determining if people like or dislike a presidential candidate - Determining if consumers like a product	Which of the following words likely has the strongest weight in determining the sentiment of tweets about a presidential candidate?	'terrible'	'president'|||'wants'|||'very'	Sentiment Analysis Presentation||slides||?8+',"#$%&"#$')#)*+,%,3 ¥!"#$% F%%&'% $"&'% ;)2&)=%45'&$&2)%5;% *)6#$&2)/ ¥&'"()*+, F%="#$%35%4)54-)%$"&*H%#150$%$")%*)=%&E"5*)/ ¥&)-.$*/,%0+$1%0+ F%"5=%&'%:5*'0+);%:5*8&3)*:)/%>'%3)'4#&;% &*:;)#'&*6/ ¥&".$+$*, F%="#$%35%4)54-)%$"&*H%#150$%$"&'%:#*3&3#$)%5;%&''0)/ ¥&'%($*+$"0 F%4;)3&:$%)-):$&5*%50$:5+)'%5;% +#;H)$% $;)*3' 8;5+%')*$&+)*$ NM
Text classification	Sentiment analysis	Natural Language Processing (NLP)
What is cognitive semantics?	Cognitive semantics is a school of thought that sees linguistic meanings as conceptual structures in the mind, rather than as relationships between language and things in an objective world. It emphasizes cognition and "embodiment" - the idea that meanings are derived from our embodied experience of the world, from perception and action. Traditional theories of meaning assume that semantics is a matter of evaluating the objective truth or falsity of sentences, while cognitive semantics says that truth is secondary, with language being thoroughly metaphorical and referring to subjective experiences.	Which of the following is not a difference between cognitive semantics and traditional semantics?	Semantics is a kind of mapping	One need not refer to reality to interpret language|||Language depends on sensory-motor cognition|||Truth is not a fundamental notion	Cognitive semantics and image schemas with embodied forces||slides||˘ˇˆ˙˝  ˛ˇˇ  ˝˚˜˜˜˜˜˘ˇ ˝  !"#ˇ!#ˇ!   $ˇ#ˇ˙˙ˇ%ˇ    !&ˇ#' #ˇ'##˙ ˇ%ˇ #(ˇ ˙˙)ˇ# #''ˇ! &ˇ*#˙  +,'#'˙ #ˇ '˙#˙  #ˇ!-˙ #˙ ˇ* ##!.˙ ##   ## ˇ! .ˇ ,#      !.#'' *' ˇ  !,#   #ˇ ''˙ˇˇ #!,'#'###  ˇ  ˙!/ #˙*'# '  !/0123 '! 4456˙ 789#˙˙#ˇˇ ''ˇ#!: ˙#   'ˇ##˙0! ! ˙##˙6; <ˇ  ˇ#˙# '˚'' ! .#  ˙ ##  #'0!!˘%123 ˘%12= 1 23 ˇ˜55> ?˜55=6! :'#@  !A'#˙  #ˇ'' *'#ˇ !:' ˇ'ˇ#  .' ˇˇˇ˙ ˇ   )   ˇ  !,,B  ,  '', ˇˇ˙ '!,˙# ## < <#%#%0ˇ ####%6!:˙ << !  .Cˇ''ˇ #˙ #˙ ˙#ˇ *'!D%0123 '!˜E6˙@;:ˇ %'@*'##
Cognitive semantics	Cognition	Semantics	Analytic philosophy	Image-schema	Semantic theories	Truth-conditions
What is the Critical Period Hypothesis?	The critical period hypothesis is a hypothesis that suggests that there is a particular window of opportunity where a child has the ability to learn a language as a native speaker of that language. This is between birth and middle childhood (13 years). This method of language acquisition can only take place in a linguistically rich environment with exposure to a certain language.	Which of the following theorists supports the Critical Period Hypothesis?	Noam Chomsky	Paul Broca|||Benjamin Bloom|||Sigmund Freud	Brain and Language presentation||slides||Brain and Language   Reported by: Alice Carrillo, RMT , LPT    Topics    The Human Brain    The Autonomy of  Language    Language and Brain  Development    Review    Exercises    The Human  Brain    The Human Brain   Cortex   Cerebral  Hemispheres   Corpus Callosum   -    decision making   - one on the right and one on the left   and is joined together by   - allows the communication of the two    hemispheres    If you point with your  right hand,   the  left hemisphere   is responsible   to your action.    Where is Language located in the brain ???   Where is Language located in brain ???     Where is Language located in the brain ???       Proposed the  theory of Localization    Language is located in the  frontal  lobes of the brain      later   known as    phrenology   Practice of determining:     Personality traits  and intellectual capacities    by examining the   on the skull   Franz Joseph Gall    Johann Spurzheim    Introduced phrenology to America   Located directly under the eye    Aphasia    Paul Broca    Proposed that language is localized to the   left hemisphere of the brain (    area)        Claimed that we speak with  the left  hemisphere    Based on a study of his patient who suffered    deficits after brain injury to the left frontal lobe       Carl Wernicke   Another type of Aphasia that occurred in patients with   lesions in the areas of the  left hemisphere temporal             Loss of fluency and articulation        Inability to repeat complex sentences        Impaired comprehension of complex sentences    Effortless, melodic speech        Unintelligible content due to  word and phoneme choice  errors (phonemic paraphasias)        Loss of repetition        Aphasia      Aphasia     Most notable characteristic   of     Aphasia is that it is  agrammatic   which  means that it  lacks articles, prepositions, pronouns, auxiliary verbs, and  typically remove inflections   such as the past tense suffix     ed   or the third  person singular verb ending   s.     Example:   DOCTOR : Could you tell me what you have been doing in the hospital?   PATIENT :   Yes, sure. Me go,  er , uh, P.T. none     rike    DOCTOR :   And have you been going home on weekends?   PATIENT:    uh..no  ba  ra   purpike          produce speech with good intonation and syntax, but their  language is often semantically incoherent.     Example:   A patient replied to a question about his health with:     PATIENT:    I felt worse because I can no longer keep in mind from the minds of the  minds to keep me from my mind and up to the ear which can be to find among  ourselves.       difficulty naming objects   presented to them.   Example:       Jargon Aphasia            1. There is under a horse a new sidesaddle.     2. In girls we see many happy days.          4. I surprise no new glamour.      Deaf Aphasics   - Deaf patients with lesions in     area show language  deficit like those of hearing  patients such as:     Severely dysfluent   Agrammatic   sign production   - Deaf patients with lesions in   have fluent but often  semantically incoherent sign  language filled with made - up signs.    I want a  table to sit  on.   Jargon Aphasia    Other Language Disorders:   Developmental Dyslexics   Acquired Dyslexics    The   tip - of - the - tongue   phenomenon   (TOT)   means   that   you   rarely   find   the   word   you   wanted   to   say .   Many   aphasics   suffer   from   severe   anomia .   Anomia   is   the   inability   to   find   the   word   you   wish   to   speak .    SLI   Specific Language Impairment   Children with SLI have  problems   with the use of function words such as  articles, prepositions,  and  auxiliary verbs .   Examples:                                  Linguistically handicapped savants    - 15 - 20 languages, and can   translate them quickly with few  errors in English   Christopher    Laura   She can produce complex sentences with multiple  phrases and sentences with other sentences inside  them, but she cant read nor write.    battery - powered watch that I loved,  and She does painting, this really  good friend of the kids who I went  to school with and really loved, and  I was like 15 or 19 when I started      CT scans and Magnetic Resonance   Imaging (MRI)   - Can reveal lesions shortly after the   damage occurs   Positron Emission Topography (PET) Scan and  functional MRI (fMRI)   scans provide images of the  brain in action.     Noninvasive brain  recording technologies   are use to locate brain lesions or to  identify the language regions of the  brain.        Magnetic Encephalography (MEG)   Measures   magnetic   fields   in   the   living   brain    and   show   us   how   the   healthy   brain   reacts   particular   linguistic   stimuli .   *The results of these studies reaffirm that  language resides in specific  areas of the left hemisphere    Brain Plasticity and Lateralization in Early Life   Left Hemisphere        ( right side)       Right Hemisphere          left side           A one week old    gugugu   babbling      is a very rare neurosurgical procedure in which one cerebral  hemisphere (half of the brain) is removed, disconnected, or disabled .   Hemispherectomy   Children who undergo  left  hemispherectomy   show   specific linguistic deficits,  while other cognitive    abilities remain intact.    having the corpus  callosum severed or  absent eliminates  the main connection  between the two  hemispheres of the  brain   Split - Brain    Dichotic listening   Is an experimental  method that uses auditory  signals to observe the  behavior of the individual  hemispheres of the brain.    The Critical  Period    Critical - age hypothesis   States that there is a  window of opportunity  between birth and  middle childhood for  learning a first  language.    Such behavior is not the result of conscious  decision, external teaching, or intensive  practice, but appears to be a mature  determined schedule that is universal across  the species.    EXERCISES    The following sentences spoken by aphasic patients were collected and analyzed  by Dr. Harry Whitaker.  In each case, state how the sentences deviates from normal  non - aphasic language.   1. Mike and Peter is happy.   2. Bill and John likes hot dogs.   3. Is there three chairs in this room?   4. In girls we see many happy days.   5. Proliferate is a complete time about a word  that is correct.            1.  flipdoor   you can see it.   2.  swish.   3.  disflined   a  sinter, minter.   4.  he.
Critical Period Hypothesis	Language acquisition	Language exposure	Psycholinguistics	Lateralization of languages
What are phonaesthemes?	Phonaesthemes are clusters of sounds that make up parts of words in a language, which are larger than phonemes, smaller than morphemes, and appear to have vague meanings, somewhat like sound-symbolism. An example is the "-mp" in mump, bump, hump, lump, dump, jump, blimp, and ramp.  Another is the "str-" in string, stripe, stretch, streak, stroke, and strap.  They have an ambiguous status in linguistics. Theorists disagree about whether they are part of the grammar of a language, governed by rules, or even real. One cannot freely use them to make new words but they do appear often in neologisms.	Which of the following is an example of phonaesthemes?	The "-ash" in crash, smash, bash, and hash.	Some words for animal noises, like "grrr," "tweet," and "bah."|||The "-oom" in room, doom, loom, boom, and broom.|||The suffix "-ish."	The Psychological Reality of Phonaesthemes||publication||0THE PSYCHOLOGICAL REALITY OF PHONAESTHEMESBENJAMIN K. BERGENDepartment of LinguisticsUniversity of Hawai`i at Manoa569 Moore Hall1890 East-West RoadHonolulu, HI 96822bergen@hawaii.edu (808)956-3242 1THE PSYCHOLOGICAL REALITY OF PHONAESTHEMES 2The psychological reality of English phonaesthemes is demonstrated through a primingexperiment with native speakers of American English. Phonaesthemes are well represented sound-meaning pairings in a language, such as English gl-, which occurs in numerous Englishwords with meanings relating to light and vision. In the experiment, phonaesthemes, despitebeing non-compositional in nature, displayed priming effects much like those that have beenreported for compositional morphemes. These effects could not be explained as the result ofsemantic or phonological priming, either alone or in combination. The results support a viewof the lexicon in which shared form and meaning across words is a key factor in theirrelatedness, and in which morphological composition is not a prerequisite for internal wordstructure to play a role in language processing.*                                                  * Thanks to Nancy Chang, Jerome Feldman, Larry Hyman, Sharon Inkelas, George Lakoff, and Madelaine Plauché forhelpful comments on drafts of related work, and to Amy Schafer and Robert Blust, in addition to two anonymous  reviewers for very useful suggestions on this paper. 3INTRODUCTION. Phonaesthemes (Wallis 1699, Firth 1930) are frequent sound-meaningpairings, which cannot be defined entirely in terms of contrast, like the English onset gl-. As withother phonaesthemes, this particular sound sequence, gl-, is relatively infrequent in English,except among words with meanings related to ‘VISION’ and ‘LIGHT’, some of which are exemplified in 1a. Another well-documented phonaestheme is the English onset sn-, whichoccurs in a large number of words relating to ‘MOUTH’ and ‘NOSE’ (1b).(1)a. gl-LIGHT, VISIONglimmer, glisten, glitter, gleam, glow, glint, etc.b. sn-NOSE, MOUTHsnore, snack, snout, snarl, snort, sniff, sneeze, etc.When viewed simply as statistically aberrant distributions of sound-meaning pairings in thelexicon, phonaesthemes are found to be pervasive in human languages. They have been documentedin such diverse languages as English (Wallis 1699, Firth 1930, Marchand 1959, Bolinger 1980), Indonesian (McCune 1983) and other Austronesian languages (Blust 1988), Japanese (Hamano 1998), Ojibwa (Rhodes 1981), and Swedish (Abelin 1999). While it is difficult to establish reliableestimates of how many of the world's languages display phonaesthemes, or how many words in agiven language's lexicon have phonaesthemes in them, no systematic studies of particular languageshave produced results suggesting that those languages have no phonaesthemes in them. In general,phonaesthemes seem to appear in content words over function words, and in more specific (orsubordinate level) rather than more general (or basic level) words.In addition to distributional evidence, phonaesthemes can be detected through their role inlanguage change, in particular by the part they play in the generation of neologisms. Phonaesthemeshave been implicated in both the production and perception of neologisms on the basis ofexperimental (Abelin 1999) and comparative studies (Blust 1988, 2003). The recurrent finding of  4note in this area is that in a given set of related languages, phonaesthemes that appear in somelangauges will also appear in other languages, but in words that are not cognates. For example (fromBlust 2003) the Austronesian phonaestheme N occurs in initial position in words that have meaningsrelated to 'NOSE' or 'MOUTH'. It appears in words meaning 'CHEW, MASTICATE,RUMINATE', such as Amis NafNaf, Toba Batak Naltok and Trukese Nú, among others, but noneof these words are cognates.While statistically significant representation in a lexicon and participation in the construction ofneologisms are important convergent indications that phonaesthemes are a fact of language, thesetwo types of evidence leave room for different interpretations of exactly what the status ofphonaesthemes is. The extent to which phonaesthemes play a role in the synchronic mentalorganization of language remains an open question.Indeed, their cognitive status remains controversial, in part because they do not fit well intolinguistic theories that view compositionality as a central characteristic of morphological complexity,such as so-called Item and Arrangement models (Hockett 1954). On these compositional types of account, phonaesthemes belong outside the scope of regular morphological analysis. By contrast,non-compositional theories of morphology predict that recurrent sound-meaning pairings in alanguage, whether compositional or not, will rise to status as organizing principles of the lexicon, onthe basis of their frequency. Examples of such non-compositional models are the Dynamic Model(Bybee 1985), the Usage-based Model (Langacker 1991), Seamless Morphology (Starosta ToAppear), and most connectionist morphological models (e.g. Plaut and Gonnerman 2000). This paper investigates the psychological reality of phonaesthemes, thereby providing anindication of the appropriate place for phonaesthemes in morphological theories. Looking ahead, itwill be shown below that presenting language users with a sequential pair of words sharing aphonaestheme produces a processing advantage for the second word (a facilitory priming effect).  5This phonaesthemic priming effect differs significantly from form priming, shown by words sharingonly a phonological onset, and from semantic priming, which occurs between semantically relatedwords. Rather, phonaesthemic priming very closely mirrors the priming effects that have beenreported for morphologically related words. Moreover, phonaesthemic priming is not observedbetween simply any two words that by chance share both some phonological form and somemeaning – it only surfaces when the form-meaning pairing is well attested in the lexicon. We willconclude from these results that while compositionality may play a role in morphologicalorganization, statistical prevalence in the lexicon of form-meaning pairings is also a sufficientcriterion for those pairings to display morpheme-like behavior in language processing tasks. As such,these results also lend support to the prediction made by non-compositional morphological modelsthat the frequency, and not just the compositionality, of recurring form-meaning pairings is crucialto their mental representation.1. PHONAESTHEMES AND COMPOSITIONALITY 1.1. COMPOSITIONALITY IN MORPHOLOGICAL THEORIES . Human language is frequentlycharacterized as a finite system with infinite capacity (e.g. Chomsky 1995). On most views, the keyproperty of linguistic systems that gives rise to this infinite capacity is the possibility of combininglinguistic units with others in novel ways. Compositional morphological theories, such as Item andArrangement models, view this property, PRODUCTIVITY, to be predicated upon the hierarchicalnature of certain linguistic structures. Complex linguistic units such as complex words are believedto be composed of smaller units, that is, they are COMPOSITIONAL. Most linguistic theories viewcompositionality, along with the resulting productivity, as a defining feature of human language.Syntactic, phonological, and morphological compositionality have been particularly central in 6shaping mainstream linguistics of the second half of the 20th century. The centrality of this capacityin theories of morphology is entirely unsurprising, since a large part of what language users knowabout the words of their language is indeed how to combine morphemes to construct novel words.Many morphological theories view complex words as compiled in production or decomposed inperception. The processes can be seen as the simple concatenation of morphological units, by Itemand Arrangement models (e.g. Lieber 1980, Di Sciullo and Williams 1987, and Halle and Marantz 1993).1 What these models share is the view that the morphological capacity is something akin to ametaphorical assembly line, which either compiles a complex word from its morphological parts inproduction, or takes apart a complex word to analyze the pieces during perception. Moreover, formany theories of the word-internal relation between sound and meaning, the ability to combineunits in novel ways to produce larger ones even defines the domain of study. On one view forexample, ‘processes of word-formation that are no longer productive [...] are of little or no interestto morphological theory’ (Baayen and Lieber 1991:802). By itself, though, compositionality makes up only part of what language users know about how the words of their language pair form and meaning. They also have unconscious knowledge ofgroups of words that share meaning and are phonologically or prosodically similar (Sereno 1994,Kelly 1992). They know subtle correlations between a person’s gender and the phonology of theirname (Cassidy et al. 1999) and between the number of syllables in a word and the complexity of theobject it describes (Kelly et al. 1990). They also know when the pieces of a word bear meaning,whether or not those parts can be productively combined with others (Bybee and Scheibman 1999). As these results indicate, and as we will see below, compositionality, as it is usually understood,does not suffice to capture what language users know about word-internal form-meaning relations.The next section describes the difficulties that assembly line models face, when confronted withphonaesthemes. 71.2. THE TROUBLE WITH PHONAESTHEMES . Phonaesthemes hold a prominent place amongmorphological phenomena that are troublesome to compositional accounts of morphology. Anotherof these difficult cases are cranberry morphs, like cran- or boysen-, which cannot stand alone and arepermitted to attach to only one stem. Unlike cranberry morphs, phonaesthemes by definition appearin numerous words. They also attach to material that is no more like conventional morphemes thanthe phonaesthemes are themselves. These two properties of phonaesthemes – their frequency andthe fact that they combine with material of a questionable morphological nature – are what makethem particularly irksome.Phonaesthemes are defined as form-meaning pairings that crucially are better attested in thelexicon of a language than would be predicted, all other things being equal. For an example of thistype of hyper-attestation, consider the distribution of four phonaesthemes in the Brown corpus(Francis & Kucera 1967). A full 39% of the word types and a whopping 60% of word tokensstarting with gl- have definitions that relate to ‘LIGHT’ or ‘VISION’ (Table 1) in an online versionof Webster’s 7th Collegiate Dictionary.2 Similarly, 28% of word types and 19% of word tokens witha sn- onset have meanings related to ‘NOSE’ or ‘MOUTH’. Complete lists of each set of word typecan be found in Appendix 1.INSERT TABLE 1 ABOUT HERE The overwhelming statistical pairings of forms like gl- and sn- with their associated meanings runcontrary to what we would predict if the only correlations between the sounds of words and theirmeanings derive from shared component subparts, i.e. constituent morpheme. That is, on the basisof the assumption that a word’s phonological form is entirely arbitrary, given its semantics, there 8should be the same portion of gl- words that have meanings related to ‘LIGHT’ or ‘VISION’ asthere are sn- words that share these meanings. And yet, as Table 1 shows, this is clearly not the case.While more than one third of gl- word types relate to ‘LIGHT’ or ‘VISION’, only one percent of sn-words do. The reverse is true for sn- words and the meanings ‘NOSE’ and ‘MOUTH’ – while nearlyone third of sn- word types relate to ‘NOSE’ or ‘MOUTH’, only four percent of gl- word types sharethis semantics. One can also compare the proportion of gl- or sn- words that bear these particularmeanings with the distribution of fl- initial words in Table 1. While some fl- initial words have‘LIGHT’ or ‘VISION’ meanings, like flicker and fluorescent, these words are less numerous than are gl-words with similar meanings.Since morphological models are attempts to understand distributions of lexical and sub-lexicalsound-meaning pairings in a language, phonaesthemes appear at first blush to be perfect fodder forthem. But phonaesthemes are problematic for compositional morphologies because words thatcontain these phonaesthemes also contain a complement which is usually not itself a meaningfulunit. For example, removing the onset phonaesthemes from glint and snarl yields -int and -arl. Itwould be problematic both semantically and structurally to say that -int and -arl were units whichcontributed to the words as a whole. Neither do -int and –arl have meanings on their own, nor canwe mix and match the phonaesthemes and their complements: glarl and snint are mostly nonsensical(though as potential neologisms we can assign possible meanings to them, as discussed in Section1.3 below).It should be noted that although all the examples we have seen thus far are drawn from onsets,other parts of syllables like rimes in words like English smash, bash, crash, and mash (Rhodes andLawler 1981 and Rhodes 1994) and even entire syllables in languages like Indonesian (Blust 1988) are appropriately classified as phonaesthemes. Thus in some cases, a word may actually appear toconstructed compositionally from entirely phonaesthemic material. For example, sneer might be seen 9as composed from the onset phonaestheme sn- plus a rime phonaestheme -eer , also found in leer andjeer, with a meaning pertaining to 'EXPRESSION OF CONTEMPT'. Nevertheless, theoverwhelming majority of words containing phonaesthemes include material that would be difficultto analyze as phonaesthemic, as a perusal of the examples in Appendix A will swiftly demonstrate. So while phonaesthemes display some potential compositionality, this is not their usual behavior. It is the tension between compositional models’ apparent need to account for phonaesthemesand their inability to do so that drives us to look further into the details of the psychological statusof phonaesthemes.1.3. EXPERIMENTAL EVIDENCE ON PHONAESTHEMES . Phonaesthemes, as noted above, arepervasive in human languages. Traditionally, they have been documented primarily on the basis ofdistributional data. While there is a long tradition of psycholinguistic experiments focusing on a howlanguage users interact with novel words containing particular purported sound symbolic elementssuch as /i/ and /a/ (see the summary in Jakobson and Waugh (1979)), only recently has the psychological reality of large numbers of phonaesthemes been placed under systematic experimentalscrutiny. As we will see below, the psychological status of phonaesthemes has been difficult to pindown for methodological reasons.The standard linguistic test for evaluating the psychological reality of a proposed linguistic unit isbased on compositionality and productivity. In her seminal study of regular English plural marking,Berko (1958) argued for the psychological reality of morphological knowledge on the basis ofsubjects’ generalizations of an existing pattern in the language to new forms, such as wug. Whensubjects accurately pluralized the novel form wug [] as wugs [], this was taken as evidencethat they had internalized a rule by for forming plural nouns. Such neologism tasks (now known asWUG-TESTS) are pervasive in psychological studies of morphological knowledge. 10But a methodological predisposition towards compositionality and productivity leads to the following quandary. It is quite difficult to assess the psychological reality of phonaesthemes, whichare generally not compositional or very productive, if the only metric we have is crucially based onproductivity and compositionality themselves. A partial solution to this dilemma has arisen from theobservation that phonaesthemes, while non-compositional, are in fact partially productive.In the past three years, three theses (Hutchins 1998 and Magnus 2000 on English and Abelin1999 on Swedish) have addressed the psychological reality of phonaesthemes from essentially thesame perspective: that of neologisms. Magnus’ work is representative of all three, so her thesis canserve as a model to demonstrate their methodologies. Magnus uses two types of experiment to testthe psychological status of phonaesthemes. Both of these are based upon subjects’ recognition andproduction of neologisms. The first methodology tests for knowledge of phonaesthemes byproviding subjects with definitions for non-existent words and asking them to invent new words forthose definitions. In Magnus’ study, subjects tended to invent novel words that made use ofphonaesthemes of their language. For example, given the definition ‘to scrape the black stuff offoverdone toast’, 27% of subjects invented a word that started with sk-. The second type ofexperiment tested the use of phonaesthemes in the perception of novel words. Subjects werepresented a non-word and were asked to provide a definition for it. Again, they responded as if theywere using phonaesthemes. For example, glon (bearing a gl- onset) evoked definitions relating to‘LIGHT’ in 25% of the cases.These studies each provide independent evidence for the role of phonaesthemes in theprocessing of neologisms. Magnus (2000) interprets her results as evidence that language usersrepresent individual phones as bearing meaning (note that this is similar to Joseph's (1994)conclusion for the Greek phoneme /ts/, made on the basis of distributional evidence). Abelin (1999) adds to this the insight that the fewer words there are that share the phonology of a 11phonaestheme (excluding the phonasthemic words themselves), and the more common that phonaestheme is among the words sharing its phonology, the more likely subjects are to treatneologisms along the lines of that phonaestheme. Hutchins’ (1998) results confirm the role ofphonaesthemes, even cross-linguistic ones, in the processing of neologisms. Interestingly, thesefindings are qualitatively similar to results of neologism experiments on purely phonologicalgeneralizations, such as the Obligatory Contour Principle in Arabic (Frisch and Zawaydeh 2001) andLabial Attraction in Turkish (Zimmer 1969) – frequent patterns, whether phonological orphonological-semantic, are used in neologism interpretation and production.The results of these studies are intuitive, given what we know about lexical innovation in general.Probably the most famous use of English language literary neologisms are in The Jabberwocky, apoem that confounds young Alice in Lewis Carroll’s (1897) Through the Looking Glass. The firststanza of this poem appears in 2 below.(2)‘Twas brillig, and the slithy toves Did gyre and gimble in the wabe: All mimsy were the borogoves,And the mome raths outgrabe.Luckily for Alice, Humpty Dumpty is gracious enough to explain the poem: ‘Well, “slithy” means“lithe and slimy [...]”’ (Carroll 1897) and so on. As we will see in the remainder of this section, thisliterary example is particularly illustrative of the problem with neologism studies.Although they provide valuable insight into how language users interact with new words,neologism studies do not by themselves constitute conclusive evidence for the psychological statusof phonaesthemes. Even in light of the neologism results from the three phonaestheme works cited 12above, one could still hold the position that phonaesthemes are only static, distributional facts aboutthe lexicon, which speakers of a language can access consciously. This is problematic sinceessentially all normal morphological processing happens unconsciously. In other words, we knowthat language-users are able to access all sorts of facts about their language upon reflection. Peoplecan come up with a word of their language that is spelled with all five vowel letters and ‘y’ in order,or a word that has three sets of double letters in a row. (See below for the answers.3) These abilitiesby themselves do not lead us to conclude, though, that orthographic order of vowel letters in a wordis a fundamental principle of implicit cognitive organization. For the same reason, subjects’ ability toconsciously access distributions of sound-meaning pairings in their language does not imply thatthose pairings are meaningful for the subjects’ linguistic system.In fact, different mechanisms could be evoked to account for the cognitive processing that theneologism studies described above demonstrate.a. Subjects could be taking a sample of the lexicon, and observing generalizations over a set ofwords that share structure with the neologisms.b. They might be randomly selecting nearest neighbors.c. They could be looking for cues in the stimulus itself; notice that the word ‘scrape’ appears inthe example for sk- provided above, and the work ‘slimy’ appears in Humpty Dumpty’sexplanation for ‘slithy’..d. Finally, subjects just might be unconsciously acting in accordance with the structuralconfiguration of their language system, which is structured in part on the basis ofphonaesthemes.Unfortunately, we cannot distinguish between any of these solutions on the basis of neologism taskslike the ones described above. 13What is needed to resolve this issue, then, is a way to tap into unconscious language processingof morphemes. Applying such a measure to phonaesthemes, we can evaluate the extent to whichtheir behavior in language processing is like or unlike concatenative, productive morphological units.To reiterate, the stakes are high. If phonaesthemes have a demonstrable psychological reality, thenthe notion of morphemes as concatenative, productive units is insufficient for defining the humancapacity to extract and use generalizations about the internal structure of words.2. METHOD. From the perspective of their distribution, phonaesthemes are statistically wellrepresented partial form-meaning pairings. In order to test whether phonaesthemes also have apsychological reality - whether they are internalized and used by speaker-hearers - we will have toanswer two related questions. First, does the presence of a phonaestheme in a word affect theprocessing of that word? If phonaesthemes play a role in language processing, then words theyoccur in should be processed differently from words that do not. Second, if phonaesthemes do infact affect lexical processing, is this effect significantly different for phonaesthemes than forsubparts of words that correlate some form and some meaning, but not in a statistically significantway in the lexicon? In other words, is there a role for the frequency of a form-meaning pairings indetermining processing effects?The morphological priming methodology (first used by Kempley and Morton 1982, summarizedby Drews 1996) provides an excellent starting point for developing a methodology for addressingsuch empirical questions. Morphological priming is the facilitation (speeding up) or inhibition(slowing down) of mental access to a TARGET word on the basis of some other PRIME word,which has been presented previously. A large number of morphological priming studies have foundpriming patterns that are unique to morphologically related PRIME and TARGET words, and are 14not shared by morphologically unrelated words (see the summary in Feldman 1995). On the basis ofthis difference between morphological priming effects on the one hand and known phonologicaland semantic priming effects on the other, it has been argued that morphemes have a psychologicalreality. In the same way, we can hypothesize that if phonaesthemes have some cognitive status, thenthey should similarly display priming effects that are different from those exhibited by words that donot share phonaesthemes. The study described below aimed to test this possibility.In this study, 20 native speakers of English, aged 18 to 36, were first presented with a PRIMEstimulus, which was an orthographic representation of a word, on the screen of a digital computer.Following the setup of Feldman and Soltano (1999), it appeared slightly above the center of the screen for 150msec, just enough to be barely perceived by the subject. 300msec later, a secondstimulus, the TARGET stimulus, also a typewritten word, was presented in the center of the screenfor 1000msec or until the subject responded to it. This inter-stimulus latency of 300msec was chosenbecause that delay has been reported to most effectively separate morphological from non-morphological priming effects (Feldman and Soltano 1999). At this inter-stimulus interval, there isthe least phonological or semantic priming relative to morphological priming. Subjects performed alexical decision task; they were asked to decide as quickly as possible whether the TARGET was aword of English or not. They were to indicate their decision by pressing one of two keys on thecomputer keyboard - ‘z’ for non-words and ‘m’ for words.The critical stimuli were 50 pairs of words, falling into five categories (Table 2). In the firstcondition, the PRIME and TARGET shared some phonological feature, always a complex onset,and some semantic feature, such as ‘NOSE’ or ‘LIGHT’. Additionally, a significant proportion ofthe words in English sharing that onset also shared the semantics, giving pairs like glitter and glow. Inthe second condition, the PRIME and TARGET shared only a phonological feature, always anonset, yielding stimuli such as druid and drip. PRIMEs and TARGETs in the third condition shared  15only some semantics, like cord and rope. In the fourth condition, the pseudo-phonaestheme case,stimuli shared phonology and semantics but were part of a very small number of words occurring inthe Brown corpus (usually only two) that did so. An example of such a pair is the nouns crony andcrook. Finally, for comparison, stimuli in the fifth, baseline condition did not share any segments intheir onset, or a meaning, giving pairs like frill and barn. The full list of stimuli presented can be seenin Appendix 2.Since the subjects were performing a lexical decision task, choosing whether a string ofcharacters was a word of English or not, an equal number of non-words was required as well. Anadditional 50 nonce words were thus created, by manipulating one or two characters in each of theexisting TARGET words (also seen in Appendix 2). For example, the real TARGET word glowbecame glone and crook became croof.  Crucially, all of these non-words shared their onset (or in thecase of the one vowel-initial word, the initial vowel) with the real TARGET word they weremanufactured from. The nonce TARGET words were paired with the PRIME words that the wordsthat were manipulated to form them were paired with. This ensured that subjects could not simplyrely on a shared onset between PRIME and TARGET, or indeed a given onset on a TARGET word(such as sn- or gl-) as a cue that the TARGET was a word or not. Each subject saw half of thePRIME words in each condition followed by a real word TARGET and the other half followed by amanipulated non-word TARGET.INSERT TABLE 2 ABOUT HERE Translated into specific hypotheses about the result of this study, the questions at the top of thissection appear as follows. First, if the presence of phonaesthemes affects processing, then responsesto condition 1 (the phonaestheme case) should be significantly different from those to condition 5 16(the baseline, unrelated condition). To be certain that this effect does not derive from knownpriming effects resulting from phonological (Zwitserlood 1996) and semantic relatedness (Thompson-Schill et al. 1998) between PRIME and TARGET, the responses to class 1 must also bedistinct from those to classes 2 and 3. Second, if this priming effect is restricted to statistically wellrepresented form-meaning pairs, then responses to condition 1 (phonaesthemes) should besignificantly different from those to condition 4 (pseudo-phonaesthemes).Before assessing the results of the experiment, we need to briefly discuss some controls appliedto it. The morphological priming methodology assumes that it takes subjects the same length of timeto make lexical decisions for the TARGETs in each of the different conditions. If a set of TARGET words in one condition requires a longer lexical decision than those in another condition, then anydifferences in response time across conditions could be a result of this difference, rather thandifferent priming effects. In fact, in most experiments using this methodology, the same TARGETs are used across conditions, with only the PRIMES varying. The current experimental design requiresdifferent TARGET words in the different conditions, as there are very few words (if any) thatsimultaneously instantiate phonaesthemes and pseudo-phonaesthemes. Two important factors thatinfluence the rate of decision to a given TARGET word must therefore be controlled for - lengthand frequency.The TARGET words in all conditions were matched as closely as possible for average tokenfrequency and length, since these factors have demonstrable effects on morphological priming(Meunier and Segui 1999). As seen in Table 3, it was possible to find stimuli whose lengths were, onaverage, very similar - no conditions differ by as much as a phone or letter. Frequency is much moredifficult to assess, since the type of corpus one selects for frequency measures radically influencesthe rates of occurrence of the words contained in it. Table 3 shows two standard written frequencymeasures, Thorndike-Lorge and Kucera-Francis, both taken from the MRC Psycholinguistic  17database (http://www.psy.uwa.edu.au/mrcdatabase/uwa_mrc.htm). While there is good consistencyacross most of the conditions for each of these measures it's clear that according to Thorndike- Lorge, TARGET words in the baseline and pseudo-phonaestheme condition are the least frequent,while the Kucera-Francis numbers place the pseudo-phonaestheme words as least frequent.INSERT TABLE 3 ABOUT HERE These frequency differences should be a matter of concern, since our main interest is indetermining whether different types of relation between a PRIME and TARGET yield differentdegrees of priming. It might take longer to make a lexical decision about less frequent TARGETwords, such as those in the pseudo-phonaestheme condition, regardless of what precedes them, inwhich case it would be impossible to determine whether any differences among the conditionsresulted from effects of the PRIME stimuli or simply from different default rates of response to theTARGET words.Fortunately, there exist measures of lexical decision times for words in isolation, which will helpus determine whether the frequency differences describes above should be of concern. Table 4shows the mean response time in a lexical decision task to the TARGET words in each of the fiveconditions, according to the English Lexicon Project (Balota et al. 2002). As we can see, thephonaestheme, form, and meaning conditions are quite similar, while TARGET words in thepseudo-phonaestheme condition are responded to more quickly and those in the baseline conditionmore slowly. We will take these differences into consideration when analyzing the results from thisexperiment, in the next section.INSERT TABLE 4 ABOUT HERE 183. RESULTS. The average reaction times per condition fell out as shown in the first row of Table5. By far, the fastest average reaction time was to the phonaestheme condition, which was 59 msecfaster than the baseline (unrelated) condition. That is to say, subjects responded to a TARGETstimulus 59 msec faster when the PRIME and TARGET shared a phonaestheme, relative to thecondition where PRIME and TARGET were semantically and phonologically unrelated. Bycomparison, PRIME and TARGET pairs that shared meaning but no form were identified only 23msec faster than the baseline condition. By contrast, words sharing an onset with their PRIME,those in the form condition, took slightly longer to decide on than did those in the baselinecondition. In other words, phonaesthemically related words led to much faster recognition of theTARGET than did semantically or phonologically related words.INSERT TABLE 5 ABOUT HERE Moreover, shared form and meaning together were not sufficient to explain the phonaesthemic priming. Recall that the phonaestheme condition elicited reaction times 59 msec faster than thebaseline. By comparison, the pseudo-phonaestheme condition, containing words that shared formand meaning but were among very few words in the lexicon sharing both, yielded reactions only 7msec faster than the baseline. This difference indicates that the extent of the form-meaning pairing’sdistribution in the language is crucial to its role in processing. Statistically prevalent form-meaningpairings yield faster responses than do similar words that are statistical loners. The significance ofthe effect of condition on reaction time was evaluated using a one-way ANOVA subject analysis.The effect was significant, with p < 0.05. The differences between the phonaestheme condition and 19each other condition were also evaluated using Fisher's PLSD and Sheffe's, which both confirmedthat the phonaestheme condition was significantly different (p < 0.5) from all other conditions.While these results very clearly indicate that TARGETs are responded to much more quickly when they share a phonaestheme with their PRIME than when they share form, meaning, both, ornothing with it. But as mentioned in the previous section, there remains the possible confound ofhow long it takes to make a lexical decision on the TARGETs in the different conditions in isolation. If the reaction time differences we observed in the first row of Table 5 are due to solely todifferent default rates to decision for the TARGETs in the different conditions, then there shouldbe a strong correlation between the default response time and the primed response time. As mightbe hypothesized from a scan of the data in Table 5, there is no such statistically significantcorrelation. (The correlation coefficient of the two rows is 0.2, which falls well below the requisite0.9 required for significance with five observations.)Giving these data a more detailed analysis, we might be interested in investigating whetherdifferences in the reaction time between the phonaestheme condition and any others are in fact dueto absolute differences in response time. One way to assess this is to compare the average differencebetween reaction time in isolation and in the priming experiment for each condition. If thephonaestheme condition is responded to faster under priming than in isolation, while the otherconditions are not, then this will be yet another indication that it is the phonaesthemic relation, rather than independent response time, that is responsible for the fast decisions in thephonaesthemic condition reported above. This is almost precisely what we find - as seen in the lastrow of Table 5, TARGETs in the phonaestheme are responded to much faster (38.3 msec faster) inthe current priming experiment than in isolation, while the form, meaning, and psudo-phonaestheme TARGETs require slower lexical decision responses in the priming experiment thanin isolation. The baseline condition showed a slightly faster response in the priming experiment than 20in isolation. Since the actual conditions of the current experiment and those that yielded the meanisolated lexical decision times differ, we cannot quantitatively compare their results. Nevertheless,they support the conclusion that even when the response times to these TARGET words inisolation are taken into account, the faster response times in the phonaestheme condition do notresult from shared form or meaning, or both.One final confound should be also addressed - the possibility that the stimuli in thephonaestheme condition are more closely related in terms of meaning than those in the otherconditions, and that this is the cause for faster response time to words in this condition. While it isdifficult to establish reliable and useful semantic similarity ratings, it is clearly useful to try to applysome quantitative analysis to the problem. One useful semantic similarity metric is a similarity ratingproduced by Latent Semantic Analysis (LSA - Landauer et al. 1998). LSA is, among other things, is a statistical method for extracting and representing the similarity between words or texts on the basisof the contexts they do and do not appear in. Two words or texts will be rated as more similar themore alike their distributions are. LSA has been shown to perform quite like humans in a range ofbehaviors, including synonym and multiple-choice tasks. Of relevance to the current discussion isthe pairwise comparison function, which produces a similarity rating from -1 to 1 for any pair oftexts. Identical texts have a rating of 1, while completely dissimilar ones would have a rating of -1.To give examples from the pairs used in the phonaestheme experiment, glitter and glow have an LSAvalue of 0.27 (somewhat similar) while barn and frill have an LSA of 0 (no particular similarity ordissimilarity).The mean LSA scores for each condition were as seen in Table 6. While the LSA values forword pairs in the form and baseline conditions were close to 0, those of the pseudo-phonaestheme,phonaestheme, and meaning conditions were higher. The significance of differences among theseconditions were evaluated using an ANOVA, which took the semantic relatedness of each PRIME- 21TARGET pair, measured as an LSA value, as the dependent variable and condition (phonaestheme,form, meaning, etc.) as the independent variable. According to this analysis, the phonaesthemecondition was not significantly different from the pseudo-phonaestheme condition (p=0.16), orfrom the meaning condition (p=0.65) but was significantly different from the form and baselineconditions (p=0.01 for each). The pseudo-phonaestheme was however distinguishable from themeaning condition (p<0.05) but not from the baseline (p=0.18) or the form condition (p=0.16). Inother words, the pairs of words in the phonaestheme condition were just as similar in terms ofmeaning as were the words in the meaning-only and pseudo-phonaestheme conditions, while wordpairs in the form-only and baseline conditions were significantly different.INSERT TABLE 6 ABOUT HERE The current experiment has demonstrated phonaesthemically related facilitory priming byPRIMEs that are both semantically and phonaesthemically related to a TARGET that cannot beaccounted for simply on the basis of other sorts of priming. The meaning priming and form primingboth yield quantifiably different priming effects from phonaesthemic priming. As a potentialcomplication, it has been proposed in the morphological priming literature that since stimuli thatshare a morpheme share both form and meaning, the sum of the priming effects from each of thesedomains might give rise to the actual priming observed with phonesthemes (see Feldman and Soltano 1999 for discussion). However, taken together, form and meaning priming are still distinctfrom phonaestheme priming no matter how they are measured. If the difference between thebaseline reaction time and the reaction time in each other condition is taken as the priming effect forthat condition, then the sum of the meaning and form priming effects (20mesc) differs a great dealfrom the phonaesthemic priming effect (59mesc). If, however, it is the difference between the 22reaction time to a condition in isolation and the time to respond in the priming context that is used,then the difference is even greater - the form and meaning priming effects would sum to a delay of -25 msec, while the phonaesthemic priming effect would be an increase in speed at 38 msec. Noother metrics aside from summation have been proposed in the literature to deal with combinedphonological and semantic effects. Phonaesthemic priming is an entity unto itself, resulting from the statistical over-representation of a particular pairing between form and meaning in the lexicon. These results disconfirm the possibility that language-users access phonaesthemic knowledgeonly during tasks allowing reflection, such as the neologism tasks described in Section 1.3., above.Instead, phonaestheme effects emerge even while individuals are performing a task that is tightlyconstrained by time pressure, and is therefore processed unconsciously, like natural language is.4. DISCUSSION. The experimental evidence described above argues for the psychological realityof phonaesthemic priming effects. These effects bear implications for questions as fundamental asthe definition of the morpheme and the role of frequency in language learning, representation, andchange.4.1. PHONAESTHEME AND MORPHEME PRIMING . Despite the fact that they are mostly non-compositional, phonaesthemes behave just like central morphological units in terms of twodefinitional criteria. First, unlike the most frequent and productive affixes, but like other semi-productive units like English en- (Baayen and Lieber 1991), phonaesthemes are partially productive.Evidence of this productivity comes from the use of phonaesthemes in interactions withneologisms, as shown in Hutchins (1998) and Magnus (2000). Second, as we will now see,phonaesthemes mirror the priming behavior of canonical morphemes. 23In general, morphologically related forms demonstrate facilitory priming that differs from bothsemantic priming and phonological priming in terms of its degree and time course. Consider theresults of an experiment described by Feldman and Soltano (1999), which was nearly identical informat to the one described above. The key difference between the two experiments was that inFeldman and Soltano, the test stimuli were morphologically related, instead of being phonaesthemically related. Unlike the phonaestheme stimuli, the morphologically related forms inFeldman and Soltano’s experiment shared a productive suffix, like the regular past tense. Table 7 lays out the priming effects from Feldman and Soltano’s experiment in comparison with those fromthe phonaestheme experiment (measured as the difference between the mean reaction time of thebaseline condition and that of each of the other conditions).INSERT TABLE 7 ABOUT HERE We cannot compare the measures of priming in these two experiments quantitatively, becausethe test conditions differed slightly. We can nevertheless draw a valuable conclusion from theirjuxtaposition. The phonaesthemic priming detected in the experiment described above is qualitatively similar to evidence for morphological priming, like the morphological priming reportedby Feldman and Soltano. In both cases, at the same inter-stimulus interval, the morphologically orphonaesthemically related condition yielded significantly greater facilitory priming than form-onlypriming, meaning-only priming, or the sum of form and meaning priming.This similarity between phonaesthemic priming and morphological priming, along with thepartial productivity of phonaesthemes, suggests that phonaesthemes have a status in the mentallanguage processing system that is similar to that of canonical morphemes. Observations of theimportance of phonaesthemes as organizing units has led some, such as Blust (1988), to suggest that 24phonaesthemes may be represented as units, below the level of the morpheme, but above the levelof the phoneme. While such a solution is enticing since it could preserve the compositionality of morphemes, it nevertheless spawns as many important difficulties. For one, phonaesthemes are notcompositional, which leaves open the question of how the remainder of a morpheme is representedat the phonaesthemic level. Second, instantiating a phonaesthemic level provides no a prioriexplanation for why frequent sound-meaning pairings would give rise to morpheme-like and notphoneme-like priming effects. Third, and perhaps most importantly, such a solution does notprovide any explanation for the importance of frequency effects at the phonaesthemic level.Phonaesthemes, as we will see below, are just the tip of the frequency iceberg.4.2. OTHER STUDIES OF STATISTICAL SOUND -MEANING PAIRINGS. It is not only phonaesthemes,but virtually any form-meaning pairing with sufficient statistical reliability that can play ademonstrable role in the mental processing of language. The range of statistical form-meaningpairings that have been shown to play a role in processing (of which only a selection are surveyedbelow) testifies to the industriousness of the human capacity to encode statistical form-meaningcorrelations within words. Research on a number of seemingly unrelated topics has indicated thatlanguage users integrate and make use of statistical correlations between sound and meaning, evenwhen these relations do not play a productive role in the linguistic system.Language users internalize subtle correlations between a person’s gender and the phonology oftheir name. Cassidy et al. (1999) documented these correlations, finding that while male names likeRichard and Arthur tend to have trochaic stress, female names like Irene and Michelle tend to haveiambic stress. Additionally, male names like Bob and Ted tend to end in consonants, while femalenames like Sue and Mary tend to end in vowels. The researchers then presented adults and four-year-old children with words that bore statistically prevalent characteristics of female or male first names, 25asking them to perform neologism and sentence completion tasks. Cassidy and her colleagues foundthat subjects tended to process the names along the lines predicted by the phonological cues togender.In another study, Kelly et al. (1990) found that adults and children have internalized and makeuse of the correlation in English between the number of syllables in a word and the complexity ofthe object it describes. Geometric shapes are particularly exemplary of this phenomenon; considerthe tendency expressed by the visual complexity and syllabicity of line, square, and point with those oftrapezoid, hexagon, and parabola. When presented with novel shapes, subjects were more likely to paircomplex shapes with polysyllabic words than they were simpler shapes.Finally, a segment’s morphological expressiveness correlates probabilistically with the extent towhich speakers are willing to reduce that segment. English has a word-final deletion process, whichelides the final coronal stops of many words in most dialects. It has consistently been shown thatindividuals reduce the final t or d of a word more, the more frequent the word in question is (Labov1972, Guy 1980, Bybee 2000). But in addition to this effect, if a final coronal stop has some morphological status – if, for example, it indicates the past tense such as in the words tossed andbuzzed – then it is less likely to be reduced. Just like probabilistic effects from gender and visualcomplexity, this meaning-bearingness effect provides external support that language users internalizestatistical form-meaning correlations and make use of them during language processing.4.3. USAGE-BASED MODELS. The key property that phonaesthemes and the three other casessummarized in the previous sub-section share is that their only manifestation is as recurrent pairingsof formal and semantic properties of words. For phonaesthemes, this recurrence is sufficient toallow them to play a role in unconscious language processing, as well as, possibly, in the inventionand interpretation of neologisms. While compositional morphologies have very little to say about the 26mechanisms by which phonaesthemes might be learned and represented, usage-based views ofmorphology do. Network models (e.g. Bybee 1985, Langacker 1991), and connectionist models (e.g. Plaut and Gonnerman 2000) both predict that statistical recurrences across words, likephonaesthemes, will automatically rise to the status of organizing structures in a language.Acquiring phonaesthemes, like all the other form-meaning pairings described above, isaccounted for in network models as the emergent product of implicit abstraction over specificlexical representations. For example, in Bybee’s (1985) network morphology, phonological and semantic similarities across words give rise to connections between them. This might be representedas in F IGURE 1, where we can see that connections between words (depicted as solid lines) emergewhere form elements or meaning elements are shared across words. Where form and meaning areshared across a set of words, a morphological relation emerges; that is, a generalization appearswhere previously there were only memorized word forms. This emergent generalization is usuallyreferred to as a schema (Bybee and Slobin 1982, etc.). Morphological schemata, as simple generalizations over individual tokens, are based not on compositionality or productivity, but ratheron the simple recurrence in the language of a form meaning association. Langacker’s (1991) modelworks similarly, generalizing over individual examples such that more abstract schemas emerge. Inboth, the only difference between compositional and non-compositional morphological units iswhether or not the remainder of the words in which they occur also instantiates a more generalschema.INSERT FIGURE 1 ABOUT HERE Distributed connectionist models of morphology share the premise that morphological relationsare emergent from semantic and phonological generalizations over stored lexical forms. They differ 27from network models like Bybee’s, however, in two main ways. First, they are quantitative models,which, when implemented in a computational environment, facilitates their evaluation. Second,rather than a unique representation for each word, they posit a network of phonological andsemantic units, where a pattern of activation over those units represents a word. In therepresentative miniature connectionist model in Figure 2, a tendency for form units g and l to co-occur with the meaning unit LIGHT is quantitatively encoded in the strength of connections amongthe three units. Activation of g and l will increase the likelihood that LIGHT will also become active,due to emergent strengthening of connections between these nodes and the network’s internalrepresentation in its hidden layer.INSERT FIGURE 2 ABOUT HERE In all usage-based models, the strength of an emergent schema, which allows it to play a role inunconscious and conscious language tasks, will depend at least on its frequency of occurrence in thewords of a language and the frequency of those words that exemplify it. Schema strength does notdepend on the compositionality of the form-meaning pairing. Thus, usage-based morphologiesprovide a natural explanation for the behavior of phonaesthemes we have seen above.It is important to note that a certain class of morphological theories are constructed from twoindependent morphologies – an Assembly Line model and a usage-based model. These "DualRoute" models (introduced by Pinker and Prince 1988) recognize that not all morphology iscompositional, and view the morphological capacity as composed of a regular, compositionalcomponent, and an irregular, lexical component. Phonaesthemes would in such a model be placed inthis second group. But separating morphological knowledge into two separate modules raises a hostof problems, particularly when phonaesthemes are on the table. From the discussion above, we can 28see that the only demonstrated difference between phonaesthemes and other morphological units istheir compositionality. They can all be more or less productive, can be used to invent and interpret neologisms, and demonstrate priming effects that are distinct from phonological and semanticpriming effects. Thus, a Dual Route model would need to explain why all these properties are shared by compositional and non-compositional morphological units, which are processed in two separateroutes. This is a less parsimonious solution than those, like usage-based models, which use a singlemechanism for both types of morphological unit.4.4. FUTURE DIRECTIONS. The results of the present work raise the question of the preciserelationship between morphological and phonaesthemic priming. Only by combining these twotypes of test stimuli in a single experiment can answer this question. Also of future interest is thespecific role that statistical frequency plays in phonaesthemic priming. Meunier and Segui (1999) have shown that frequency plays a role in morphological priming, but we don’t yet know what theexact character of the effect of frequency in phonaesthemic priming is. Is being a phonaestheme anall-or-none enterprise, or does frequency correlate gradedly with degree of phonaesthemic priming? The present study did not include frequency as a graded parameter, but rather as a discrete one;statistically significant phonaesthemes were assigned to the same condition, regardless of howsignificant they were. Another factor that might play a role in phonaesthemic priming is the degreeof semantic overlap between PRIME and TARGET. This factor has been shown to play a role inthe degree of morphological priming between stimuli, and might also do so for phonaesthemes(Laura Gonnerman, p.c.). In addressing the psychological reality of phonaesthemes, we have addressed only the degree towhich phonaesthemic knowledge has been internalized by language users. Other questions aboutphonaesthemes deserve attention, though. For example, the study described above assumes that the 29potential universality of or motivation for phonaesthemes is orthogonal to their representation. Incontrast with this approach, phonaesthemes have predominantly been investigated not in terms oftheir representation, but in terms of their motivation – that is, with respect to their sound-symbolicproperties. Various studies have attempted to document cross-linguistic phonaesthemes for thepurpose of demonstrating that there is some correlation between the sound structure of words thathave phonaesthemes and the meanings they bear. Famous cases include the often reported use ofhigh vowels to express small size and low vowels to indicate largeness, as in English teeny and tinyversus humungous and large (e.g. Ohala 1984). The results of these studies are frequently interpreted asdisproving the arbitrariness of the sign, e.g. Bolinger (1949). Both the results and this conclusion,though, are hotly contested. While the motivation describes ultimate historical or acquisitionalcauses for the phonaestheme’s existence, there may in fact be some relationship between the semantic motivation for a phonaestheme and its distribution. More semantically unified or moresemantically ‘basic’ meanings, however basicness is measured, might be more prevalent. One way totest these hypotheses would be to repeat the experiment presented above with phonaesthemestimuli, which varied in terms of their proposed universality.And finally, if statistical prevalence of a sound-meaning pairing in a language results in ademonstrable representation of that pairing in the cognitive system, which itself may give rise to theability of individuals to invent new words based on that pairing, and to interpret others’ neologismsmaking use of that pairing, then, we would expect statistical distribution of a phonaestheme to affectits extension in a language over time. Phonaesthemes that are well represented should yield newforms over time for which there is no cognate in closely related languages. One such case, discussedby Hock and Joseph (1996) (from Samuels 1972), concerns the rime of words like drag, flag, lag, andsag, which all share the meaning of ‘slow, tiring, tedious motion’. According to Hock and Joseph,this cluster gained a new member in the 16th century when English sacke became sag through an 30irregular sound change. Elsewhere, Blust (1988) reports on the historical developments ofAustronesian roots, which are like English phonaesthemes, except that they occur word-finally andcan encompass as much as an entire syllable. Blust has found numerous cases of cognateless formsin various Austronesian languages which it seems is best explained as a case of the richer rootsgetting richer, or as Blust (2003) calls it, a "snowballing effect". In the future, we hope to investigatehow much of a role the precise distributional statistics of phonaesthemes influences their extensionover time.5. CONCLUSION. Like other non-categorical pairings between phonology and semantics,phonaesthemes have a significant psychological status. Specifically, when a form-meaning pairingrecurs with sufficient statistical weight, it comes to take on priming behavior that cannot beexplained as the result of form or meaning priming, alone or in combination. Assembly line and dualroute models of morphology are unable to account for the similarity between phonaesthemicpriming and morphological priming, and the experimental results have been interpreted assupporting usage-based models. Phonaesthemes are a testament to the diligence of the human abilityto encode and use subtle statistical associations in the linguistic environment. 31REFERENCESAbelin, Asa. 1999. Studies in Sounds Symbolism. Goteborg, Sweden: Goteborg UniversityDissertation..Baayen, H. and R. Lieber. 1991. Productivity and English derivation: a corpus based study.Linguistics 29.801-43.Balota, D.A., Cortese, M.J., Hutchison, K.A., Neely, J.H., Nelson, D., Simpson, G.B., Treiman, R.(2002). The English Lexicon Project: A web-based repository of descriptive and behavioralmeasures for 40,481 English words and nonwords. http://elexicon.wustl.edu/, Washington University.Barlow, Michael and Suzanne Kemmer (eds.). 2000. Usage-Based Models of Language. Stanford:CSLI.Berko, Jean. 1958.  The child’s acquisition of English morphology,  Word 14.150-77.Blust, Robert. 1988. Austronesian root theory: an essay on the limits of morphology.  Philadelphia: J.Benjamins.Blust, Robert. 2003. The phonestheme N in Austronesian languages, Oceanic Linguistics 42.1.Bolinger, Dwight. 1949. The Sign Is Not Arbitrary. Boletín del Instituto Caro y Cuervo, 5.52-62. Bolinger, Dwight. 1980. Language; The Loaded Weapon: The Use and Abuse of Language Today.New York: Longman.Bybee, Joan. 1985. Morphology: A study of the relation between meaning and form. Amsterdam:John Benjamins.Bybee, Joan. 2000. The Phonology of the Lexicon: Evidence from Lexical Diffusion. Usage-BasedModels of Language, ed by Barlow, Michael and Suzanne Kemmer, 65-85. Stanford: CSLI.  32Bybee, Joan and Joanne Scheibman. 1999. The effect of usage on degrees of constituency thereduction of don't in English. Linguistics 37-4.575-596.Bybee, Joan and Dan Slobin. 1982. Rules and Schemas in the development and use of the Englishpast tense. Language 58.265-289.Carroll, Lewis. 1897. Alice’s adventures in wonderland, and Through the looking-glass. New York,Macmillan.Cassidy, Kimberly, Michael Kelly, and Lee-at Sharoni. 1999. Inferring gender from name phonology.Journal of Experimental Psychology: General 128(3).362-381.Chomsky, Noam. 1995. The Minimalist Program. Cambridge: MIT Press.Cowan, William. (ed.) 1981. Papers of the Twelfth Algonquian Conference, Ottawa: CarletonUniversity Press.Di Sciullo, Anna-Maria, and Edwin Williams. 1987. On the Definition of Word. Cambridge: MIT Press.Drews, Etta. 1996. Morphological priming. Language and Cognitive Processes 11(6).629-634Feldman, Laurie. 1995. Morphological aspects of language processing. Hilldale, NJ: Erlbaum.Feldman, Laurie and Emily Soltano. 1999. Morphological Priming: The Role of Prime Duration,Semantic Transparency, and Affix Position. Brain and Language 68.33-39.Firth, J. R., 1930, Speech. London: Oxford University Press.Francis, W. Nelson and Henry Kucera. 1982. Frequency analysis of English usage: lexicon and grammar. Boston: Houghton Mifflin.Frisch, Stephen and Bushra Zawaydeh. 2001. The psychological reality of OCP-Place in Arabic.Language 77.Guy, Gregory. 1980. Variation in the group and the individual: The case of final stop deletion.Locating language in time and space, ed. by William Labov, 1-36. New York: Academic Press. 33Hale, Ken. and Sylvain Bromberger. (eds.) 1993. The View from Building 20. Cambridge, MA: MITPress.Halle, Morris and Alec Marantz. 1993. Distributed Morphology and the Pieces of Inflection. The View from Building 20, ed. by Ken Hale and Sylvain Bromberger, 111-176. Cambridge, MA:MIT Press.Hamano, Shoko. 1998. The Sound-Symbolic System of Japanese. Cambridge University Press. Hinton, Leanne, Johanna Nicols and John Ohala. (eds.) 1994. Sound Symbolism, Cambridge University Press.Hock, Hans Henrich and Brian D. Joseph. 1996. Language history, language change, and languagerelationship. An introduction to historical and comparative linguistics. Berlin: Mouton deGruyter.Hockett, Charles F. 1954. Two Models of Grammatical Description. Word 10.210-231Hutchins, Sharon Suzanne. 1998. The Psychological Reality, Variability, and Compositionality ofEnglish Phonesthemes. Emory University Dissertation. Jakobson, Roman and Linda Waugh. 1979. The Sound Shape of Language. Bloomington: IndianaUniversity Press.Joseph, Brian. 1994. Modern Greek ts beyond sound symbolism, in Leanne Hinton, Johanna Nicolsand John J. Ohala, (Eds.), Sound Symbolism, Cambridge University Press.Kelly, Michael. 1992. Using sound to solve syntactic problems: The role of phonology ingrammatical category assignments. Psychological Review 99(2).349-364. Kelly, Michael, Ken Springer, and Frank Keil. 1990. The relation between syllable number and visual complexity in the acquisition of word meanings. Memory and Cognition 18(5).528-536. Kempley, S. and J. Morton. 1982. The Effects of Priming with Regularly and Irregularly RelatedWords in Auditory Word Recognition. British Journal of Psychology 73.441-445.  34Labov, William. 1972. Sociolinguistic Patterns. Philadelphia: University of Pennsylvania Press.Labov, William (Ed.). 1980. Locating language in time and space. New York: Academic Press.Landauer, T., Foltz, P., & Laham, D. 1998. Introduction to Latent Semantic Analysis. Discourse Processes, 25, 259-284.Langacker, Ronald. 1991. Concept, Image, and Symbol: The Cognitive Basis of Grammar. NewYork: Mouton De Gruyter. Lieber, Rochelle. 1980. On the Organization of the Lexicon. Ph.D. dissertation, MIT; published1981 by the Indiana University Linguistics Club.Magnus, Margaret. 2000. What’s in a Word? Evidence for Phonosemantics. Ph.D. Dissertation, University of Trondheim, Norway.Marchand, Hans. 1959. Phonetic symbolism in English word formations. IndogermanischeForschungen 64: 146-168.McCune, Keith. 1983. The Internal Structure of Indonesian Roots, Ph.D. dissertation, University ofMichigan.Meunier, Fanny and Juan Segui. 1999. Morphological Priming Effect: The Role of SurfaceFrequency. Brain and Language 68: 54-60.Ohala, John. 1984. An ethological perspective on common cross-language utilization of F0 of voice.Phonetica 41:1-16. Pinker, Steven and Alan Prince. (1988) On language and connectionism: Analysis of a paralleldistributed processing model of language acquisition. Cognition, 28, 73-193.Plaut, David and Laura Gonnerman. (2000). Are non-semantic morphological effects incompatiblewith a distributed connectionist approach to lexical processing? Language and CognitiveProcesses, 15, 445-485. 35Rhodes, Richard. 1981. On the Semantics of Ojibwa Verbs of Breaking, in W. Cowan, (Ed.) Papersof the Twelfth Algonquian Conference, Ottawa: Carleton University Press.Rhodes, Richard. 1994. Aural Images, in Leanne Hinton, Johanna Nicols and John J. Ohala, (Eds.), Sound Symbolism, Cambridge University Press.Rhodes, Richard, and John Lawler. 1981. Athematic Metaphors, in Roberta A. Hendrick, Carrie S.Masek, and Mary Frances Miller, (Eds.) Papers from the Seventeenth Annual Regional Meetingof the Chicago Linguistic Society. Chicago: Chicago Linguistic Society, 318-42.Samuels, Michael. 1972. Linguistic evolution; with special reference to English. Cambridge:Cambridge University Press.Sereno, Joan. 1994. Phonsyntactics. In L. Hinton, J. Nichols, and J. Ohala (Eds.), Sound Symbolism.Cambridge: Cambridge University Press, 263-275.Singh, Rajendra and Stanley Starosta To appear. Explorations In Seamless Morphology.  New Delhi, London, and Thousand Oaks: Sage Publications.Starosta, Stanley.  To appear. Do compounds have internal structure?  A seamless analysis.  In Rajendra Singh and Stanley Starosta, Explorations In Seamless Morphology.  New Delhi,London, and Thousand Oaks: Sage Publications.Thompson-Schill, Sharon L., Kenneth Kurtz, and John Gabrieli. 1998. Effects of Semantic and Associative Relatedness on Automatic Priming. Journal of Memory and Language 38: 440-458.Wallis, John. 1699. Grammar of the English Language (Fifth Edition). Oxford: L. Lichfield.Zimmer, Karl. 1969.  Psychological Correlates of Some Turkish Morpheme Structure Conditions.Language 45: 309-321.Zwitserlood, Pienie. 1996. Form Priming. Language and Cognitive Processes 11(6), 589-596. 36gl-sn-sm-fl- TypesTokensTypesTokensTypesTokensTypesTokens LIGHT /VISION38.7%(48/124)59.8%(363/607)1.1%(1/8)0.3%(1/31)0%(0/7)0%(0/1042)10.9%(28/256)10.1%(146/1441)NOSE /MOUTH4%(5/12)1.4%(9/607)28.4%(25/88)19.0%(59/311)25.3%(19/75)27.3%(284/1042)5.6%(15/256)2.7%(39/1441)TABLE 1.Phonaesthemes gl-, sn-, sm-, and fl- in the Brown Corpus 37ConditionCharacteristicsExamples PhonaesthemePRIME and TARGET shared a semantic feature and aphonological  onset and were a statistically significantsubclass of the lexicon.glitter:glowFormPRIME and TARGET shared an onset. druid:dripMeaningPRIME and TARGET shared a semantic feature. cord:ropePseudo-PhonaesthemePRIME and TARGET shared a semantic feature and anonset but were not a statistically significant subclass of thelexiconcrony:crookBaselinePRIME and TARGET were unrelated in form andmeaningfrill:barnTABLE 2.Five test conditions with examples 38Thorndike-LorgeKucera-FrancisLettersPhonesSyllables Phonaestheme181.415.755.241 Form136.5612.754.51.1 Meaning160.513.14.74.11.4 Pseudophonaestheme111.55.55.14.51.1 Baseline96.312.224.83.81.2 TABLE 3.For each condition, mean frequency (written frequency in Thorndike-Lorge and Kucera andFrancis), and length (in number of letter, number of phones, and number of syllables) 39Mean RT in isolationPhonaestheme645 Meaning631 Form655 Pseudo-phonaestheme610 Baseline681 TABLE 4.For each condition, mean reaction time to the stimuli in isolation (according to the English Lexiconproject) 40PhonaesthemeFormMeaningPseudo-PBaseline Average RT under priming(msec)606.7668.2642.7658.7665.3 Average RT in isolation(msec)645655631610681 Difference (msec) -38.313.211.748.7-15.7 TABLE 5.Average TARGET reaction times by condition, along with mean reaction time to the stimuli inisolation (according to the English Lexicon project) and the difference between the two measures 41Mean LSA rating for PIME-TARGET pairsPhonaestheme0.23 Meaning0.26 Form0.06 Pseudo-phonaestheme0.12 Baseline0.06 TABLE 6.For each condition, mean LSA semantic similarity rating for word pairs 42Relation Between PRIME and TARGETMorphologicalPhonaesthemicFormalSemantic Feldman & Soltano 49---1834 The present study--59-323 TABLE 7.Morphological priming (Feldman and Soltano 1999) and phonaesthemic priming (the current work) in milliseconds 43im [LIGHT] [LIGHT]  brat    [LIGHT] s   [LIGHT]   [LIGHT] mr    [LIGHT] d    [MOTION]  [EMOTION] FIGURE 1.A piece of a network model of generalization over phonaesthemes 44LIGHT    EMOTION   MOTION  HEAT     COLOR Meani ng Layer Hidden Layer Form layer l  b       r FIGURE 2.A miniature connectionist model for learning and representing phonaesthemes 45APPENDIX 1These words were among those extracted from the Brown corpus. They were selected as containingthe phonaesthemic meanings if one of their senses in Webster's 7th dictionary referred to someaspect of 'LIGHT' or 'VISION' for the case of gl- and 'NOSE' or 'MOUTH' for sn-.  gl-glanceglanced glances glancingglareglared glaring glaringlyglassglass-bottom glasses glass-fiberglasslessglass-like glassy glaucomaglazeglazed glazes glazinggleamgleamed gleaming glimmerglimmeringglimpseglimpsed glimpses glintglintedglinting glisten glistenedglisteningglitter glittered glitteringgloomgloomily gloomy glossglossyglow glowed gloweredgloweringglowing glowssn-snacksnacks snarled snarlingsneersneered sneering sneerssneezedsneezing snickered sniffsniffedsniffing sniggered snippysnivelingssnoring snorkle snortsnortedsnout snuffboxes snuffedsnuffer 1APPENDIX 2PhonaesthemePrimeWord TargetNonceTargetglimmergleamgleat glintglimpseglim glistenglareglame glitterglowglone smirksmellsmett smoochsmokesmofe snifflesneezesneeg snoresnacksnab snortsniffsnick snoutsnarlsnart BaselinePrimeWord TargetNonceTargetdialuglyudly frillbarnbarm jestquillquive luretwintwim muckwranglewrongle poppyshivershigger sedanroastroalt thirsthymnhymp vigilyearnyeart wrynestnisk MeaningPrimeWord TargetNonceTargetcollarbuttonbottin cordroperone forsakequitquin keenclevercledat pollenblossomblossot somberdimdiz tenorbassbiss thugbanditbanlit varnishwaxwap weedlawnlafe FormPrimeWord TargetNonceTargetdrabdreaddrode druiddripdrit flogflakeflike flourflagflad frayfrostfrossel glibgloveglope grazegruntgrent plightplungeplung propprovokepronoke stoutstabsteb Pseudo-PhonPrimeWord TargetNonceTargetbleachblankblang blemishblotblog blimpblobbloff bludgeonblastblask blurblinkbling cratecradlecradon cronycrookcroof skipperskiffskidge skitsketchskelt throttlethroatthroam  1Notes                                                  1Word-and-Paradigm and Item-and-Process models (Hockett 1954) do not make strong use ofcompositionality, though they do depend on productivity.2 The distinction between word types, the list of word entries that appear in the corpus, and wordtokens, the number of actual instances of each of those words, is a common and necessary one instudies of corpus frequency.3Facetiously and bookkeeper, respectively, although Brian Joseph (p.c.) notes that if one is willing toadopt a flexible definition of word, then a word with five double sets of letters in sequence isboobbookkeeper (a bookkeeper who is a boob).
Phonaesthemes	Phonemes	Morphemes	Phonology	Word structures	Morphology
In sentiment analysis, what are some strategies to identify the following sentence as a negative sentiment: "I did not like the movie."	The "not like" part of this sentence makes it difficult for a standard unigram bag-of-words model to handle, since 'like' would strongly weight the sentence as a positive sentiment. One strategy to get around this is to either consider bigram features ("not like" would be one token, which would be strongly weighted as a negative sentiment) or to prepend the term 'NOT_' before any terms that begin with 'not' (or related, such as "didn't") during preprocessing (text normalization). Then, 'NOT_like' would strongly weight the sentence as a negative sentiment.	Consider the following sentence: "The movie was not really good." How might a classifier correctly label this sentence as a negative sentiment?	Either B or C.	Prepend 'NOT_' to 'really' before building a model around unigrams.|||Prepend 'NOT_' to 'really' before building a model around bigrams.|||Build a model around trigrams.|||Either A or B.	Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||8 C HAPTER 4  N AIVE B AYESAND S ENTIMENT C LASSIFICATION multinomialnaiveBayes or binaryNB .ThevariantusesthesameEq. 4.10 except binaryNB thatforeachdocumentweremoveallduplicatewordsbeforeconcatenatingthem intothesinglebigdocument.Fig. 4.3 showsanexampleinwhichasetoffour documents(shortenedandtext-normalizedforthisexample)areremappedtobinary, withthecountsshowninthetableontheright.Theexampleisworked withoutadd-1smoothingtomakethedifferencesclearer.Notethattheresultscounts neednotbe1;theword great hasacountof2evenforBinaryNB,becauseitappears inmultipledocuments. Fouroriginaldocuments:  itwaspathetictheworstpartwasthe boxingscenes  noplottwistsorgreatscenes + andsatireandgreatplottwists + greatscenesgreat Afterper-documentbinarization:  itwaspathetictheworstpartboxing scenes  noplottwistsorgreatscenes + andsatiregreatplottwists + greatscenes NBBinary CountsCounts +  +  and 2 0 1 0 boxing0101 1010 great 3 1 2 1 it0101 no0101 or0101 part0101 pathetic0101 plot1111 satire1010 scenes1212 the 0 2 0 1 twists1111 was 0 2 0 1 worst0101 Figure4.3 AnexampleofbinarizationforthebinarynaiveBayesalgorithm. Asecondimportantadditioncommonlymadewhendoingtextfor sentimentistodealwithnegation.Considerthedifferencebetween Ireallylikethis movie (positive)and Ididn'tlikethismovie (negative).Thenegationexpressedby didn't completelyalterstheinferenceswedrawfromthepredicate like .Similarly, negationcanmodifyanegativewordtoproduceapositivereview( don'tdismissthis  , doesn'tletusgetbored ). Averysimplebaselinethatiscommonlyusedinsentimentanalysistodealwith negationisthefollowing:duringtextnormalization,prependthe NOT to everywordafteratokenoflogicalnegation( n't,not,no,never )untilthenextpunc- tuationmark.Thusthephrase didntlikethismovie,butI becomes didntNOT_likeNOT_thisNOT_movie,butI Newlyformed`words'like NOT like , NOT recommend willthusoccurmoreof- teninnegativedocumentandactascuesfornegativesentiment,whilewordslike NOT bored , NOT dismiss willacquirepositiveassociations.WewillreturninChap- ter17totheuseofparsingtodealmoreaccuratelywiththescoperelationshipbe- tweenthesenegationwordsandthepredicatestheymodify,butthissimplebaseline worksquitewellinpractice. Finally,insomesituationswemighthaveinsuflabeledtrainingdatato trainaccuratenaiveBayesusingallwordsinthetrainingsettoestimate positiveandnegativesentiment.Insuchcaseswecaninsteadderivethepositive
Text classification	Normalization	Unigrams	Bigrams	Bag-of-words	Sentiment analysis
What is the intuition behind gradient descent? Why is it particularly advantageous for Logistic Regression?	Gradient descent aims to take in a loss from the current point (which represents the parameters that are being trained, such as weights and a bias for Logistic Regression), and inch this point in the opposite direction until it has reached the local minimum (which are the optimal values for our parameters). While complications are added for multi-layer neural networks, which have a non-convex loss function (i.e., multiple local minima), the loss function for Logistic Regression is convex and thus the minimum reached via gradient descent in this case is guaranteed to be our global minimum.	Which of the following statements is not true about using gradient descent for logistic regression?	If we initialize our weights and bias to 0 and our first training example has the label y=0, our weights and bias will all still be 0 after performing our update.	Each value in our gradient corresponds with each of our current weights and bias.|||We want to update our current weights and bias in the opposite direction as our gradient.|||Each value in our gradient is multiplied by a learning rate––the bigger the learning rate, the more drastically we will update our current weights and bias.|||(All of the above are true statements.)	Speech and Language Processing: Logistic Regression||publication||8 C HAPTER 5  L OGISTIC R EGRESSION Bycontrast,let'spretendinsteadthattheexampleinFig. 5.2 wasactuallynegative, i.e. y = 0(perhapsthereviewerwentontosayﬁButbottomline,themovieis terrible!Ibegyounottoseeit!ﬂ).Inthiscaseourmodelisconfusedandwe'dwant thelosstobehigher.Nowifweplug y = 0and1  s ( w  x + b )= : 31fromEq. 5.6 intoEq. 5.11 ,theleftsideoftheequationdropsout: L CE ( w ; b )=  [ y log s ( w  x + b )+( 1  y ) log ( 1  s ( w  x + b ))] =  [ log ( 1  s ( w  x + b ))] =  log ( : 31 ) = 1 : 17 Sureenough,thelossforthe(.37)islessthanthelossforthesecond (1.17). Whydoesminimizingthisnegativelogprobabilitydowhatwewant?Aper- fectwouldassignprobability1tothecorrectoutcome(y=1ory=0)and probability0totheincorrectoutcome.Thatmeansthehigher‹ y (thecloseritisto 1),thebetterthethelower‹ y is(thecloseritisto0),theworsetheclas- .Thenegativelogofthisprobabilityisaconvenientlossmetricsinceitgoes from0(negativelogof1,noloss)to(negativelogof0,loss).This lossfunctionalsoensuresthatastheprobabilityofthecorrectanswerismaximized, theprobabilityoftheincorrectanswerisminimized;sincethetwosumtoone,any increaseintheprobabilityofthecorrectansweriscomingattheexpenseofthein- correctanswer.It'scalledthecross-entropyloss,becauseEq. 5.9 isalsotheformula forthe cross-entropy betweenthetrueprobabilitydistribution y andourestimated distribution‹ y . Nowweknowwhatwewanttominimize;inthenextsection,we'llseehowto theminimum. 5.4GradientDescent Ourgoalwithgradientdescentistotheoptimalweights:minimizetheloss functionwe'veforthemodel.InEq. 5.12 below,we'llexplicitlyrepresent thefactthatthelossfunction L isparameterizedbytheweights,whichwe'llreferto inmachinelearningingeneralas q (inthecaseoflogisticregression q = w ; b ): ‹ q = argmin q 1 m m X i = 1 L CE ( y ( i ) ; x ( i ) ; q ) (5.12) Howshallwetheminimumofthis(orany)lossfunction?Gradientdescent isamethodthataminimumofafunctionbyoutinwhichdirection (inthespaceoftheparameters q )thefunction'sslopeisrisingthemoststeeply, andmovingintheoppositedirection.Theintuitionisthatifyouarehikingina canyonandtryingtodescendmostquicklydowntotheriveratthebottom,youmight lookaroundyourself360degrees,thedirectionwherethegroundisslopingthe steepest,andwalkdownhillinthatdirection. Forlogisticregression,thislossfunctionisconveniently convex .Aconvexfunc- convex tionhasjustoneminimum;therearenolocalminimatogetstuckin,sogradient descentstartingfromanypointisguaranteedtotheminimum.(Bycontrast,  5.4  G RADIENT D ESCENT 9 thelossformulti-layerneuralnetworksisnon-convex,andgradientdescentmay getstuckinlocalminimaforneuralnetworktrainingandnevertheglobalopti- mum.) Althoughthealgorithm(andtheconceptofgradient)aredesignedfordirection vectors ,let'sconsideravisualizationofthecasewheretheparameterofour systemisjustasinglescalar w ,showninFig. 5.3 . Givenarandominitializationof w atsomevalue w 1 ,andassumingtheloss function L happenedtohavetheshapeinFig. 5.3 ,weneedthealgorithmtotellus whetheratthenextiterationweshouldmoveleft(making w 2 smallerthan w 1 )or right(making w 2 biggerthan w 1 )toreachtheminimum. Figure5.3 Thestepiniterativelytheminimumofthislossfunction,bymoving w inthereversedirectionfromtheslopeofthefunction.Sincetheslopeisnegative,weneed tomove w inapositivedirection,totheright.Heresuperscriptsareusedforlearningsteps, so w 1 meanstheinitialvalueof w (whichis0), w 2 atthesecondstep,andsoon. Thegradientdescentalgorithmanswersthisquestionbythe gradient gradient ofthelossfunctionatthecurrentpointandmovingintheoppositedirection.The gradientofafunctionofmanyvariablesisavectorpointinginthedirectionofthe greatestincreaseinafunction.Thegradientisamulti-variablegeneralizationofthe slope,soforafunctionofonevariableliketheoneinFig. 5.3 ,wecaninformally thinkofthegradientastheslope.ThedottedlineinFig. 5.3 showstheslopeofthis hypotheticallossfunctionatpoint w = w 1 .Youcanseethattheslopeofthisdotted lineisnegative.Thustotheminimum,gradientdescenttellsustogointhe oppositedirection:moving w inapositivedirection. Themagnitudeoftheamounttomoveingradientdescentisthevalueoftheslope d dw f ( x ; w ) weightedbya learningrate h .Ahigher(faster)learningratemeansthat learningrate weshouldmove w moreoneachstep.Thechangewemakeinourparameteristhe learningratetimesthegradient(ortheslope,inoursingle-variableexample): w t + 1 = w t  h d dw f ( x ; w ) (5.13) Nowlet'sextendtheintuitionfromafunctionofonescalarvariable w tomany variables,becausewedon'tjustwanttomoveleftorright,wewanttoknowwhere intheN-dimensionalspace(ofthe N parametersthatmakeup q )weshouldmove. The gradient isjustsuchavector;itexpressesthedirectionalcomponentsofthe sharpestslopealongeachofthose N dimensions.Ifwe'rejustimaginingtwoweight dimensions(sayforoneweight w andonebias b ),thegradientmightbeavectorwith twoorthogonalcomponents,eachofwhichtellsushowmuchthegroundslopesin the w dimensionandinthe b dimension.Fig. 5.4 showsavisualization:  10 C HAPTER 5  L OGISTIC R EGRESSION Figure5.4 Visualizationofthegradientvectorintwodimensions w and b . Inanactuallogisticregression,theparametervector w ismuchlongerthan1or 2,sincetheinputfeaturevector x canbequitelong,andweneedaweight w i for each x i .Foreachdimension/variable w i in w (plusthebias b ),thegradientwillhave acomponentthattellsustheslopewithrespecttothatvariable.Essentiallywe're asking:ﬁHowmuchwouldasmallchangeinthatvariable w i thetotalloss function L ?ﬂ Ineachdimension w i ,weexpresstheslopeasapartialderivative ¶ ¶ w i oftheloss function.Thegradientisthenasavectorofthesepartials.We'llrepresent‹ y as f ( x ; q ) tomakethedependenceon q moreobvious: Ñ q L ( f ( x ; q ) ; y ))= 2 6 6 6 6 4 ¶ ¶ w 1 L ( f ( x ; q ) ; y ) ¶ ¶ w 2 L ( f ( x ; q ) ; y ) . . . ¶ ¶ w n L ( f ( x ; q ) ; y ) 3 7 7 7 7 5 (5.14) Theequationforupdating q basedonthegradientisthus q t + 1 = q t  h Ñ L ( f ( x ; q ) ; y ) (5.15) 5.4.1TheGradientforLogisticRegression Inordertoupdate q ,weneedaforthegradient Ñ L ( f ( x ; q ) ; y ) .Recallthat forlogisticregression,thecross-entropylossfunctionis: L CE ( w ; b )=  [ y log s ( w  x + b )+( 1  y ) log ( 1  s ( w  x + b ))] (5.16) Itturnsoutthatthederivativeofthisfunctionforoneobservationvector x is Eq. 5.17 (theinterestedreadercanseeSection 5.8 forthederivationofthisequation): ¶ L CE ( w ; b ) ¶ w j =[ s ( w  x + b )  y ] x j (5.17) NoteinEq. 5.17 thatthegradientwithrespecttoasingleweight w j representsa veryintuitivevalue:thedifferencebetweenthetrue y andourestimated‹ y = s ( w  x + b ) forthatobservation,multipliedbythecorrespondinginputvalue x j .  5.4  G RADIENT D ESCENT 11 5.4.2TheStochasticGradientDescentAlgorithm Stochasticgradientdescentisanonlinealgorithmthatminimizesthelossfunction bycomputingitsgradientaftereachtrainingexample,andnudging q intheright direction(theoppositedirectionofthegradient).Fig. 5.5 showsthealgorithm. function S TOCHASTIC G RADIENT D ESCENT ( L () , f () , x , y ) returns q #where:Listhelossfunction #fisafunctionparameterizedby q #xisthesetoftraininginputs x ( 1 ) ; x ( 2 ) ;:::; x ( n ) #yisthesetoftrainingoutputs(labels) y ( 1 ) ; y ( 2 ) ;:::; y ( n ) q   0 repeat tildone#seecaption Foreachtrainingtuple ( x ( i ) ; y ( i ) ) (inrandomorder) 1.Optional(forreporting):#Howarewedoingonthistuple? Compute‹ y ( i ) = f ( x ( i ) ; q ) #Whatisourestimatedoutput‹ y ? Computetheloss L ( ‹ y ( i ) ; y ( i ) ) #Howfaroffis‹ y ( i ) ) fromthetrueoutput y ( i ) ? 2. g   Ñ q L ( f ( x ( i ) ; q ) ; y ( i ) ) #Howshouldwemove q tomaximizeloss? 3. q   q  h g #Gotheotherwayinstead return q Figure5.5 Thestochasticgradientdescentalgorithm.Step1(computingtheloss)isused toreporthowwellwearedoingonthecurrenttuple.Thealgorithmcanterminatewhenit converges(orwhenthegradient < e ),orwhenprogresshalts(forexamplewhentheloss startsgoinguponaheld-outset). Thelearningrate h isaparameterthatmustbeadjusted.Ifit'stoohigh,the learnerwilltakestepsthataretoolarge,overshootingtheminimumofthelossfunc- tion.Ifit'stoolow,thelearnerwilltakestepsthataretoosmall,andtaketoolongto gettotheminimum.Itiscommontobeginthelearningrateatahighervalue,and thenslowlydecreaseit,sothatitisafunctionoftheiteration k oftraining;youwill sometimesseethenotation h k tomeanthevalueofthelearningrateatiteration k . 5.4.3Workingthroughanexample Let'swalkthoughasinglestepofthegradientdescentalgorithm.We'lluseasim- versionoftheexampleinFig. 5.2 asitseesasingleobservation x ,whose correctvalueis y = 1(thisisapositivereview),andwithonlytwofeatures: x 1 = 3(countofpositivelexiconwords) x 2 = 2(countofnegativelexiconwords) Let'sassumetheinitialweightsandbiasin q 0 areallsetto0,andtheinitiallearning rate h is0.1: w 1 = w 2 = b = 0 h = 0 : 1 Thesingleupdatesteprequiresthatwecomputethegradient,multipliedbythe learningrate q t + 1 = q t  h Ñ q L ( f ( x ( i ) ; q ) ; y ( i ) )  12 C HAPTER 5  L OGISTIC R EGRESSION Inourminiexampletherearethreeparameters,sothegradientvectorhas3dimen- sions,for w 1 , w 2 ,and b .Wecancomputethegradientasfollows: Ñ w ; b = 2 6 4 ¶ L CE ( w ; b ) ¶ w 1 ¶ L CE ( w ; b ) ¶ w 2 ¶ L CE ( w ; b ) ¶ b 3 7 5 = 2 4 ( s ( w  x + b )  y ) x 1 ( s ( w  x + b )  y ) x 2 s ( w  x + b )  y 3 5 = 2 4 ( s ( 0 )  1 ) x 1 ( s ( 0 )  1 ) x 2 s ( 0 )  1 3 5 = 2 4  0 : 5 x 1  0 : 5 x 2  0 : 5 3 5 = 2 4  1 : 5  1 : 0  0 : 5 3 5 Nowthatwehaveagradient,wecomputethenewparametervector q 1 bymoving q 0 intheoppositedirectionfromthegradient: q 2 = 2 4 w 1 w 2 b 3 5  h 2 4  1 : 5  1 : 0  0 : 5 3 5 = 2 4 : 15 : 1 : 05 3 5 Soafteronestepofgradientdescent,theweightshaveshiftedtobe: w 1 = : 15, w 2 = : 1,and b = : 05. Notethatthisobservation x happenedtobeapositiveexample.Wewouldexpect thatafterseeingmorenegativeexampleswithhighcountsofnegativewords,that theweight w 2 wouldshifttohaveanegativevalue. 5.4.4Mini-batchtraining Stochasticgradientdescentiscalledstochasticbecauseitchoosesasinglerandom exampleatatime,movingtheweightssoastoimproveperformanceonthatsingle example.Thatcanresultinverychoppymovements,soit'scommontocomputethe gradientoverbatchesoftraininginstancesratherthanasingleinstance. Forexamplein batchtraining wecomputethegradientovertheentiredataset. batchtraining Byseeingsomanyexamples,batchtrainingoffersasuperbestimateofwhichdi- rectiontomovetheweights,atthecostofspendingalotoftimeprocessingevery singleexampleinthetrainingsettocomputethisperfectdirection. Acompromiseis mini-batch training:wetrainonagroupof m examples(per-  haps512,or1024)thatislessthanthewholedataset.(If m isthesizeofthedataset, thenwearedoing batch gradientdescent;if m = 1,wearebacktodoingstochas- ticgradientdescent).Mini-batchtrainingalsohastheadvantageofcomputational efy.Themini-batchescaneasilybevectorized,choosingthesizeofthemini- batchbasedonthecomputationalresources.Thisallowsustoprocessalltheexam- plesinonemini-batchinparallelandthenaccumulatetheloss,somethingthat'snot possiblewithindividualorbatchtraining. Wejustneedtomini-batchversionsofthecross-entropylossfunction weinSection 5.3 andthegradientinSection 5.4.1 .Let'sextendthecross- entropylossforoneexamplefromEq. 5.10 tomini-batchesofsize m .We'llcontinue tousethenotationthat x ( i ) and y ( i ) meanthe i thtrainingfeaturesandtraininglabel, respectively.Wemaketheassumptionthatthetrainingexamplesareindependent: log p ( traininglabels )= log m Y i = 1 p ( y ( i ) j x ( i ) ) (5.18) = m X i = 1 log p ( y ( i ) j x ( i ) ) (5.19) =  m X i = 1 L CE ( ‹ y ( i ) ; y ( i ) ) (5.20)
Logistic regression	Natural Language Processing (NLP)	Gradient descent
What is word normalization and why might it be helpful for text classification tasks?	Word normalization is the task of converting words to one standard format before processing. For instance, this can include "case folding," which is the conversion of all words into their lowercased equivalents. This can be beneficial to text classification tasks, such as sentiment analysis, because the difference in weighting between "Amazing," "amazing," and "AMAZING" in determining sentiment is likely less significant, and we can essentially combine our data for all three casings (or any other casing variations) in building our model that may be a more accurate representation. To further think about this, "AMAZING" may only include a few examples in a training dataset, so its weights may be much more accurate if we combine this token with all other casings.  Lemmatization is another strategy that can be employed, which converts words to a common root for a model. For instance, "am" and "are" would be considered as "be," and "amazing" and "amazingly" would be considered to be "amaze," which could have similar benefits to a model as case folding.	Which of the following is least likely to be an example of normalization?	Converting all instances of 'very' to 'really'.	Converting all instances of 'United States' to 'US'.|||Converting all instances of 'was' and 'is' to 'be'.|||Converting all instances of 'Building' to 'building'.	Speech and Language Processing: Regular Expressions, Text Normalization, Edit Distance||publication||20 C HAPTER 2  R EGULAR E XPRESSIONS ,T EXT N ORMALIZATION ,E DIT D ISTANCE 2018) .BPEandwordpiecebothassumethatwealreadyhavesomeinitialtokeniza- tionofwords(suchasbyspaces,orfromsomeinitialdictionary)andsowenever triedtoinducewordpartsacrossspaces.Bycontrast,theSentencePiecemodel worksfromrawtext;evenwhitespaceishandledasanormalsymbol.Thusitdoesn't needaninitialtokenizationorword-list,andcanbeusedinlanguageslikeChinese orJapanesethatdon'thavespaces. 2.4.4WordNormalization,LemmatizationandStemming Word normalization isthetaskofputtingwords/tokensinastandardformat,choos- normalization ingasinglenormalformforwordswithmultipleformslike USA and US or uh-huh and uhhuh .Thisstandardizationmaybevaluable,despitethespellinginformation thatislostinthenormalizationprocess.Forinformationretrievalorinformation extractionabouttheUS,wemightwantseeinformationfromdocumentswhether theymentionthe US orthe USA . Casefolding isanotherkindofnormalization.Mappingeverythingtolower casefolding casemeansthat Woodchuck and woodchuck arerepresentedidentically,whichis veryhelpfulforgeneralizationinmanytasks,suchasinformationretrievalorspeech recognition.Forsentimentanalysisandothertexttasks,information extraction,andmachinetranslation,bycontrast,casecanbequitehelpfulandcase foldingisgenerallynotdone.Thisisbecausemaintainingthedifferencebetween, forexample, US thecountryand us thepronouncanoutweightheadvantagein generalizationthatcasefoldingwouldhaveprovidedforotherwords. Formanynaturallanguageprocessingsituationswealsowanttwomorpholog- icallydifferentformsofawordtobehavesimilarly.Forexampleinwebsearch, someonemaytypethestring woodchucks butausefulsystemmightwanttoalso returnpagesthatmention woodchuck withno s .Thisisespeciallycommoninmor- phologicallycomplexlanguageslikeRussian,whereforexampletheword Moscow hasdifferentendingsinthephrases Moscow , ofMoscow , toMoscow ,andsoon. Lemmatization isthetaskofdeterminingthattwowordshavethesameroot, despitetheirsurfacedifferences.Thewords am , are ,and is havethesharedlemma be ;thewords dinner and dinners bothhavethelemma dinner .Lemmatizingeachof theseformstothesamelemmawillletusallmentionsofwordsinRussianlike Moscow.Thelemmatizedformofasentencelike Heisreadingdetectivestories wouldthusbe Hebereaddetectivestory . Howislemmatizationdone?Themostsophisticatedmethodsforlemmatization involvecomplete morphologicalparsing oftheword. Morphology isthestudyof thewaywordsarebuiltupfromsmallermeaning-bearingunitscalled morphemes . morpheme Twobroadclassesofmorphemescanbedistinguished: stems Šthecentralmor- stem phemeoftheword,supplyingthemainmeaningŠand  Šaddingﬁadditionalﬂ  meaningsofvariouskinds.So,forexample,theword fox consistsofonemorpheme (themorpheme fox )andtheword cats consistsoftwo:themorpheme cat andthe morpheme -s .Amorphologicalparsertakesawordlike cats andparsesitintothe twomorphemes cat and s ,oraSpanishwordlike amaren (`ifinthefuturethey wouldlove')intothemorphemes amar `tolove', 3PL ,and futuresubjunctive . ThePorterStemmer Lemmatizationalgorithmscanbecomplex.Forthisreasonwesometimesmakeuse ofasimplerbutcrudermethod,whichmainlyconsistsofchoppingoffw afes.Thisnaiveversionofmorphologicalanalysisiscalled stemming .Oneof stemming themostwidelyusedstemmingalgorithmsisthe Porter(1980) .ThePorterstemmer Porterstemmer
Lemmatization	Text classification	Normalization	Bag-of-words	Natural Language Processing (NLP)	Case folding
What happens if the learning rate \( \eta \) is too high in gradient descent?	If the learning rate \( \eta \) is too high in gradient descent, the learner will take too large of steps, causing it to overshoot the desired minimum.  Figure A shows a visualization of what happens with different learning rates.	What happens when the learning rate \( \eta \) is too low in gradient descent?	Too slow of a learning rate causes us to reach the local minimum too slowly, which can be a problem if we are under a time constraint or our number of epochs is too low to reach the local minimum.	We will ultimately hit the local minimum. There is no such thing as "too slow of a learning rate," we just need to make sure the learning rate is not too high.|||Too slow of a learning rate is expensive in terms of space, and we want to optimize our use of memory.|||Too slow of a learning rate can cause our learner to have drastic updates and potentially overshoot the local minimum.	Speech and Language Processing: Logistic Regression||publication||5.4  G RADIENT D ESCENT 9 thelossformulti-layerneuralnetworksisnon-convex,andgradientdescentmay getstuckinlocalminimaforneuralnetworktrainingandnevertheglobalopti- mum.) Althoughthealgorithm(andtheconceptofgradient)aredesignedfordirection vectors ,let'sconsideravisualizationofthecasewheretheparameterofour systemisjustasinglescalar w ,showninFig. 5.3 . Givenarandominitializationof w atsomevalue w 1 ,andassumingtheloss function L happenedtohavetheshapeinFig. 5.3 ,weneedthealgorithmtotellus whetheratthenextiterationweshouldmoveleft(making w 2 smallerthan w 1 )or right(making w 2 biggerthan w 1 )toreachtheminimum. Figure5.3 Thestepiniterativelytheminimumofthislossfunction,bymoving w inthereversedirectionfromtheslopeofthefunction.Sincetheslopeisnegative,weneed tomove w inapositivedirection,totheright.Heresuperscriptsareusedforlearningsteps, so w 1 meanstheinitialvalueof w (whichis0), w 2 atthesecondstep,andsoon. Thegradientdescentalgorithmanswersthisquestionbythe gradient gradient ofthelossfunctionatthecurrentpointandmovingintheoppositedirection.The gradientofafunctionofmanyvariablesisavectorpointinginthedirectionofthe greatestincreaseinafunction.Thegradientisamulti-variablegeneralizationofthe slope,soforafunctionofonevariableliketheoneinFig. 5.3 ,wecaninformally thinkofthegradientastheslope.ThedottedlineinFig. 5.3 showstheslopeofthis hypotheticallossfunctionatpoint w = w 1 .Youcanseethattheslopeofthisdotted lineisnegative.Thustotheminimum,gradientdescenttellsustogointhe oppositedirection:moving w inapositivedirection. Themagnitudeoftheamounttomoveingradientdescentisthevalueoftheslope d dw f ( x ; w ) weightedbya learningrate h .Ahigher(faster)learningratemeansthat learningrate weshouldmove w moreoneachstep.Thechangewemakeinourparameteristhe learningratetimesthegradient(ortheslope,inoursingle-variableexample): w t + 1 = w t  h d dw f ( x ; w ) (5.13) Nowlet'sextendtheintuitionfromafunctionofonescalarvariable w tomany variables,becausewedon'tjustwanttomoveleftorright,wewanttoknowwhere intheN-dimensionalspace(ofthe N parametersthatmakeup q )weshouldmove. The gradient isjustsuchavector;itexpressesthedirectionalcomponentsofthe sharpestslopealongeachofthose N dimensions.Ifwe'rejustimaginingtwoweight dimensions(sayforoneweight w andonebias b ),thegradientmightbeavectorwith twoorthogonalcomponents,eachofwhichtellsushowmuchthegroundslopesin the w dimensionandinthe b dimension.Fig. 5.4 showsavisualization:  10 C HAPTER 5  L OGISTIC R EGRESSION Figure5.4 Visualizationofthegradientvectorintwodimensions w and b . Inanactuallogisticregression,theparametervector w ismuchlongerthan1or 2,sincetheinputfeaturevector x canbequitelong,andweneedaweight w i for each x i .Foreachdimension/variable w i in w (plusthebias b ),thegradientwillhave acomponentthattellsustheslopewithrespecttothatvariable.Essentiallywe're asking:ﬁHowmuchwouldasmallchangeinthatvariable w i thetotalloss function L ?ﬂ Ineachdimension w i ,weexpresstheslopeasapartialderivative ¶ ¶ w i oftheloss function.Thegradientisthenasavectorofthesepartials.We'llrepresent‹ y as f ( x ; q ) tomakethedependenceon q moreobvious: Ñ q L ( f ( x ; q ) ; y ))= 2 6 6 6 6 4 ¶ ¶ w 1 L ( f ( x ; q ) ; y ) ¶ ¶ w 2 L ( f ( x ; q ) ; y ) . . . ¶ ¶ w n L ( f ( x ; q ) ; y ) 3 7 7 7 7 5 (5.14) Theequationforupdating q basedonthegradientisthus q t + 1 = q t  h Ñ L ( f ( x ; q ) ; y ) (5.15) 5.4.1TheGradientforLogisticRegression Inordertoupdate q ,weneedaforthegradient Ñ L ( f ( x ; q ) ; y ) .Recallthat forlogisticregression,thecross-entropylossfunctionis: L CE ( w ; b )=  [ y log s ( w  x + b )+( 1  y ) log ( 1  s ( w  x + b ))] (5.16) Itturnsoutthatthederivativeofthisfunctionforoneobservationvector x is Eq. 5.17 (theinterestedreadercanseeSection 5.8 forthederivationofthisequation): ¶ L CE ( w ; b ) ¶ w j =[ s ( w  x + b )  y ] x j (5.17) NoteinEq. 5.17 thatthegradientwithrespecttoasingleweight w j representsa veryintuitivevalue:thedifferencebetweenthetrue y andourestimated‹ y = s ( w  x + b ) forthatobservation,multipliedbythecorrespondinginputvalue x j .  5.4  G RADIENT D ESCENT 11 5.4.2TheStochasticGradientDescentAlgorithm Stochasticgradientdescentisanonlinealgorithmthatminimizesthelossfunction bycomputingitsgradientaftereachtrainingexample,andnudging q intheright direction(theoppositedirectionofthegradient).Fig. 5.5 showsthealgorithm. function S TOCHASTIC G RADIENT D ESCENT ( L () , f () , x , y ) returns q #where:Listhelossfunction #fisafunctionparameterizedby q #xisthesetoftraininginputs x ( 1 ) ; x ( 2 ) ;:::; x ( n ) #yisthesetoftrainingoutputs(labels) y ( 1 ) ; y ( 2 ) ;:::; y ( n ) q   0 repeat tildone#seecaption Foreachtrainingtuple ( x ( i ) ; y ( i ) ) (inrandomorder) 1.Optional(forreporting):#Howarewedoingonthistuple? Compute‹ y ( i ) = f ( x ( i ) ; q ) #Whatisourestimatedoutput‹ y ? Computetheloss L ( ‹ y ( i ) ; y ( i ) ) #Howfaroffis‹ y ( i ) ) fromthetrueoutput y ( i ) ? 2. g   Ñ q L ( f ( x ( i ) ; q ) ; y ( i ) ) #Howshouldwemove q tomaximizeloss? 3. q   q  h g #Gotheotherwayinstead return q Figure5.5 Thestochasticgradientdescentalgorithm.Step1(computingtheloss)isused toreporthowwellwearedoingonthecurrenttuple.Thealgorithmcanterminatewhenit converges(orwhenthegradient < e ),orwhenprogresshalts(forexamplewhentheloss startsgoinguponaheld-outset). Thelearningrate h isaparameterthatmustbeadjusted.Ifit'stoohigh,the learnerwilltakestepsthataretoolarge,overshootingtheminimumofthelossfunc- tion.Ifit'stoolow,thelearnerwilltakestepsthataretoosmall,andtaketoolongto gettotheminimum.Itiscommontobeginthelearningrateatahighervalue,and thenslowlydecreaseit,sothatitisafunctionoftheiteration k oftraining;youwill sometimesseethenotation h k tomeanthevalueofthelearningrateatiteration k . 5.4.3Workingthroughanexample Let'swalkthoughasinglestepofthegradientdescentalgorithm.We'lluseasim- versionoftheexampleinFig. 5.2 asitseesasingleobservation x ,whose correctvalueis y = 1(thisisapositivereview),andwithonlytwofeatures: x 1 = 3(countofpositivelexiconwords) x 2 = 2(countofnegativelexiconwords) Let'sassumetheinitialweightsandbiasin q 0 areallsetto0,andtheinitiallearning rate h is0.1: w 1 = w 2 = b = 0 h = 0 : 1 Thesingleupdatesteprequiresthatwecomputethegradient,multipliedbythe learningrate q t + 1 = q t  h Ñ q L ( f ( x ( i ) ; q ) ; y ( i ) )
Gradient descent	Learning rate	Logistic regression	Neural models	Natural Language Processing (NLP)
What is a 'Knowledge Graph'	A Knowledge Graph is a digital matrix that contains infinite amounts of related facts and information about different entities. Through keyword searches, these connections can be identified and processed.	What is the most contributing field of study that supports this system?	Semantics	Phonetics|||Language Acquisition	NLP Crash Course clip||youtube||N/A
Semantics	Computational linguistics	Computer science	Natural Language Processing (NLP)
What is the difference between an "adverb phrase" and an "adverbial"?	"Adverb" is a part-of-speech, or more properly, "lexical category." "Adverbial" is a functional category, like subject and predicate; it describes what a phrase does, not what it is. While an adverb phrase must have an adverb for its head, such as in "almost always" or "very quickly," an adverbial phrase does not even need to contain an adverb; it just functions like one. For example, in "He chewed his food with his mouth open," the phrase with his mouth open is a prepositional phrase, but it is an adverbial, because it modifies the verb "chewed."	Which of the following sentences does NOT contain an adverbial?	The man at the help desk looked up the number.	We picnicked on the hill until the sun went down.|||The house sat on the hill watching over the town.|||I often daydream.	NONE
Grammatical relations	Grammar	Syntax	Phrasal categories	Lexical categories	Adverbs	Adverb phrases	Adverbials
How is intonation used in linguistics?	Intonation refers to a change in the pitch, timing, or volume of speech to indicate a speaker's intent. It can provide the emotion or attitude of the speaker.	Of the branches of linguistics, which one studies intonation?	Phonetics	Applied linguistics|||Discourse analysis|||Phonology	Introduction to Stress and Intonation||youtube||N/A
Intonation	Phonetics	Branches of linguistics
What is a syntactic constituent?	A syntactic constituent is a chunk of a sentence or utterance that is a complete grammatical (functional) unit and has a meaning; all phrases and words are constituents, but many pieces of sentences are not. For example. In the previous sentence, "is a chunk of" and "that is a" are among the non-constituents. Three tests for constituency are substitution, movement, and questionability. (1) Any constituent can be replaced by one word without making the sentence ungrammatical. (2) Any constituent can be moved around as a unit to different places in the sentence (sometimes with other changes in the sentence being necessary). (3) Any constituent should be the complete answer to some question about the sentence. For example, if the sentence is "The woman standing on the corner is my professor" . . . to test the constituency of "the woman standing on the corner": (1) substitution: "She is my professor." (2) movement: "My professor is the woman standing on the corner." (3) questionability: "Who is your professor?" Therefore, we know that the word sequence in question is a constituent. (NOTE: There are a few other tests for constituency, also.)	Which of the following is NOT true about testing for constituency?	If the tests of substitution, movement, and questioning fail fail, it is not a constituent.	If it is coordinated with other phrases by a conjunction, it is a constituent.|||No words can belong to more than one constituent except in nested constituents.|||One constituent can have many constituents within it.	Constituents and Tests for Constituency from Washington State University||document||Constituents and Tests for Constituency  Constituent : "a syntactic unit that functions as part of a larger unit within a sentence" (Finegan and Besnier: 525) 1. Single words are constituents. (exceptions: certain contractions, certain possessives) Complete sentences are constituents. 2. Any sequence of words which can be functionally replaced by a single word must be a constituent. The man in the black hat  is my brother. You were sick  when I saw you. Tom  is my brother. You were sick  yesterday .A specific subcase of (2): any sequence of words which can be replaced by a proform is a constituent. The woman with red hair  won first prize.  She  won it. Tommy studied hard for the test and Jane  studied hard for the test  too. Tommy studied hard for the test and Jane  did  too. 3. Any sequence of words which can stand alone in answer to a question or conversely, any sequence of words which can be questioned as a unit is a constituent. Why did John win?  Because he paid off the judges .Who did John pay?  The judges. Where did he go?  To Spain. This does not merely argue that these are constituents in the ellipted answer form given above, but in the full sentence forms from which these are apparently derived: John won  because he paid off the judges .John paid  the judges. John went  to Spain. 4. Any sequence of words conjoined to an independently identifiable constituent is a constituent. Mary helped  the man in the green hat  and  Michael .Since  Michael  is a single word, and so a constituent, and it is can be conjoined to  the man in the green hat , then  the man in the green hat must be a constituent. How can we tell that  Michael  is conjoined to  the man in the green hat  and not to  hat  or the green hat  or man in the green hat ?Compare  Mary helped Michael and the man in the green hat  with the sentence above. Notice that it is possible to change the order of Michael  and  the man in the green hat . It is not possible to change the order of  hat  and  Michael  and get a grammatical sentence: * Mary helped the man in the green Michael and hat . Similarly * Mary helped the man in Michael and the green hat  and  Mary helped the Michael and man in the green hat  are ungrammatical. Coordinate conjunction joins structurally parallel units; any order of the conjuncts should produce a grammatical sentence. As a corollary, conjoined units form a constituent: Mary helped  the man in the green hat and Michael  as can be demonstrated by other tests:Mary helped  them . (substitution) Who did Mary help?  The man in the green hat and Michael . (question) 5. Any sequence of words which can be moved as a unit and still produce a grammatical sentence with the same meaning is a constituent. I worked on the ancient text  with great care .With great care  I worked on the ancient text. Mary put  the books and the new computer  into her car. Mary put into her car  the books and the new computer .6. The tests in (1-5) are all one-way, positive tests: If you can't find a good sentence where the test works, that does not prove that the sequence of words in question is not a constituent. If you can find a good sentences where one of these tests works (or better where several of them do), that gives strong evidence that the sequence of words is a constituent. There is one way demonstrate that a continuous sequence of words does not form a constituent: A sequence of words cannot be a constituent if its parts are part of other  constituents which are not subunits of the same constituent. In other words, in the sentence  The man stole the money  we can demonstrate that  the man  is a constituent;  stole the money  is a constituent; and  the money  is a constituent. Substitution tests: He stole the money. Who stole the money? The man  did. The man stole  it.Question tests: Who stole the money?  The man .What did the man do?  Stole the money. What did he steal?  The money. Conjunction tests: Tom and  the man  stole the money. The man  stole the money  and disappeared. The man stole furniture and  the money. Given that these are the constituents of this sentence,  man stole , for example, cannot be a constituent. Since  man  is a constituent of a larger constituent that does not include  stole  and  stole  is part of a larger constituent that does not include  man ; they cannot together form a constituent. What other sequences of words in this sentence cannot form a constituent?
Sentence structure	Constituents	Constituency	Syntax
How would an inference system use backwards or forwards chaining to answer the question, “How can I make my mother happy?”	An inference system based on chaining would need to use backwards chaining to answer this question. Backwards chaining is the process of finding the premises (antecedents) of a conclusion, and then the premises of those premises, and so on . . . or to put it another way, one needs to find out what leads to the conclusion that my mother will be happy, and then what leads to that. For example, “if she believes you are successful, your mother will be happy.” Therefore, backwards chaining tells us that we need to find out how to make our mothers believe we are successful. Then we might find, “if you tell your mother something, she will believe it,” so we get the answer: “Tell your mother you are successful.” In practice, the knowledge base would likely consist of implications more basic and general than these, such as "if you tell somebody something, they may think it is true."	Which of the following is not a potential disadvantage of using backwards and forwards chaining to answer questions?	They are more computationally expensive than the alternative method, ‘resolution’	Even together, they are not ‘complete’; they may not be able to yield all true inferences.|||The conclusions derived through backwards chaining are not necessarily true||| Knowledge needs to be encoded in a way specifically designed to support the process of chaining implications	Chapter section on inference||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 16 LogicalRepresentationsof SentenceMeaning I SHMAEL : Surelyallthisisnotwithoutmeaning. HermanMelville, MobyDick Inthischapterweintroducetheideathatthemeaningoflinguisticexpressionscan becapturedinformalstructurescalled meaningrepresentations .Considertasks meaning representations thatrequiresomeformofsemanticprocessing,likelearningtouseanewpieceof softwarebyreadingthemanual,decidingwhattoorderatarestaurantbyreading amenu,orfollowingarecipe.Accomplishingthesetasksrequiresrepresentations thatlinkthelinguisticelementstothenecessarynon-linguistic knowledgeofthe world .Readingamenuanddecidingwhattoorder,givingadviceaboutwhereto gotodinner,followingarecipe,andgeneratingnewrecipesallrequireknowledge aboutfoodanditspreparation,whatpeopleliketoeat,andwhatrestaurantsarelike. Learningtouseapieceofsoftwarebyreadingamanual,orgivingadviceonusing software,requiresknowledgeaboutthesoftwareandsimilarapps,computers,and usersingeneral. Inthischapter,weassumethatlinguisticexpressionshavemeaningrepresenta- tionsthataremadeupofthe samekindofstuff thatisusedtorepresentthiskindof everydaycommon-senseknowledgeoftheworld.Theprocesswherebysuchrepre- sentationsarecreatedandassignedtolinguisticinputsiscalled semanticparsing or semantic parsing semanticanalysis ,andtheentireenterpriseofdesigningmeaningrepresentations andassociatedsemanticparsersisreferredtoas computationalsemantics . computational semantics 9 e ; yHaving ( e ) ^ Haver ( e ; Speaker ) ^ HadThing ( e ; y ) ^ Car ( y ) Figure16.1 Alistofsymbols,twodirectedgraphs,andarecordstructure:asamplerof meaningrepresentationsfor Ihaveacar. ConsiderFig. 16.1 ,whichshowsexamplemeaningrepresentationsforthesen- tence Ihaveacar usingfourcommonlyusedmeaningrepresentationlanguages. Thetoprowillustratesasentencein First-OrderLogic ,coveredindetailinSec- tion 16.3 ;thedirectedgraphanditscorrespondingtextualformisanexampleof an AbstractMeaningRepresentation(AMR) form (Banarescuetal.,2013) ,and ontherightisa frame-based or  representation,discussedinSection 16.5 andagaininChapter18.  2 C HAPTER 16  L OGICAL R EPRESENTATIONSOF S ENTENCE M EANING Whiletherearenon-trivialdifferencesamongtheseapproaches,theyallshare thenotionthatameaningrepresentationconsistsofstructurescomposedfroma setofsymbols,orrepresentationalvocabulary.Whenappropriatelyarranged,these symbolstructuresaretakento correspond toobjects,propertiesofobjects,andrela- tionsamongobjectsinsomestateofaffairsbeingrepresentedorreasonedabout.In thiscase,allfourrepresentationsmakeuseofsymbolscorrespondingtothespeaker, acar,andarelationdenotingthepossessionofonebytheother. Importantly,theserepresentationscanbeviewedfromatleasttwodistinctper- spectivesinalloftheseapproaches:asrepresentationsofthemeaningofthepar- ticularlinguisticinput Ihaveacar ,andasrepresentationsofthestateofaffairsin someworld.Itisthisdualperspectivethatallowstheserepresentationstobeused tolinklinguisticinputstotheworldandtoourknowledgeofit. Inthenextsectionswegivesomebackground:ourdesiderataforameaning representationlanguageandsomeguaranteesthattheserepresentationswillactually dowhatweneedthemtodoŠprovideacorrespondencetothestateofaffairsbeing represented.InSection 16.3 weintroduceFirst-OrderLogic,historicallytheprimary techniqueforinvestigatingnaturallanguagesemantics,andseeinSection 16.4 how itcanbeusedtocapturethesemanticsofeventsandstatesinEnglish.Chapter17 thenintroducestechniquesfor semanticparsing :generatingtheseformalmeaning representationsgivenlinguisticinputs. 16.1ComputationalDesiderataforRepresentations Let'sconsiderwhymeaningrepresentationsareneededandwhattheyshoulddofor us.Tofocusthisdiscussion,let'sconsiderasystemthatgivesrestaurantadviceto touristsbasedonaknowledgebase. V Considerthefollowingsimplequestion: (16.1) DoesMaharaniservevegetarianfood? Toanswerthisquestion,wehavetoknowwhatit'sasking,andknowwhetherwhat it'saskingistrueofMahariniornot. v isasystem'sabilitytocompare v thestateofaffairsdescribedbyarepresentationtothestateofaffairsinsomeworld asmodeledinaknowledgebase.Forexamplewe'llneedsomesortofrepresentation like Serves ( Maharani ; VegetarianFood ) ,whichasystemcancanmatchagainstits knowledgebaseoffactsaboutparticularrestaurants,andifitarepresentation matchingthisproposition,itcanansweryes.Otherwise,itmusteithersay No ifits knowledgeoflocalrestaurantsiscomplete,orsaythatitdoesn'tknowifitknows itsknowledgeisincomplete. UnambiguousRepresentations Semantics,likealltheotherdomainswehavestudied,issubjecttoambiguity.Words andsentenceshavedifferentmeaningrepresentationsindifferentcontexts.Consider thefollowingexample: (16.2) Iwannaeatsomeplacethat'scloseto ICSI . Thissentencecaneithermeanthatthespeakerwantstoeat at somenearbylocation, orunderaGodzilla-as-speakerinterpretation,thespeakermaywanttodevoursome  16.1  C OMPUTATIONAL D ESIDERATAFOR R EPRESENTATIONS 3 nearbylocation.Thesentenceisambiguous;asinglelinguisticexpressioncanhave oneoftwomeanings.Butour meaningrepresentations itselfcannotbeambiguous. Therepresentationofaninput'smeaningshouldbefreefromanyambiguity,sothat thethesystemcanreasonoverarepresentationthatmeanseitheronethingorthe otherinordertodecidehowtoanswer. Aconceptcloselyrelatedtoambiguityis vagueness :inwhichameaningrepre- vagueness sentationleavessomepartsofthemeaningVaguenessdoesnotgive risetomultiplerepresentations.Considerthefollowingrequest: (16.3) IwanttoeatItalianfood. While Italianfood mayprovideenoughinformationtoproviderecommendations,it isnevertheless vague astowhattheuserreallywantstoeat.Avaguerepresentation ofthemeaningofthisphrasemaybeappropriateforsomepurposes,whileamore representationmaybeneededforotherpurposes. CanonicalForm Thedoctrineof canonicalform saysthatdistinctinputsthatmeanthesamething canonicalform shouldhavethesamemeaningrepresentation.Thisapproachgreatlyrea- soning,sincesystemsneedonlydealwithasinglemeaningrepresentationfora potentiallywiderangeofexpressions. Considerthefollowingalternativewaysofexpressing( 16.1 ): (16.4) DoesMaharanihavevegetariandishes? (16.5) DotheyhavevegetarianfoodatMaharani? (16.6) ArevegetariandishesservedatMaharani? (16.7) DoesMaharaniservevegetarianfare? Despitethefactthesealternativesusedifferentwordsandsyntax,wewantthem tomaptoasinglecanonicalmeaningrepresentations.Iftheywerealldifferent, assumingthesystem'sknowledgebasecontainsonlyasinglerepresentationofthis fact,mostoftherepresentationswouldn'tmatch.Wecould,ofcourse,storeall possiblealternativerepresentationsofthesamefactintheknowledgebase,butdoing sowouldleadtoenormousdifinkeepingtheknowledgebaseconsistent. Canonicalformdoescomplicatethetaskofsemanticparsing.Oursystemmust concludethat vegetarianfare , vegetariandishes ,and vegetarianfood refertothe samething,that having and serving areequivalenthere,andthatalltheseparse structuresstillleadtothesamemeaningrepresentation.Orconsiderthispairof examples: (16.8) Maharaniservesvegetariandishes. (16.9) VegetariandishesareservedbyMaharani. Despitethedifferentplacementoftheargumentsto serve ,asystemmuststillassign Maharani and vegetariandishes tothesamerolesinthetwoexamplesbydraw- ingongrammaticalknowledge,suchastherelationshipbetweenactiveandpassive sentenceconstructions. InferenceandVariables Whataboutmorecomplexrequestssuchas: (16.10) CanvegetarianseatatMaharani? Thisrequestresultsinthesameanswerastheothersnotbecausetheymeanthesame thing,butbecausethereisacommon-senseconnectionbetweenwhatvegetarianseat
Backwards chaining	Forwards chaining
How to import linguistic data using NLTK?	(a) import nltk data = nltk.corpus.brown.tagged_sents() (b) import nltk data = nltk.corpus.brown.words() (c) import nltk data = nltk.corpus.brown.sents()	Which of the following kinds of corpora are not available through nltk.corpus?	Twitter posts	Chatroom transcripts|||The Bible|||Romance novels	NONE
Text corpora	nltk corpora	NLTK	nltk.corpus	Tokenized sentences	Tagged sentences
Briefly describe the three perspectives of language acquisition?	1. Behavioristic position: All humans begin with no knowledge of language, but possess the innate ability to learn through imitation.</p><p>2. Psycholinguistic position: All humans possess an innate ability to understand language, and the language input is merely a trigger for these mental processes.</p><p>3. Interactionistic perspective: All humans possess an innate ability to learn language, and this learning happens as the result of one's environment and experiences.	Which approach views infants as especially prepared to organize speech that is heard into a human language system?	Psycholinguistic	Behavioristic|||Interactionistic	A Brief Discussion on the Biological Factors in the Acquisition of Language||publication||Ronivaldo Braz da SILVA psycholinguistic and interactionistic proponents alike. In examining the biological component of language, this paper also discusses the critical period for language acquisition, theories of language development from the biological perspective, as well as information from studies ofindividuals with neurological or biological dysfunction. The study ofthe language development ofpersons with specific types ofbrain damage or other biological disturbances has provided much information regarding the brain's role in the language process. Finally, this paper discusses how these innate, biological factors, influence the acquisition oflanguage. The three perspectives of language acquisition The first perspective of language acquisition to be discussed in this paper is the behaviouristic position (SKINNER, 1987), which builds on learning principles to explain language acquisition. Behaviourists believe that the learner begins withno knowledge of language but possesses the competence to learn it. Specifically, they contend that one learns through the reinforcement ofimitation. For instance, infants repeat words or babbles after their parents without having a clear knowledge ofthe meaning ofthose words. This reinforcement ofbabbling and the shaping ofvocal behaviour account for the very first stage ofleaming. The child's babbling will later tum into words that will subsequently be associated with meanings and promote communication. According to the behaviouristic perspective, language is acquired from factors in the environment Behaviourists believe thatthe developmentoflanguage is afirnctionofstimulus, response, and reinforcement2. Behaviourists view the language learner as a language-producing machine. Language input is made available to the learner in the form ofstimuli/ feedback. In the behaviouristic model, the learner is passive, and the environment is the determining factor. Anotherperspective oflanguage learning isthepsycholinguisticposition. The proponents ofthis approach argue thatthe leameris the grand initiator ofall language learning. The learner possesses an innate capacity for dealing with language and activates a theory orprocess of grammar (grammatical theory) to help understand and produce an innumerable number of phrases or sentences. Language input is, therefore, oflittle consequence otherthan being only atriggerofthe innatementalprocessesto beginlanguage formation. Psycholinguists claimthat none ofthe learner's output can be explained in terms ofthe characteristics ofthe input Instead, the learner is biologically predisposed to learn languages as the brain develops, and the environment simply triggers its emergence. Noam Chomsky (1965, 1980, 2005Y, the See for example the study developed by Moerk (1983). 3  See also Ellis (1985). 154 Revista do GEL, S. J. do Rio Preto, v. 4, n. 2, p. 153-169, 2007  A brief discussion on the biological factors in the acquisition oflanguage most famous ofthe psycholinguists, calledthis innate or biological component the Language AcquisitionDevice (LAD). A third perspective is that of the interactionists. Proponents of the interactionistic perspective claim that the development of language is the result of interaction between the learner's mental abilities and the linguistic environment (see for example CHAPMAN, 2000; HUITT; HUMMEL, 2003; PIAGET, 1954, 1999). The learner acquires language through the interaction ofperceptual-cognitive capacities and experiences. The learner's environment and neurological maturation determine learning. Therefore, language and thought are simultaneously developed as the learner passes through a series of fixed developmental stages requiring more and more complex strategies of cognitive organization. Interactionists consider the capacity for learning language to be innate. Interactionists claim that the learner must internalize linguistic structures from the environment and must become aware of the social function of communication. Thus, the important data are not only the utterances produced by the learner, but the discourse which learner and caretaker (e.g., father and/or mother) jointly construct. Piaget (1954, 1999; PIAGET; INHELDER, 1969), the major proponent of the interactionistic position, believed that the child's environment and neurological maturation determine learning. As a result, language development programs based on the interactionistic perspective are based on two ideas: "(a) meaning is brought to a child's language through interaction with the environment, and (b) the child uses speech to control the environment" (MERCER, 1997, p.418). In summary, the above discussion shows that these three perspectives overlap and complement each other. While behaviourists claim that learning is the result of input from the linguistic environment, psycholinguists believe that language input works only as a trigger ofthe innate mental processes that is responsible for language formation. This means that the linguistic environment contributes little to language learning. Interactionists, on the other hand, combine the behaviouristic and the psycholinguistic perspectives, as they believe that the interaction between the learner's mental abilities and the linguistic environment promotes the development of language. Innate human language component Lenneberg (1964) set forth the seminal arguments for the innate human component, by presenting four arguments for biological innateness ofpsychological Revista do GEL, S. J. do Rio Preto, v. 4, n. 2, p. 153-169, 2007 155
Language acquisition
Describe "second language anxiety."	Feelings of anxiety related to second language acquisition. Affective and cognitive: fear of performing tasks related to learning a second language and fear of doing poorly, ex. getting bad grades at school.	Which of the following have the potential to cause language anxiety?	All of the above	Teacher-learner interaction|||Bad learning experiences|||Consistently poor learning performance	Mathematics and English, Two Languages: Teachers' Views||publication||Journal of Education and Learning; Vol. 2, No. 1; 2013  ISSN 1927-5250   E-ISSN 1927-5269  Published by Canadian Center  of Science and Education  211   Mathematics and English, Two Languages: Teachers™ Views Shosh Leshem 1 & Zvia Markovits 1 1 Oranim Academic College of Education, Tivon, Israel  Correspondence: Shosh Leshem, Oranim, Academic  College of Education, Tivon, Israel. E-mail:  shosh-l@zahav.net.il   Received: December 8, 2012      Accepted: January  10, 2013      Online Pub lished: February 22, 2013  doi:10.5539/jel.v2n1p211              URL: http://dx.doi.org/10.5539/jel.v2n1p211    Abstract   English is an international language used all over the world.  Mathematics is the language of sciences but it is also  a language used in everyday life. Although both are perceived as languages, mathematics and English are   considered as two completely distinct disciplines. In  this paper we first discuss English and mathematics as  languages. Then we present interviews conducted with five  mathematics and English teachers in order to explore  their views regarding issues of lan guage features, ways of teaching and issues of anxiety and collaboration  between English and mathematics teach ers. The interviews s uggest some communalities in  the way English and  mathematics teachers relate to the two languages. The co nnection between the two disciplines seems to challenge  teachers™ thinking about their teaching.  Keywords:  teacher education, mathematics and Englis h, universal languages, teachers™ beliefs  1. Introduction   English is an international language used all over  the world allowing people from different countries to  communicate for different needs. Mathematics is the language  of sciences but it is also a language which is used  for communication and for describing different situations in everyday life. Although both English and   mathematics are perceived as languages, they are considered by schools and colleges as two completely distinct  disciplines.   We are both teacher educators in an academic college of ed ucation. Author 1 is an expe rt in teaching English as a  foreign language (TEFL) and Author 2 is an expert in ma thematics education. We both  have experience as teachers  in schools and at present we are head of departments in the faculty of education in our college. Our collaboration   commenced when we were inv ited to make a contribution to a book on t eacher education in ou r country. We then  realized that, in fact, our two ﬁdistin ct disciplinesﬂ have communal features. This realization made us aware of  various interesting incidents that emerged in our classes and sparked some thoughts concerning the connection  between our two areas of expertise.  In the course of one of Author™s 1 academic writing cour ses at the college, we discussed the different processes  that writing entails, in order to achieve an elegant cohere nt piece of text. Students were  invited to share how they  perceive the art of writing.  ﬁThis is just like problem solvingﬂ , said one of the students.  ﬁThere are so many  linguistic elements that we have to thin k of and knit together into a colorful  tapestry of words so that everybody can  read and understand. This is not easy when you have to wr ite in a language different from your mother tongue. You  have to draw the right words from your command of vocabulary, be familiar with the right lexicon, know the   grammar and convey meaning in an unambiguous manner.  This is high order problem solving.ﬂ  This description  brought home some thoughts about the possible similarity to mathematical problem solving.  The following took place in one of Au thor™s 2 mathematics classes when discussing a research study which  investigated the understanding of 7 th grade students of the relationships between fractions and decimals. We  realized that although most of the students know that 0.5 and 1/2 are equal, this might be difficult to grasp for some   of them, since the two numbers do not look alike. One of the students commented that this is just like in languages.   ﬁThe word 'cat' ,ﬂ he said,  ﬁis pronounced differently in Hebrew and in English. When written down, this word does  not look alike in the two languages. Still the words have the same meaning. Thus, we can use the examples from  language in order to explain and make sense of the problem in mathematics.ﬂ   Colleagues from both disciplines showed kin interest in the topic and encouraged us to further investigate it. This  made us ponder on the  nature of mathematics and English as langua ges. As a result, the following questions   www.ccsenet.org/jel Journal of Educati on and Learning Vol. 2, No. 1; 2013  212  emerged: Are there commonalities in the teaching strategies  of English and mathematics? Do teachers perceive  these two disciplines as two languages? What could be the implications of such awareness or lack of awareness?   Could there possibly be any instructional collaboration betw een teachers of English and teachers of mathematics?  How could the awareness of these issues help students who are good at mathematics but find English difficult, and  vice versa?   Thus, the purpose of this study is firstly to discuss the disciplines of English and mathematics as languages, and  secondly, to examine the views of mathematics and English  teachers regarding language fe atures, ways of teaching,  issues of anxiety and collaboration be tween English and mathemat ics teachers. We believe that the way English  teachers perceive the language of English would gene rally influence how it is taught. Similarly, the way  mathematics is taught in class is influenced by the way te achers perceive the nature of  mathematics. Moreover, if  mathematics is perceived as a language by English an d mathematics teachers, they could have common grounds  for collaboration, enriching the  teaching of both disciplines.   1.1 What is Language?  A language is based on a set of rules relating symbols to  meaning which allow the forming of an infinite number of  utterances from a finite number of el ements. The rules are what we call the  grammar of the language, the system  underlying our use of language. These rules are abstract rules of a language which we use to construct our  sentences in speech and wr iting (Freeborn, 1987).   The fact that human language is a learned symbolic  system allows flexibility: la nguage changes, words are  transformed and new words are created. Language evolves in  response to changing historical and social conditions  and people in different places in the world speak divergen t languages. However, the need for social or commercial  communication brought about ﬁlingua francasﬂ, one language used by common agreement. Having this in mind,  let's see how English has become the lingua franca of the whole world (Fromkin & Rodman, 1974).   1.2 The English Language  English is the language for international communication and is nowadays used by more nonnative than native  speakers. English has developed from ﬁthe native language of a small island nation to the most widely taught read   and spoken language that the world has ever knownﬂ (Kachru & Nelson 2001, p. 9). Since the second half of the   twentieth century, the English language has spread around the world to an extent unknown in any other historical   period or for any other language (Jenkins, 2006). It has turned to be the nativized language for what Kachru terms   as the Outer Circle (Kachru, 1985). English also serves as a lingua franca among non-native speakers of English.   Beneke (1991) estimates that about 80  percent of verbal exchanges in which English is used as a second or foreign  language do not involve any native speakers of English.   For the first time in history, a language  has reached truly global dimensions, an d as a consequence, is being shaped,  in its international uses, at least as much by its nonn ative speakers as its native sp eakers (Seidlhofer, 2004). The  globalism of the English language is well described by Brumfit: ﬁThe members of the expanding circle who do use   English are an increasingly significant  group who operate in an increasingly  global economy which has an impact  on the economy in all countries–and the internet, mobile  phones and other technology increasingly establish the  potential for use of English which is quite independent of  the controls offered by traditional educational systems,  publishing outlets and radio/televisionﬂ (Brumifit, 2002, p. 5). The language is also used more and more for   practical purposes by people with varied norms and scop es of proficiency. Thus, En glish as a language, according  to many linguists, is undergoing a process of internationalization and destandarization (Melchers & Shaw, 2003).   The native speaker norms, in light of the internationalization of English, have  been a debated issue for quite some  time. Actually, the 1990s were a revolutionary decade, acc ording to Crystal (2003) due to proliferation of new  linguistic varieties arising out of the world wide implementation of the Internet. The consequences were a public   recognition of the global position of English. English is  now the language most widely taught as a foreign  language-in over 100 countries and in most of these countries it is emerging as the chief foreign language to be   encountered in schools, often displacing another language in the process. Since the 1960s, English has become the   normal medium of instruction in higher education for many countries. About 80 percent of the world's  electronically stored information is currently in English.  Crystal (2003) argues that a global language is particul arly appreciated by the international academic and business  communities. English is the medium of a great deal of th e world's knowledge, especially in areas as science and  technology. The reason why so many nations have in recent years made English an official language, or chosen it   as their chief foreign language in schools, is educational.  The rapid change, the growth in international contacts,  the mobility of people, the ﬁglobal villageﬂ have provided the circumstances needed for a global language: ﬁThere   www.ccsenet.org/jel Journal of Educati on and Learning Vol. 2, No. 1; 2013  213  has never been a time when so many nations were needing to talk to each other so much. There has never been a  time when so many people wished to travel to so  many places. Never has the need for more widespread  bilingualism been greater, to  ease the burden placed on the professional  few: And never has there been a more  urgent need for a global language (p. 14).ﬂ  This has also aroused thoughts among TESOL (Teaching Eng lish for speakers of other  languages) teachers about  the consequences it might have on the conceptualization, development and teaching of English: how far should   students, classroom teachers  and teacher educators conform to native-sp eaker norms (Timmis, 2002)? What is the  best way to prepare learners for international communication if second language pedagogy should not aspire for  intelligibility for native speakers receivers (Jenkins, 2002)?  1.3 The Language of Mathematics  Galileo Galilei said that: ﬁMathematics is the language with  which God has written the universe.ﬂ(Note 1) It is the  language of numbers, symbols notations and grammar. Using numbers and mathematical symbols, one can write   ﬁwordsﬂ and ﬁsentencesﬂ. When appropriate, several ﬁs entencesﬂ together might fo rm a ﬁmathematical storyﬂ  (solutions of exercises, problems, et c.). Mathematics has also grammar Œ th e mathematical logic which determines  whether statements are valid or not. Jamison (2000) is  using linguistic terms to describe mathematics and argues  that not many see mathematics as a la nguage. He suggests that treating math ematics as a language would help to  increase its understanding: ﬁI want to show how making th e syntactical and rhetorical  structure of mathematical  language clear and explicit to students can increase their  understanding of fundamenta l mathematical concepts. –  Regrettably, many people see mathematics as a collection of arcane rules for manipulating bizarre symbols Œ   something far removed from speech and  writing (p. 45).ﬂ Gough (2007) suggests that mathematics is not a natural  language but a formal language, artificially constructed, using our natural everyday language in teaching the   mathematical language. In the document ﬁMaking Mathematics Count™ released by the Department of Education   and Skills of Great Britain, mathematics is described as a ‚powerful universal languageﬂ (Making Mathematics  Count, 2004).   And indeed mathematics is a universal language, since the same numbers and symbols (in most of the cases) are  used around the world by billions of people. If a person  in China would write a letter (written in Chinese) to a  person in the United States it is almost sure that the person  in the United States will not be able to read it. But if,  instead, he will send a simple mathematical equation with its solution, it is almost certain that the person in the  States will be able to ﬁreadﬂ the solution and most probably understand it.  Mathematics as a universal language is important to the m odern society. It is being used in technology, sciences,  business and financial services and at  many workplaces around the world (Mak ing Mathematics Count, 2004). It is  a language taught in school and learned all around the worl d from first to twelfth grade and it is also part of the  activities children are engaged in during pre-school.   Woodin (1995) refers to mathematics  as a language and suggests teaching it as a language to children with learning  disabilities. He even compares math to English and says  that: ﬁMath may be viewed as a language Œ a simpler,  more consistent and more regular language than English.  This is especially the case with math facts. Numbers  represent nouns, while operational signs (+, -, x, /, =) serve as verbs. Both components are governed by rules of   syntax. Math facts, such as 2 x 3 = 6, may be thought of as math sentences. Students should be encouraged to speak   in complete sentences, to convey an entire thought, and  to develop a consistent rehearsal pattern for the math  fact–. Math at the simple sentence fact level is a mu ch easier language than English. Although math has an  infinite number of nouns, it has only five verbs (+, -, x, /, =) associated with four basic operations. Some students,   however, may need to have the syntax, as well as the codi ng (place value) and number theory, explicitly taught to  them.ﬂ   Mathematics is also a language of communication in our everyday life. Newspapers and television broadcasts, for  example, include graphs, percentages and other mathematical concepts to convey important information.   Mathematics as a language of communication is also emph asized in mathematics curricula around the world. For  example, The NCTM (National Council of  Teachers of Mathematics) in the United States refers to mathematics as  a language of communication in The Standards for School Mathematics (NCTM, 2000) and states that   ﬁInstructional programs from pre-kindergarten through grade 12 should enable all students to: communicate their   mathematical thinking coherently and clearly to peers, an d others and use the language of mathematics to express  mathematical ideas.ﬂ  The language of mathematics and mathematical skills are an integral part of our  everyday life. Small children have  in their lexicon phrases such as, "3 candies", or "one half". We use mathematical language in recipes when we   www.ccsenet.org/jel Journal of Educati on and Learning Vol. 2, No. 1; 2013  214  describe ingredients (200 grams of flower or 3/4 cup of sugar). During sales prices go down by 20 percent. Thus  we all actually speak ﬁmathematicsﬂ without even being aware of it.  Mathematics as well as English as a second language do  not develop naturally as a child develops a natural  language, they need to be learned. Learning entails co nscious practice which adheres to ﬁskill learning theoryﬂ  (Dekeyser, 2007): providing concise rules which the lear ner can rehearse; offering ab undant repetition and drilling;  providing open-ended activities to practice the skill in  a wider applicability range. Classroom activities in  mathematics and English should engage learners in  authentic, real-life functi onal use of the language.  Thus English and mathematics as languages are similar and yet different. They both are languages of signs and  symbols which combine to ﬁwordsﬂ ﬁsentencesﬂ and ﬁstories ﬂ. These ﬁstoriesﬂhave a gr ammar, which is universal  and hence makes them languages of international communication. Albeit these similarities, there are some   principal differences. The language of mathematics for example, is precise and less flexible - it cannot afford   ambiguity, while natural languages contain constructive ambiguity derived from their cultural and contextual   wealth of meanings. Having this in mind, we wondere d whether these commonalities an d differences are part of  teachers™ beliefs and how they coul d affect the teaching strategies.  1.4 Teachers™ Beliefs and Ways of Teaching   Numerous studies on the relationships  between teachers™ beliefs and classroom  practices have been conducted in  the last decade (Clark & Peterson, 1986; Tillema, 2000; Borg, 2006; Fulton, 2012). Horwitz (1985), Roberts   (1998), Kern (1995) and others suggest, for example, that  pre-service teachers™ beliefs  about language learning  originate from their second language learning experiences, particularly in secondary school. Connely and   Clandinin (1994) also note that teachers™ personal valu es and beliefs are very much shaped by their personal  experiences. For example, teachers wh o believe that language learning  means memorizing grammar might spend  most of their time on these activities (P eacock, 2001). Johnson (1 994) found that pre-serv ice teachers™ acceptance  or rejection of the content of their t eacher preparation courses appeared to re st on their prior formal and informal  language learning experiences . Wood and Bennet (2000) argue that beliefs  assumptions and knowledge of teachers  affect the way a teacher interprets  teaching events and hence the teaching  decisions that are made. This is  supported by later studies by Tsui (2003) pointing out how  teachers™ personal assumptions, values and beliefs filter  through to their classroom practices and the way they define problems and manage dilemmas in the classroom.   Keys (2005) demonstrated that teacher s often had two sets of beliefs, those  they exhibited in the classroom and  those they stated but never followed th rough within the cl assroom. Many teachers needed ongoing support as they  attempted to implement a new practice that was not aligned with their beliefs.   Studies by Ng and Farrell (2003) found evidence to sugge st that what teachers say  and do in the classroom is  governed by their beliefs. In a previous study, Farrell (1999 ) examined the beliefs system  of pre-service teachers of  English grammar in terms of its influence on teaching practice and found evidence to suggest that these beliefs   may be resistant to change. Similarly, Richards, Gallo, and Renandya (2001) discovered that although many   teachers stated they followed a communica tive approach to teaching, they held  strongly to the belief that grammar  is central to language learning. Thus , in understanding teachers™ classroo m practices and the knowledge embodied  in these practices, it is important to understand their c onceptions of teaching and learning and the sources of  influence that shape such conceptions. This reinforces previous studies suggesting that it may be particularly  important to help trainees reflect on their beliefs (Brown & Mcgannon,1998).   Goldin, Rosker and Torner (2009) claim that ﬁbeliefs now  constitute ‚a no longer hidden variable™ in research on  the teaching and learning of mathematicsﬂ  (p. 14). Findings suggest that teacher s™ beliefs regarding the nature of  mathematics are significant factors in the way they teach  mathematics in class (Cross,  2009; Handal, 2003; Spitek  et al., 2001). Many mathematics teachers c onceive of mathematics as a static bo dy of knowledge, involving a set of  rules and procedures that are applied to yield one ri ght answer (Thompson, 1992; Nisbet & Warren, 2000).  Inquiry-oriented mathematics educators take a more  dynamic view of mathematics conceptualizing it as a  discipline that is continually undergoing change and revision (Prawat, 1992). Handal (2003) in his review of   teachers™ mathematical beliefs argues  that a large number of teachers st ill perceive mathematics in traditional  terms dominated by a-priori rules and procedures. Zakaria and Musiran (2010) investigated beliefs among one   hundred pre-service teachers. They found that many of th e teachers believe that mathematics is a formal way of  representing the real world and that it should be taught as a collection of skills and algorithms.  It is interesting to note that studies on teachers' beliefs regarding the nature of mathematics have no explicit  reference to mathematics as a language . Exposing teachers™ beliefs toward  mathematics and English as languages  is important for understanding the influence, if any, of thes e beliefs on the way they teach in class. This might also  open channels for collaboration between mathematics and English teachers.   www.ccsenet.org/jel Journal of Educati on and Learning Vol. 2, No. 1; 2013  215  1.5 Anxiety  English and mathematics are considered core subjects and are highly regarded by students, parents and educational  systems in our country, as in many other countries. Math ematics and English are the ga te keepers to universities,  either in the form of psychometric  exams, or as threshold standards  required for admission into academic  institutions. The consequences are that parents and school st akeholders are ready to invest much effort in order to  promote success in these two subjects.  But for some students these subject s are accompanied by a feeling of  anxiety.   Mathematics has a reputation of one of the most difficult subjects in school and many students suffer from math  anxiety. Mathematics anxiety is expressed by a feeling of fear, tension and panic when asked to perform   mathematical tasks. Many students in  school and later as adu lts suffer from math anxiety. Wigfield and Meece  (1988) identified two components of mathematics anxiety,  affective and cognitive: ne gative affective reaction to  mathematics, such as nervousness, fear and discom fort and worries about doing well in mathematics.  Jackson and Leffingwell (1999) examined the type of  teacher's behavior which might create or exacerbate  mathematics anxiety in students. They asked prospective el ementary mathematics school teachers to describe their  worst or most challenging mathematics classroom experience from kindergarten through college and found that for   some, math anxiety occurred as early as 3 rd grade of elementary school. Res earch studies suggest that the way  teachers teach mathematics is the key to reducing mathemat ics anxiety (Arem, 2010; Rayner et al., 2009; Scarpello,  2007; Ashcraft, 2002).   In the last 20 years there has also been a great deal of research into second language anxiety (Woodrow, 2006). It  has been regarded as one of the most important affective factors that influence language acquisition (Na, 2007).   Much earlier Young (1991) id entified six potential causes of language an xiety which related to learners, teachers  and instructional practice. He mainly pointed at personal and interpersonal aspects, learner beliefs about language   learning, teachers™ beliefs about language teaching, teach er-learner interaction, cla ssroom procedure and tests.  MacIntyre and Gardner (1995) contend that anxiety might increase as a result of a learner™s bad learning   experience or continued bad learning performance. It can also be caused by the need to perform in a language other  than one™s native tongue (Jackson, 2002).  In a study investigating the relationship between anxiety and achievement in language among Israeli seventh grade  students it was found that anxiety was negatively and signifi cantly correlated to FL achievement on all FL Tests.  Also, the teachers™ attitudes to teaching as perceived by  the students indicated a significant prediction of L2  anxiety (Abu-Rabia, 2004).  2. Design of Study   The study is exploratory and part of a larger study investigating different aspects of English and mathematics as  two languages. The present study explor es what teachers of both disciplines  think of mathematics and English as  languages. Our research questions focused mainly on th e similarities and differences between the two languages as  perceived by the teachers, pertaining to ways of teachin g and anxiety. We were also  interested to know about  teachers™ opinion on the feas ibility of collaboration between teachers of th e two disciplines. We found interviews  as the most suitable way to achieve our aims as they en able the respondents to demonstrate their unique way of  looking at the phenomenon of investigation and to provid e their definition of the situation (Silverman, 1993). Thus  the study employs an inductive approach (Bogdan & Bikl en, 2003) in order to gain  insights on the teachers™  perceptions of mathematics and English as languages.   Four in-depth interviews with five  teachers studying at a teacher educa tion college are presented. All five  participants are the authors™ students in  different courses: two are mathematic s teachers, three are English teachers.  One of the English teachers used to be a mathematics t eacher. They are all in-service teachers and study at the  college for different degrees: three for an M. Ed degree and two for a teaching certificate.  Both researchers conducted th e interview according to a set of predetermined themes not organized in a particular  sequence. The interviews were conducted  in the office of one of the authors.  Each interview lasted between 40-50  minutes. One joint interview was conducted with two of the five teachers assuming that a discussion will develop   and the interviewees™ interaction between each other would yield insights that might not otherwise have been  available in a straight forward interview. This interview lasted 60 minutes.   All interviews were recorded and transcribed. Transcrip tions were read by the two researchers independently in  order to obtain a holistic view for emergent themes related to English and Mathematics as languages. The   independent reading was followed by discussions between the researches in order to arrive at common grounded   www.ccsenet.org/jel Journal of Educati on and Learning Vol. 2, No. 1; 2013  216  themes. The interviews are described in a narrative style in order to maintain the holism of the interview and also to  capture the dynamics within the interviews (especially, the joint interview).   2.1 Interview with Neal  Neal is a mathematics teacher with 30  years of teaching experience. He teaches  in the upper grades of elementary  school. The first question addressed to Neal was the possi ble relationship between mathematics and English. Neal  hesitated for a second and said that he never thought about it. We further probed whether mathematics and English   are both languages, he replied: ﬁMathematics to me is the  language of all languages. It is the language of reality.  However, while English is a language with an element of  emotion–a means by which reality is reflected by words  of sentiment– mathematics describes reality in an objective wayﬂ.   Neal noted that although English an d mathematics are both languages they  differ from each other. He described  throwing a stone as an example. ﬁIn mathematics this will be described saying that the stone has a parabolic   trajectory. In English this can be described by saying that the stone cut through the blue sky disappearing behind   the mountains.ﬂ He added that different languages have different emotional elements, suggesting that Shakespeare   would not be the same when translated into other languages; mathematics however, seems to be the same   everywhere. ﬁIf mathematics is the same everywhere™ we  asked, ‚isn™t it more of a communication language than  English?ﬂ Neal explained that mathematics might be compared to the language spoken by all people before the   ﬁTower of Babylonﬂ after which each created their own language and creativity flourished. Neal mentioned the   issue of creativity and said that in English creativity is  expressed by the variety of interpretations that can be  attributed to the same concept. He gave the example that ﬁa rose is a rose is a roseﬂ can have multiple   interpretations, but in mathematics there is rigidity: ﬁ3 is 3 and 3+4 will always be 7ﬂ. It becomes creative when  mathematics interprets reality by a ﬁword problemﬂ.  To the question concerning students™ anxiety in English an d mathematics, Neal said that mathematics causes much  more concern among students than English. He explained that the stronger the ﬁemotional elementﬂ involved in the   subject, the lower the anxiety level. He further explained that every language has its music and you just "hearﬂ   when a sentence is correct or incorrect  by its musicality. The same applies to mathematics: ﬁmathematics has its  own music; the right thinking is th e musical thinking– A child who can ﬁcat chﬂ this musicality, who can ﬁhearﬂ  the logic of mathematics is good at math, but if they  cannot identify connections and relationships, they are  doomed to much difficulty.ﬂ To make himself more explicit, he added that ﬁmusicality of mathematicsﬂ means to   understand the reality. For example: ﬁto understand ‚ratio ™, is music– the relationship between two tunes is a  mathematical ratio and the more beautiful the harmony  between two tunes, the more mathematical ratios are  created.ﬂ  Neal seemed to be taken by surprise at the question concerning collaboration between English and mathematics  teachers and said that it never occurred to  him, albeit the logicality of it. He al so highlighted similarities in the way  the two languages are taught and explained that a child learns his mother tongue by listening and speaking. English   taught in the first grades is very much focused on listeni ng skills, and only afterwards th e child learns the grammar.  Similarly, in math, one first learns how to solve problems and only later the mathematical logic behind it. In music  it is quite the same, you first learn the song and then you decompose the melody.   2.2 Interview with Sean   Sean is a mathematics teacher with eight years of expe rience in elementary school. He did not hesitate for a  moment when we asked about English and mathematics  as languages: ﬁthey are both interesting international  languages existing from the beginning of history, used in everyday lifeﬂ. His concern was about the way it is   taught. He said that it is taught in a manner which  is detached from reality detracting from its beauty and  usefulness. His personal belief is that ﬁchildren need to understand that mathematics embodies meaning, it needs   to be taught in context. One cannot escape the fact that it is part of the media, the culture around us–it is a   language containing grammar just like any other languageﬂ. He further explained that mathematics is meaningful   and creative: ﬁwhen you describe the classroom as a th ree dimensional shape, you are actually telling a story.  Mathematics can be a joy and make children wonder and curious about things around them. It all depends on  how you teach it.ﬂ   As far as anxiety towards mathematics is concerned, Sean  criticized the educational system and asserted that it  had failed: ﬁteachers teach for the test  and destroy the ‚wonder™ of mathema tics. It raises antagonism, unhealthy  competition and high level of anxiety am ong students. Teaching mathematics in  this way, is not teaching it as a  language should be taught. Students are engrossed with mechanical exercises and do not see the story in the   numbers and formulasﬂ. He compared it with teaching  English: ﬁwe have two teachers one is teaching the  language showing its usefulness in life, the other bombar ds the students with gramma r exercisesﬂ. The teachers,   www.ccsenet.org/jel Journal of Educati on and Learning Vol. 2, No. 1; 2013  217  he said, also play an important role in forming a positive stance to the subject. His opinion is that the fact that  English and mathematics are co nsidered the most important subjects in sc hool causes a lot of social pressure on  the students. In his school there is  no collaboration between the Engl ish and mathematics teachers but the  questions provided some food for thought.  2.3 Interview with Nina  Nina is an English teacher in elementary school and  has 12 years of teaching experience. She has no doubts  about the logical connection between English and mathema tics. ﬁThey are definitely languages,ﬂ she said, ﬁin  both languages there are patterns and rules. They are  both taught through visualizations and illustrations.  Teaching vocabulary is like teaching fractions. You need to  illustrate in both languages and practice a lot. First  you drill and then exercises become more meaningful until you gain autonomy and speak the language  automatically. Thus the sequence in both languages is cumulative and incremental.ﬂ  She believes that anxiety might be caused by a traumatic e xperience. ﬁThere are lots and lots of rulesﬂ, she says,  ﬁand one needs to learn them or even  memorize them. This is not easy for some learners.ﬂ She also mentions  social pressure, mainly from parents. These are two subj ects that open doors on the employment market. Parents  put a lot of pressure. They see them as the key to success.  Regarding collaboration, Nina  notes that in her school  there is no collaboration between  English and mathematics teachers.  2.4 Sue and Ann (Joint Interview)  Sue teaches English as a foreign langu age in elementary school. She has  eight years of teaching experience.  Hebrew is her first language and English is her foreign language. She used to teach Hebrew and Mathematics in   a school in the United States. Ann teaches English as a  foreign language in elementary school .English is her  native language and Hebrew is her second language. She also speaks fluently French.  As a teacher of both languages, Sue could immediately  see the similarities between English and mathematics.  ﬁThe analytical thinking, she said, is applicable to both  languages. The left hemisphere helps learn grammatical  structures in English just like it helps learning formulas  in mathematics.ﬂ Ann moved uncomfortably in her chair,  ﬁI disagreeﬂ, she says ﬁIt did not work for me and many others I know. I am very good at languages; I speak   fluently three languages and learned them quite easily. I almost failed mathematics. Sue explained that certain   skills in English, such as, learning new vocabulary or l earning grammatical structures, are a sort of mathematics.  Some pupils would look at a ‚problem solving™ activity  as a problem from real life  and not as a mathematical  problem.ﬂ She then provided  an example from her own learning experien ce: ﬁI remember when I learned math in  school I used to look at the mathematical problems as a  puzzle, or even a game. This is why I enjoyed math. The  language, serves as a means to express id eas and turn the problem into a game.ﬂ  Ann was hesitant and said: ﬁWell–this sounds quite reasonable. If I see a math problem, I will not be able to  solve it even though I understand the words. The same is when I encounter a language I do not speak or read. If I   do not have the grammar and the literacy, I will not be ab le to read or understand. ﬂ Concerning ﬁanxietyﬂ Ann  added that this is something that comes from home: ﬁIf you have good grades in these two subjects, you are on   the road! All the rest receives less esteem .ﬂ Sue thought that anxiety develops  later when students realize that it  is not as easy as they thought it would be. Ann said th at the negative attitude develops due to failure or an  unpleasant experience with a particular teacher. She explained that in languages or math it is inborn: ﬁone can   have a mathematical talent and succeed  without exerting too much energy. In  history, for example, one can make  some more effort and do well.ﬂ Ann seemed to be convinced that mathematics is a language after all.  This led us to the question on the t eaching strategies of the two subjects.  Ann was probably still hesitant due to  her ﬁrevelationﬂ that mathematics is a language and ch ose not to react. Sue believed that teaching mathematics  and teaching English is, in a sense, similar: ﬁIt is presen ted through real life situations, just like in English. If I  want to present a math problem of percentage, for exampl e, I will tell my pupils that they are going on holiday  and have a certain amount of money. They have to plan  the expenses by percentage. This is something taken  from their own life. It is real and not a mere math problem.ﬂ   Collaboration between math and English teachers was a bit too overwhelming for both Ann and Sue. Ann was  surprised: ﬁit seemed illogical before the interview and I would have never thought of it. Now, I am quite   convinced we can learn from each othe r. Sue™s final comment was that it  brought back memories from her  teaching math. When she started teaching English, she comple tely shut off from math. Now, we gave her an idea.  Maybe she could feed on some of th e ideas she used in teaching math.ﬂ     www.ccsenet.org/jel Journal of Educati on and Learning Vol. 2, No. 1; 2013  218  3. Discussion  3.1 Interpreting the Interviews  The five teachers seemed to be enthused by the topic.  Linking mathematics and English opened new ways of  thinking about their teaching. Their views as displayed in the interviews were based on their beliefs concerning the  two disciplines, and on their personal  experiences as teachers and students.  All teachers, except Ann, related to  mathematics as a language. Ann™s (English teacher) initial belief was that  mathematics is not a language, but then the interaction  with Sue convinced her otherwise. Several characteristics  of languages were mentioned by the teachers, most of  them supported by the literature: English and mathematics  are both universal languages ((Making Mathematics Count, 2004); they are languages of communication (Kachru   & Nelson, 2001); they both contain rules and structures  and share analytical thinking  (Dekeyser, 2007). Neal  added one more insight, suggesting that both English and  mathematics describe reality, however, descriptions in  mathematics are more objective; while in English descriptio ns are more subjective using emotive discourse. Hence,  a ﬁsentenceﬂ in mathematics has always one interpretation  (3=3), while a sentence in English might have different  interpretations.   The teachers expressed their beliefs a bout the way mathematics and English ar e taught. Two main aspects emerged:  Using real life situations as a way of teaching and the as pect of rules and logic. Re garding the first aspect, one  mathematics teacher was concerned that  many teachers teach mathematics in  a technical way, detached from  reality. Regarding the second aspect, English and mathematic s were compared in the sense that in learning English  one learns first to speak and only then learns the grammar, and in mathematics one first learns to cope with  problems and then learns th e logic that underlies them.   The metaphor of music was used to compare between the two languages, explaining that both English and  mathematics have their unique music. One needs only to ﬁcatch the musicalityﬂ of the sentence or the formula to   know whether it is correct or not.  Teaching vocabulary was compared to  teaching fractions as both entail  illustrations and practice.   Teachers™ views on anxiety acco rd with some as pects mentioned in the literature. Firstly, they agreed that anxiety  exists in both English and mathematics, though it might be more acute in English due to the element of emotion   which might reduce the level of anxiety. Teachers™ be havior (Arem, 2010) and bad learning experiences  (MacIntyre & Gardner1995) were mentioned as a source of anxiety. The need to learn many rules, to solve   mathematical problems or perform in a language different  from one™s own, were mentioned as reasons for failure  and anxiety. The aspect of social pressure by parents a nd education institutions was strongly emphasised as a cause  for tension.  All interviews clearly showed  that in spite of the fact  that teachers highlighted co mmonalities between English and  mathematics they were quite surprised at our question co ncerning possible collaboratio n between teachers of both  disciplines. This might be due to the fact that in th e educational system English and mathematics are commonly  conceived as two completely distinct disciplines which do not seem to have anything in common. Consequently,   English and mathematics teachers would us ually not find any reasons to collabo rate and discuss issues related to  teaching and learning  the two subjects.  3.2 Emerging Insights  The study set out to explore teachers™ perceptions of En glish and mathematics as two languages. Five teachers,  including two mathematics teachers, two English teachers a nd one English teacher who us ed to teach mathematics  in the past, were interviewed. The interviews exposed  some interesting insights concerning communal features  between English and mathematics, and raised practical qu estions pertaining to teach ing in schools and teacher  education.   The interviews with the teachers made us wonder: what  could be the mutual contribution of collaboration between  teachers of English and teachers of mathematics? Could  such collaboration encourage  sharing of ideas and thus  enhancement of professional development of teachers? Th e interviews also reinforce the contention that it is  important to understand the influences  that shape teachers conceptions about  teaching and learning and it might be  useful to help teacher to reflect on their beliefs (Bro wn & Mcgannon, 1998). Ann st arted off with a firm belief  based on her own experience that mathematics could not be a language since she was very good at learning   languages, but never took to mathematics. In the course  of the interview, her views were challenged when she  realized that there are similarities between the two disciplines and mathematics might indeed be a language. Sue,   as well, was surprised at her own state of mind when she reali zed that in spite of the fact that she considered both as  languages, she never thought of the si milar teaching strategies she herself us ed being an English and mathematics   www.ccsenet.org/jel Journal of Educati on and Learning Vol. 2, No. 1; 2013  219  teacher. Thus, the interviews unveiled  predispositions and values that have not been challenged before and  provided food for thought that might have  an impact on the teachers™ teaching.   We believe that teachers from the two disciplines could fe ed on each other™s knowledge and reflect to the students  the possible links between mathematics and English as two languages. The students who are strong in one   discipline could be encouraged to try and get better at the weaker one. The modeling of collaboration could reduce   the level of anxiety for those students who have difficultie s in these disciplines. The re-framing of Ann™s beliefs  about mathematics as a language in the course of the in terview could also happen to pupils who are concerned  about failing mathematics. As teacher educators it provided food for thought concerning our teacher education   programmes in our respective  faculties. Gilles and Wilson (2 004) suggest that teachers need to reflect critically on  their beliefs and dissonance between beliefs and actions. They need opportunities to learn about, discuss and   reflect upon how their knowledge is put into practice (Helmann, 2006). Could we enhance this notion by  collaboration between our English and mathematics faculties in our college?   This initial small scale study emerged fr om our mutual professional practice  as teacher educator s. It explored  teachers™ beliefs concerning English and mathematics as  two languages. Our intention  is to further explore the  issue in a wider population of teachers from elementa ry and secondary schools. Our interests will focus on  thinking processes in problem solving in the two languages. This will entail observations of English and   mathematics lessons in schools to iden tify unique teaching strategies of both  subjects. As teacher educators we  believe that insights from the study pose challenges for thinking outside the box and enhance interconnectivity and  expansion of traditional boundaries (Tsui & Law, 2007).  References   AbuŒRabia, S. (2004). Teacher s™ role, learners™ gender differences, an d FL anxiety among se venth-grade students  studying English as a FL.  Educational Psychology, 24 (5), 711-721.  http://dx.doi.org/10.1080/0144341042000263006  Arem, C. (2010).  Conquering Math Anxiety  (3rd ed)  Brooks: Cole Cengage Learning Inc..  Ashcraft, M. H. (2002). Math anxiety: Personal, educational and cognitive sequences.  Current Directions in  Psychological Science, 11,  181-185. http://dx.doi.org/10.1111/1467-8721.00196  Beneke, J. (1991). English as a lingua franca or as a medium of intercultural communication. In R. Grebing (Ed.),  Grenzenloses Sprachenlernen  (pp. 54-66). Berlin: Cornelsen.  Borg, S. (2006). The  distinctive characteristics of  foreign language teachers.  Language Teaching Research, 10,  3-31.   Bogdan, R. C., & Biklen, S. K. (2003).  Qualitative Research in Education . Boston, MA: Allyn & Bacon . http://dx.doi.org/10.1191/1362168806lr182oa  Brown, J., & Mcgannon, J. (1 998). What do I know  about language learning? The story of the beginning teacher.  23rd ALAA Congress. Retrieved fr om http://www. Cltr.uq.edu.au /alsaa/proceed/bro-mcgan.html  Brumfit, C. H.  (2002).  Global English and language teaching in the twenty-first century . Centre for Language in  Education Occasional Papers No. 59, University of Southampton.  Clark, C. M., & Peterson, P. L. (1986). Teachers™  thought processes. In  M. C. Wittrock (Ed.),  Handbook of  Research on Teaching  (3rd ed, pp. 255-296). New York: Macmillan.  Connely, F. M., & Clandinin, D.  J. (1994). Telling teaching stories.  Teacher Education Quarterly,  21(1), 145-158.  Cross, D. I. (2009). Alignment, cohesion, and change:  Examining mathematics teachers' beliefs structures and  their influence on instructional practices.  Journal of Mathematics Teacher Education, 12,  325-346.  http://dx.doi.org/10.1007/s10857-009-9120-5  Crystal, D. (2003).  English as a Global Language  (3rd ed). Cambridge: Cambridge University Press.  http://dx.doi.org/10.1017/CBO9780511486999  Dekeyser, R. (2007).  Practice in a Second Language: Perspectives from Applied Linguistics and Cognitive  Psychology. Cambridge: Cambridge University Press.  Farrell, T. S. C. (1999). The Reflective assignment: Unlo cking pre-service English teachers' beliefs on grammar  teaching. RELC Journal, 30 (2), 1-17. http://dx.doi.org/10.1177/003368829903000201  Freeborn, D. (1987).  A Course Book in English Grammar . Macmilan Education LTD: London.  Fromkin, V., & Rodman, R. (1974).  An Introduction to Language . Holt, Rinehart and Winston, Inc.
Second language anxiety	Language acquisition
Provide Stanford / universal dependency relationships for the following sentence (list the dependency relations with their terms, including the root; don’t worry about morphology):	[the relations need not be formatted precisely as below) root(slipped) nsubj(slipped, linguist) dobj(slipped, pun) amod(linguist, cunning) advmod(slipped, slyly) det(cunning, the) det(pun, a) det(problem, the) nmod(into, problem) advcl(slipped, into)	Which of the following dependency relations is incorrect?	I sent him a message. dobj(sent, him)	We ran around the block: advcl(run, around)|||  Elizabeth is running for president: aux(running, is) |||I want him to win: ccomp(want, win)	Universal / Stanford dependency relations||slides||UniversalStanfordDependencies:Across-linguistictypology Marie-CatherinedeMarneffe  ,TimothyDozat ? ,NataliaSilveira ? , KatriHaverinen  ,FilipGinter  ,JoakimNivre / ,ChristopherD.Manning ?   LinguisticsDepartment,TheOhioStateUniversity ? Linguisticsand  ComputerScienceDepartments,StanfordUniversity  DepartmentofInformationTechnology,UniversityofTurku / DepartmentofLinguisticsandPhilology,UppsalaUniversity Abstract RevisitingthenowdefactostandardStanforddependencyrepresentation,weproposeanimprovedtaxonomytocapturegrammatical relationsacrosslanguages,includingmorphologicallyrichones.Wesuggestatwo-layeredtaxonomy:asetofbroadlyattested universalgrammaticalrelations,towhichrelationscanbeadded.WeemphasizethelexicaliststanceoftheStanford Dependencies,whichleadstoaparticular,partiallynewtreatmentofcompounding,prepositions,andmorphology.Weshowhow existingdependencyschemesforseverallanguagesmapontotheuniversaltaxonomyproposedhereandclosewithconsiderationof practicalimplicationsofdependencyrepresentationchoicesforNLPapplications,inparticularparsing. Keywords: dependencygrammar,StanfordDependencies,grammaticaltaxonomy 1.Introduction TheStanfordDependencies(SD)representation(de Marneffeetal.,2006)wasoriginallydevelopedasaprac- ticalrepresentationofEnglishsyntax,aimedatnaturallan- guageunderstanding(NLU)applications.However,itwas deeplyrootedingrammaticalrelation-basedsyntactictra- ditions,whichhavelongemphasizedcross-linguisticde- scription.Faithfulnesstotheseoriginswasattenuatedby desideratafromourNLUapplicationsandthedesirefora simple,uniformrepresentation,whichwaseasilyintelligi- blebynon-experts(deMarneffeandManning,2008).Nev- ertheless,itisreasonabletosupposethattheseadditional goalsdonotdetractfromcross-linguisticapplicability. Inthispaper,weattempta(post-hoc)reconstructionof theunderlyingtypologyoftheStanfordDependenciesrep- resentation.Thisnotonlygivesinsightsintohowtheap- proachmightbeappliedtootherlanguages,butalsogives anopportunitytoreconsidersomeofthedecisionsmadein theoriginalscheme,aimingtoproposeanimprovedtaxon- omy,evenforEnglish.Wesuggestataxonomywhichhas atitscoreasetofverybroadlyattestedgrammaticalrela- tions,supplementedasneededbysubtypesforlanguage- particularrelations,whichcapturephenomenaimportantto thesyntaxofindividuallanguagesorlanguagefamilies. Weattempttomakethebasiccoremoreapplicable,both cross-linguisticallyandacrossgenres,andmorefaithfulto thedesignprinciplesindeMarneffe&Manning(2008). Weconsiderhowtotreatgrammaticalrelationsinmorpho- logicallyrichlanguages,includingachievinganappropri- ateparallelismbetweenexpressinggrammaticalrelations byprepositionsversusmorphology.Weshowhowexisting dependencyschemesforotherlanguageswhichdrawfrom StanfordDependencies(Changetal.,2009;Boscoetal., 2013;Haverinenetal.,2013;Serajietal.,2013;McDon- aldetal.,2013;Tsarfaty,2013)canbemappedontothe newtaxonomyproposedhere.Weemphasizethelexicalist stanceofbothmostworkinNLPandthesyntactictheory onwhichStanfordDependenciesisbased,andhenceargue foraparticulartreatmentofcompoundingandmorphology. Wealsodiscussthedifferentformsofdependencyrepre- sentationthatshouldbeavailableforSDtobemaximally usefulforawiderangeofNLPapplications,convergingon threeversions:thebasicone,theenhancedone(whichadds extradependencies),andaparticularformforparsing. 2.Aproposeduniversaltaxonomy Table1givesthetaxonomyweproposefortheuniversal grammaticalrelations,withatotalof42relations.These relationsaretakentobebroadlysupportedacrossmanylan- guagesinthetypologicallinguisticliterature. 1 2.1.Therepresentationbuildsonlexicalism Anunder-elaboratedpartofthedesignprinciplein (deMarneffeandManning,2008)isthatSDadoptsthe lex- icalisthypothesis insyntax,wherebygrammaticalrelations shouldbebetweenwholewords(or lexemes ).Thereisa longstanding,unresolveddebateinlinguisticsbetweenthe- orieswhichattempttobuildupbothwordsandphrases usingthesamecompositionalsyntacticmechanisms(and inwhichthenotionofawordhasminimalprivilegedex- istence)versusthosetheorieswherethewordisafunda- mentalunitandwhichseethemorphologicalprocessesthat buildupwordsasfundamentallydifferentfromandhid- dentothosethatbuildupsentences,sometimestermedthe lexicalintegrityprinciple (Chomsky,1970;Bresnanand Mchombo,1995;Aronoff,2007).Forapracticalcompu- tationalmodel,therearegreatadvantagestoalexicalistap- proach.However,thereremaincertaindifcases,such ashowtodealwithcertain clitics (ZwickyandPullum, 1983),phonologicallyboundwordswhichbehavelikesyn- tacticwords(wefollowmanytreebanksinseparatingthem aswordsandhavingthemparticipateinthesyntax)and howtotreatwordsthataresplitinuneditedwriting(see section2.4.). 1 Thisisnottosaythatalllanguageshaveallthesegrammatical relations.E.g.,manylanguages,fromEnglishtoChiche ‹ wa,allow verbstotakemorethanoneobject,butotherlanguages,suchas French,donot.Nevertheless, iobj isbroadlyattested.  Coredependentsofclausalpredicates NominaldepPredicatedep nsubjcsubj nsubjpasscsubjpass dobjccompxcomp iobj Non-coredependentsofclausalpredicates NominaldepPredicatedepword advcladvmod neg nmodncmod Specialclausaldependents NominaldepAuxiliaryOther vocativeauxmark discourseauxpasspunct explcop Coordination conjcc Noundependents NominaldepPredicatedepword nummodrelclamod apposdet nmodncmodneg Compoundingandunanalyzed compoundmwegoeswith nameforeign Case-marking,prepositions,possessive case Loosejoiningrelations listparataxisremnant dislocatedreparandum Other Sentenceheaddependency rootdep Table1:DependenciesinuniversalStanfordDependencies. Note: nmod , ncmod ,  ,and neg appearintwoplaces. 2.2.Dependentsofclausalpredicates Thesystemofclausaldependentsmostcloselyfol- lowsLexical-FunctionalGrammar(LFG)(Bresnan,2001). However,thetaxonomydiffersfromLFGinseveralre- spects.First,thecleardistinctionbetweencorearguments andotherdependentsismade,butthedistinctionbetween adjunctsandobliquearguments(Radford,1988)istaken tobesufsubtle,unclear,andarguedoverthatitis eliminated. 2 Second,themodelrevertstothetraditional grammarnotionofdirectobjectandotherobjects.Incases 2 TheoriginalPennTreebankannotatorsalsodecidednottotry tomarkargumentsvs.adjuncts(Tayloretal.,2003).Conversely, thePennChineseTreebankdoestrytomakeanadjunct/argument distinction(Xueetal.,2005)andeffectivelyPropBank(Palmer etal.,2005)addsanargument/adjunctdistinctionoverlaytothe EnglishPennTreebank. ofmultipleobjects,thetheme/patientargumentisthedirect object.Finally,thetaxonomyaimstoclearlyindicateinthe dependencylabel(i)anon-canonicalvoicesubject(where theproto-agentargumentisnotsubject,i.e.,inpassives) and(ii)whetherdependentsarenounphrase(NP)argu- mentsversusintroducinganewclause/predicate.Adesign goalofSDhasbeentodistinguishindependencynames whereanewclauseisbeingintroduced(andsowedistin- guish nsubj and csubj ; dobj and ccomp ,butalso advmod and advcl ).WefollowLFGinincludingadistinctionbe- tween ccomp and xcomp forclausalcomplementsthatare standalone(haveaninternalsubject)versusthosehaving necessarycontrol(omission)ofthedependentpredicate's subject(haveanexternalsubject). 3 Otheraspectsofthetypologyarelessregularbutstill important.Thenon-coreclausaldependentsareallmodi- Thedistinctionbetweenafulladverbialclause advcl andaparticipialorveclause  issimi- larbutnotexactlyparalleltothe ccomp / xcomp distinction. 4 Clauseheadshavemanyotherspecialdependents,includ- ingperiphrasticauxiliaries,markers(complementizersand subordinatingconjunctions)aswellasvocativesanddis- courseelements(like well or um ).Conjunctionscombine elementsofmanycategories.UndertheSDdesignprinci- ples,the conj relationconnectslexicalheads. 2.3.Treatmentofcopulas SDhasadvocatedatreatmentofthecopulaﬁbeﬂwhereit isnottheheadofaclause,butratherthedependentofalex- icalpredicate,asin(1a).Suchananalysisismotivatedby thefactthatmanylanguagesoftenoralwayslackanovert copulainsuchconstructions,asintheRussian(1b).Similar constructionsariseeveninEnglishifweconsiderso-called raising-to-objectorsmallclauseconstructions.Underthe basicanalysisproposedforSD,thepredicatecomplement isnotlinkedtoitssubjectargument,butintheenhanced representation(seebelow),thelinkageisthenparallelto thetreatmentinazerocopulalanguage,asin(1c). (1)a. Ivan is the best dancer nsubj cop det amod 3 Thelatterisusedtorepresentcontrolandraisingconstruc- tions,includingcasesofraising-to-objectorso-calledﬁexcep- tionalcasemarkingﬂ.Thetreatmentofthislastgroupofphenom- enaisoneofthelargestsubstantivebreakswiththePennTree- bankannotationtradition,whichfollowsChomskyanapproaches fromtheextendedstandardtheory,Government-BindingTheory (Chomsky,1981)etseq.,intreatingsuchconstructionsashaving acompleteclausalcomplementwithﬁexceptionalcasemarkingﬂ, ratherthananobjectinthehigherclause. 4 For ccomp vs. xcomp ,thedifferenceisacontrolled subject.Whilean xcomp isalwaystherearealsonon-  ccomp ,suchasina for-to ve(ﬁIarranged[forher togobybus]ﬂ).For advcl vs.  thedistinctionisa clausevs.aclause,thoughusuallyareducedonelack- ingasubject.Wegeneralizetheprevious partmod and infmod to  formoregeneralityandcross-linguisticapplicability.We use  ratherthan vmod sincethiscanalsobeanad- jective,asinﬁShehesitated[unabletorememberhisname]ﬂ.  b.Russian: Ivan lu  c  sij tancor Ivan best dancer nsubj amod c. I judge Ivan the best dancer nsubj dobj xcomp det amod nsubj 2.4.vs.compounding Theuniversalschemekeepstherichtaxonomyofnoun dependentsthatareoneofthestrengthsofSD.Anim- portantpartofthetypologyistodifferentiatecompounds (multi-rootlexemes)fromorcomplementa- tion.Underalexicalistapproach,compoundwordsarefun- damentallydifferentfromcasesofphrasal Therearethreerelationsforcompounding.Weuse mwe forxedgrammaticizedexpressionswithfunctionwords (e.g., insteadof : mwe (of,instead),Fr. plut ‹ otque ﬁrather thanﬂ: mwe (que,plut ‹ ot)), name forpropernounsconstituted ofmultiplenominalelements,asintheFinnishandItalian dependencytreebanks, 5 and compound tolabelothertypes ofmulti-wordlexemes.Thus compound isusedforany kindofX 0 compounding:nouncompounds(e.g., phone book ),butalsoverbandadjectivecompoundsthataremore commoninotherlanguages(suchasPersianorJapanese lightverbconstructions);fornumbers(e.g., threethou- sandbooks gives compound (thousand,three));forparticles ofphrasalverbs(e.g., putup : compound (put,up)). 2.5.Treatmentofprepositionsandcasemarking Amajorproposedchangefromtheextantversionsof SDisanewtreatmentofprepositionstoprovideauniform analysisofprepositionsandcaseinmorphologicallyrich languages.Theanalysiswechoseistopushalltheway thedesignprincipleofhavingdirectlinksbetweencon- tentwords.Weabandontreatingaprepositionasame- diatorbetweenawordanditsobject,and,in- stead,anycase-markingelement(includingprepositions, postpositions,andcliticcasemarkers)willbetreatedasa dependentofthenounitattachestoorintroduces.Thepro- posedanalysisisshownin(2): nmod labelstherelation betweenthetwocontentwords,whereastheprepositionis nowviewedasa case dependingonitscomplement.In general, nmod expressessomeformofobliqueoradjunct relationfurtherbythe case . (2)a. the Chair 's of det nmod case b. the of of the Chair det nmod case det c.French: le bureau du pr ´ esident the of of theChair det nmod case 5 Thatis, name wouldbeusedbetweenthewordsofﬁHillary RodhamClintonﬂbutnottoreplacetheusualrelationsinaphrasal orclausalnamelikeﬁTheLordoftheRingsﬂ. Thetreatmentofcasemarkingisillustratedin(3).In (3a), at inHebrewisaseparatetokenindicatinganac- cusativeobject:thecasemarkerdependsontheobject.In (3c),weshowtheanalysiswhencasemarkersaremor- phemes.Thecasemorphemeisnotdividedoffthenoun asaseparate case dependent,butthenounasawholeis analyzedasa nmod oftheverb.Toovertlymarkcase,we includePOStagsintherepresentationasshownin(3b)and (3d).WeusetheuniversalPOStagsetfromPetrovetal. (2012)towhichweappendcaseinformation. (3)a.Hebrew: wkfraiti at hsrj andwhenIsaw ACC themovie dobj case b. dobj (wkfraiti/ VERB ,hsrj/ NOUN ) case (hsrj/ NOUN ,at/ PRT - ACC ) c.Russian: Ya napisal pis'mo perom I wrote theletter withaquill nsubj dobj nmod d. nsubj (napisal/ VERB ,Ya/ NOUN - NOM ) dobj (napisal/ VERB ,pis'mo/ NOUN - ACC ) nmod (napisal/ VERB ,perom/ NOUN - INSTR ) Thistreatmentprovidesparallelismbetweendifferent constructionsacrossandwithinlanguages.Agoodresultis thatwenowhavegreaterparallelismbetweenprepositional phrasesandsubordinateclauses,whichareinpracticeoften introducedbyapreposition,asin(4). (4)a. Sue left after the rehearsal nsubj nmod case det b. Sue left after we did nsubj advcl mark nsubj Wealsoobtainparallelconstructionsforthepossessive alternationasshownin(2),forvariantformswithcase,a prepositionorapostpositioninFinnish,asshownin(5), andforthedativealternationwheretheprepositionalcon- structiongetsasimilaranalysistothedoubleobjectcon- struction,see(6). (5)a.Finnish: etsi ¨ a ilman johtolankaa tosearch without clue . PARTITIVE case nmod b. etsi ¨ a taskulampun kanssa tosearch torch . GENITIVE with nmod case c. etsi ¨ a johtolangatta tosearch clue . ABESSIVE nmod (6)a. give thechildren thetoys iobj dobj  b. give thetoys to thechildren nmod dobj case c.French: donner lesjouets aux enfants give thetoys tothe children dobj nmod case Anotheradvantageofthisnewanalysisisthatitpro- videsatreatmentofprepositionalphrasesthatarepredica- tivecomplementsofﬁbeﬂasin(7)thatisconsistentwiththe treatmentofnominalpredicativecomplements,asin(1). (7) Sue is in shape nsubj cop case SDisasurfacesyntacticrepresentation,whichdoesnot representsemanticroles.Thesemanticrolesof arehardtocategorizeandhardtodetermine.Wefeelthat thereisalotofuseforarepresentationwhichworkssolely intermsoftheovertrole-markingresourcesofeachlan- guage.Thisissupportedbymanyrichlanguage-particular traditionsofgrammaticalanalysis,whetherviaSanskrit casesorthecaseparticlesonJapanese bunsetsu . Prepositionssometimesintroduceaclauseastheircom- plement,e.g.,(8a).Followingtheprinciplethatdependen- ciesdomarkwherenewclausesareintroduced,thisrelation shouldhaveadifferentnamefrom nmod ,andwesuggest callingit ncmod ﬁnominalizedclauseUnderthe proposednewanalysis,theheadoftheof data will be upset .Theresultwillbetheanalysisin(8b). (8)a.Wehavenodataaboutwhetherusersareupset. b. data about whether users are upset ncmod case mark nsubj cop Anotherissueiswhatanalysistogivetocasesofstacked prepositions,suchas outof .Ourproposalisthatallsuch casesshouldberegardedassomeformof mwe ,asin(9b). (9)a.Outofallthis,somethinggoodwillcome. b. Out of all this ... come mwe predet case nmod 2.6.Informaltextgenres Followingthepracticalapproachusedinpart-of-speech taggingofrecentLDCtreebanks,weintroducetherelation goeswith toconnectmultipletokensthatcorrespondtoa singlestandardword,asaresultofreanalysisofwordsas compounds(ﬁhandsomeﬂforﬁhandsomeﬂ)orinputerror (ﬁotherﬂforﬁotherﬂ).Weuse foreign tolabelsequences offoreignwords.Toindicateoverriddenina speechrepair,weuse reparandum ,asin(10). (10) Go to the righ- to the left. reparandum case det case det nmod Theﬁloosejoiningrelationsﬂaimatarobustanalysisof moreinformalformsoftext,whicharenowcommonin NLPapplications.Informalwrittentextoftencontainslists ofcomparableitems,whichareparsedassinglesentences. Emailsignaturesinparticularcontainthesestructures,in theformofcontactinformation.FollowingdeMarneffeet al.(2013),weusethe list , parataxis ,(and appos )relations tolabelthesekindsofstructures.Therelation parataxis is alsousedinmoreformalwritingforconstructionssuchas sentencesjoinedwithacolon. The dislocated relationcapturespreposed(topics)and postposedelements.The remnant relationisusedtopro- videatreatmentofellipsis(inthecaseofgappingorstrip- ping,wherepredicationalorverbalmaterialgetselided), somethingthatwaslackinginearlierversionsofSD.It providesabasisforbeingabletoreconstructdependencies intheenhancedversionofSD.Forexample,in(11),the remnant relationsenableustocorrectlyretrievethesub- jectsandobjectsintheclauseswithanelidedverb. (11) John won bronze, Mary silver, and Sandy gold. nsubj dobj remnant remnant remnant remnant Incontrast,inright-node-raising(RNR)(12)andVP- ellipsis(13)constructionsinwhichsomekindofpredica- tionalorverbalmaterialisstillpresent,the remnant relation isnotused.InRNR,theverbsarecoordinatedandtheob- jectisa dobj oftheverb.InVP-ellipsis,wekeepthe auxiliaryasthehead,asshownin(13). (12) John bought and ate anapple. nsubj cc conj dobj (13) John will win gold and Mary will too. nsubj aux dobj cc nsubj conj advmod 2.7.Language-particularrelations Inadditiontoauniversaldependencytaxonomy,itisde- sirabletorecognizegrammaticalrelationsthatareparticu- lartoonelanguageorasmallgroupofrelatedlanguages. Suchlanguage-particularrelationsarenecessarytoaccu- ratelycapturethegeniusofaparticularlanguagebutwill notinvolveconceptsthatgeneralizebroadly.Thesugges- tionhereisthattheserelationsshouldalwaysberegarded asasubtypeofanexistingUniversalSDrelation.The SDrelationshaveataxonomicorganization(deMarneffe etal.,2006),andsomeoftheuniversalrelationsareal- readysubtypesofeachother(e.g., auxpass isasubtypeof aux ).Language-particularrelationsthatseemusefultodis- tinguishforEnglishareincludedatthebottomofTable2: npmod forbarenominalofpredicateslackinga preposition,amongwhichinparticularthereis tmod for bareNPtemporal poss forpossessives,sincethe syntaxofprenominalpossessionisverydistinctfrompost- nominal(whichmayalsoexpresspossession); predet forwordssuchas all thatprecederegulardetermin- ersand preconj forwordsprecedingaconjunctionlike ei- ther ;and prt forverbparticles.  3.Mappingtoexistingschemes Therehasrecentlybeenanefforttopushtowardsho- mogeneityacrossresourcesfordifferentlanguagesand tocomeupwithcross-linguisticallyconsistentannotation aimedatmulti-lingualtechnology,forpart-of-speechtagset (Petrovetal.,2012)aswellasfordependencyrepresenta- tion(McDonaldetal.,2013).TheschemeproposedinMc- Donaldetal.(2013)tookSDasastartingpoint.Annota- torsforsixdifferentlanguages(German,English,Swedish, Spanish,FrenchandKorean)producedannotationguide- linesforthelanguagetheywereworkingon,keepingthe labelandconstructionsetascloseaspossibletotheoriginal EnglishSDrepresentation.Theywereonlyallowedtoadd labelsforphenomenathatdonotexistinEnglish.Giventhe setsofrelationsobtainedforthedifferentlanguages,ahar- monizationstepwasperformedtomaximizeconsistencyof annotationacrosslanguages.However,thisrigidstrategy lostsomeimportantdistinctions,suchasthedistinctionbe- tweencompoundingandphrasalwhilemain- tainingsomedistinctionsthatarebestabandoned,suchasa distinctionbetweenvalandparticipial McDonaldetal.(2013)doesnotaddressgivinganele- ganttreatmentofmorphologicallyrichlanguages.Incon- trast,Tsarfaty(2013)proposestotreatmorphologyassyn- taxinherdependencyproposal,illustratedwithHebrew. However,thisrepresentationchoicectswiththelexi- calistapproachofSD.Herewetakeuphergoaloftrying togiveauniformtreatmentofbothmorphologicallyrich andmorphologicallypoorlanguages,butsuggestachieving thegoalinadifferentway,whichmaintainsalexicalistap- proach(seeSection2.5.).Table2showsacomparisonbe- tweentheevolutionoftheSDscheme(StanfordDependen- ciesv2.0.0,usedintheSANCLsharedtask,andStanford Dependenciesv3.3.0,the2013/11/12version),thepropos- alsinMcDonaldetal.(2013)(GSD)andinTsarfaty(2013) (TSD),andthedependencysetproposedhere(USD). Existingdependencysetsforotherlanguagescanbe fairlystraightforwardlymappedontoournewproposal. EveniftheschemesexaminedhereareﬁSD-centricﬂ,they dealtwithparticularconstructionspresentineachlanguage andpositednewrelationswhennecessary.Themappingis lessdifbecauseUSDadoptssomeoftheideasandre- lationsthatweredevelopedfortheseothertreebanks, suchasthecontentwordasheadanalysisofprepositional phrasesfromtheTurkuDependencyTreebank(TDT).In table3,weshowhowtheFinnish(Haverinenetal.,2013), Italian(Boscoetal.,2013),Chinese(Changetal.,2009) andPersian(Serajietal.,2013)schemescanbemapped ontotheproposeduniversaltaxonomy(USD).Theboldla- belsarerelations,andtheyaresubtypes ofthecorrespondingUSDrelationintherow.Gapsin- dicateexistingconstructionsinthelanguagethatwerenot capturedintheoriginalscheme(theUSDlabelisapplicable there);  indicatesconstructionsthatarenotpresentinthe language.Sincecopularverbsarenotheadsanymore,the attr relationisremoved,requiringtotheex- istinganalysesofcopularsentencesforItalianandChinese. Wealsointroducedextrarelationsforcertainconstructions, whichsomeschemeshadnotincorporatedyet. ForFinnish,therelation rel (forrelativemarker)willbe mappedtowhateversyntacticroletherelativeisplayingin therelativeclause( nsubj , dobj ,etc.),informationwhichis presentinthesecondannotationlayeroftheTDTcorpus. ISDTistheconversionoftheMIDTItaliandependency schemetoSD.Someofthe clit usesinISDT(forx- ivepronounsinpronominalverbsŒfrequentinRomance languagesŒsuchasFr. sedouter ﬁtosuspectﬂ)willbeen- compassedby expl .However,whenthexivepronoun cantrulybeadirectorindirectobject,itgetsassignedthe correspondingobjectrelation. Chinesehasserialverbconstructionswhichmightnow be compound (andnot conj ).Wetreatpost-nominallocal- izersandprepositionsasaformofcase. InPersian,therearenorelativepronounsand rel was usedfortheedrelativemarker,butitcanbemappedto mark .UPDThasa fw relationbetweensequencesoffor- eignwords,unanalyzedwithinPersiangrammar,whichwe adopt,namingit foreign .WealsoadopttheUPDT dep-top relationusedforafrontedelementthatintroducesthetopic ofasentence,butwegeneralizeitto dislocated toaccount forpostposedelementsaswellas.Rightdislocatedele- mentsarefrequentinspokenlanguages:e.g.,Fr. fautpas lamanger,lap ‹ ate (literally,ﬁneednotiteat,thedoughﬂ). Labelsofrelationswillbeharmonized tobesharedbetweenlanguages:Chinese assmod willbe mappedto poss ,andPersian dep-top to topic . 4.DifferentformsofStanfordDependencies ThecurrentStanfordconverterprovidesanumberof variantformsofSD,ofwhichthemostusedarethe ba- sic dependencytree,andthe collapsed,cc-processed form thataddsextradependencyarcs,restructuresprepositions tonotbeheads,andspreadsrelationsacrossconjunctions. Thissectionsuggestssomenewideasforhowtoprovide potentiallylessbutdifferentoptions. Oneconcernaboutourproposedtaxonomyisthat straightforwardparsingtoUSDislikelytobeharderfor parsersthanthecurrentrepresentation(forEnglish).It isnowfairlywellknownthat,whiledependencyrepre- sentationsinwhichcontentwordsaremadeheadstend tohelpsemanticallyorienteddownstreamapplications,de- pendencyparsingnumbersarehigherifyoumakeauxiliary verbsheads,ifyouanalyzelongconjunctionsbychaining (ratherthanhavingasingleheadofthewholeconstruction), andifyoumakeprepositionstheheadofprepositional phrases(Schwartzetal.,2012;Elmingetal.,2013).The generalizationisthatdependencyparsers,perhapsinpar- ticulartheefshift-reduce-styledependencyparsers likeMaltParser(Nivreetal.,2007),workbestthemorethe dependencyrepresentationlookslikeachainofshortde- pendenciesalongthesentence.UndertheproposedUSD, SDwouldthenbemakingtheﬁwrongﬂchoiceineachcase. However,itseemswrong-headedtochoosealinguis- ticrepresentationtomaximizeparserperformance,rather thanbasedonthelinguisticqualityoftherepresentation anditsusefulnessforapplicationsthatbuildfurtherpro- cessingontopofit.Rather,itmaybeusefultodopars- ingusingatransformationofthetargetdependencysys- tem.Inconstituencyparsing,itiscompletelyusualforthe targetrepresentationtobetransformedsoastoimprove  SDv2.0.0SDv3.3.0GSDTSDUSDNotes nsubjnsubjnsubjnsubjnsubj X csubjcsubjcsubjcsubjcsubj X nsubjpassnsubjpassnsubjpassnsubjpassnsubjpass X csubjpasscsubjpasscsubjpasscsubjpasscsubjpass X dobjdobjdobjdobjdobj X iobjiobjiobjiobjiobj X (TSDalsohas gobj forgenitiveobject) ccompccompccompccompccompUSD&TSDasclausewithinternalsubject,not xcompxcompxcompxcompxcompUSD&TSDasclausewithexternalsubject,not acompacompacompacompŒ acomp canbegeneralizedinto xcomp attrŒattrŒŒ attr removed: wh- isheador xcomp (withcopulaheadoption) advmodadvmodadvmodadvmodadvmod X advcladvcladvclŒadvclTSDomitsbutneededtopreserveclauseboundaries purpclŒŒŒŒFoldedinto advcl negnegnegnegnegAswellasadverbial not,never ,USDextendstonegative det like no detdetdetdetdet X (TSD dem and def assubtypesof det ) amodamodamodamodamod X apposapposapposapposappos X abbrevŒŒabbrevŒParentheticalabbreviationsbecomecasesof appos numnumnumnummodnummodRenamedforclarity rcmodrcmodrcmodrcmodrelcl X partmodpartmodpartmod?Make partmod , infmod into  ;use(rich)POStodistinguish infmodinfmodinfmodinfmodMake partmod , infmod into  ;use(rich)POStodistinguish quantmodquantmodadvmod?ŒGenerallyfoldedinto advmod rootrootROOTrootroot punctpunctppunctpunct auxauxauxauxaux X (TSDadds qaux forquestionauxiliary.veisnow mark .) auxpassauxpassauxpassauxpassauxpass X copcopcopcopcopGSDhas cop onlyincontent-headversion explexplexplexplexplSubjectandobjectexpletives,frozenxives(Fr. sedouter ) markmarkmarkmarkmark X to introducinganvewillnowbe mark (insteadof aux ) complmŒŒcomplmŒRemoveanduse mark morebroadly ŒdiscourseŒparataxis?discourseAgapinoriginalandothertypologies ŒŒŒŒvocativeAgapinoriginalandothertypologies depdepdepdepdepGSDusesfor vocative and discourse relŒrelrelŒConverter'sunresolvedonesnow dep ;TSD rel isreally mark prepprepadpmodprepmodcaseUSD case isdependentofNPnotofthing ŒŒnmodŒŒEquivalentto nmod below pobjpobjadpobjpobjnmod nmod nowgoesfromthingtoNPofPP pcomppcompadpcomppcompncmod ncmod goesfromthingtoclause ŒŒadpprep/casecaseTSDhasN/A/G/Dsubtypes,butcan'tkeepaddingforallcases possessivepossessiveadpgenŒViewasamanifestationof case nnnncompmodnncompoundGeneralize nn tolightverbs,etc.;X 0 compoundingnot ŒŒmweŒnameMulti-wordpropernouns(e.g., JohnSmith )asinTDTandISDT numbernumbernumnummod?ŒRegardedastypeofcompound;using nummod iswrong mwemwemwemwemweFixedexpressionswithfunctionwords( sothat,allbut,dueto,... ) ŒgoeswithŒŒgoeswithFororthographicerrors: other ŒŒŒŒforeignLinearanalysisofforeignwords(headisleft-most)asinUPDT ŒŒŒŒreparandumForoverriddeninspeechrepairs conjconjconjconjconj X cccccccccc X parataxisparataxisparataxisparataxisparataxis X ŒŒŒŒlistUsedforinformalliststructures,signatureblocks,etc. ŒŒŒŒremnantUsedtogiveatreatmentofellipsiswithoutemptynodes ŒŒŒŒdislocatedPreposedtopicsanddislocatedelementsasinUPDT Englishparticular npadvmodnpadvmodnmodadvmod? npmod Asubtypeof nmod tmodtmodadvmodtmod tmod Asubtypeof npmod predetpredetŒpredet predet Asubtypeof det preconjpreconjccpreconj preconj Asubtypeof cc prtprtprt? prt Asubtypeof compound posspossposspossmod poss Asubtypeof case Table2:ComparisonofproposalsonEnglish:SD,McDonaldetal.(2013)(GSD),Tsarfaty(2013)(TSD)andours(USD).  TDTISDTChineseUPDTUSD nsubjnsubjnsubj,topnsubjnsubj csubjcsubj  csubj  nsubjpassnsubjpassnsubjpassnsubjpass  csubjpass  csubjpass dobjdobj,clitdobjdobjdobj  iobj,clitiobj  iobj ccomp, iccomp ccompccomp,rcompccompccomp xcomp,acompxcomp,acompxcomp,acompxcomp  attrattr  advmod,quantmodadvmodadvmod,dvpmodadvmod,quantmodadvmod advcladvclsomeadvmodadvcladvcl negnegnegnegneg detdet, predet detdet, predet det amodamodamodamodamod apposapposprnmodapposappos numnumnummod,ordmodnumnummod rcmodrcmodrcmodrcmodrelcl partmod,infmodpartmodvmod   rootrootrootrootroot punctpunctpunctpunctpunct auxauxasp,mmodauxaux auxpassauxpasspassauxpassauxpass copcopcopcopcop  expl,clit  expl complm,markmarkcpmcomplm,mark, rel mark intjdiscourse discourse vocdep-vocvocative depcomp,moddepdepdep poss , gobj , gsubj ,pobj, poss ,pobj, lobj , assmod ,pobj, poss , cpobj nmod nommodnpadvmod, tmod clf,range, tmod npadvmod, tmod  pcomppccomp, lccomp  ncmod adpospossessive,prepassm,prep, ba , dvpm , locacc ,prep, cprep case number,nn, prt number,nnnn,someconjnumber,nn, prt , f nsubj j dobj j acomp j prep g -lvc compound namennp name somedepmweprtmodmwemwe goeswith goeswith fwforeign reparandum conjconjconj,etc,comodconjconj cc, preconj cc, preconj cccc, preconjunct cc parataxisparataxisparataxisparataxis list ellipsis remnant dep-top dislocated Table3:MappingsoftheFinnish(TDT),Italian(ISDT),ChineseandPersian(UPDT)schemestoUSD. parsingnumbers,suchasbyhead-lexicalization(Collins, 2003),bymanualorautomaticsubcategorizationofcat- egories(KleinandManning,2003;Petrovetal.,2006), andevenbyothermethodssuchasunarychaincontrac- tion(Finkeletal.,2008).Afterparsing,adetransformation processreconstructstreesinthetargetrepresentation.This kindoftransform-detransformarchitectureisatpresentless commonindependencyparsing,althoughNilsson,Nivre &Hall(2006;2007)dothisforcoordinationandverb groups,andpseudo-projectiveparsing(NivreandNilsson, 2005)canalsobeseenasaninstanceofthisarchitecture. Atransform-detransformarchitectureshouldbecomemore standardindependencyparsing.Wethereforeproposea parsing representationthatchangessomeofthedepen- dencyheadchoicestomaximizeparsingperformance.This requiresdevelopingtoolstoconvertseamlesslybothways betweenthe basic and parsing representations. 6 Sincethenewtreatmentofprepositionalphrasesbasi- callydoeswhatthe collapsed representationwasdesigned todo(puttingadirectlinkbetweenthenouncomplementof aprepositionandwhatitexceptfornotrenaming 6 AsmallpartofthisisinplaceintheStanfordconverter,inthe abilitytogeneratecopula-andcontent-headversionsfromtrees.  thedependencyrelation,the collapsed representationonits ownhaslessutility.However,theideasofhavingextrade- pendenciestomarkexternalsubjectsandtheexternalrole inrelativeclausesisuseful,therenamingofdependencies toincludecaseorprepositioninformationhelpsinmany applications,andspreadingrelationsoverconjunctionsis usefulforrelationextraction.Thesetransforma- tionscanbeprovidedinan enhanced representation. WethussuggestprovidingthreeversionsofStanfordDe- pendencies: basic , enhanced ,and parsing . 5.Conclusion Weproposedataxonomyofgrammaticalrelationsappli- cabletoavarietyoflanguages,developingtheimplications ofalexicalistapproachforthetreatmentofmorphology andcompounding.Someofthedecisionsmadeonlinguis- ticgroundsareatoddswithwhatworksbestforprocessing tools.Wesuggestedthatthetransform-detransformarchi- tecturestandardlyusedinconstituencyparsingisthesolu- tiontoadoptfordependencyparsing.Weworkedoutthe mappingofexistingdependencyresourcesfordifferentlan- guagestothetaxonomyproposedhere.Wehopethiswork willenhanceconsistencyinannotationbetweenlanguages andfurtherfacilitatecross-lingualapplications. 6.References Aronoff,M.(2007).Inthebeginningwastheword. Lan- guage ,83:803Œ830. Bosco,C.,Montemagni,S.,andSimi,M.(2013).Convert- ingItaliantreebanks:TowardsanItalianStanforddepen- dencytreebank.In SeventhLinguisticAnnotationWork- shop&InteroperabilitywithDiscourse . Bresnan,J.andMchombo,S.A.(1995).Thelexicalin- tegrityprinciple:EvidencefromBantu. NaturalLan- guageandLinguisticTheory ,13:181Œ254. Bresnan,J.(2001). Lexical-FunctionalSyntax .Blackwell, Oxford. Chang,P.-C.,Tseng,H.,Jurafsky,D.,andManning,C.D. (2009).DiscriminativereorderingwithChinesegram- maticalrelationsfeatures.In ThirdWorkshoponSyntax andStructureinStatisticalTranslation ,pages51Œ59. Chomsky,N.(1970).Remarksonnominalization.InJa- cobs,R.A.andRosenbaum,P.S.,editors, Readings inEnglishtransformationalgrammar ,pages184Œ221. Ginn,Waltham,MA. Chomsky,N.(1981). LecturesonGovernmentandBind- ing .Foris,Dordrecht. Collins,M.(2003).Head-drivenstatisticalmodelsfor naturallanguageparsing. ComputationalLinguistics , 29:589Œ637. deMarneffe,M.-C.andManning,C.D.(2008).TheStan- fordtypeddependenciesrepresentation.In Workshopon Cross-frameworkandCross-domainParserEvaluation . deMarneffe,M.-C.,MacCartney,B.,andManning, C.D.(2006).Generatingtypeddependencyparsesfrom phrasestructureparses.In LREC . deMarneffe,M.-C.,Connor,M.,Silveira,N.,Bowman, S.R.,Dozat,T.,andManning,C.D.(2013).Morecon- structions,moregenres:ExtendingStanforddependen- cies.In DepLing2013 . Elming,J.,Johannsen,A.,Klerke,S.,Lapponi,E.,Mar- tinez,H.,andSøgaard,A.(2013).Down-streamef- fectsoftree-to-dependencyconversions.In NAACLHLT 2013 . Finkel,J.R.,Kleeman,A.,andManning,C.D.(2008).Ef- feature-based,conditionalrandomparsing. In ACL46 ,pages959Œ967. Haverinen,K.,Nyblom,J.,Viljanen,T.,Laippala,V.,Ko- honen,S.,Missil ¨ a,A.,Ojala,S.,Salakoski,T.,and Ginter,F.(2013).Buildingtheessentialresourcesfor Finnish:theTurkudependencytreebank. LanguageRe- sourcesandEvaluation .Inpress.Availableonline. Klein,D.andManning,C.D.(2003).Accurateunlexical- izedparsing.In ACL41 ,pages423Œ430. McDonald,R.,Nivre,J.,Quirmbach-Brundage,Y.,Gold- berg,Y.,Das,D.,Ganchev,K.,Hall,K.,Petrov, S.,Zhang,H.,T ¨ ackstr ¨ om,O.,Bedini,C.,Bertomeu Castell ´ o,N.,andLee,J.(2013).Universaldependency annotationformultilingualparsing.In ACL51 . Nilsson,J.,Nivre,J.,andHall,J.(2006).Graphtransfor- mationsindata-drivendependencyparsing.In COLING 21andACL44 ,pages257Œ264. Nilsson,J.,Nivre,J.,andHall,J.(2007).Treetransforma- tionsforinductivedependencyparsing.In ACL45 . Nivre,J.andNilsson,J.(2005).Pseudo-projectivedepen- dencyparsing.In ACL43 ,pages99Œ106. Nivre,J.,Hall,J.,Nilsson,J.,Chanev,A.,Eryigit,G., K ¨ ubler,S.,Marinov,S.,andMarsi,E.(2007).Malt- Parser:Alanguage-independentsystemfordata-driven dependencyparsing. NaturalLanguageEngineering , 13:95Œ135. Palmer,M.,Gildea,D.,andKingsbury,P.(2005).The propositionbank:Anannotatedcorpusofsemanticroles. ComputationalLinguistics ,31:71Œ105. Petrov,S.,Barrett,L.,Thibaux,R.,andKlein,D.(2006). Learningaccurate,compact,andinterpretabletreeanno- tation.In COLING21andACL44 ,pages433Œ440. Petrov,S.,Das,D.,andMcDonald,R.(2012).Auniversal part-of-speechtagset.In LREC . Radford,A.(1988). TransformationalGrammar .Cam- bridgeUniversityPress,Cambridge. Schwartz,R.,Abend,O.,andRappoport,A.(2012). Learnability-basedsyntacticannotationdesign.In COL- ING24 ,pages2405Œ2422. Seraji,M.,Jahani,C.,Megyesi,B.,andNivre,J.(2013). UppsalaPersiandependencytreebankannotationguide- lines.Technicalreport,UppsalaUniversity. Taylor,A.,Marcus,M.,andSantorini,B.(2003).ThePenn treebank:Anoverview.InAbeill ´ e,A.,editor, Building andUsingParsedCorpora ,volume20of Text,Speech andLanguageTechnology .Springer. Tsarfaty,R.(2013).Amorpho-syntacticschemeof Stanforddependencies.In ACL51 . Xue,N.,Xia,F.,Chiou,F.-D.,andPalmer,M.(2005).The PennChinesetreebank:Phrasestructureannotationof alargecorpus. NaturalLanguageEngineering ,11:207Œ 238. Zwicky,A.M.andPullum,G.K.(1983).Cliticizationvs. English n't . Language ,59:502Œ513.
Dependency relations	Grammatical relations	Universal dependencies	Stanford dependencies	Parsing
What makes the Khoi-San languages notably different from other languages around them?	The Khoi-San languages of Africa are notable for many reasons. They are among the rarest of languages, spoken by only scattered groups of a few thousand "Bushmen" in central and southern Africa.  They are unrelated to the many other African languages and language families surrounding them. For these last two reasons and others, they are believed to be among the oldest, perhaps THE oldest languages in Africa, having been around for perhaps 60,000 years. And most famously, they are "click" languages. Thus, Khoi-San could be evidence that early human languages had clicks, and Khoi-San could be the closest living relative to the parent-family of all living languages, although this is speculative.	Which of the following is NOT true of the Khoi-San languages?	They are grammatically similar to each other.	They are dying as their speakers choose to use other languages instead.|||They are phonologically similar to each other.|||They are among the least well studied or documented languages in the world.	"The Khoisan Language Family" webpage from Fu Jen Catholic University||document||IŸThe Khoisan Language Family introduction structure writing resources  Introduction Introduction The Khoisan language family is the smallest of the languages families of Africa. The name  Khoisan  derives from the name of the Khoi-Khoi group of South Africa and the San (Bushmen) group of Namibia. It is used for several ethnic groups who were the original inhabitants of southern Africa before the Bantu migrations southward and later European colonization. Archaelogical evidence suggests that the Khoisan people appeared in southern Africa some 60,000 years ago. Thus, the Khoisan languages may well be among the most ancient of all human tongues. Even though the Khoisan languages share similarities in their sound systems, their grammatical systems are quite unique. In the absence of historical records, it is difÞcult to determine their genetic relationship to each other and to other African languages. Today, the Khoisan languages are spoken only in southwestern Africa, in the region around the Kalahari Desert extending from Angola to South Africa, and in one small area of Tanzania. The  Hadza  and  Sandawe  languages in Tanzania are generally classiÞed as Khoisan, but are extremely distant geographically and linguistically from the others. It is fair to say that of all the language families of the world, the Khoisan languages are among the most neglected by language scholars and the least studied.  Afro-Asiatic Nilo- Saharan Niger-Congo Khoisan Austronesian  Status The Khoisan languages are becoming increasingly rare. Few of them have more than 1,000 speakers. The number of speakers is fast diminishing, and several are known to have become extinct. One of the main reasons is that bilingual Khoisan speakers shift to the dominant language of the area and stop teaching their mother tongue to their children. There are some exceptions, for instance, the  Sandawe language in Tanzania whose speakers have maintained a relatively stable linguistic community. Unfortunatately, many of these languages have left behind no written records, so their loss is permanent.   Ethnologue  lists 13 Khoisan languages with populations of 1,000 and over: Sandawe 40,000Tanzania Hai||om  (San) IŸ16,000Namibia Nama (Khoekhoegowab) 233,701Namibia, Botswana, South Africa Shua 6,000Botswana Tsoa 5,000Botswana ||Ani 1,000Botswana Gana 2,000Botswana Kxoe 10,000Namibia, Angola, Botswana, South Africa, Zambia |Gwi2,500Botswana Naro 14,000Botswana, Namibia =|Kx'au||'ein 2,000Namibia, Botswana Kung-Ekoka 6,900Botswana, Angola, South Africa Ju|'hoan 5,000Botswana, Namibia Maligo 2,200Angola Nama is an ofÞcial language of Namibia. The language is used at all levels of education and in the media.  Kxoe  is used as a spoken, but not a written language in primary schools i.e., textbooks are in English.  Naro  is used as a  lingua franca  among speakers of other Khoisan languages. Click here  to view a compelling National Geographic multimedia presentation on the San people. IŸ
African languages	Historical linguistics	Comparative linguistics	Joseph Greenberg	Language typology	Phonology	Khoi-San	Language death	Language isolates	Areal linguistics
What are conditional frequency distributions and how are they related to n-grams and Markov models?	These three things are different ways of looking at the same thing. Their most general purpose in NLP is predicting the next most appropriate word in a string of words, such as in auto-correct, or in order to produce the most probable of several possible sentences in text generation (and they have many other applications). A frequency distribution is the assignment of frequencies (the frequency of occurrence) to each target item in a corpus of data (such as the words of a text). A conditional frequency distribution is one which tells you the frequency of each item under various conditions (contexts), such as when following one or more previous words. This is where n-gram models come in. An n-gram model tells you the frequency of each ngram in the text and therefore the probability for each word to occur at the end of each n-gram, which is the probability of the word following each n-1 previous words. For example, if the text is modeled with trigrams, the frequency of the trigram “chocolate ice cream” is also the probability of “cream” following the words, “chocolate ice.” Finally, this is also a Markov model. A Markov model is a probabilistic model in which layers of context, such as those n-1 previous words, impact on the prediction of the next item (rather than just the items raw frequency of occurrence).	Which of the following is NOT true about these kinds of language models?	They capture the grammar of a language.	They can apply to units other than words.|||In the language of Markov models for NLP, âwordsâ are âstates.â|||Their applications include grammar checking and speech recognition.	Introduction to language modeling with ngrams||slides||Introduction to N-gram Models  COSI 114 Ð Computational Linguistics  James Pustejovsky   January 23, 2015  Brandeis University   Outline !!Human Word  Presentation  !!Language Model  !!N-gram  !!N-gram Language  Model  !!Markov chain  !!Training and Testing  !!Example  !!Reduce Number of  Parameters in N- gram  !!Maximum Likelihood  Estimation(MLE)  !!Sparse Data  Problem  !!Smoothing and  Backoff  !!ZipfÕs  Law !!Evaluation  !!Application   Language model   !!Predicting the next word  !!Evaluating how ÔEnglishÕ a sequence of  words is   Next Word Presentation  !!From  The Boston Globe 1/19/15  !!A mother  !  !!A mother was  !. !!A mother was found  !.  !!A mother was found dead  ! !!A mother was found dead and her   !.  !!A mother was found dead and her son was  shot and  ! !!A mother was found dead and her son was  shot and killed by police early Monday.   Human Word Prediction  !!We may have ability to predict future  words in an utterance  !!How? !!Domain Knowledge   "!red blood   !!Syntactic knowledge   "!jÕai vu la  !<feminine  adj | feminine noun>  !!Lexical knowledge  "!baked  <potato,beans,cod >  Predictive keyboard   Grammar checker  !!The difference is that we donÕt eat beef.  The difference are that we donÕt eat beef.  !!The difference is that we donÕt eat beef.  The difference is is that we donÕt eat beef.  !!I wish I was there.  I wish I were there.    Statistical Machine Translation  !!Make the translation sound fluent  !!Which one sounds the best?  !!he introduced reporters to the main contents  of the statement  !!he briefed to reporters the main contents of  the statement  !!he briefed reporters on the main contents of  the statement    Applications  !!LM usually does not stand alone.   !!spelling correction  "!Detect the error (low probability)  "!Correct the error by increasing probability  "!Theatre owners say popcorn/unicorn sales have doubled...  !!speech recognition   "!providing context  "!along with Starbucks lovers  "!a long list of ex loves    Language Model  !!Language Model (LM)  !!A language model is a probability  distribution over entire sentences or texts  "!N-gram: unigrams, bigrams, trigrams, !  !!In a simple  n-gram language model , the probability of a word, conditioned  on some number of previous words    N-grams  !!In other words, using the previous N-1  words in a sequence we want to  predict the next word  !!Sue swallowed a large green ____.  "!A. frog  "!B. mountain  "!C. car  "!D. pill  What is an N-gram?  !!An n-gram  is a subsequence of  n items  from a given  sequence . !!Unigram:  n-gram of size 1   !!Bigram:  n-gram of size 2  !!Trigram:  n-gram of size 3  !!Item:  "!Phonemes  "!Syllables  "!Letters  "!Words    "!Anything else depending on the application.   Example   input=the dog smelled like a skunk  "!# =  bos and eos !!Bigram : !!# the, the dog, dog smelled, smelled like,  like a, a skunk, skunk#   !!Trigram : !!"# the dog", "the dog smelled", "dog  smelled like", "smelled like a", "like a  skunk" and "a skunk #".   How to predict the next word?  !!Assume a language has T word types in its  lexicon, how likely is word  x to follow word  y? !!Solutions:  !!Estimate likelihood of  x occurring in new text,  based on its  general frequency of occurrence  estimated from a corpus  "!popcorn  is more likely to occur than  unicorn  !!Condition the likelihood of  x occurring  in the context of previous words  (bigrams , trigrams , !) "!mythical unicorn  is more likely than  mythical popcorn    Estimate PDF  !!With large enough dataset   P(X=x, S=s) can be estimated with    C(X=x, S=s) in the dataset               size of dataset  !!P(X= x|S =s) can be estimated with    C(X=x, S=s) in the dataset           C(S=s) in the dataset  !!More examples in lab next week   Predicting the next word from  corpora  !!P(love| but he will) =    C(but he will love) / C (he will love)   Statistical View  !!The task of predicting the next word can  be stated as:  !!attempting to estimate the probability  distribution function  P:  !!In other words:  !!we use a classification of the previous history  words (or context),  to predict the next word.  !!On the basis of having looked at a lot of text,  we know which words tend to follow other  words.   How to assign probabilities to a  sequence of words  !!Statistical Machine Learning  !!We use language model to give probabilities  to the translation outputs.  !!High probability = better translation  !!P(Òhe briefed reporters on the main contents of the statementÓ) =   C(Òhe briefed reporters on the main contents of the statementÓ)                       C( all possible sequence of words)   !!To solve this, we use Chain Rule +  Markov Assumption   N-Gram Models  !!Models Sequences using the  Statistical Properties  of N-Grams  !!Idea: Shannon  !!given a sequence of letters, what is the  likelihood  of the next letter?   !!From training data, derive a  probability  distribution  for the next letter given a  history  of size n.   N-gram Model is a Markov Chain  ¥!Give a set of  states, S = {s 1, s2, ... , sr}.  ¥!The process starts in a random state based on a  probability distribution  ¥!Change states in sequence based on a probability  distribution of the next state given the previous state  ¥!This model could generate the sequence {A,C,D,B,C}  of length 5 with probability:   Markov Assumption  !!Markov assumption : the probability of the next  word depends only on the previous k words.  !!Common N-grams:
Language models	Markov model	Conditional frequency distribution	N-grams	Statistical Natural Language Processing (NLP)
What is an n-gram language model and what are they used for?	An n-gram language model is a kind of probabilistic model of a language – a model which tells you the probability of a sequence of words being part of the language, or the probability of the next word in a sequence. N-grams are sequences of words taken from data, such as two-word sequences (bigrams) and three-word sequences (trigrams). An n-gram model records the frequency of every n-gram in the corpus of linguistic data used – or the probability of one word following another, or of following a preceding sequence if you are using n-grams with n larger than two. These models have many applications, including auto-correct, automatic text generation, and speech recognition.  In all these cases, the model helps by allowing the computer to guess the most likely next word or phrase.	Which of the following describe how to compute the probability P of word "w" following a sequence of words, using an n-gram model.	P = the number of times that word follows that sequence in the data divided by the number of times the sequence appears in the data.	P = the number of times that word follows that sequence in the data, divided by the number of times that word appears in the data.|||P = the number of times that word follows that sequence in the data divided by the number of phrases of that length in the data.|||P = the probability of the sequence+word appearing in the data, divided by (the probability of that word appearing in the data, multiplied by the probability that the sequence appears in the data).	N-Grams chapter||document||DRAFT SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2014.All rightsreserved.DraftofSeptember1,2014. CHAPTER 4 N-Grams ﬁYouareuniformlycharming!ﬂcriedhe,withasmileofassociatingandnow andthenIbowedandtheyperceivedachaiseandfourtowishfor. RandomsentencegeneratedfromaJaneAustentrigrammodel Beingabletopredictthefutureisnotalwaysagoodthing.CassandraofTroyhad thegiftofforeseeingbutwascursedbyApollothatherpredictionswouldneverbe believed.HerwarningsofthedestructionofTroywereignoredandtosimplify,let's justsaythatthingsjustdidn'tgowellforherlater. Inthischapterwetakeupthesomewhatlessfraughttopicofpredictingwords. Whatword,forexample,islikelytofollow Pleaseturnyourhomework... Hopefully,mostofyouconcludedthataverylikelywordis in ,orpossibly over , butprobablynot refrigerator or the .Inthefollowingsectionswewillformalize thisintuitionbyintroducingmodelsthatassigna probability toeachpossiblenext word.Thesamemodelswillalsoservetoassignaprobabilitytoanentiresentence. Suchamodel,forexample,couldpredictthatthefollowingsequencehasamuch higherprobabilityofappearinginatext: allofasuddenInoticethreeguysstandingonthesidewalk thandoesthissamesetofwordsinadifferentorder: onguysallIofnoticesidewalkthreeasuddenstandingthe Whywouldyouwanttopredictupcomingwords,orassignprobabilitiestosen- tences?Probabilitiesareessentialinanytaskinwhichwehavetoidentifywords innoisy,ambiguousinput,like speechrecognition or handwritingrecognition .In themovie TaketheMoneyandRun ,WoodyAllentriestorobabankwithasloppily writtenhold-upnotethatthetellerincorrectlyreadsasﬁIhaveagubﬂ.As Rus- sellandNorvig(2002) pointout,alanguageprocessingsystemcouldavoidmaking thismistakebyusingtheknowledgethatthesequenceﬁIhaveagunﬂisfarmore probablethanthenon-wordﬁIhaveagubﬂorevenﬁIhaveagullﬂ. In spellingcorrection ,weneedtoandcorrectspellingerrorslike Their aretwomidtermsinthisclass ,inwhich There wasmistypedas Their .Asentence startingwiththephrase Thereare willbemuchmoreprobablethanonestartingwith Theirare ,allowingaspellcheckertobothdetectandcorrecttheseerrors. Assigningprobabilitiestosequencesofwordsisalsoessentialin machinetrans- lation .SupposewearetranslatingaChinesesourcesentence: Ö  °  Ë Í ƒ; † – ¹ Hetoreportersintroducedmaincontent  DRAFT 2 C HAPTER 4  N-G RAMS Aspartoftheprocesswemighthavebuiltthefollowingsetofpotentialrough Englishtranslations: heintroducedreporterstothemaincontentsofthestatement hebriefedtoreportersthemaincontentsofthestatement hebriefedreportersonthemaincontentsofthestatement Aprobabilisticmodelofwordsequencescouldsuggestthat briefedreporterson isamoreprobableEnglishphrasethan briefedtoreporters (whichhasanawkward to after briefed )or introducedreportersto (whichusesaverbwhichisless Englishinthiscontext),allowingustocorrectlyselecttheboldfacedsentenceabove. Probabilitiesareisalsoimportantfor augmentativecommunication (Newell etal.,1998) systems.PeoplelikethephysicistStephenHawkingwhoareunable tophysicallytalkorsigncaninsteadusesimplemovementstoselectwordsfrom amenutobespokenbythesystem.Wordpredictioncanbeusedtosuggestlikely wordsforthemenu. Modelsthatassignprobabilitiestosequencesofwordsarecalled languagemod- els or LMs .Inthischapterweintroducethesimplestmodelthatassignsprobabilities languagemodel LM tosentencesandsequencesofwords,the N-gram .AnN-gramisasequenceof N  words:a2-gram(or bigram )isatwo-wordsequenceofwordslikeﬁpleaseturnﬂ, ﬁturnyourﬂ,orﬂyourhomeworkﬂ,anda3-gram(or trigram )isathree-wordse- quenceofwordslikeﬁpleaseturnyourﬂ,orﬁturnyourhomeworkﬂ.We'llseehow touseN-grammodelstoestimatetheprobabilityofthelastwordofanN-gram giventhepreviouswords,andalsotoassignprobabilitiestoentiresequences.In abitofterminologicalambiguity,weusuallydropthewordﬁmodelﬂ,andthusthe term N-gram isusedtomeaneitherthewordsequenceitselforthepredictivemodel thatassignsitaprobability. Whetherestimatingprobabilitiesofnextwordsorofwholesequences,theN- grammodelisoneofthemostimportanttoolsinspeechandlanguageprocessing. 4.1N-Grams Let'sbeginwiththetaskofcomputing P ( w j h ) ,theprobabilityofaword w given somehistory h .Supposethehistory h isﬁ itswaterissotransparentthat ﬂandwe wanttoknowtheprobabilitythatthenextwordis the : P ( the j itswaterissotransparentthat ) : (4.1) Onewayistoestimatethisprobabilityisfromrelativefrequencycounts:takea verylargecorpus,countthenumberoftimeswesee itswaterissotransparentthat , andcountthenumberoftimesthisisfollowedby the .Thiswouldbeansweringthe questionﬁOutofthetimeswesawthehistory h ,howmanytimeswasitfollowedby theword w ﬂ,asfollows: P ( the j itswaterissotransparentthat )= C ( itswaterissotransparentthatthe ) C ( itswaterissotransparentthat ) (4.2) Withalargeenoughcorpus,suchastheweb,wecancomputethesecountsand estimatetheprobabilityfromEq. 4.2 .Youshouldpausenow,gototheweb,and computethisestimateforyourself.
N-grams	Bigrams	N-gram model	Probabilistic language model	Natural Language Processing (NLP)	Computational linguistics	Word prediction
What are linguistic feedback, nonlinguistic feedback, and paralinguistic feedback?	In a verbal conversation, linguistic feedback is spoken words; paralinguistic feedback is pitch, volume, and frequency; and nonlinguistic feedback is all other non-oral feedback, such as eye contact, facial expressions, and postures.	In a conversation, the listener looks on in disgust as the speaker describes finding an unexpected insect in their breakfast. The listener is employing which type of feedback?	Nonlinguistic feedback	Paralinguistic feedback|||Linguistic feedback	On the Semantics and Pragmatics of Linguistic Feedback||publication||3characterized by different functions. Interaction functions can, for example, besubdivided into mechanisms for:(i)sequencing (of activities and subactivities, communicative acts and/or topics)(ii)turntaking(iii)giving and eliciting feedback.The literature on conversation analysis and discourse analysis (see e.g. Levinson1985 or Brown and Yule 1983) contains much discussion of the former two types ofmechanisms, whereas there has been less discussion of feedback (cf. Allwood 1988a,1988b). This paper is intended as a contribution to the further exploration oflinguistic feedback mechanisms, especially with regard to the semantic/pragmaticfunctions of such mechanisms.12.2.Linguistic feedback: basic functionsThe term feedback originates in cybernetics (Wiener 1948), where it is used todenote processes by which a control unit gets information about the effects andconsequences of its actions.Here we are concerned with linguistic (interindividual) feedback (Allwood 1979,1988a, 1988b, 1988c), i.e., linguistic mechanisms which enable the participants of aconversation to exchange information about four basic communicative functions,which are essential in human direct face-to-face communication. These functionsare:(i)contact (i.e., whether the interlocutor is willing and able to continue theinteraction)(ii)perception (i.e., whether the interlocutor is willing and able to perceive themessage)(iii)understanding (i.e., whether the interlocutor is willing and able to understandthe message)                                                1 As can he seen. we are here making no attempt to distinguish semantics from pragmatics. This isso because we believe that such a distinction runs into serious practical and theoretical difficulties(cf. Allwood 1981). 4(iv)attitudinal reactions (i.e. whether the interlocutor is willing and able to reactand (adequately) respond to the message, specifically whether he/she acceptsor rejects it).These four basic functions of linguistic feedback arise from four basic requirementsof human communication. First, communication requires that at least two agents arewilling and able to communicate. Second, communication requires that the receivingagent is willing and able to perceive the behavioral or other means whereby thesending agent is displaying or signalling information. Third, communication requiresthat the receiving agent is willing and able to understand the content that the senderis displaying or signalling. It is also often helpful if the receiver can perceive andunderstand various types of indicated information.2 Finally, communication requiresthat the receiving agent is willing and able to react attitudinally and behaviorally tovarious aspects of the content that the sender is displaying or signalling. Again, it issometimes beneficial for communication, if the receiver also reacts to indicatedinformation. Certain conventional features of the displayed or signalled content hereseem particularly important for the interpretation of the content of feedbackexpressions. Among these are polarity (positive or negative) and mood(conventionally signalled evocative intention; cf. Allwood 1978).Every language appears to have conventionalized means (verbal and prosodic meansas well as body movements) for giving and eliciting information about the four basiccommunicative functions. Linguistic feedback mechanisms on a primary levelusually involve very short morphemes (yes, no, m), or basic mechanisms such asrepetition, simple body movements (head nods, head shakes) in combination, on asecondary level, with fairly simple phonological, morphological and syntacticoperations for modifying and expanding the primary feedback expressions.Earlier studies that have discussed feedback and related phenomena include Allwood(1976, 1979, 1988a, 1988b), Anward (1986), Clark & Schaefer (1989), Ehlich(1986), Fries (1952), Hellberg (1985), Heritage (1984), James (1972), Nivre (1991),Schegloff (1982), Severinson-Eklundh (1986), Sigurd (1984), Yngve (1970).Allwood (1988b) gives a taxonomy for the structure of linguistic feedback and, inparticular, describes the Swedish system. In the present paper, we want to focus onthe content features of linguistic feedback.                                                2 For the distinction between indicated, displayed and signalled information, see section 3.3 below.
Key terms in linguistics
What are five features of sentence constituents one would extract in order to train a Semantic Role classifier?	Five of the following: (1) governing predicate (the word whose roles are being classified), (2) the phrase-type of the classified constituent, (3) the head-word of the classified constituent, (4) the path between the predicate and the constituent (the grammatical categories of the nodes), (5) the position of the constituent in the sentence, (6) and the sub-categorization of the predicate (e.g. transitive, ditransitive, etc.), (7) the voice of the clause (active or passive)	Which of the following is NOT a significant challenge with creating a simple “1-of-N” classifier for semantic roles?	The semantic roles in one sentence are not independent of each other.	There can only be one of most roles in a particular sentence.|||It is difficult to automatically select all and only the constituents that should be classified.|||The classifier must be able to assign “no-role” to some constituents	Chapter Section on Semantic Role classifiers||publication||20.6  S EMANTIC R OLE L ABELING 9 Recallthatthedifferencebetweenthesetwomodelsofsemanticrolesisthat FrameNet( 20.27 )employsmanyframeelementsasroles,whileProp- Bank( 20.28 )usesasmallernumberofnumberedargumentlabelsthatcanbeinter- pretedasvlabels,alongwiththemoregeneralARGMlabels.Some examples: (20.27) [You]can't[blame][theprogram][forbeingunabletoidentifyit] COGNIZERTARGETEVALUEEREASON (20.28) [TheSanFranciscoExaminer]issued[aspecialedition][yesterday] ARG 0 TARGETARG 1 ARGM - TMP 20.6.1AFeature-basedAlgorithmforSemanticRoleLabeling Afeature-basedsemanticrolelabelingalgorithmissketchedinFig. 20.4 . Feature-basedalgorithmsŠfromtheveryearliestsystemslike (Simmons,1973) Š beginbyparsing,usingbroad-coverageparserstoassignaparsetotheinputstring. Figure 20.5 showsaparseof( 20.28 )above.Theparseisthentraversedtoall wordsthatarepredicates. Foreachofthesepredicates,thealgorithmexamineseachnodeintheparse treeandusessupervisedclasstodecidethesemanticrole(ifany)itplays forthispredicate.GivenalabeledtrainingsetsuchasPropBankorFrameNet,a featurevectorisextractedforeachnode,usingfeaturetemplatesdescribedinthe nextsubsection.A1-of-Nisthentrainedtopredictasemanticrolefor eachconstituentgiventhesefeatures,whereNisthenumberofpotentialsemantic rolesplusanextraNONErolefornon-roleconstituents.Anystandard algorithmscanbeused.Finally,foreachtestsentencetobelabeled,theis runoneachrelevantconstituent. function S EMANTIC R OLE L ABEL ( words ) returns labeledtree parse   P ARSE ( words ) foreach predicate in parse do foreach node in parse do featurevector   E XTRACT F EATURES ( node , predicate , parse ) C LASSIFY N ODE ( node , featurevector , parse ) Figure20.4 Agenericsemantic-role-labelingalgorithm.C LASSIFY N ODE isa1-of- N clas- thatassignsasemanticrole(orNONEfornon-roleconstituents),trainedonlabeleddata suchasFrameNetorPropBank. Insteadoftrainingasingle-stageasinFig. 20.5 ,thenode-levelclassi- taskcanbebrokendownintomultiplesteps: 1. Pruning: Sinceonlyasmallnumberoftheconstituentsinasentenceare argumentsofanygivenpredicate,manysystemsusesimpleheuristicstoprune unlikelyconstituents. 2.  abinaryclasofeachnodeasanargumenttobela- beledora NONE . 3.  a1-of- N ofalltheconstituentsthatwerelabeled asargumentsbythepreviousstage Theseparationofandmayleadtobetteruseoffea- tures(differentfeaturesmaybeusefulforthetwotasks)ortocomputationalef ciency.  10 C HAPTER 20  S EMANTIC R OLE L ABELING Figure20.5 ParsetreeforaPropBanksentence,showingthePropBankargumentlabels.Thedottedline showsthe path featureNP " S # VP # VBDforARG0,theNP-SBJconstituent TheSanFranciscoExaminer. GlobalOptimization ThealgorithmofFig. 20.5 eachargumentseparately(`lo- cally'),makingthesimplifyingassumptionthateachargumentofapredicatecanbe labeledindependently.Thisassumptionisfalse;thereareinteractionsbetweenargu- mentsthatrequireamore`global'assignmentoflabelstoconstituents.Forexample, constituentsinFrameNetandPropBankarerequiredtobenon-overlapping.More ,thesemanticrolesofconstituentsarenotindependent.Forexample PropBankdoesnotallowmultipleidenticalarguments;twoconstituentsofthesame verbcannotbothbelabeled ARG 0. Rolelabelingsystemsthusoftenaddafourthsteptodealwithglobalconsistency acrossthelabelsinasentence.Forexample,thelocalcanreturnalistof possiblelabelsassociatedwithprobabilitiesforeachconstituent,andasecond-pass Viterbidecodingorre-rankingapproachcanbeusedtochoosethebestconsensus label.Integerlinearprogramming(ILP)isanothercommonwaytochooseasolution thatconformsbesttomultipleconstraints. FeaturesforSemanticRoleLabeling Mostsystemsusesomegeneralizationofthecoresetoffeaturesintroducedby GildeaandJurafsky(2000) .Commonbasicfeaturestemplates(demonstratedon the NP-SBJ constituent TheSanFranciscoExaminer inFig. 20.5 )include:  Thegoverning predicate ,inthiscasetheverb issued .Thepredicateisacru- cialfeaturesincelabelsareonlywithrespecttoaparticularpredicate.  The phrasetype oftheconstituent,inthiscase, NP (or NP-SBJ ).Somese- manticrolestendtoappearas NP s,othersas S or PP ,andsoon.  The headword oftheconstituent, Examiner .Theheadwordofaconstituent canbecomputedwithstandardheadrules,suchasthosegiveninChapter12 inFig. ?? .Certainheadwords(e.g.,pronouns)placestrongconstraintsonthe possiblesemanticrolestheyarelikelyto  The headwordpartofspeech oftheconstituent, NNP .  The path intheparsetreefromtheconstituenttothepredicate.Thispathis markedbythedottedlineinFig. 20.5 .Following GildeaandJurafsky(2000) , wecanuseasimplelinearrepresentationofthepath,NP " S # VP # VBD. " and # representupwardanddownwardmovementinthetree,respectively.The  20.6  S EMANTIC R OLE L ABELING 11 pathisveryusefulasacompactrepresentationofmanykindsofgrammatical functionrelationshipsbetweentheconstituentandthepredicate.  The voice oftheclauseinwhichtheconstituentappears,inthiscase, active (ascontrastedwith passive ).Passivesentencestendtohavestronglydifferent linkingsofsemanticrolestosurfaceformthandoactiveones.  Thebinary linearposition oftheconstituentwithrespecttothepredicate, either before or after .  The subcategorization ofthepredicate,thesetofexpectedargumentsthat appearintheverbphrase.Wecanextractthisinformationbyusingthephrase- structurerulethatexpandstheimmediateparentofthepredicate;VP ! VBD NPPPforthepredicateinFig. 20.5 .  Thenamedentitytypeoftheconstituent.  Thewordsandthelastwordoftheconstituent. ThefollowingfeaturevectorthusrepresentstheNPinourexample(recall thatmostobservationswillhavethevalueNONEratherthan,forexample, ARG 0, sincemostconstituentsintheparsetreewillnotbearasemanticrole): ARG 0:[issued,NP,Examiner,NNP,NP " S # VP # VBD,active,before,VP ! NPPP, ORG,The,Examiner] Otherfeaturesareoftenusedinaddition,suchassetsofn-gramsinsidethe constituent,ormorecomplexversionsofthepathfeatures(theupwardordownward halves,orwhetherparticularnodesoccurinthepath). It'salsopossibletousedependencyparsesinsteadofconstituencyparsesasthe basisoffeatures,forexampleusingdependencyparsepathsinsteadofconstituency paths. 20.6.2ANeuralAlgorithmforSemanticRoleLabeling Thestandardneuralalgorithmforsemanticrolelabelingisbasedonthebi-LSTM IOBtaggerintroducedinChapter9,whichwe'veseenappliedtopart-of-speech taggingandnamedentitytagging,amongothertasks.RecallthatwithIOBtagging, wehaveabeginandendtagforeachpossiblerole( B - ARG 0, I - ARG 0; B - ARG 1, I - ARG 1,andsoon),plusanoutsidetag O . Aswithallthetaggers,thegoalistocomputethehighestprobabilitytagse- quence‹ y ,giventheinputsequenceofwords w : ‹ y = argmax y 2 T P ( y j w ) Inalgorithmslike Heetal.(2017) ,eachinputwordismappedtopre-trainedem- beddings,andalsoassociatedwithanembeddingfora(0/1)variableindicating whetherthatinputwordisthepredicate.Theseconcatenatedembeddingsarepassed throughmultiplelayersofbi-directionalLSTM.State-of-the-artalgorithmstendto bedeeperthanforPOSorNERtagging,using3to4layers(6to8totalLSTMs). Highwaylayerscanbeusedtoconnecttheselayersaswell. Outputfromthelastbi-LSTMcanthenbeturnedintoanIOBsequenceasfor POSorNERtagging.Tagscanbelocallyoptimizedbytakingthebi-LSTMoutput, passingitthroughasinglelayerintoasoftmaxforeachwordthatcreatesaproba- bilitydistributionoverallSRLtagsandthemostlikelytagforword x i ischosenas t i ,computingforeachwordessentially: ‹ y i = argmax t 2 tags P ( t j w i )
Classifiers	Semantic role labeling (SRL)	Feature extraction
Identify the grammatical relations / dependencies of the constituents following the verbs below.	In (a), “her” is an indirect object (iobj) and “roses” is a direct object (dobj) of “sent.” The direct object is the thing directly acted on by the subject, and the indirect object is a thing indirectly acted on by the subject. In (b) the verb is intransitive and the following constituent is an adverbial modifier of “performed”; the parser should create an advmod relation from “better” to “performed.” In (c) “a teacher” is an intentional complement; the complement describes the subject. In automatic parsers, it is labeled as as the copula (cop) of “teacher” and some parsers may make “teacher” the “root.” In (d) the entire sentence following the verb is a sentential complement, not an object, and would be labeled by the “ccomp” dependency from the verb “lying” to the verb “knew.” This is the dependency for sentential complements with defined subjects.	Which of the following contains an xcomp dependency?	She loves to act.	She is acting in the school play.|||She excels at acting.|||She loves acting. 	Guide to Stanford (universal) dependencies||publication||UniversalStanfordDependencies:Across-linguistictypology Marie-CatherinedeMarneffe  ,TimothyDozat ? ,NataliaSilveira ? , KatriHaverinen  ,FilipGinter  ,JoakimNivre / ,ChristopherD.Manning ?   LinguisticsDepartment,TheOhioStateUniversity ? Linguisticsand  ComputerScienceDepartments,StanfordUniversity  DepartmentofInformationTechnology,UniversityofTurku / DepartmentofLinguisticsandPhilology,UppsalaUniversity Abstract RevisitingthenowdefactostandardStanforddependencyrepresentation,weproposeanimprovedtaxonomytocapturegrammatical relationsacrosslanguages,includingmorphologicallyrichones.Wesuggestatwo-layeredtaxonomy:asetofbroadlyattested universalgrammaticalrelations,towhichrelationscanbeadded.WeemphasizethelexicaliststanceoftheStanford Dependencies,whichleadstoaparticular,partiallynewtreatmentofcompounding,prepositions,andmorphology.Weshowhow existingdependencyschemesforseverallanguagesmapontotheuniversaltaxonomyproposedhereandclosewithconsiderationof practicalimplicationsofdependencyrepresentationchoicesforNLPapplications,inparticularparsing. Keywords: dependencygrammar,StanfordDependencies,grammaticaltaxonomy 1.Introduction TheStanfordDependencies(SD)representation(de Marneffeetal.,2006)wasoriginallydevelopedasaprac- ticalrepresentationofEnglishsyntax,aimedatnaturallan- guageunderstanding(NLU)applications.However,itwas deeplyrootedingrammaticalrelation-basedsyntactictra- ditions,whichhavelongemphasizedcross-linguisticde- scription.Faithfulnesstotheseoriginswasattenuatedby desideratafromourNLUapplicationsandthedesirefora simple,uniformrepresentation,whichwaseasilyintelligi- blebynon-experts(deMarneffeandManning,2008).Nev- ertheless,itisreasonabletosupposethattheseadditional goalsdonotdetractfromcross-linguisticapplicability. Inthispaper,weattempta(post-hoc)reconstructionof theunderlyingtypologyoftheStanfordDependenciesrep- resentation.Thisnotonlygivesinsightsintohowtheap- proachmightbeappliedtootherlanguages,butalsogives anopportunitytoreconsidersomeofthedecisionsmadein theoriginalscheme,aimingtoproposeanimprovedtaxon- omy,evenforEnglish.Wesuggestataxonomywhichhas atitscoreasetofverybroadlyattestedgrammaticalrela- tions,supplementedasneededbysubtypesforlanguage- particularrelations,whichcapturephenomenaimportantto thesyntaxofindividuallanguagesorlanguagefamilies. Weattempttomakethebasiccoremoreapplicable,both cross-linguisticallyandacrossgenres,andmorefaithfulto thedesignprinciplesindeMarneffe&Manning(2008). Weconsiderhowtotreatgrammaticalrelationsinmorpho- logicallyrichlanguages,includingachievinganappropri- ateparallelismbetweenexpressinggrammaticalrelations byprepositionsversusmorphology.Weshowhowexisting dependencyschemesforotherlanguageswhichdrawfrom StanfordDependencies(Changetal.,2009;Boscoetal., 2013;Haverinenetal.,2013;Serajietal.,2013;McDon- aldetal.,2013;Tsarfaty,2013)canbemappedontothe newtaxonomyproposedhere.Weemphasizethelexicalist stanceofbothmostworkinNLPandthesyntactictheory onwhichStanfordDependenciesisbased,andhenceargue foraparticulartreatmentofcompoundingandmorphology. Wealsodiscussthedifferentformsofdependencyrepre- sentationthatshouldbeavailableforSDtobemaximally usefulforawiderangeofNLPapplications,convergingon threeversions:thebasicone,theenhancedone(whichadds extradependencies),andaparticularformforparsing. 2.Aproposeduniversaltaxonomy Table1givesthetaxonomyweproposefortheuniversal grammaticalrelations,withatotalof42relations.These relationsaretakentobebroadlysupportedacrossmanylan- guagesinthetypologicallinguisticliterature. 1 2.1.Therepresentationbuildsonlexicalism Anunder-elaboratedpartofthedesignprinciplein (deMarneffeandManning,2008)isthatSDadoptsthe lex- icalisthypothesis insyntax,wherebygrammaticalrelations shouldbebetweenwholewords(or lexemes ).Thereisa longstanding,unresolveddebateinlinguisticsbetweenthe- orieswhichattempttobuildupbothwordsandphrases usingthesamecompositionalsyntacticmechanisms(and inwhichthenotionofawordhasminimalprivilegedex- istence)versusthosetheorieswherethewordisafunda- mentalunitandwhichseethemorphologicalprocessesthat buildupwordsasfundamentallydifferentfromandhid- dentothosethatbuildupsentences,sometimestermedthe lexicalintegrityprinciple (Chomsky,1970;Bresnanand Mchombo,1995;Aronoff,2007).Forapracticalcompu- tationalmodel,therearegreatadvantagestoalexicalistap- proach.However,thereremaincertaindifcases,such ashowtodealwithcertain clitics (ZwickyandPullum, 1983),phonologicallyboundwordswhichbehavelikesyn- tacticwords(wefollowmanytreebanksinseparatingthem aswordsandhavingthemparticipateinthesyntax)and howtotreatwordsthataresplitinuneditedwriting(see section2.4.). 1 Thisisnottosaythatalllanguageshaveallthesegrammatical relations.E.g.,manylanguages,fromEnglishtoChiche ‹ wa,allow verbstotakemorethanoneobject,butotherlanguages,suchas French,donot.Nevertheless, iobj isbroadlyattested.  Coredependentsofclausalpredicates NominaldepPredicatedep nsubjcsubj nsubjpasscsubjpass dobjccompxcomp iobj Non-coredependentsofclausalpredicates NominaldepPredicatedepword advcladvmod neg nmodncmod Specialclausaldependents NominaldepAuxiliaryOther vocativeauxmark discourseauxpasspunct explcop Coordination conjcc Noundependents NominaldepPredicatedepword nummodrelclamod apposdet nmodncmodneg Compoundingandunanalyzed compoundmwegoeswith nameforeign Case-marking,prepositions,possessive case Loosejoiningrelations listparataxisremnant dislocatedreparandum Other Sentenceheaddependency rootdep Table1:DependenciesinuniversalStanfordDependencies. Note: nmod , ncmod ,  ,and neg appearintwoplaces. 2.2.Dependentsofclausalpredicates Thesystemofclausaldependentsmostcloselyfol- lowsLexical-FunctionalGrammar(LFG)(Bresnan,2001). However,thetaxonomydiffersfromLFGinseveralre- spects.First,thecleardistinctionbetweencorearguments andotherdependentsismade,butthedistinctionbetween adjunctsandobliquearguments(Radford,1988)istaken tobesufsubtle,unclear,andarguedoverthatitis eliminated. 2 Second,themodelrevertstothetraditional grammarnotionofdirectobjectandotherobjects.Incases 2 TheoriginalPennTreebankannotatorsalsodecidednottotry tomarkargumentsvs.adjuncts(Tayloretal.,2003).Conversely, thePennChineseTreebankdoestrytomakeanadjunct/argument distinction(Xueetal.,2005)andeffectivelyPropBank(Palmer etal.,2005)addsanargument/adjunctdistinctionoverlaytothe EnglishPennTreebank. ofmultipleobjects,thetheme/patientargumentisthedirect object.Finally,thetaxonomyaimstoclearlyindicateinthe dependencylabel(i)anon-canonicalvoicesubject(where theproto-agentargumentisnotsubject,i.e.,inpassives) and(ii)whetherdependentsarenounphrase(NP)argu- mentsversusintroducinganewclause/predicate.Adesign goalofSDhasbeentodistinguishindependencynames whereanewclauseisbeingintroduced(andsowedistin- guish nsubj and csubj ; dobj and ccomp ,butalso advmod and advcl ).WefollowLFGinincludingadistinctionbe- tween ccomp and xcomp forclausalcomplementsthatare standalone(haveaninternalsubject)versusthosehaving necessarycontrol(omission)ofthedependentpredicate's subject(haveanexternalsubject). 3 Otheraspectsofthetypologyarelessregularbutstill important.Thenon-coreclausaldependentsareallmodi- Thedistinctionbetweenafulladverbialclause advcl andaparticipialorveclause  issimi- larbutnotexactlyparalleltothe ccomp / xcomp distinction. 4 Clauseheadshavemanyotherspecialdependents,includ- ingperiphrasticauxiliaries,markers(complementizersand subordinatingconjunctions)aswellasvocativesanddis- courseelements(like well or um ).Conjunctionscombine elementsofmanycategories.UndertheSDdesignprinci- ples,the conj relationconnectslexicalheads. 2.3.Treatmentofcopulas SDhasadvocatedatreatmentofthecopulaﬁbeﬂwhereit isnottheheadofaclause,butratherthedependentofalex- icalpredicate,asin(1a).Suchananalysisismotivatedby thefactthatmanylanguagesoftenoralwayslackanovert copulainsuchconstructions,asintheRussian(1b).Similar constructionsariseeveninEnglishifweconsiderso-called raising-to-objectorsmallclauseconstructions.Underthe basicanalysisproposedforSD,thepredicatecomplement isnotlinkedtoitssubjectargument,butintheenhanced representation(seebelow),thelinkageisthenparallelto thetreatmentinazerocopulalanguage,asin(1c). (1)a. Ivan is the best dancer nsubj cop det amod 3 Thelatterisusedtorepresentcontrolandraisingconstruc- tions,includingcasesofraising-to-objectorso-calledﬁexcep- tionalcasemarkingﬂ.Thetreatmentofthislastgroupofphenom- enaisoneofthelargestsubstantivebreakswiththePennTree- bankannotationtradition,whichfollowsChomskyanapproaches fromtheextendedstandardtheory,Government-BindingTheory (Chomsky,1981)etseq.,intreatingsuchconstructionsashaving acompleteclausalcomplementwithﬁexceptionalcasemarkingﬂ, ratherthananobjectinthehigherclause. 4 For ccomp vs. xcomp ,thedifferenceisacontrolled subject.Whilean xcomp isalwaystherearealsonon-  ccomp ,suchasina for-to ve(ﬁIarranged[forher togobybus]ﬂ).For advcl vs.  thedistinctionisa clausevs.aclause,thoughusuallyareducedonelack- ingasubject.Wegeneralizetheprevious partmod and infmod to  formoregeneralityandcross-linguisticapplicability.We use  ratherthan vmod sincethiscanalsobeanad- jective,asinﬁShehesitated[unabletorememberhisname]ﬂ.  b.Russian: Ivan lu  c  sij tancor Ivan best dancer nsubj amod c. I judge Ivan the best dancer nsubj dobj xcomp det amod nsubj 2.4.vs.compounding Theuniversalschemekeepstherichtaxonomyofnoun dependentsthatareoneofthestrengthsofSD.Anim- portantpartofthetypologyistodifferentiatecompounds (multi-rootlexemes)fromorcomplementa- tion.Underalexicalistapproach,compoundwordsarefun- damentallydifferentfromcasesofphrasal Therearethreerelationsforcompounding.Weuse mwe forxedgrammaticizedexpressionswithfunctionwords (e.g., insteadof : mwe (of,instead),Fr. plut ‹ otque ﬁrather thanﬂ: mwe (que,plut ‹ ot)), name forpropernounsconstituted ofmultiplenominalelements,asintheFinnishandItalian dependencytreebanks, 5 and compound tolabelothertypes ofmulti-wordlexemes.Thus compound isusedforany kindofX 0 compounding:nouncompounds(e.g., phone book ),butalsoverbandadjectivecompoundsthataremore commoninotherlanguages(suchasPersianorJapanese lightverbconstructions);fornumbers(e.g., threethou- sandbooks gives compound (thousand,three));forparticles ofphrasalverbs(e.g., putup : compound (put,up)). 2.5.Treatmentofprepositionsandcasemarking Amajorproposedchangefromtheextantversionsof SDisanewtreatmentofprepositionstoprovideauniform analysisofprepositionsandcaseinmorphologicallyrich languages.Theanalysiswechoseistopushalltheway thedesignprincipleofhavingdirectlinksbetweencon- tentwords.Weabandontreatingaprepositionasame- diatorbetweenawordanditsobject,and,in- stead,anycase-markingelement(includingprepositions, postpositions,andcliticcasemarkers)willbetreatedasa dependentofthenounitattachestoorintroduces.Thepro- posedanalysisisshownin(2): nmod labelstherelation betweenthetwocontentwords,whereastheprepositionis nowviewedasa case dependingonitscomplement.In general, nmod expressessomeformofobliqueoradjunct relationfurtherbythe case . (2)a. the Chair 's of det nmod case b. the of of the Chair det nmod case det c.French: le bureau du pr ´ esident the of of theChair det nmod case 5 Thatis, name wouldbeusedbetweenthewordsofﬁHillary RodhamClintonﬂbutnottoreplacetheusualrelationsinaphrasal orclausalnamelikeﬁTheLordoftheRingsﬂ. Thetreatmentofcasemarkingisillustratedin(3).In (3a), at inHebrewisaseparatetokenindicatinganac- cusativeobject:thecasemarkerdependsontheobject.In (3c),weshowtheanalysiswhencasemarkersaremor- phemes.Thecasemorphemeisnotdividedoffthenoun asaseparate case dependent,butthenounasawholeis analyzedasa nmod oftheverb.Toovertlymarkcase,we includePOStagsintherepresentationasshownin(3b)and (3d).WeusetheuniversalPOStagsetfromPetrovetal. (2012)towhichweappendcaseinformation. (3)a.Hebrew: wkfraiti at hsrj andwhenIsaw ACC themovie dobj case b. dobj (wkfraiti/ VERB ,hsrj/ NOUN ) case (hsrj/ NOUN ,at/ PRT - ACC ) c.Russian: Ya napisal pis'mo perom I wrote theletter withaquill nsubj dobj nmod d. nsubj (napisal/ VERB ,Ya/ NOUN - NOM ) dobj (napisal/ VERB ,pis'mo/ NOUN - ACC ) nmod (napisal/ VERB ,perom/ NOUN - INSTR ) Thistreatmentprovidesparallelismbetweendifferent constructionsacrossandwithinlanguages.Agoodresultis thatwenowhavegreaterparallelismbetweenprepositional phrasesandsubordinateclauses,whichareinpracticeoften introducedbyapreposition,asin(4). (4)a. Sue left after the rehearsal nsubj nmod case det b. Sue left after we did nsubj advcl mark nsubj Wealsoobtainparallelconstructionsforthepossessive alternationasshownin(2),forvariantformswithcase,a prepositionorapostpositioninFinnish,asshownin(5), andforthedativealternationwheretheprepositionalcon- structiongetsasimilaranalysistothedoubleobjectcon- struction,see(6). (5)a.Finnish: etsi ¨ a ilman johtolankaa tosearch without clue . PARTITIVE case nmod b. etsi ¨ a taskulampun kanssa tosearch torch . GENITIVE with nmod case c. etsi ¨ a johtolangatta tosearch clue . ABESSIVE nmod (6)a. give thechildren thetoys iobj dobj  b. give thetoys to thechildren nmod dobj case c.French: donner lesjouets aux enfants give thetoys tothe children dobj nmod case Anotheradvantageofthisnewanalysisisthatitpro- videsatreatmentofprepositionalphrasesthatarepredica- tivecomplementsofﬁbeﬂasin(7)thatisconsistentwiththe treatmentofnominalpredicativecomplements,asin(1). (7) Sue is in shape nsubj cop case SDisasurfacesyntacticrepresentation,whichdoesnot representsemanticroles.Thesemanticrolesof arehardtocategorizeandhardtodetermine.Wefeelthat thereisalotofuseforarepresentationwhichworkssolely intermsoftheovertrole-markingresourcesofeachlan- guage.Thisissupportedbymanyrichlanguage-particular traditionsofgrammaticalanalysis,whetherviaSanskrit casesorthecaseparticlesonJapanese bunsetsu . Prepositionssometimesintroduceaclauseastheircom- plement,e.g.,(8a).Followingtheprinciplethatdependen- ciesdomarkwherenewclausesareintroduced,thisrelation shouldhaveadifferentnamefrom nmod ,andwesuggest callingit ncmod ﬁnominalizedclauseUnderthe proposednewanalysis,theheadoftheof data will be upset .Theresultwillbetheanalysisin(8b). (8)a.Wehavenodataaboutwhetherusersareupset. b. data about whether users are upset ncmod case mark nsubj cop Anotherissueiswhatanalysistogivetocasesofstacked prepositions,suchas outof .Ourproposalisthatallsuch casesshouldberegardedassomeformof mwe ,asin(9b). (9)a.Outofallthis,somethinggoodwillcome. b. Out of all this ... come mwe predet case nmod 2.6.Informaltextgenres Followingthepracticalapproachusedinpart-of-speech taggingofrecentLDCtreebanks,weintroducetherelation goeswith toconnectmultipletokensthatcorrespondtoa singlestandardword,asaresultofreanalysisofwordsas compounds(ﬁhandsomeﬂforﬁhandsomeﬂ)orinputerror (ﬁotherﬂforﬁotherﬂ).Weuse foreign tolabelsequences offoreignwords.Toindicateoverriddenina speechrepair,weuse reparandum ,asin(10). (10) Go to the righ- to the left. reparandum case det case det nmod Theﬁloosejoiningrelationsﬂaimatarobustanalysisof moreinformalformsoftext,whicharenowcommonin NLPapplications.Informalwrittentextoftencontainslists ofcomparableitems,whichareparsedassinglesentences. Emailsignaturesinparticularcontainthesestructures,in theformofcontactinformation.FollowingdeMarneffeet al.(2013),weusethe list , parataxis ,(and appos )relations tolabelthesekindsofstructures.Therelation parataxis is alsousedinmoreformalwritingforconstructionssuchas sentencesjoinedwithacolon. The dislocated relationcapturespreposed(topics)and postposedelements.The remnant relationisusedtopro- videatreatmentofellipsis(inthecaseofgappingorstrip- ping,wherepredicationalorverbalmaterialgetselided), somethingthatwaslackinginearlierversionsofSD.It providesabasisforbeingabletoreconstructdependencies intheenhancedversionofSD.Forexample,in(11),the remnant relationsenableustocorrectlyretrievethesub- jectsandobjectsintheclauseswithanelidedverb. (11) John won bronze, Mary silver, and Sandy gold. nsubj dobj remnant remnant remnant remnant Incontrast,inright-node-raising(RNR)(12)andVP- ellipsis(13)constructionsinwhichsomekindofpredica- tionalorverbalmaterialisstillpresent,the remnant relation isnotused.InRNR,theverbsarecoordinatedandtheob- jectisa dobj oftheverb.InVP-ellipsis,wekeepthe auxiliaryasthehead,asshownin(13). (12) John bought and ate anapple. nsubj cc conj dobj (13) John will win gold and Mary will too. nsubj aux dobj cc nsubj conj advmod 2.7.Language-particularrelations Inadditiontoauniversaldependencytaxonomy,itisde- sirabletorecognizegrammaticalrelationsthatareparticu- lartoonelanguageorasmallgroupofrelatedlanguages. Suchlanguage-particularrelationsarenecessarytoaccu- ratelycapturethegeniusofaparticularlanguagebutwill notinvolveconceptsthatgeneralizebroadly.Thesugges- tionhereisthattheserelationsshouldalwaysberegarded asasubtypeofanexistingUniversalSDrelation.The SDrelationshaveataxonomicorganization(deMarneffe etal.,2006),andsomeoftheuniversalrelationsareal- readysubtypesofeachother(e.g., auxpass isasubtypeof aux ).Language-particularrelationsthatseemusefultodis- tinguishforEnglishareincludedatthebottomofTable2: npmod forbarenominalofpredicateslackinga preposition,amongwhichinparticularthereis tmod for bareNPtemporal poss forpossessives,sincethe syntaxofprenominalpossessionisverydistinctfrompost- nominal(whichmayalsoexpresspossession); predet forwordssuchas all thatprecederegulardetermin- ersand preconj forwordsprecedingaconjunctionlike ei- ther ;and prt forverbparticles.  3.Mappingtoexistingschemes Therehasrecentlybeenanefforttopushtowardsho- mogeneityacrossresourcesfordifferentlanguagesand tocomeupwithcross-linguisticallyconsistentannotation aimedatmulti-lingualtechnology,forpart-of-speechtagset (Petrovetal.,2012)aswellasfordependencyrepresenta- tion(McDonaldetal.,2013).TheschemeproposedinMc- Donaldetal.(2013)tookSDasastartingpoint.Annota- torsforsixdifferentlanguages(German,English,Swedish, Spanish,FrenchandKorean)producedannotationguide- linesforthelanguagetheywereworkingon,keepingthe labelandconstructionsetascloseaspossibletotheoriginal EnglishSDrepresentation.Theywereonlyallowedtoadd labelsforphenomenathatdonotexistinEnglish.Giventhe setsofrelationsobtainedforthedifferentlanguages,ahar- monizationstepwasperformedtomaximizeconsistencyof annotationacrosslanguages.However,thisrigidstrategy lostsomeimportantdistinctions,suchasthedistinctionbe- tweencompoundingandphrasalwhilemain- tainingsomedistinctionsthatarebestabandoned,suchasa distinctionbetweenvalandparticipial McDonaldetal.(2013)doesnotaddressgivinganele- ganttreatmentofmorphologicallyrichlanguages.Incon- trast,Tsarfaty(2013)proposestotreatmorphologyassyn- taxinherdependencyproposal,illustratedwithHebrew. However,thisrepresentationchoicectswiththelexi- calistapproachofSD.Herewetakeuphergoaloftrying togiveauniformtreatmentofbothmorphologicallyrich andmorphologicallypoorlanguages,butsuggestachieving thegoalinadifferentway,whichmaintainsalexicalistap- proach(seeSection2.5.).Table2showsacomparisonbe- tweentheevolutionoftheSDscheme(StanfordDependen- ciesv2.0.0,usedintheSANCLsharedtask,andStanford Dependenciesv3.3.0,the2013/11/12version),thepropos- alsinMcDonaldetal.(2013)(GSD)andinTsarfaty(2013) (TSD),andthedependencysetproposedhere(USD). Existingdependencysetsforotherlanguagescanbe fairlystraightforwardlymappedontoournewproposal. EveniftheschemesexaminedhereareﬁSD-centricﬂ,they dealtwithparticularconstructionspresentineachlanguage andpositednewrelationswhennecessary.Themappingis lessdifbecauseUSDadoptssomeoftheideasandre- lationsthatweredevelopedfortheseothertreebanks, suchasthecontentwordasheadanalysisofprepositional phrasesfromtheTurkuDependencyTreebank(TDT).In table3,weshowhowtheFinnish(Haverinenetal.,2013), Italian(Boscoetal.,2013),Chinese(Changetal.,2009) andPersian(Serajietal.,2013)schemescanbemapped ontotheproposeduniversaltaxonomy(USD).Theboldla- belsarerelations,andtheyaresubtypes ofthecorrespondingUSDrelationintherow.Gapsin- dicateexistingconstructionsinthelanguagethatwerenot capturedintheoriginalscheme(theUSDlabelisapplicable there);  indicatesconstructionsthatarenotpresentinthe language.Sincecopularverbsarenotheadsanymore,the attr relationisremoved,requiringtotheex- istinganalysesofcopularsentencesforItalianandChinese. Wealsointroducedextrarelationsforcertainconstructions, whichsomeschemeshadnotincorporatedyet. ForFinnish,therelation rel (forrelativemarker)willbe mappedtowhateversyntacticroletherelativeisplayingin therelativeclause( nsubj , dobj ,etc.),informationwhichis presentinthesecondannotationlayeroftheTDTcorpus. ISDTistheconversionoftheMIDTItaliandependency schemetoSD.Someofthe clit usesinISDT(forx- ivepronounsinpronominalverbsŒfrequentinRomance languagesŒsuchasFr. sedouter ﬁtosuspectﬂ)willbeen- compassedby expl .However,whenthexivepronoun cantrulybeadirectorindirectobject,itgetsassignedthe correspondingobjectrelation. Chinesehasserialverbconstructionswhichmightnow be compound (andnot conj ).Wetreatpost-nominallocal- izersandprepositionsasaformofcase. InPersian,therearenorelativepronounsand rel was usedfortheedrelativemarker,butitcanbemappedto mark .UPDThasa fw relationbetweensequencesoffor- eignwords,unanalyzedwithinPersiangrammar,whichwe adopt,namingit foreign .WealsoadopttheUPDT dep-top relationusedforafrontedelementthatintroducesthetopic ofasentence,butwegeneralizeitto dislocated toaccount forpostposedelementsaswellas.Rightdislocatedele- mentsarefrequentinspokenlanguages:e.g.,Fr. fautpas lamanger,lap ‹ ate (literally,ﬁneednotiteat,thedoughﬂ). Labelsofrelationswillbeharmonized tobesharedbetweenlanguages:Chinese assmod willbe mappedto poss ,andPersian dep-top to topic . 4.DifferentformsofStanfordDependencies ThecurrentStanfordconverterprovidesanumberof variantformsofSD,ofwhichthemostusedarethe ba- sic dependencytree,andthe collapsed,cc-processed form thataddsextradependencyarcs,restructuresprepositions tonotbeheads,andspreadsrelationsacrossconjunctions. Thissectionsuggestssomenewideasforhowtoprovide potentiallylessbutdifferentoptions. Oneconcernaboutourproposedtaxonomyisthat straightforwardparsingtoUSDislikelytobeharderfor parsersthanthecurrentrepresentation(forEnglish).It isnowfairlywellknownthat,whiledependencyrepre- sentationsinwhichcontentwordsaremadeheadstend tohelpsemanticallyorienteddownstreamapplications,de- pendencyparsingnumbersarehigherifyoumakeauxiliary verbsheads,ifyouanalyzelongconjunctionsbychaining (ratherthanhavingasingleheadofthewholeconstruction), andifyoumakeprepositionstheheadofprepositional phrases(Schwartzetal.,2012;Elmingetal.,2013).The generalizationisthatdependencyparsers,perhapsinpar- ticulartheefshift-reduce-styledependencyparsers likeMaltParser(Nivreetal.,2007),workbestthemorethe dependencyrepresentationlookslikeachainofshortde- pendenciesalongthesentence.UndertheproposedUSD, SDwouldthenbemakingtheﬁwrongﬂchoiceineachcase. However,itseemswrong-headedtochoosealinguis- ticrepresentationtomaximizeparserperformance,rather thanbasedonthelinguisticqualityoftherepresentation anditsusefulnessforapplicationsthatbuildfurtherpro- cessingontopofit.Rather,itmaybeusefultodopars- ingusingatransformationofthetargetdependencysys- tem.Inconstituencyparsing,itiscompletelyusualforthe targetrepresentationtobetransformedsoastoimprove  SDv2.0.0SDv3.3.0GSDTSDUSDNotes nsubjnsubjnsubjnsubjnsubj X csubjcsubjcsubjcsubjcsubj X nsubjpassnsubjpassnsubjpassnsubjpassnsubjpass X csubjpasscsubjpasscsubjpasscsubjpasscsubjpass X dobjdobjdobjdobjdobj X iobjiobjiobjiobjiobj X (TSDalsohas gobj forgenitiveobject) ccompccompccompccompccompUSD&TSDasclausewithinternalsubject,not xcompxcompxcompxcompxcompUSD&TSDasclausewithexternalsubject,not acompacompacompacompŒ acomp canbegeneralizedinto xcomp attrŒattrŒŒ attr removed: wh- isheador xcomp (withcopulaheadoption) advmodadvmodadvmodadvmodadvmod X advcladvcladvclŒadvclTSDomitsbutneededtopreserveclauseboundaries purpclŒŒŒŒFoldedinto advcl negnegnegnegnegAswellasadverbial not,never ,USDextendstonegative det like no detdetdetdetdet X (TSD dem and def assubtypesof det ) amodamodamodamodamod X apposapposapposapposappos X abbrevŒŒabbrevŒParentheticalabbreviationsbecomecasesof appos numnumnumnummodnummodRenamedforclarity rcmodrcmodrcmodrcmodrelcl X partmodpartmodpartmod?Make partmod , infmod into  ;use(rich)POStodistinguish infmodinfmodinfmodinfmodMake partmod , infmod into  ;use(rich)POStodistinguish quantmodquantmodadvmod?ŒGenerallyfoldedinto advmod rootrootROOTrootroot punctpunctppunctpunct auxauxauxauxaux X (TSDadds qaux forquestionauxiliary.veisnow mark .) auxpassauxpassauxpassauxpassauxpass X copcopcopcopcopGSDhas cop onlyincontent-headversion explexplexplexplexplSubjectandobjectexpletives,frozenxives(Fr. sedouter ) markmarkmarkmarkmark X to introducinganvewillnowbe mark (insteadof aux ) complmŒŒcomplmŒRemoveanduse mark morebroadly ŒdiscourseŒparataxis?discourseAgapinoriginalandothertypologies ŒŒŒŒvocativeAgapinoriginalandothertypologies depdepdepdepdepGSDusesfor vocative and discourse relŒrelrelŒConverter'sunresolvedonesnow dep ;TSD rel isreally mark prepprepadpmodprepmodcaseUSD case isdependentofNPnotofthing ŒŒnmodŒŒEquivalentto nmod below pobjpobjadpobjpobjnmod nmod nowgoesfromthingtoNPofPP pcomppcompadpcomppcompncmod ncmod goesfromthingtoclause ŒŒadpprep/casecaseTSDhasN/A/G/Dsubtypes,butcan'tkeepaddingforallcases possessivepossessiveadpgenŒViewasamanifestationof case nnnncompmodnncompoundGeneralize nn tolightverbs,etc.;X 0 compoundingnot ŒŒmweŒnameMulti-wordpropernouns(e.g., JohnSmith )asinTDTandISDT numbernumbernumnummod?ŒRegardedastypeofcompound;using nummod iswrong mwemwemwemwemweFixedexpressionswithfunctionwords( sothat,allbut,dueto,... ) ŒgoeswithŒŒgoeswithFororthographicerrors: other ŒŒŒŒforeignLinearanalysisofforeignwords(headisleft-most)asinUPDT ŒŒŒŒreparandumForoverriddeninspeechrepairs conjconjconjconjconj X cccccccccc X parataxisparataxisparataxisparataxisparataxis X ŒŒŒŒlistUsedforinformalliststructures,signatureblocks,etc. ŒŒŒŒremnantUsedtogiveatreatmentofellipsiswithoutemptynodes ŒŒŒŒdislocatedPreposedtopicsanddislocatedelementsasinUPDT Englishparticular npadvmodnpadvmodnmodadvmod? npmod Asubtypeof nmod tmodtmodadvmodtmod tmod Asubtypeof npmod predetpredetŒpredet predet Asubtypeof det preconjpreconjccpreconj preconj Asubtypeof cc prtprtprt? prt Asubtypeof compound posspossposspossmod poss Asubtypeof case Table2:ComparisonofproposalsonEnglish:SD,McDonaldetal.(2013)(GSD),Tsarfaty(2013)(TSD)andours(USD).  TDTISDTChineseUPDTUSD nsubjnsubjnsubj,topnsubjnsubj csubjcsubj  csubj  nsubjpassnsubjpassnsubjpassnsubjpass  csubjpass  csubjpass dobjdobj,clitdobjdobjdobj  iobj,clitiobj  iobj ccomp, iccomp ccompccomp,rcompccompccomp xcomp,acompxcomp,acompxcomp,acompxcomp  attrattr  advmod,quantmodadvmodadvmod,dvpmodadvmod,quantmodadvmod advcladvclsomeadvmodadvcladvcl negnegnegnegneg detdet, predet detdet, predet det amodamodamodamodamod apposapposprnmodapposappos numnumnummod,ordmodnumnummod rcmodrcmodrcmodrcmodrelcl partmod,infmodpartmodvmod   rootrootrootrootroot punctpunctpunctpunctpunct auxauxasp,mmodauxaux auxpassauxpasspassauxpassauxpass copcopcopcopcop  expl,clit  expl complm,markmarkcpmcomplm,mark, rel mark intjdiscourse discourse vocdep-vocvocative depcomp,moddepdepdep poss , gobj , gsubj ,pobj, poss ,pobj, lobj , assmod ,pobj, poss , cpobj nmod nommodnpadvmod, tmod clf,range, tmod npadvmod, tmod  pcomppccomp, lccomp  ncmod adpospossessive,prepassm,prep, ba , dvpm , locacc ,prep, cprep case number,nn, prt number,nnnn,someconjnumber,nn, prt , f nsubj j dobj j acomp j prep g -lvc compound namennp name somedepmweprtmodmwemwe goeswith goeswith fwforeign reparandum conjconjconj,etc,comodconjconj cc, preconj cc, preconj cccc, preconjunct cc parataxisparataxisparataxisparataxis list ellipsis remnant dep-top dislocated Table3:MappingsoftheFinnish(TDT),Italian(ISDT),ChineseandPersian(UPDT)schemestoUSD. parsingnumbers,suchasbyhead-lexicalization(Collins, 2003),bymanualorautomaticsubcategorizationofcat- egories(KleinandManning,2003;Petrovetal.,2006), andevenbyothermethodssuchasunarychaincontrac- tion(Finkeletal.,2008).Afterparsing,adetransformation processreconstructstreesinthetargetrepresentation.This kindoftransform-detransformarchitectureisatpresentless commonindependencyparsing,althoughNilsson,Nivre &Hall(2006;2007)dothisforcoordinationandverb groups,andpseudo-projectiveparsing(NivreandNilsson, 2005)canalsobeseenasaninstanceofthisarchitecture. Atransform-detransformarchitectureshouldbecomemore standardindependencyparsing.Wethereforeproposea parsing representationthatchangessomeofthedepen- dencyheadchoicestomaximizeparsingperformance.This requiresdevelopingtoolstoconvertseamlesslybothways betweenthe basic and parsing representations. 6 Sincethenewtreatmentofprepositionalphrasesbasi- callydoeswhatthe collapsed representationwasdesigned todo(puttingadirectlinkbetweenthenouncomplementof aprepositionandwhatitexceptfornotrenaming 6 AsmallpartofthisisinplaceintheStanfordconverter,inthe abilitytogeneratecopula-andcontent-headversionsfromtrees.  thedependencyrelation,the collapsed representationonits ownhaslessutility.However,theideasofhavingextrade- pendenciestomarkexternalsubjectsandtheexternalrole inrelativeclausesisuseful,therenamingofdependencies toincludecaseorprepositioninformationhelpsinmany applications,andspreadingrelationsoverconjunctionsis usefulforrelationextraction.Thesetransforma- tionscanbeprovidedinan enhanced representation. WethussuggestprovidingthreeversionsofStanfordDe- pendencies: basic , enhanced ,and parsing . 5.Conclusion Weproposedataxonomyofgrammaticalrelationsappli- cabletoavarietyoflanguages,developingtheimplications ofalexicalistapproachforthetreatmentofmorphology andcompounding.Someofthedecisionsmadeonlinguis- ticgroundsareatoddswithwhatworksbestforprocessing tools.Wesuggestedthatthetransform-detransformarchi- tecturestandardlyusedinconstituencyparsingisthesolu- tiontoadoptfordependencyparsing.Weworkedoutthe mappingofexistingdependencyresourcesfordifferentlan- guagestothetaxonomyproposedhere.Wehopethiswork willenhanceconsistencyinannotationbetweenlanguages andfurtherfacilitatecross-lingualapplications. 6.References Aronoff,M.(2007).Inthebeginningwastheword. Lan- guage ,83:803Œ830. Bosco,C.,Montemagni,S.,andSimi,M.(2013).Convert- ingItaliantreebanks:TowardsanItalianStanforddepen- dencytreebank.In SeventhLinguisticAnnotationWork- shop&InteroperabilitywithDiscourse . Bresnan,J.andMchombo,S.A.(1995).Thelexicalin- tegrityprinciple:EvidencefromBantu. NaturalLan- guageandLinguisticTheory ,13:181Œ254. Bresnan,J.(2001). Lexical-FunctionalSyntax .Blackwell, Oxford. Chang,P.-C.,Tseng,H.,Jurafsky,D.,andManning,C.D. (2009).DiscriminativereorderingwithChinesegram- maticalrelationsfeatures.In ThirdWorkshoponSyntax andStructureinStatisticalTranslation ,pages51Œ59. Chomsky,N.(1970).Remarksonnominalization.InJa- cobs,R.A.andRosenbaum,P.S.,editors, Readings inEnglishtransformationalgrammar ,pages184Œ221. Ginn,Waltham,MA. Chomsky,N.(1981). LecturesonGovernmentandBind- ing .Foris,Dordrecht. Collins,M.(2003).Head-drivenstatisticalmodelsfor naturallanguageparsing. ComputationalLinguistics , 29:589Œ637. deMarneffe,M.-C.andManning,C.D.(2008).TheStan- fordtypeddependenciesrepresentation.In Workshopon Cross-frameworkandCross-domainParserEvaluation . deMarneffe,M.-C.,MacCartney,B.,andManning, C.D.(2006).Generatingtypeddependencyparsesfrom phrasestructureparses.In LREC . deMarneffe,M.-C.,Connor,M.,Silveira,N.,Bowman, S.R.,Dozat,T.,andManning,C.D.(2013).Morecon- structions,moregenres:ExtendingStanforddependen- cies.In DepLing2013 . Elming,J.,Johannsen,A.,Klerke,S.,Lapponi,E.,Mar- tinez,H.,andSøgaard,A.(2013).Down-streamef- fectsoftree-to-dependencyconversions.In NAACLHLT 2013 . Finkel,J.R.,Kleeman,A.,andManning,C.D.(2008).Ef- feature-based,conditionalrandomparsing. In ACL46 ,pages959Œ967. Haverinen,K.,Nyblom,J.,Viljanen,T.,Laippala,V.,Ko- honen,S.,Missil ¨ a,A.,Ojala,S.,Salakoski,T.,and Ginter,F.(2013).Buildingtheessentialresourcesfor Finnish:theTurkudependencytreebank. LanguageRe- sourcesandEvaluation .Inpress.Availableonline. Klein,D.andManning,C.D.(2003).Accurateunlexical- izedparsing.In ACL41 ,pages423Œ430. McDonald,R.,Nivre,J.,Quirmbach-Brundage,Y.,Gold- berg,Y.,Das,D.,Ganchev,K.,Hall,K.,Petrov, S.,Zhang,H.,T ¨ ackstr ¨ om,O.,Bedini,C.,Bertomeu Castell ´ o,N.,andLee,J.(2013).Universaldependency annotationformultilingualparsing.In ACL51 . Nilsson,J.,Nivre,J.,andHall,J.(2006).Graphtransfor- mationsindata-drivendependencyparsing.In COLING 21andACL44 ,pages257Œ264. Nilsson,J.,Nivre,J.,andHall,J.(2007).Treetransforma- tionsforinductivedependencyparsing.In ACL45 . Nivre,J.andNilsson,J.(2005).Pseudo-projectivedepen- dencyparsing.In ACL43 ,pages99Œ106. Nivre,J.,Hall,J.,Nilsson,J.,Chanev,A.,Eryigit,G., K ¨ ubler,S.,Marinov,S.,andMarsi,E.(2007).Malt- Parser:Alanguage-independentsystemfordata-driven dependencyparsing. NaturalLanguageEngineering , 13:95Œ135. Palmer,M.,Gildea,D.,andKingsbury,P.(2005).The propositionbank:Anannotatedcorpusofsemanticroles. ComputationalLinguistics ,31:71Œ105. Petrov,S.,Barrett,L.,Thibaux,R.,andKlein,D.(2006). Learningaccurate,compact,andinterpretabletreeanno- tation.In COLING21andACL44 ,pages433Œ440. Petrov,S.,Das,D.,andMcDonald,R.(2012).Auniversal part-of-speechtagset.In LREC . Radford,A.(1988). TransformationalGrammar .Cam- bridgeUniversityPress,Cambridge. Schwartz,R.,Abend,O.,andRappoport,A.(2012). Learnability-basedsyntacticannotationdesign.In COL- ING24 ,pages2405Œ2422. Seraji,M.,Jahani,C.,Megyesi,B.,andNivre,J.(2013). UppsalaPersiandependencytreebankannotationguide- lines.Technicalreport,UppsalaUniversity. Taylor,A.,Marcus,M.,andSantorini,B.(2003).ThePenn treebank:Anoverview.InAbeill ´ e,A.,editor, Building andUsingParsedCorpora ,volume20of Text,Speech andLanguageTechnology .Springer. Tsarfaty,R.(2013).Amorpho-syntacticschemeof Stanforddependencies.In ACL51 . Xue,N.,Xia,F.,Chiou,F.-D.,andPalmer,M.(2005).The PennChinesetreebank:Phrasestructureannotationof alargecorpus. NaturalLanguageEngineering ,11:207Œ 238. Zwicky,A.M.andPullum,G.K.(1983).Cliticizationvs. English n't . Language ,59:502Œ513.
Stanford dependencies	Verb valency	Grammar	Parsing	Universal dependencies
What is a feature-based context-free grammar and what features are usually specified for nouns and verbs in English?	A feature based context-free grammar is one in which the elements (variables and terminals) are tagged with features recording their grammatical and sometimes semantic characteristics, which are used by production rules to ensure that the words in the resulting productions agree grammatically (or semantically).  The features for nouns in English include number, countability, and sometimes gender, and animacy. The features for verbs in English include tense, number, person, and transitivity.	Which of the following types of words also needs features to agree with nouns and verbs?	determiners	adjectives |||adverbs |||prepositions	Intro to feature-based context free grammars (slides 1-10)||slides||Feature-Based  Grammar  Ling571  Deep Processing Techniques for NLP  February 3, 2016   Features in CFGs:  Agreement  !!Goal:   !!Support agreement of NP/VP, Det Nominal  !!Approach:  !!Augment CFG rules with features  !!Employ head features  !!Each phrase: VP, NP has head  !!Head: child that provides features to phrase  !!Associates grammatical role with word   !!VP Ð V; NP Ð Nom, etc   Simple Feature Grammars  !!S -> NP[NUM=?n] VP[NUM=?n]  !!NP[NUM=?n] -> N[NUM=?n]  !!NP[NUM=?n] ->  PropN[NUM=?n]  !!NP[NUM=?n] ->  Det[NUM=?n] N[NUM=?n]  !!Det[NUM=sg] -> 'this' | 'everyÕ  !!Det[NUM=pl] -> 'these' | 'allÕ  !!N[NUM=sg] -> ' dog ' | ' girl' | 'car' | ' child Õ !!N[NUM=pl] -> ' dogs ' | ' girls' | ' cars ' | ' children '  Parsing with Features  !!>>> cp =  load_parser ('grammars/ book_grammars /feat0.fcfgÕ)   !!>>> for tree in  cp.parse (tokens):   !!... print(tree)  !!(S[] (NP[NUM=' sg']   !!(PropN[NUM=' sg'] Kim))   !!(VP[NUM=' sg', TENSE=' pres '] !! (TV[NUM=' sg', TENSE=' pres '] likes)  !! (NP[NUM=' pl'] (N[NUM=' pl'] children))))   Feature Applications  !!Subcategorization:  !!Verb-Argument constraints  !!Number, type, characteristics of args (e.g. animate)  !!Also adjectives, nouns  !!Long distance dependencies  !!E.g. filler-gap relations in wh-questions, rel   Morphosyntactic  Features  !!Grammatical feature that influences morphological  or syntactic behavior   !!English:  !!Number:   !!Dog, dogs  !!Person:  !!Am; are; is  !!Case:  !!I Ð me; he Ð him;  etc  !!Countability :  Semantic Features  !!Grammatical features that influence semantic  (meaning)  behavior of associated units  !!E.g.:  !!?The rocks slept.  !!Many proposed:  !!Animacy : +/-  !!Natural gender: masculine, feminine, neuter  !!Human: +/-  !!Adult: +/-  !!Liquid: +/-   Aspect (J&M 17.4.2)  !!The climber hiked for six hours.  !!The climber hiked on Saturday.  !!The climber reached the summit on Saturday.  !!*The climber reached the summit for six hours.  !!Contrast:  !!Achievement (in an instant)  vs activity (for a time)   Unification and the Earley  Parser  !!Employ constraints to restrict addition to chart  !!Actually pretty straightforward  !!Augment rules with feature structure  !!Augment state (chart entries) with DAG  !!Prediction adds DAG from rule  !!Completion applies unification (on copies)  !!Adds entry only if current DAG is NOT subsumed    Summary  !!Features   !!Enable compact representation of grammatical  constraints  !!Capture basic linguistic patterns  !!Unification  !!Creates and maintains consistency over features  !!Integration with parsing allows filtering of ill- formed analyses    HW #5  Ling 571  Deep Techniques for NLP  February 3, 2016   Feature-based Parsing  !!Goals:  !!Explore the role of features in implementing linguistic  constraints.  !! Identify some of the challenges in building compact  constraints to define a precise grammar.   !!Gain some further familiarity with NLTK.   !!Apply feature-based grammars to perform grammar  checking.  !!Individual work   Task  !!Create grammar rules with features  !!Produce a single parse for grammatical sentences  !!Single parse per line  !!Reject ungrammatical sentences  !!Print blank line  !!Homework includes sentences and ÒkeyÓ   Feature Grammar in NLTK  !!NLTK supports feature-based grammars, including  !!ways of associating features with CFG rules  !!readers for feature grammars  !!.fcfg  files  !!parsers   !!Nltk.parse.FeatureEarleyChartParser  !!Nice discussion, examples in NLTK book CH. 9 (Ch. 8, ed1)  !!NOTE: HPSG-style comps list <NP,PP,..> NOT built into NLTK  !!Can be approximated with pseudo-list: e.g. [FIRST=?a, REST=?b]    !!For Extra-credit   Feature Structures  !!>>> fs1 =   nltk.FeatStruct (Ò[NUM=Ô plÕ]Ó)  !!>>> print fs1  !![NUM=Ô plÕ]  !!>>> print fs1[ÔNUMÕ]  !!pl !!More complex structure  !!>>> fs2 =  nltk.FeatStruct (Ò[POS=ÔNÕ,  !!                                          AGR=[NUM=Ô plÕ,PER=3]]Ó)   Reentrant Feature  Structures  !!First instance  !!Parenthesized integer: (1)  !!Subsequent instances:  !!ÔPointerÕ: -> (1)  !!>>> print(nltk.FeatStruct ("[A='a', B=(1)[C='c'],   D->(1)]Ó))  !![ A  =  ÔaÕ               ]  !![ B  = (1)  [ C = ÔcÕ]]  !![ D -> (1)               ]    Augmenting Grammars  !!Attach feature information to non-terminals, on  !!N[AGR=[NUM=' pl']] -> 'studentsÕ  !!N[AGR=[NUM=Õ sg']] -> 'studentÕ  !!So far, all values are literal or reentrant  !!Variables allow generalization: ?a  !!Allows  underspecification , e.g.  Det[GEN=?a]  !!NP[AGR=?a] ->  Det[AGR=?a] N[AGR=?a]   Mechanics  !!>>> fs3 =  nltk.FeatStruct (NUM=Ô plÕ,PER=3)  !!>>> fs4 =  nltk.FeatStruct (NUM=Ô plÕ)  !!>>> print fs4.unify(fs3)  !![NUM = Ô plÕ]  !![PER  =  3  ]
Grammatical agreement	Morphology	Context-free grammars
What is maternal language?	Scientists use the concept of "maternal language" to describe child-directed language. Scholars who study language acquisition began studying maternal language because they wanted to know how children's language learning is affected by the way mothers (or caregivers) talk to them.	What is an example of maternal language?	Baby talk	Using knowledge from your mother tongue to understand a new language.|||Using a different tone of voice when you talk to your mother than when you talk to other people|||Not being able to learn a second language as well as a first	NONE
Maternal language	Language acquisition
How do Skinner and Chomsky differ in their views on language as an instinct?	B.F. Skinner is known for the "Imitation" theory and the behaviorism perspective of learning language. That is, he believed that language is learned via imitation and practice. Oppositely, Noam Chomsky's viewpoint is that language is innate and develops naturally.	Who proposed the Language Acquisition Device (LAD) hypothetical module of the human mind?	Noam Chomsky	Neither|||B.F. Skinner	Chomsky's and Skinner's Theory of Language Acquisition||slides||N/A|||Language Acquisition - Skinner vs. Chomsky Clip||youtube||N/A
Language acquisition	B.F. Skinner	Noam Chomsky
How many features of language are there and what are they?	There are six features of language: cultural transmission, arbitrariness, discreteness, duality, displacement, and productivity.	Which of the following is an example of how the properties of language are related to branches of linguistics?	Duality is related to phonology.	Language is not arbitrary.|||Displacement is related to the study of morphology.|||Theoretical linguistics are related to language production.	Characteristics of Language presentation||slides||CHARACTERISTICS OF HUMAN  LANGUAGE 1. Productivity/ Creativity 2. Cultural transmission 3. Displacement 4. Arbitrariness 5. Duality 6. Discreetness.   PRODUCTIVITY/ CREATIVITY: Whatever we speak or create is productivity.                   Speaking itself is productivity.  Humans can talk about topics that are  displaced. They can talk about the things that  may be present or not, or talk about anything  in past, present or future.    CULTURAL TRANSMISSION: a. Language can be culturally transmitted. b. It cannot be transmitted through heredity. c. Animals transmit their cries through  heredity. d. What language the baby is going to speak is  determined by the culture the baby is born  into.    DISPLACEMENT : Human language can refer to the past and  future times. We can also refer to the things and  events that are not present, intangible, non  existance and non visible. FOR EXAMPLE: moon, star, dragons,  maths equations, heaven and hell etc.   ARBITRARINESS:  This feature was first proposed by Saussure. The  forms of linguistics signs bear no logical,  intrinsic, natural relationship to their meaning.  FOR EXAMPLE: There is nothing in the word      DUALITY:  Human language is organized at two levels  simultaneously. For example, at speech production  we have a physical level at which we can produce  individual sounds like n, b and i.  another level producing a meaning that is different    In animals there is only one level that is sound.   DISCREETNESS : Sounds in human  language are different, they are  separate from one another whereas sounds of animals are the same,  they cannot  be separated.  THANK YOU
Language features	Cultural transmission	Arbitrariness	Discreteness	Duality	Displacement	Productivity	Characteristics of languages
Provide universal dependency relations for the following sentence:	root(want) nsubj(want, I) dobj(want, you) xcomp(want, send) mark(send, to) dobj(send, invitation) iobj(send, Elon_Musk) det(invitation, an) case(invitation, for) nmod(for, party) amod(party, my)	Which phrase in this sentence could be an advmod without changing the sentence's meaning?	to Elon Musk	to send|||for my party|||an invitation	Summary of Stanford (universal) dependencies||publication||UniversalStanfordDependencies:Across-linguistictypology Marie-CatherinedeMarneffe  ,TimothyDozat ? ,NataliaSilveira ? , KatriHaverinen  ,FilipGinter  ,JoakimNivre / ,ChristopherD.Manning ?   LinguisticsDepartment,TheOhioStateUniversity ? Linguisticsand  ComputerScienceDepartments,StanfordUniversity  DepartmentofInformationTechnology,UniversityofTurku / DepartmentofLinguisticsandPhilology,UppsalaUniversity Abstract RevisitingthenowdefactostandardStanforddependencyrepresentation,weproposeanimprovedtaxonomytocapturegrammatical relationsacrosslanguages,includingmorphologicallyrichones.Wesuggestatwo-layeredtaxonomy:asetofbroadlyattested universalgrammaticalrelations,towhichrelationscanbeadded.WeemphasizethelexicaliststanceoftheStanford Dependencies,whichleadstoaparticular,partiallynewtreatmentofcompounding,prepositions,andmorphology.Weshowhow existingdependencyschemesforseverallanguagesmapontotheuniversaltaxonomyproposedhereandclosewithconsiderationof practicalimplicationsofdependencyrepresentationchoicesforNLPapplications,inparticularparsing. Keywords: dependencygrammar,StanfordDependencies,grammaticaltaxonomy 1.Introduction TheStanfordDependencies(SD)representation(de Marneffeetal.,2006)wasoriginallydevelopedasaprac- ticalrepresentationofEnglishsyntax,aimedatnaturallan- guageunderstanding(NLU)applications.However,itwas deeplyrootedingrammaticalrelation-basedsyntactictra- ditions,whichhavelongemphasizedcross-linguisticde- scription.Faithfulnesstotheseoriginswasattenuatedby desideratafromourNLUapplicationsandthedesirefora simple,uniformrepresentation,whichwaseasilyintelligi- blebynon-experts(deMarneffeandManning,2008).Nev- ertheless,itisreasonabletosupposethattheseadditional goalsdonotdetractfromcross-linguisticapplicability. Inthispaper,weattempta(post-hoc)reconstructionof theunderlyingtypologyoftheStanfordDependenciesrep- resentation.Thisnotonlygivesinsightsintohowtheap- proachmightbeappliedtootherlanguages,butalsogives anopportunitytoreconsidersomeofthedecisionsmadein theoriginalscheme,aimingtoproposeanimprovedtaxon- omy,evenforEnglish.Wesuggestataxonomywhichhas atitscoreasetofverybroadlyattestedgrammaticalrela- tions,supplementedasneededbysubtypesforlanguage- particularrelations,whichcapturephenomenaimportantto thesyntaxofindividuallanguagesorlanguagefamilies. Weattempttomakethebasiccoremoreapplicable,both cross-linguisticallyandacrossgenres,andmorefaithfulto thedesignprinciplesindeMarneffe&Manning(2008). Weconsiderhowtotreatgrammaticalrelationsinmorpho- logicallyrichlanguages,includingachievinganappropri- ateparallelismbetweenexpressinggrammaticalrelations byprepositionsversusmorphology.Weshowhowexisting dependencyschemesforotherlanguageswhichdrawfrom StanfordDependencies(Changetal.,2009;Boscoetal., 2013;Haverinenetal.,2013;Serajietal.,2013;McDon- aldetal.,2013;Tsarfaty,2013)canbemappedontothe newtaxonomyproposedhere.Weemphasizethelexicalist stanceofbothmostworkinNLPandthesyntactictheory onwhichStanfordDependenciesisbased,andhenceargue foraparticulartreatmentofcompoundingandmorphology. Wealsodiscussthedifferentformsofdependencyrepre- sentationthatshouldbeavailableforSDtobemaximally usefulforawiderangeofNLPapplications,convergingon threeversions:thebasicone,theenhancedone(whichadds extradependencies),andaparticularformforparsing. 2.Aproposeduniversaltaxonomy Table1givesthetaxonomyweproposefortheuniversal grammaticalrelations,withatotalof42relations.These relationsaretakentobebroadlysupportedacrossmanylan- guagesinthetypologicallinguisticliterature. 1 2.1.Therepresentationbuildsonlexicalism Anunder-elaboratedpartofthedesignprinciplein (deMarneffeandManning,2008)isthatSDadoptsthe lex- icalisthypothesis insyntax,wherebygrammaticalrelations shouldbebetweenwholewords(or lexemes ).Thereisa longstanding,unresolveddebateinlinguisticsbetweenthe- orieswhichattempttobuildupbothwordsandphrases usingthesamecompositionalsyntacticmechanisms(and inwhichthenotionofawordhasminimalprivilegedex- istence)versusthosetheorieswherethewordisafunda- mentalunitandwhichseethemorphologicalprocessesthat buildupwordsasfundamentallydifferentfromandhid- dentothosethatbuildupsentences,sometimestermedthe lexicalintegrityprinciple (Chomsky,1970;Bresnanand Mchombo,1995;Aronoff,2007).Forapracticalcompu- tationalmodel,therearegreatadvantagestoalexicalistap- proach.However,thereremaincertaindifcases,such ashowtodealwithcertain clitics (ZwickyandPullum, 1983),phonologicallyboundwordswhichbehavelikesyn- tacticwords(wefollowmanytreebanksinseparatingthem aswordsandhavingthemparticipateinthesyntax)and howtotreatwordsthataresplitinuneditedwriting(see section2.4.). 1 Thisisnottosaythatalllanguageshaveallthesegrammatical relations.E.g.,manylanguages,fromEnglishtoChiche ‹ wa,allow verbstotakemorethanoneobject,butotherlanguages,suchas French,donot.Nevertheless, iobj isbroadlyattested.  Coredependentsofclausalpredicates NominaldepPredicatedep nsubjcsubj nsubjpasscsubjpass dobjccompxcomp iobj Non-coredependentsofclausalpredicates NominaldepPredicatedepword advcladvmod neg nmodncmod Specialclausaldependents NominaldepAuxiliaryOther vocativeauxmark discourseauxpasspunct explcop Coordination conjcc Noundependents NominaldepPredicatedepword nummodrelclamod apposdet nmodncmodneg Compoundingandunanalyzed compoundmwegoeswith nameforeign Case-marking,prepositions,possessive case Loosejoiningrelations listparataxisremnant dislocatedreparandum Other Sentenceheaddependency rootdep Table1:DependenciesinuniversalStanfordDependencies. Note: nmod , ncmod ,  ,and neg appearintwoplaces. 2.2.Dependentsofclausalpredicates Thesystemofclausaldependentsmostcloselyfol- lowsLexical-FunctionalGrammar(LFG)(Bresnan,2001). However,thetaxonomydiffersfromLFGinseveralre- spects.First,thecleardistinctionbetweencorearguments andotherdependentsismade,butthedistinctionbetween adjunctsandobliquearguments(Radford,1988)istaken tobesufsubtle,unclear,andarguedoverthatitis eliminated. 2 Second,themodelrevertstothetraditional grammarnotionofdirectobjectandotherobjects.Incases 2 TheoriginalPennTreebankannotatorsalsodecidednottotry tomarkargumentsvs.adjuncts(Tayloretal.,2003).Conversely, thePennChineseTreebankdoestrytomakeanadjunct/argument distinction(Xueetal.,2005)andeffectivelyPropBank(Palmer etal.,2005)addsanargument/adjunctdistinctionoverlaytothe EnglishPennTreebank. ofmultipleobjects,thetheme/patientargumentisthedirect object.Finally,thetaxonomyaimstoclearlyindicateinthe dependencylabel(i)anon-canonicalvoicesubject(where theproto-agentargumentisnotsubject,i.e.,inpassives) and(ii)whetherdependentsarenounphrase(NP)argu- mentsversusintroducinganewclause/predicate.Adesign goalofSDhasbeentodistinguishindependencynames whereanewclauseisbeingintroduced(andsowedistin- guish nsubj and csubj ; dobj and ccomp ,butalso advmod and advcl ).WefollowLFGinincludingadistinctionbe- tween ccomp and xcomp forclausalcomplementsthatare standalone(haveaninternalsubject)versusthosehaving necessarycontrol(omission)ofthedependentpredicate's subject(haveanexternalsubject). 3 Otheraspectsofthetypologyarelessregularbutstill important.Thenon-coreclausaldependentsareallmodi- Thedistinctionbetweenafulladverbialclause advcl andaparticipialorveclause  issimi- larbutnotexactlyparalleltothe ccomp / xcomp distinction. 4 Clauseheadshavemanyotherspecialdependents,includ- ingperiphrasticauxiliaries,markers(complementizersand subordinatingconjunctions)aswellasvocativesanddis- courseelements(like well or um ).Conjunctionscombine elementsofmanycategories.UndertheSDdesignprinci- ples,the conj relationconnectslexicalheads. 2.3.Treatmentofcopulas SDhasadvocatedatreatmentofthecopulaﬁbeﬂwhereit isnottheheadofaclause,butratherthedependentofalex- icalpredicate,asin(1a).Suchananalysisismotivatedby thefactthatmanylanguagesoftenoralwayslackanovert copulainsuchconstructions,asintheRussian(1b).Similar constructionsariseeveninEnglishifweconsiderso-called raising-to-objectorsmallclauseconstructions.Underthe basicanalysisproposedforSD,thepredicatecomplement isnotlinkedtoitssubjectargument,butintheenhanced representation(seebelow),thelinkageisthenparallelto thetreatmentinazerocopulalanguage,asin(1c). (1)a. Ivan is the best dancer nsubj cop det amod 3 Thelatterisusedtorepresentcontrolandraisingconstruc- tions,includingcasesofraising-to-objectorso-calledﬁexcep- tionalcasemarkingﬂ.Thetreatmentofthislastgroupofphenom- enaisoneofthelargestsubstantivebreakswiththePennTree- bankannotationtradition,whichfollowsChomskyanapproaches fromtheextendedstandardtheory,Government-BindingTheory (Chomsky,1981)etseq.,intreatingsuchconstructionsashaving acompleteclausalcomplementwithﬁexceptionalcasemarkingﬂ, ratherthananobjectinthehigherclause. 4 For ccomp vs. xcomp ,thedifferenceisacontrolled subject.Whilean xcomp isalwaystherearealsonon-  ccomp ,suchasina for-to ve(ﬁIarranged[forher togobybus]ﬂ).For advcl vs.  thedistinctionisa clausevs.aclause,thoughusuallyareducedonelack- ingasubject.Wegeneralizetheprevious partmod and infmod to  formoregeneralityandcross-linguisticapplicability.We use  ratherthan vmod sincethiscanalsobeanad- jective,asinﬁShehesitated[unabletorememberhisname]ﬂ.  b.Russian: Ivan lu  c  sij tancor Ivan best dancer nsubj amod c. I judge Ivan the best dancer nsubj dobj xcomp det amod nsubj 2.4.vs.compounding Theuniversalschemekeepstherichtaxonomyofnoun dependentsthatareoneofthestrengthsofSD.Anim- portantpartofthetypologyistodifferentiatecompounds (multi-rootlexemes)fromorcomplementa- tion.Underalexicalistapproach,compoundwordsarefun- damentallydifferentfromcasesofphrasal Therearethreerelationsforcompounding.Weuse mwe forxedgrammaticizedexpressionswithfunctionwords (e.g., insteadof : mwe (of,instead),Fr. plut ‹ otque ﬁrather thanﬂ: mwe (que,plut ‹ ot)), name forpropernounsconstituted ofmultiplenominalelements,asintheFinnishandItalian dependencytreebanks, 5 and compound tolabelothertypes ofmulti-wordlexemes.Thus compound isusedforany kindofX 0 compounding:nouncompounds(e.g., phone book ),butalsoverbandadjectivecompoundsthataremore commoninotherlanguages(suchasPersianorJapanese lightverbconstructions);fornumbers(e.g., threethou- sandbooks gives compound (thousand,three));forparticles ofphrasalverbs(e.g., putup : compound (put,up)). 2.5.Treatmentofprepositionsandcasemarking Amajorproposedchangefromtheextantversionsof SDisanewtreatmentofprepositionstoprovideauniform analysisofprepositionsandcaseinmorphologicallyrich languages.Theanalysiswechoseistopushalltheway thedesignprincipleofhavingdirectlinksbetweencon- tentwords.Weabandontreatingaprepositionasame- diatorbetweenawordanditsobject,and,in- stead,anycase-markingelement(includingprepositions, postpositions,andcliticcasemarkers)willbetreatedasa dependentofthenounitattachestoorintroduces.Thepro- posedanalysisisshownin(2): nmod labelstherelation betweenthetwocontentwords,whereastheprepositionis nowviewedasa case dependingonitscomplement.In general, nmod expressessomeformofobliqueoradjunct relationfurtherbythe case . (2)a. the Chair 's of det nmod case b. the of of the Chair det nmod case det c.French: le bureau du pr ´ esident the of of theChair det nmod case 5 Thatis, name wouldbeusedbetweenthewordsofﬁHillary RodhamClintonﬂbutnottoreplacetheusualrelationsinaphrasal orclausalnamelikeﬁTheLordoftheRingsﬂ. Thetreatmentofcasemarkingisillustratedin(3).In (3a), at inHebrewisaseparatetokenindicatinganac- cusativeobject:thecasemarkerdependsontheobject.In (3c),weshowtheanalysiswhencasemarkersaremor- phemes.Thecasemorphemeisnotdividedoffthenoun asaseparate case dependent,butthenounasawholeis analyzedasa nmod oftheverb.Toovertlymarkcase,we includePOStagsintherepresentationasshownin(3b)and (3d).WeusetheuniversalPOStagsetfromPetrovetal. (2012)towhichweappendcaseinformation. (3)a.Hebrew: wkfraiti at hsrj andwhenIsaw ACC themovie dobj case b. dobj (wkfraiti/ VERB ,hsrj/ NOUN ) case (hsrj/ NOUN ,at/ PRT - ACC ) c.Russian: Ya napisal pis'mo perom I wrote theletter withaquill nsubj dobj nmod d. nsubj (napisal/ VERB ,Ya/ NOUN - NOM ) dobj (napisal/ VERB ,pis'mo/ NOUN - ACC ) nmod (napisal/ VERB ,perom/ NOUN - INSTR ) Thistreatmentprovidesparallelismbetweendifferent constructionsacrossandwithinlanguages.Agoodresultis thatwenowhavegreaterparallelismbetweenprepositional phrasesandsubordinateclauses,whichareinpracticeoften introducedbyapreposition,asin(4). (4)a. Sue left after the rehearsal nsubj nmod case det b. Sue left after we did nsubj advcl mark nsubj Wealsoobtainparallelconstructionsforthepossessive alternationasshownin(2),forvariantformswithcase,a prepositionorapostpositioninFinnish,asshownin(5), andforthedativealternationwheretheprepositionalcon- structiongetsasimilaranalysistothedoubleobjectcon- struction,see(6). (5)a.Finnish: etsi ¨ a ilman johtolankaa tosearch without clue . PARTITIVE case nmod b. etsi ¨ a taskulampun kanssa tosearch torch . GENITIVE with nmod case c. etsi ¨ a johtolangatta tosearch clue . ABESSIVE nmod (6)a. give thechildren thetoys iobj dobj  b. give thetoys to thechildren nmod dobj case c.French: donner lesjouets aux enfants give thetoys tothe children dobj nmod case Anotheradvantageofthisnewanalysisisthatitpro- videsatreatmentofprepositionalphrasesthatarepredica- tivecomplementsofﬁbeﬂasin(7)thatisconsistentwiththe treatmentofnominalpredicativecomplements,asin(1). (7) Sue is in shape nsubj cop case SDisasurfacesyntacticrepresentation,whichdoesnot representsemanticroles.Thesemanticrolesof arehardtocategorizeandhardtodetermine.Wefeelthat thereisalotofuseforarepresentationwhichworkssolely intermsoftheovertrole-markingresourcesofeachlan- guage.Thisissupportedbymanyrichlanguage-particular traditionsofgrammaticalanalysis,whetherviaSanskrit casesorthecaseparticlesonJapanese bunsetsu . Prepositionssometimesintroduceaclauseastheircom- plement,e.g.,(8a).Followingtheprinciplethatdependen- ciesdomarkwherenewclausesareintroduced,thisrelation shouldhaveadifferentnamefrom nmod ,andwesuggest callingit ncmod ﬁnominalizedclauseUnderthe proposednewanalysis,theheadoftheof data will be upset .Theresultwillbetheanalysisin(8b). (8)a.Wehavenodataaboutwhetherusersareupset. b. data about whether users are upset ncmod case mark nsubj cop Anotherissueiswhatanalysistogivetocasesofstacked prepositions,suchas outof .Ourproposalisthatallsuch casesshouldberegardedassomeformof mwe ,asin(9b). (9)a.Outofallthis,somethinggoodwillcome. b. Out of all this ... come mwe predet case nmod 2.6.Informaltextgenres Followingthepracticalapproachusedinpart-of-speech taggingofrecentLDCtreebanks,weintroducetherelation goeswith toconnectmultipletokensthatcorrespondtoa singlestandardword,asaresultofreanalysisofwordsas compounds(ﬁhandsomeﬂforﬁhandsomeﬂ)orinputerror (ﬁotherﬂforﬁotherﬂ).Weuse foreign tolabelsequences offoreignwords.Toindicateoverriddenina speechrepair,weuse reparandum ,asin(10). (10) Go to the righ- to the left. reparandum case det case det nmod Theﬁloosejoiningrelationsﬂaimatarobustanalysisof moreinformalformsoftext,whicharenowcommonin NLPapplications.Informalwrittentextoftencontainslists ofcomparableitems,whichareparsedassinglesentences. Emailsignaturesinparticularcontainthesestructures,in theformofcontactinformation.FollowingdeMarneffeet al.(2013),weusethe list , parataxis ,(and appos )relations tolabelthesekindsofstructures.Therelation parataxis is alsousedinmoreformalwritingforconstructionssuchas sentencesjoinedwithacolon. The dislocated relationcapturespreposed(topics)and postposedelements.The remnant relationisusedtopro- videatreatmentofellipsis(inthecaseofgappingorstrip- ping,wherepredicationalorverbalmaterialgetselided), somethingthatwaslackinginearlierversionsofSD.It providesabasisforbeingabletoreconstructdependencies intheenhancedversionofSD.Forexample,in(11),the remnant relationsenableustocorrectlyretrievethesub- jectsandobjectsintheclauseswithanelidedverb. (11) John won bronze, Mary silver, and Sandy gold. nsubj dobj remnant remnant remnant remnant Incontrast,inright-node-raising(RNR)(12)andVP- ellipsis(13)constructionsinwhichsomekindofpredica- tionalorverbalmaterialisstillpresent,the remnant relation isnotused.InRNR,theverbsarecoordinatedandtheob- jectisa dobj oftheverb.InVP-ellipsis,wekeepthe auxiliaryasthehead,asshownin(13). (12) John bought and ate anapple. nsubj cc conj dobj (13) John will win gold and Mary will too. nsubj aux dobj cc nsubj conj advmod 2.7.Language-particularrelations Inadditiontoauniversaldependencytaxonomy,itisde- sirabletorecognizegrammaticalrelationsthatareparticu- lartoonelanguageorasmallgroupofrelatedlanguages. Suchlanguage-particularrelationsarenecessarytoaccu- ratelycapturethegeniusofaparticularlanguagebutwill notinvolveconceptsthatgeneralizebroadly.Thesugges- tionhereisthattheserelationsshouldalwaysberegarded asasubtypeofanexistingUniversalSDrelation.The SDrelationshaveataxonomicorganization(deMarneffe etal.,2006),andsomeoftheuniversalrelationsareal- readysubtypesofeachother(e.g., auxpass isasubtypeof aux ).Language-particularrelationsthatseemusefultodis- tinguishforEnglishareincludedatthebottomofTable2: npmod forbarenominalofpredicateslackinga preposition,amongwhichinparticularthereis tmod for bareNPtemporal poss forpossessives,sincethe syntaxofprenominalpossessionisverydistinctfrompost- nominal(whichmayalsoexpresspossession); predet forwordssuchas all thatprecederegulardetermin- ersand preconj forwordsprecedingaconjunctionlike ei- ther ;and prt forverbparticles.  3.Mappingtoexistingschemes Therehasrecentlybeenanefforttopushtowardsho- mogeneityacrossresourcesfordifferentlanguagesand tocomeupwithcross-linguisticallyconsistentannotation aimedatmulti-lingualtechnology,forpart-of-speechtagset (Petrovetal.,2012)aswellasfordependencyrepresenta- tion(McDonaldetal.,2013).TheschemeproposedinMc- Donaldetal.(2013)tookSDasastartingpoint.Annota- torsforsixdifferentlanguages(German,English,Swedish, Spanish,FrenchandKorean)producedannotationguide- linesforthelanguagetheywereworkingon,keepingthe labelandconstructionsetascloseaspossibletotheoriginal EnglishSDrepresentation.Theywereonlyallowedtoadd labelsforphenomenathatdonotexistinEnglish.Giventhe setsofrelationsobtainedforthedifferentlanguages,ahar- monizationstepwasperformedtomaximizeconsistencyof annotationacrosslanguages.However,thisrigidstrategy lostsomeimportantdistinctions,suchasthedistinctionbe- tweencompoundingandphrasalwhilemain- tainingsomedistinctionsthatarebestabandoned,suchasa distinctionbetweenvalandparticipial McDonaldetal.(2013)doesnotaddressgivinganele- ganttreatmentofmorphologicallyrichlanguages.Incon- trast,Tsarfaty(2013)proposestotreatmorphologyassyn- taxinherdependencyproposal,illustratedwithHebrew. However,thisrepresentationchoicectswiththelexi- calistapproachofSD.Herewetakeuphergoaloftrying togiveauniformtreatmentofbothmorphologicallyrich andmorphologicallypoorlanguages,butsuggestachieving thegoalinadifferentway,whichmaintainsalexicalistap- proach(seeSection2.5.).Table2showsacomparisonbe- tweentheevolutionoftheSDscheme(StanfordDependen- ciesv2.0.0,usedintheSANCLsharedtask,andStanford Dependenciesv3.3.0,the2013/11/12version),thepropos- alsinMcDonaldetal.(2013)(GSD)andinTsarfaty(2013) (TSD),andthedependencysetproposedhere(USD). Existingdependencysetsforotherlanguagescanbe fairlystraightforwardlymappedontoournewproposal. EveniftheschemesexaminedhereareﬁSD-centricﬂ,they dealtwithparticularconstructionspresentineachlanguage andpositednewrelationswhennecessary.Themappingis lessdifbecauseUSDadoptssomeoftheideasandre- lationsthatweredevelopedfortheseothertreebanks, suchasthecontentwordasheadanalysisofprepositional phrasesfromtheTurkuDependencyTreebank(TDT).In table3,weshowhowtheFinnish(Haverinenetal.,2013), Italian(Boscoetal.,2013),Chinese(Changetal.,2009) andPersian(Serajietal.,2013)schemescanbemapped ontotheproposeduniversaltaxonomy(USD).Theboldla- belsarerelations,andtheyaresubtypes ofthecorrespondingUSDrelationintherow.Gapsin- dicateexistingconstructionsinthelanguagethatwerenot capturedintheoriginalscheme(theUSDlabelisapplicable there);  indicatesconstructionsthatarenotpresentinthe language.Sincecopularverbsarenotheadsanymore,the attr relationisremoved,requiringtotheex- istinganalysesofcopularsentencesforItalianandChinese. Wealsointroducedextrarelationsforcertainconstructions, whichsomeschemeshadnotincorporatedyet. ForFinnish,therelation rel (forrelativemarker)willbe mappedtowhateversyntacticroletherelativeisplayingin therelativeclause( nsubj , dobj ,etc.),informationwhichis presentinthesecondannotationlayeroftheTDTcorpus. ISDTistheconversionoftheMIDTItaliandependency schemetoSD.Someofthe clit usesinISDT(forx- ivepronounsinpronominalverbsŒfrequentinRomance languagesŒsuchasFr. sedouter ﬁtosuspectﬂ)willbeen- compassedby expl .However,whenthexivepronoun cantrulybeadirectorindirectobject,itgetsassignedthe correspondingobjectrelation. Chinesehasserialverbconstructionswhichmightnow be compound (andnot conj ).Wetreatpost-nominallocal- izersandprepositionsasaformofcase. InPersian,therearenorelativepronounsand rel was usedfortheedrelativemarker,butitcanbemappedto mark .UPDThasa fw relationbetweensequencesoffor- eignwords,unanalyzedwithinPersiangrammar,whichwe adopt,namingit foreign .WealsoadopttheUPDT dep-top relationusedforafrontedelementthatintroducesthetopic ofasentence,butwegeneralizeitto dislocated toaccount forpostposedelementsaswellas.Rightdislocatedele- mentsarefrequentinspokenlanguages:e.g.,Fr. fautpas lamanger,lap ‹ ate (literally,ﬁneednotiteat,thedoughﬂ). Labelsofrelationswillbeharmonized tobesharedbetweenlanguages:Chinese assmod willbe mappedto poss ,andPersian dep-top to topic . 4.DifferentformsofStanfordDependencies ThecurrentStanfordconverterprovidesanumberof variantformsofSD,ofwhichthemostusedarethe ba- sic dependencytree,andthe collapsed,cc-processed form thataddsextradependencyarcs,restructuresprepositions tonotbeheads,andspreadsrelationsacrossconjunctions. Thissectionsuggestssomenewideasforhowtoprovide potentiallylessbutdifferentoptions. Oneconcernaboutourproposedtaxonomyisthat straightforwardparsingtoUSDislikelytobeharderfor parsersthanthecurrentrepresentation(forEnglish).It isnowfairlywellknownthat,whiledependencyrepre- sentationsinwhichcontentwordsaremadeheadstend tohelpsemanticallyorienteddownstreamapplications,de- pendencyparsingnumbersarehigherifyoumakeauxiliary verbsheads,ifyouanalyzelongconjunctionsbychaining (ratherthanhavingasingleheadofthewholeconstruction), andifyoumakeprepositionstheheadofprepositional phrases(Schwartzetal.,2012;Elmingetal.,2013).The generalizationisthatdependencyparsers,perhapsinpar- ticulartheefshift-reduce-styledependencyparsers likeMaltParser(Nivreetal.,2007),workbestthemorethe dependencyrepresentationlookslikeachainofshortde- pendenciesalongthesentence.UndertheproposedUSD, SDwouldthenbemakingtheﬁwrongﬂchoiceineachcase. However,itseemswrong-headedtochoosealinguis- ticrepresentationtomaximizeparserperformance,rather thanbasedonthelinguisticqualityoftherepresentation anditsusefulnessforapplicationsthatbuildfurtherpro- cessingontopofit.Rather,itmaybeusefultodopars- ingusingatransformationofthetargetdependencysys- tem.Inconstituencyparsing,itiscompletelyusualforthe targetrepresentationtobetransformedsoastoimprove  SDv2.0.0SDv3.3.0GSDTSDUSDNotes nsubjnsubjnsubjnsubjnsubj X csubjcsubjcsubjcsubjcsubj X nsubjpassnsubjpassnsubjpassnsubjpassnsubjpass X csubjpasscsubjpasscsubjpasscsubjpasscsubjpass X dobjdobjdobjdobjdobj X iobjiobjiobjiobjiobj X (TSDalsohas gobj forgenitiveobject) ccompccompccompccompccompUSD&TSDasclausewithinternalsubject,not xcompxcompxcompxcompxcompUSD&TSDasclausewithexternalsubject,not acompacompacompacompŒ acomp canbegeneralizedinto xcomp attrŒattrŒŒ attr removed: wh- isheador xcomp (withcopulaheadoption) advmodadvmodadvmodadvmodadvmod X advcladvcladvclŒadvclTSDomitsbutneededtopreserveclauseboundaries purpclŒŒŒŒFoldedinto advcl negnegnegnegnegAswellasadverbial not,never ,USDextendstonegative det like no detdetdetdetdet X (TSD dem and def assubtypesof det ) amodamodamodamodamod X apposapposapposapposappos X abbrevŒŒabbrevŒParentheticalabbreviationsbecomecasesof appos numnumnumnummodnummodRenamedforclarity rcmodrcmodrcmodrcmodrelcl X partmodpartmodpartmod?Make partmod , infmod into  ;use(rich)POStodistinguish infmodinfmodinfmodinfmodMake partmod , infmod into  ;use(rich)POStodistinguish quantmodquantmodadvmod?ŒGenerallyfoldedinto advmod rootrootROOTrootroot punctpunctppunctpunct auxauxauxauxaux X (TSDadds qaux forquestionauxiliary.veisnow mark .) auxpassauxpassauxpassauxpassauxpass X copcopcopcopcopGSDhas cop onlyincontent-headversion explexplexplexplexplSubjectandobjectexpletives,frozenxives(Fr. sedouter ) markmarkmarkmarkmark X to introducinganvewillnowbe mark (insteadof aux ) complmŒŒcomplmŒRemoveanduse mark morebroadly ŒdiscourseŒparataxis?discourseAgapinoriginalandothertypologies ŒŒŒŒvocativeAgapinoriginalandothertypologies depdepdepdepdepGSDusesfor vocative and discourse relŒrelrelŒConverter'sunresolvedonesnow dep ;TSD rel isreally mark prepprepadpmodprepmodcaseUSD case isdependentofNPnotofthing ŒŒnmodŒŒEquivalentto nmod below pobjpobjadpobjpobjnmod nmod nowgoesfromthingtoNPofPP pcomppcompadpcomppcompncmod ncmod goesfromthingtoclause ŒŒadpprep/casecaseTSDhasN/A/G/Dsubtypes,butcan'tkeepaddingforallcases possessivepossessiveadpgenŒViewasamanifestationof case nnnncompmodnncompoundGeneralize nn tolightverbs,etc.;X 0 compoundingnot ŒŒmweŒnameMulti-wordpropernouns(e.g., JohnSmith )asinTDTandISDT numbernumbernumnummod?ŒRegardedastypeofcompound;using nummod iswrong mwemwemwemwemweFixedexpressionswithfunctionwords( sothat,allbut,dueto,... ) ŒgoeswithŒŒgoeswithFororthographicerrors: other ŒŒŒŒforeignLinearanalysisofforeignwords(headisleft-most)asinUPDT ŒŒŒŒreparandumForoverriddeninspeechrepairs conjconjconjconjconj X cccccccccc X parataxisparataxisparataxisparataxisparataxis X ŒŒŒŒlistUsedforinformalliststructures,signatureblocks,etc. ŒŒŒŒremnantUsedtogiveatreatmentofellipsiswithoutemptynodes ŒŒŒŒdislocatedPreposedtopicsanddislocatedelementsasinUPDT Englishparticular npadvmodnpadvmodnmodadvmod? npmod Asubtypeof nmod tmodtmodadvmodtmod tmod Asubtypeof npmod predetpredetŒpredet predet Asubtypeof det preconjpreconjccpreconj preconj Asubtypeof cc prtprtprt? prt Asubtypeof compound posspossposspossmod poss Asubtypeof case Table2:ComparisonofproposalsonEnglish:SD,McDonaldetal.(2013)(GSD),Tsarfaty(2013)(TSD)andours(USD).  TDTISDTChineseUPDTUSD nsubjnsubjnsubj,topnsubjnsubj csubjcsubj  csubj  nsubjpassnsubjpassnsubjpassnsubjpass  csubjpass  csubjpass dobjdobj,clitdobjdobjdobj  iobj,clitiobj  iobj ccomp, iccomp ccompccomp,rcompccompccomp xcomp,acompxcomp,acompxcomp,acompxcomp  attrattr  advmod,quantmodadvmodadvmod,dvpmodadvmod,quantmodadvmod advcladvclsomeadvmodadvcladvcl negnegnegnegneg detdet, predet detdet, predet det amodamodamodamodamod apposapposprnmodapposappos numnumnummod,ordmodnumnummod rcmodrcmodrcmodrcmodrelcl partmod,infmodpartmodvmod   rootrootrootrootroot punctpunctpunctpunctpunct auxauxasp,mmodauxaux auxpassauxpasspassauxpassauxpass copcopcopcopcop  expl,clit  expl complm,markmarkcpmcomplm,mark, rel mark intjdiscourse discourse vocdep-vocvocative depcomp,moddepdepdep poss , gobj , gsubj ,pobj, poss ,pobj, lobj , assmod ,pobj, poss , cpobj nmod nommodnpadvmod, tmod clf,range, tmod npadvmod, tmod  pcomppccomp, lccomp  ncmod adpospossessive,prepassm,prep, ba , dvpm , locacc ,prep, cprep case number,nn, prt number,nnnn,someconjnumber,nn, prt , f nsubj j dobj j acomp j prep g -lvc compound namennp name somedepmweprtmodmwemwe goeswith goeswith fwforeign reparandum conjconjconj,etc,comodconjconj cc, preconj cc, preconj cccc, preconjunct cc parataxisparataxisparataxisparataxis list ellipsis remnant dep-top dislocated Table3:MappingsoftheFinnish(TDT),Italian(ISDT),ChineseandPersian(UPDT)schemestoUSD. parsingnumbers,suchasbyhead-lexicalization(Collins, 2003),bymanualorautomaticsubcategorizationofcat- egories(KleinandManning,2003;Petrovetal.,2006), andevenbyothermethodssuchasunarychaincontrac- tion(Finkeletal.,2008).Afterparsing,adetransformation processreconstructstreesinthetargetrepresentation.This kindoftransform-detransformarchitectureisatpresentless commonindependencyparsing,althoughNilsson,Nivre &Hall(2006;2007)dothisforcoordinationandverb groups,andpseudo-projectiveparsing(NivreandNilsson, 2005)canalsobeseenasaninstanceofthisarchitecture. Atransform-detransformarchitectureshouldbecomemore standardindependencyparsing.Wethereforeproposea parsing representationthatchangessomeofthedepen- dencyheadchoicestomaximizeparsingperformance.This requiresdevelopingtoolstoconvertseamlesslybothways betweenthe basic and parsing representations. 6 Sincethenewtreatmentofprepositionalphrasesbasi- callydoeswhatthe collapsed representationwasdesigned todo(puttingadirectlinkbetweenthenouncomplementof aprepositionandwhatitexceptfornotrenaming 6 AsmallpartofthisisinplaceintheStanfordconverter,inthe abilitytogeneratecopula-andcontent-headversionsfromtrees.  thedependencyrelation,the collapsed representationonits ownhaslessutility.However,theideasofhavingextrade- pendenciestomarkexternalsubjectsandtheexternalrole inrelativeclausesisuseful,therenamingofdependencies toincludecaseorprepositioninformationhelpsinmany applications,andspreadingrelationsoverconjunctionsis usefulforrelationextraction.Thesetransforma- tionscanbeprovidedinan enhanced representation. WethussuggestprovidingthreeversionsofStanfordDe- pendencies: basic , enhanced ,and parsing . 5.Conclusion Weproposedataxonomyofgrammaticalrelationsappli- cabletoavarietyoflanguages,developingtheimplications ofalexicalistapproachforthetreatmentofmorphology andcompounding.Someofthedecisionsmadeonlinguis- ticgroundsareatoddswithwhatworksbestforprocessing tools.Wesuggestedthatthetransform-detransformarchi- tecturestandardlyusedinconstituencyparsingisthesolu- tiontoadoptfordependencyparsing.Weworkedoutthe mappingofexistingdependencyresourcesfordifferentlan- guagestothetaxonomyproposedhere.Wehopethiswork willenhanceconsistencyinannotationbetweenlanguages andfurtherfacilitatecross-lingualapplications. 6.References Aronoff,M.(2007).Inthebeginningwastheword. Lan- guage ,83:803Œ830. Bosco,C.,Montemagni,S.,andSimi,M.(2013).Convert- ingItaliantreebanks:TowardsanItalianStanforddepen- dencytreebank.In SeventhLinguisticAnnotationWork- shop&InteroperabilitywithDiscourse . Bresnan,J.andMchombo,S.A.(1995).Thelexicalin- tegrityprinciple:EvidencefromBantu. NaturalLan- guageandLinguisticTheory ,13:181Œ254. Bresnan,J.(2001). Lexical-FunctionalSyntax .Blackwell, Oxford. Chang,P.-C.,Tseng,H.,Jurafsky,D.,andManning,C.D. (2009).DiscriminativereorderingwithChinesegram- maticalrelationsfeatures.In ThirdWorkshoponSyntax andStructureinStatisticalTranslation ,pages51Œ59. Chomsky,N.(1970).Remarksonnominalization.InJa- cobs,R.A.andRosenbaum,P.S.,editors, Readings inEnglishtransformationalgrammar ,pages184Œ221. Ginn,Waltham,MA. Chomsky,N.(1981). LecturesonGovernmentandBind- ing .Foris,Dordrecht. Collins,M.(2003).Head-drivenstatisticalmodelsfor naturallanguageparsing. ComputationalLinguistics , 29:589Œ637. deMarneffe,M.-C.andManning,C.D.(2008).TheStan- fordtypeddependenciesrepresentation.In Workshopon Cross-frameworkandCross-domainParserEvaluation . deMarneffe,M.-C.,MacCartney,B.,andManning, C.D.(2006).Generatingtypeddependencyparsesfrom phrasestructureparses.In LREC . deMarneffe,M.-C.,Connor,M.,Silveira,N.,Bowman, S.R.,Dozat,T.,andManning,C.D.(2013).Morecon- structions,moregenres:ExtendingStanforddependen- cies.In DepLing2013 . Elming,J.,Johannsen,A.,Klerke,S.,Lapponi,E.,Mar- tinez,H.,andSøgaard,A.(2013).Down-streamef- fectsoftree-to-dependencyconversions.In NAACLHLT 2013 . Finkel,J.R.,Kleeman,A.,andManning,C.D.(2008).Ef- feature-based,conditionalrandomparsing. In ACL46 ,pages959Œ967. Haverinen,K.,Nyblom,J.,Viljanen,T.,Laippala,V.,Ko- honen,S.,Missil ¨ a,A.,Ojala,S.,Salakoski,T.,and Ginter,F.(2013).Buildingtheessentialresourcesfor Finnish:theTurkudependencytreebank. LanguageRe- sourcesandEvaluation .Inpress.Availableonline. Klein,D.andManning,C.D.(2003).Accurateunlexical- izedparsing.In ACL41 ,pages423Œ430. McDonald,R.,Nivre,J.,Quirmbach-Brundage,Y.,Gold- berg,Y.,Das,D.,Ganchev,K.,Hall,K.,Petrov, S.,Zhang,H.,T ¨ ackstr ¨ om,O.,Bedini,C.,Bertomeu Castell ´ o,N.,andLee,J.(2013).Universaldependency annotationformultilingualparsing.In ACL51 . Nilsson,J.,Nivre,J.,andHall,J.(2006).Graphtransfor- mationsindata-drivendependencyparsing.In COLING 21andACL44 ,pages257Œ264. Nilsson,J.,Nivre,J.,andHall,J.(2007).Treetransforma- tionsforinductivedependencyparsing.In ACL45 . Nivre,J.andNilsson,J.(2005).Pseudo-projectivedepen- dencyparsing.In ACL43 ,pages99Œ106. Nivre,J.,Hall,J.,Nilsson,J.,Chanev,A.,Eryigit,G., K ¨ ubler,S.,Marinov,S.,andMarsi,E.(2007).Malt- Parser:Alanguage-independentsystemfordata-driven dependencyparsing. NaturalLanguageEngineering , 13:95Œ135. Palmer,M.,Gildea,D.,andKingsbury,P.(2005).The propositionbank:Anannotatedcorpusofsemanticroles. ComputationalLinguistics ,31:71Œ105. Petrov,S.,Barrett,L.,Thibaux,R.,andKlein,D.(2006). Learningaccurate,compact,andinterpretabletreeanno- tation.In COLING21andACL44 ,pages433Œ440. Petrov,S.,Das,D.,andMcDonald,R.(2012).Auniversal part-of-speechtagset.In LREC . Radford,A.(1988). TransformationalGrammar .Cam- bridgeUniversityPress,Cambridge. Schwartz,R.,Abend,O.,andRappoport,A.(2012). Learnability-basedsyntacticannotationdesign.In COL- ING24 ,pages2405Œ2422. Seraji,M.,Jahani,C.,Megyesi,B.,andNivre,J.(2013). UppsalaPersiandependencytreebankannotationguide- lines.Technicalreport,UppsalaUniversity. Taylor,A.,Marcus,M.,andSantorini,B.(2003).ThePenn treebank:Anoverview.InAbeill ´ e,A.,editor, Building andUsingParsedCorpora ,volume20of Text,Speech andLanguageTechnology .Springer. Tsarfaty,R.(2013).Amorpho-syntacticschemeof Stanforddependencies.In ACL51 . Xue,N.,Xia,F.,Chiou,F.-D.,andPalmer,M.(2005).The PennChinesetreebank:Phrasestructureannotationof alargecorpus. NaturalLanguageEngineering ,11:207Œ 238. Zwicky,A.M.andPullum,G.K.(1983).Cliticizationvs. English n't . Language ,59:502Œ513.
Parsing	Grammatical relations	Universal dependencies	Dependency relations	Dependency parse
What's the difference between multilabel and multinomial text classification?	Multilabel and multinomial classifiers are both multiclass classifiers, meaning there are more than two labels that exist as potential outputs (as opposed to binary classifiers that determine if something is or is not a label, such as if an email is or is not spam).  Multilabel classifiers ("any of") mean that one input can have multiple labels as outputs. One example of such a classifier is one that assigns topics to a news article. A news article could be tagged as "sports," "basketball," and "Cleveland Cavaliers" all at once. With multilabel classifiers, each topic would essentially have its own binary classification model––such as "is this sports or not"––and each decision is made independently.  Conversely, multinomial classifiers ("one of") assign just one label to each input. As an example, sentiment analysis commonly has three labels––positive, neutral, and negative––and one input document would never be labeled as both positive and neutral (or positive and negative, etc.). Multinomial classifiers also build out binary classifiers for each topic, but only the label that returns the highest score or the highest probability is selected as the label. For instance, a multinomial sentiment analysis classifier may output a 97% chance the document is positive, 2% it is neutral, and 1% it is negative, and thus, we would predict that the document is positive.	You want to build a classifier that determines if an article is fake news or not. What type of classifier should you build?	Binary classifier	Multinomial classifier|||Multilabel classifier	Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||4.7  E VALUATION :P RECISION ,R ECALL ,F- MEASURE 13 4.7.1Morethantwoclasses Uptonowwehavebeenassumingtextclassitaskswithonlytwoclasses. Butlotsoftasksinlanguageprocessinghavemorethantwoclasses. Forsentimentanalysiswegenerallyhave3classes(positive,negative,neutral)and evenmoreclassesarecommonfortaskslikepart-of-speechtagging,wordsense disambiguation,semanticrolelabeling,emotiondetection,andsoon. Therearetwokindsofmulti-classtasks.In any-of or multi-label   ,eachdocumentoritemcanbeassignedmorethanonelabel.Wecan solve any-of bybuildingseparatebinaryclassiforeachclass c , trainedonpositiveexampleslabeled c andnegativeexamplesnotlabeled c .Given atestdocumentoritem d ,theneachmakestheirdecisionindependently, andwemayassignmultiplelabelsto d . Morecommoninlanguageprocessingis one-of or multinomial ,  multinomial  inwhichtheclassesaremutuallyexclusiveandeachdocumentoritemappearsin exactlyoneclass.Hereweagainbuildaseparatebinarytrainedonpositive examplesfrom c andnegativeexamplesfromallotherclasses.Nowgivenatest documentoritem d ,werunalltheandchoosethelabelfromthe withthehighestscore.Considerthesampleconfusionmatrixforahypothetical3- way one-of emailcategorizationdecision(urgent,normal,spam)showninFig. 4.5 . Figure4.5 Confusionmatrixforathree-classcategorizationtask,showingforeachpairof classes ( c 1 ; c 2 ) ,howmanydocumentsfrom c 1 were(in)correctlyassignedto c 2 Thematrixshows,forexample,thatthesystemmistakenlylabeled1spamdoc- umentasurgent,andwehaveshownhowtocomputeadistinctprecisionandrecall valueforeachclass.Inordertoderiveasinglemetricthattellsushowwellthe systemisdoing,wecancombinethesevaluesintwoways.In macroaveraging ,we macroaveraging computetheperformanceforeachclass,andthenaverageoverclasses.In microav- eraging ,wecollectthedecisionsforallclassesintoasinglecontingencytable,and microaveraging thencomputeprecisionandrecallfromthattable.Fig. 4.6 showsthecontingency tableforeachclassseparately,andshowsthecomputationofmicroaveragedand macroaveragedprecision. Astheshows,amicroaverageisdominatedbythemorefrequentclass(in thiscasespam),sincethecountsarepooled.Themacroaveragebetterthe statisticsofthesmallerclasses,andsoismoreappropriatewhenperformanceonall theclassesisequallyimportant.
Text classification	Multilabel classification (any-of classification)	Multinomial classification (one-of classification)	Multiclass classification	Sentiment analysis	Natural Language Processing (NLP)
What is the sigmoid function and why is it useful?	The sigmoid function is \( y = { 1 \over (1 + e^{-z}) } \), where z is the standard output from a Logistic Regression classifier, or other probabilistic machine learning classifier. The sigmoid function always outputs a number between 0 and 1, which is perfect for classification tasks because this number can be used to represent a probability. This probability can then be used to standardize a classification function via a decision boundary. For instance:  $$\begin{equation} \hat y =\begin{cases}1, & \text{if}\ P(y = 1|x) > 0.5 \\0, & \text{otherwise}\end{cases} \end{equation} $$  Additionally, the sigmoid function is differentiable, which comes in handy for learning functions when training the model.	Which of the following is not an advantage of using the sigmoid function?	It allows us to see which prediction label is most likely.	It always outputs numbers between 0 and 1.|||The distribution of probabilities will always add up to 1.|||It is differentiable.	Speech and Language Processing: Logistic Regression||publication||2 C HAPTER 5  L OGISTIC R EGRESSION Moreformally,recallthatthenaiveBayesassignsaclass c toadocument d not bydirectlycomputing P ( c j d ) butbycomputingalikelihoodandaprior ‹ c = argmax c 2 C likelihood z }| { P ( d j c ) prior z }| { P ( c ) (5.1) A generativemodel likenaiveBayesmakesuseofthis likelihood term,which generative model expresseshowtogeneratethefeaturesofadocument ifweknewitwasofclassc . Bycontrasta discriminativemodel inthistextcategorizationscenarioattempts discriminative model to directly compute P ( c j d ) .Perhapsitwilllearntoassignahighweighttodocument featuresthatdirectlyimproveitsabilityto discriminate betweenpossibleclasses, evenifitcouldn'tgenerateanexampleofoneoftheclasses. Componentsofaprobabilisticmachinelearning LikenaiveBayes, logisticregressionisaprobabilisticthatmakesuseofsupervisedmachine learning.Machinelearningrequireatrainingcorpusof M input/output pairs ( x ( i ) ; y ( i ) ) .(We'llusesuperscriptsinparenthesestorefertoindividualinstances inthetrainingsetŠforsentimentcleachinstancemightbeanindividual documenttobeAmachinelearningsystemforthenhas fourcomponents: 1. A featurerepresentation oftheinput.Foreachinputobservation x ( i ) ,this willbeavectoroffeatures [ x 1 ; x 2 ;:::; x n ] .Wewillgenerallyrefertofeature i forinput x ( j ) as x ( j ) i ,sometimesas x i ,butwewillalsoseethe notation f i , f i ( x ) ,or,formulticlass f i ( c ; x ) . 2. Afunctionthatcomputes‹ y ,theestimatedclass,via p ( y j x ) .In thenextsectionwewillintroducethe sigmoid and softmax toolsfor cation. 3. Anobjectivefunctionforlearning,usuallyinvolvingminimizingerroron trainingexamples.Wewillintroducethe cross-entropylossfunction 4. Analgorithmforoptimizingtheobjectivefunction.Weintroducethe stochas- ticgradientdescent algorithm. Logisticregressionhastwophases: training: wetrainthesystemtheweights w and b )usingstochastic gradientdescentandthecross-entropyloss. test: Givenatestexample x wecompute p ( y j x ) andreturnthehigherprobability label y = 1or y = 0. 5.1thesigmoid Thegoalofbinarylogisticregressionistotrainathatcanmakeabinary decisionabouttheclassofanewinputobservation.Hereweintroducethe sigmoid thatwillhelpusmakethisdecision. Considerasingleinputobservation x ,whichwewillrepresentbyavectorof features [ x 1 ; x 2 ;:::; x n ] (we'llshowsamplefeaturesinthenextsubsection).Theclas- output y canbe1(meaningtheobservationisamemberoftheclass)or0 (theobservationisnotamemberoftheclass).Wewanttoknowtheprobability P ( y = 1 j x ) thatthisobservationisamemberoftheclass.Soperhapsthedecision  5.1  C LASSIFICATION : THESIGMOID 3 isﬁpositivesentimentﬂversusﬁnegativesentimentﬂ,thefeaturesrepresentcounts ofwordsinadocument,and P ( y = 1 j x ) istheprobabilitythatthedocumenthas positivesentiment,whileand P ( y = 0 j x ) istheprobabilitythatthedocumenthas negativesentiment. Logisticregressionsolvesthistaskbylearning,fromatrainingset,avectorof weights anda biasterm .Eachweight w i isarealnumber,andisassociatedwithone oftheinputfeatures x i .Theweight w i representshowimportantthatinputfeatureis tothedecision,andcanbepositive(meaningthefeatureisassociated withtheclass)ornegative(meaningthefeatureisnotassociatedwiththeclass). Thuswemightexpectinasentimenttasktheword awesome tohaveahighpositive weight,and abysmal tohaveaverynegativeweight.The biasterm ,alsocalledthe biasterm intercept ,isanotherrealnumberthat'saddedtotheweightedinputs. intercept TomakeadecisiononatestinstanceŠafterwe'velearnedtheweightsin trainingŠthemultiplieseach x i byitsweight w i ,sumsuptheweighted features,andaddsthebiasterm b .Theresultingsinglenumber z expressesthe weightedsumoftheevidencefortheclass. z =   n X i = 1 w i x i ! + b (5.2) Intherestofthebookwe'llrepresentsuchsumsusingthe dotproduct notationfrom dotproduct linearalgebra.Thedotproductoftwovectors a and b ,writtenas a  b isthesumof theproductsofthecorrespondingelementsofeachvector.Thusthefollowingisan equivalentformationtoEq. 5.2 : z = w  x + b (5.3) ButnotethatnothinginEq. 5.3 forces z tobealegalprobability,thatis,tolie between0and1.Infact,sinceweightsarereal-valued,theoutputmightevenbe negative; z rangesfrom  ¥ to ¥ . Figure5.1 Thesigmoidfunction y = 1 1 + e  z takesarealvalueandmapsittotherange [ 0 ; 1 ] . Itisnearlylineararound0butoutliervaluesgetsquashedtoward0or1. Tocreateaprobability,we'llpass z throughthe sigmoid function, s ( z ) .The sigmoid sigmoidfunction(namedbecauseitlookslikean s )isalsocalledthe logisticfunc- tion ,andgiveslogisticregressionitsname.Thesigmoidhasthefollowingequation, logistic function showngraphicallyinFig. 5.1 : y = s ( z )= 1 1 + e  z (5.4) Thesigmoidhasanumberofadvantages;ittakesareal-valuednumberandmaps itintotherange [ 0 ; 1 ] ,whichisjustwhatwewantforaprobability.Becauseitis  4 C HAPTER 5  L OGISTIC R EGRESSION nearlylineararound0buthasasharpslopetowardtheends,ittendstosquashoutlier valuestoward0or1.Andit'sdifferentiable,whichaswe'llseeinSection 5.8 will behandyforlearning. We'realmostthere.Ifweapplythesigmoidtothesumoftheweightedfeatures, wegetanumberbetween0and1.Tomakeitaprobability,wejustneedtomake surethatthetwocases, p ( y = 1 ) and p ( y = 0 ) ,sumto1.Wecandothisasfollows: P ( y = 1 )= s ( w  x + b ) = 1 1 + e  ( w  x + b ) P ( y = 0 )= 1  s ( w  x + b ) = 1  1 1 + e  ( w  x + b ) = e  ( w  x + b ) 1 + e  ( w  x + b ) (5.5) Nowwehaveanalgorithmthatgivenaninstance x computestheprobability P ( y = 1 j x ) .Howdowemakeadecision?Foratestinstance x ,wesayyesiftheprobability P ( y = 1 j x ) ismorethan.5,andnootherwise.Wecall.5the decisionboundary : decision boundary ‹ y = ˆ 1if P ( y = 1 j x ) > 0 : 5 0otherwise 5.1.1Example:sentiment Let'shaveanexample.Supposewearedoingbinarysentimenton moviereviewtext,andwewouldliketoknowwhethertoassignthesentimentclass + or  toareviewdocument doc .We'llrepresenteachinputobservationbythe6 features x 1 ::: x 6 oftheinputshowninthefollowingtable;Fig. 5.2 showsthefeatures inasampleminitestdocument. VarValueinFig. 5.2 x 1 count(positivelexicon) 2 doc ) 3 x 2 count(negativelexicon) 2 doc ) 2 x 3 ˆ 1ifﬁnoﬂ 2 doc 0otherwise 1 x 4 count ( 1stand2ndpronouns 2 doc ) 3 x 5 ˆ 1ifﬁ!ﬂ 2 doc 0otherwise 0 x 6 log ( wordcountofdoc ) ln ( 66 )= 4 : 19 Let'sassumeforthemomentthatwe'vealreadylearnedareal-valuedweightfor eachofthesefeatures,andthatthe6weightscorrespondingtothe6featuresare [ 2 : 5 ;  5 : 0 ;  1 : 2 ; 0 : 5 ; 2 : 0 ; 0 : 7 ] ,while b =0.1.(We'lldiscussinthenextsectionhow theweightsarelearned.)Theweight w 1 ,forexampleindicateshowimportanta featurethenumberofpositivelexiconwords( great , nice , enjoyable ,etc.)isto apositivesentimentdecision,while w 2 tellsustheimportanceofnegativelexicon words.Notethat w 1 = 2 : 5ispositive,while w 2 =  5 : 0,meaningthatnegativewords arenegativelyassociatedwithapositivesentimentdecision,andareabouttwiceas importantaspositivewords.  5.7  I NTERPRETINGMODELS 17 5.7Interpretingmodels Oftenwewanttoknowmorethanjustthecorrectionofanobservation. Wewanttoknowwhythemadethedecisionitdid.Thatis,wewantour decisiontobe interpretable .Interpretabilitycanbehardtostrictly,butthe interpretable coreideaisthatashumansweshouldknowwhyouralgorithmsreachtheconclu- sionstheydo.Becausethefeaturestologisticregressionareoftenhuman-designed, onewaytounderstandasdecisionistounderstandtheroleeachfeature playsinthedecision.Logisticregressioncanbecombinedwithstatisticaltests(the likelihoodratiotest,ortheWaldtest);investigatingwhetheraparticularfeatureis byoneofthesetests,orinspectingitsmagnitude(howlargeistheweight w associatedwiththefeature?)canhelpusinterpretwhythemadethe decisionitmakes.Thisisenormouslyimportantforbuildingtransparentmodels. Furthermore,inadditiontoitsuseasa,logisticregressioninNLPand manyotheriswidelyusedasananalytictoolfortestinghypothesesaboutthe effectofvariousexplanatoryvariables(features).Intextperhapswe wanttoknowiflogicallynegativewords( no,not,never )aremorelikelytobeasso- ciatedwithnegativesentiment,orifnegativereviewsofmoviesaremorelikelyto discussthecinematography.However,indoingsoit'snecessarytocontrolforpo- tentialconfounds:otherfactorsthatmightsentiment(themoviegenre,the yearitwasmade,perhapsthelengthofthereviewinwords).Orwemightbestudy- ingtherelationshipbetweenNLP-extractedlinguisticfeaturesandnon-linguistic outcomes(hospitalreadmissions,politicaloutcomes,orproductsales),butneedto controlforconfounds(theageofthepatient,thecountyofvoting,thebrandofthe product).Insuchcases,logisticregressionallowsustotestwhethersomefeatureis associatedwithsomeoutcomeaboveandbeyondtheeffectofotherfeatures. 5.8Advanced:DerivingtheGradientEquation Inthissectionwegivethederivationofthegradientofthecross-entropylossfunc- tion L CE forlogisticregression.Let'sstartwithsomequickcalculusrefreshers. First,thederivativeofln ( x ) : d dx ln ( x )= 1 x (5.36) Second,the(veryelegant)derivativeofthesigmoid: d s ( z ) dz = s ( z )( 1  s ( z )) (5.37) Finally,the chainrule ofderivatives.Supposewearecomputingthederivative chainrule ofacompositefunction f ( x )= u ( v ( x )) .Thederivativeof f ( x ) isthederivativeof u ( x ) withrespectto v ( x ) timesthederivativeof v ( x ) withrespectto x : df dx = du dv  dv dx (5.38) First,wewanttoknowthederivativeofthelossfunctionwithrespecttoasingle weight w j (we'llneedtocomputeitforeachweight,andforthebias):  18 C HAPTER 5  L OGISTIC R EGRESSION ¶ LL ( w ; b ) ¶ w j = ¶ ¶ w j  [ y log s ( w  x + b )+( 1  y ) log ( 1  s ( w  x + b ))] =   ¶ ¶ w j y log s ( w  x + b )+ ¶ ¶ w j ( 1  y ) log [ 1  s ( w  x + b )]  (5.39) Next,usingthechainrule,andrelyingonthederivativeoflog: ¶ LL ( w ; b ) ¶ w j =  y s ( w  x + b ) ¶ ¶ w j s ( w  x + b )  1  y 1  s ( w  x + b ) ¶ ¶ w j 1  s ( w  x + b ) (5.40) Rearrangingterms: ¶ LL ( w ; b ) ¶ w j =   y s ( w  x + b )  1  y 1  s ( w  x + b )  ¶ ¶ w j s ( w  x + b ) (5.41) Andnowplugginginthederivativeofthesigmoid,andusingthechainruleone moretime,weendupwithEq. 5.42 : ¶ LL ( w ; b ) ¶ w j =   y  s ( w  x + b ) s ( w  x + b )[ 1  s ( w  x + b )]  s ( w  x + b )[ 1  s ( w  x + b )] ¶ ( w  x + b ) ¶ w j =   y  s ( w  x + b ) s ( w  x + b )[ 1  s ( w  x + b )]  s ( w  x + b )[ 1  s ( w  x + b )] x j =  [ y  s ( w  x + b )] x j =[ s ( w  x + b )  y ] x j (5.42) 5.9Summary Thischapterintroducedthe logisticregression modelof  .  Logisticregressionisasupervisedmachinelearningthatextracts real-valuedfeaturesfromtheinput,multiplieseachbyaweight,sumsthem, andpassesthesumthrougha sigmoid functiontogenerateaprobability.A thresholdisusedtomakeadecision.  Logisticregressioncanbeusedwithtwoclasses(e.g.,positiveandnegative sentiment)orwithmultipleclasses( multinomiallogisticregression ,forex- ampleforn-arytextpart-of-speechlabeling,etc.).  Multinomiallogisticregressionusesthe softmax functiontocomputeproba- bilities.  Theweights(vector w andbias b )arelearnedfromalabeledtrainingsetviaa lossfunction,suchasthe cross-entropyloss ,thatmustbeminimized.  Minimizingthislossfunctionisa convexoptimization problem,anditerative algorithmslike gradientdescent areusedtotheoptimalweights.  Regularization isusedtoavoidov  Logisticregressionisalsooneofthemostusefulanalytictools,becauseofits abilitytotransparentlystudytheimportanceofindividualfeatures.
Logistic regression	Sigmoid function	Probabilistic language model	Text classification	Natural Language Processing (NLP)
What are prescriptive versus descriptive rules?	Prescriptive linguistic rules are rules that tell a person what they "should" and "should"not do with a language, like "do not split an infinitive." They are the kind of rules that appear in grammar books for non-linguists and constitute what non-linguists think of as "grammar." In linguistics, a lot of these are not considered grammatical rules at all, but rather rules of social convention; they reflect our ideas about language, which are often incorrect. Descriptive rules are the rules linguists write to describe the natural patterns occurring in what people actually say. A lot of these rules, such as "a phrase must be headed by a word with the same grammatical category as the phrase" are so abstract that they are never intentionally followed or broken. These rules are taken by linguists to describe the true contents of "grammar," which are sub-conscious, not learned from books.	Which of the following sentences would be considered grammatical by linguists?	I ain' seen all y'all in a coon's age!	She was attracted to European young tall men.|||I believe that you to come to my party.|||It makes the fun to see the movie.	Understanding Prescriptive vs. Descriptive Grammar||image||N/A
Descriptive grammar	Prescriptive grammar	Grammar	Grammatical rules
Describe the similarities and differences between the Propbank entities called “frame files” and Framenet “frames” in regards to their notion of “frame” and the way they label the complements of verbs.	Propbank “frame files” are verb-specific and their verb-specific arguments do not have rule-governed semantic roles. They are labeled arg1, arg2, etc. with human readable descriptions of their semantic roles. Other complements of verbs appearing in Propbank sentences, such as adverbials, may (some of them) be labeled with a set of “ArgM” semantic tags such as TMP (temporal) and MNR (manner). Propbank files describe a variety of syntactic realizations for one verb and set of roles, thus capturing phenomena such as “diathesis.” Framenet frames are not verb-centered; they describe types of situation, each with a range of verbs and other vocabulary items that might be used in a particular frame, and they specify many optional “frame elements”; for example “mode of travel” and “velocity” might be in a frame with the verb “travel” even though the verb “travel” often appears without these elements.	Which of the following are the rough equivalents in Framenet of Propbank’s “ArgM” roles? 	non-core roles	core roles|||modifiers|||frame elements	Chapter Sections on Propbank and Framenet||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 20 SemanticRoleLabeling Sometimebetweenthe7thand4thcenturiesBCE,theIndiangrammarianP ¯ an . ini 1 wroteafamoustreatiseonSanskritgrammar,theAs . t . ¯ adhy ¯ ay ¯ (`8books'),atreatise thathasbeencalledﬁoneofthegreatestmonumentsof humanintelligenceﬂ 1933,11) .Thework describesthelinguisticsoftheSanskritlanguageinthe formof3959sutras,eachveryef(sinceithadto bememorized!)expressingpartofaformalrulesystem thatbrilliantlymodernmechanismsofformal languagetheory (PennandKiparsky,2012) .Onesetof rules,relevanttoourdiscussioninthischapter,describes the k ¯ arakas ,semanticrelationshipsbetweenaverband nounarguments,roleslike agent , instrument ,or destina- tion .P ¯ an . ini'sworkwastheearliestweknowofthattried tounderstandthelinguisticrealizationofeventsandtheirparticipants.Thistask ofunderstandinghowparticipantsrelatetoeventsŠbeingabletoanswertheques- tionﬁWhodidwhattowhomﬂ(andperhapsalsoﬁwhenandwhereﬂ)Šisacentral questionofnaturallanguageunderstanding. Let'smoveforward2.5millenniatothepresentandconsidertheverymundane goalofunderstandingtextaboutapurchaseofstockbyXYZCorporation.This purchasingeventanditsparticipantscanbedescribedbyawidevarietyofsurface forms.Theeventcanbedescribedbyaverb( sold,bought )oranoun( purchase ), andXYZCorpcanbethesyntacticsubject(of bought ),theindirectobject(of sold ), orinagenitiveornouncompoundrelation(withthenoun purchase )despitehaving notionallythesameroleinallofthem:  XYZcorporationboughtthestock.  TheysoldthestocktoXYZcorporation.  ThestockwasboughtbyXYZcorporation.  ThepurchaseofthestockbyXYZcorporation...  ThestockpurchasebyXYZcorporation... Inthischapterweintroducealevelofrepresentationthatcapturesthecommon- alitybetweenthesesentences:therewasapurchaseevent,theparticipantswere XYZCorpandsomestock,andXYZCorpwasthebuyer.Theseshallowsemantic representations, semanticroles ,expresstherolethatargumentsofapredicatetake intheevent,indatabaseslikePropBankandFrameNet.We'llintroduce semanticrolelabeling ,thetaskofassigningrolestospansinsentences,and selec- tionalrestrictions ,thepreferencesthatpredicatesexpressabouttheirarguments, suchasthefactthatthethemeof eat isgenerallysomethingedible. 1 FigureshowsabirchbarkmanuscriptfromKashmiroftheRupavatra,agrammaticaltextbookbased ontheSanskritgrammarofPanini.ImagefromtheWellcomeCollection.  2 C HAPTER 20  S EMANTIC R OLE L ABELING 20.1SemanticRoles ConsiderhowinChapter16werepresentedthemeaningofargumentsforsentences likethese: (20.1) Sashabrokethewindow. (20.2) Patopenedthedoor. Aneo-Davidsonianeventrepresentationofthesetwosentenceswouldbe 9 e ; x ; yBreaking ( e ) ^ Breaker ( e ; Sasha ) ^ BrokenThing ( e ; y ) ^ Window ( y ) 9 e ; x ; yOpening ( e ) ^ Opener ( e ; Pat ) ^ OpenedThing ( e ; y ) ^ Door ( y ) Inthisrepresentation,therolesofthesubjectsoftheverbs break and open are Breaker and Opener respectively.These deeproles aretoeachevent; Break- deeproles ing eventshave Breakers , Opening eventshave Openers ,andsoon. Ifwearegoingtobeabletoanswerquestions,performinferences,ordoany furtherkindsofnaturallanguageunderstandingoftheseevents,we'llneedtoknow alittlemoreaboutthesemanticsofthesearguments. Breakers and Openers have somethingincommon.Theyarebothvolitionalactors,oftenanimate,andtheyhave directcausalresponsibilityfortheirevents. Thematicroles areawaytocapturethissemanticcommonalitybetween Break- thematicroles ers and Eaters .Wesaythatthesubjectsofboththeseverbsare agents .Thus, AGENT agents isthethematicrolethatrepresentsanabstractideasuchasvolitionalcausation.Sim- ilarly,thedirectobjectsofboththeseverbs,the BrokenThing and OpenedThing ,are bothprototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction. Thesemanticrolefortheseparticipantsis theme . theme ThematicRole AGENT Thevolitionalcauserofanevent EXPERIENCER Theexperiencerofanevent FORCE Thenon-volitionalcauseroftheevent THEME Theparticipantmostdirectlyaffectedbyanevent RESULT Theendproductofanevent CONTENT Thepropositionorcontentofapropositionalevent INSTRUMENT Aninstrumentusedinanevent BENEFICIARY Theofanevent SOURCE Theoriginoftheobjectofatransferevent GOAL Thedestinationofanobjectofatransferevent Figure20.1 Somecommonlyusedthematicroleswiththeir Althoughthematicrolesareoneoftheoldestlinguisticmodels,aswesawabove, theirmodernformulationisdueto Fillmore(1968) and Gruber(1965) .Although thereisnouniversallyagreed-uponsetofroles,Figs. 20.1 and 20.2 listsomethe- maticrolesthathavebeenusedinvariouscomputationalpapers,togetherwithrough andexamples.Mostthematicrolesetshaveaboutadozenroles,butwe'll seesetswithsmallernumbersofroleswithevenmoreabstractmeanings,andsets withverylargenumbersofrolesthataretosituations.We'llusethegeneral term semanticroles forallsetsofroles,whethersmallorlarge. semanticroles  20.2  D IATHESIS A LTERNATIONS 3 ThematicRoleExample AGENT Thewaiter spilledthesoup. EXPERIENCER John hasaheadache. FORCE Thewind blowsdebrisfromthemallintoouryards. THEME OnlyafterBenjaminFranklinbroke theice ... RESULT Thecitybuilta regulation-sizebaseballdiamond ... CONTENT Monaasked ﬁYoumetMaryAnnatasupermarket?ﬂ INSTRUMENT Hepoachedstunningthem withashockingdevice ... BENEFICIARY WheneverAnnCallahanmakeshotelreservations forherboss ... SOURCE Iwin fromBoston . GOAL Idrove toPortland . Figure20.2 Someprototypicalexamplesofvariousthematicroles. 20.2DiathesisAlternations Themainreasoncomputationalsystemsusesemanticrolesistoactasashallow meaningrepresentationthatcanletusmakesimpleinferencesthataren'tpossible fromthepuresurfacestringofwords,orevenfromtheparsetree.Toextendthe earlierexamples,ifadocumentsaysthat CompanyAacquiredCompanyB ,we'd liketoknowthatthisanswersthequery WasCompanyBacquired? despitethefact thatthetwosentenceshaveverydifferentsurfacesyntax.Similarly,thisshallow semanticsmightactasausefulintermediatelanguageinmachinetranslation. Semanticrolesthushelpgeneralizeoverdifferentsurfacerealizationsofpred- icatearguments.Forexample,whilethe AGENT isoftenrealizedasthesubjectof thesentence,inothercasesthe THEME canbethesubject.Considerthesepossible realizationsofthethematicargumentsoftheverb break : (20.3) John AGENT brokethewindow. THEME (20.4) John AGENT brokethewindow THEME witharock. INSTRUMENT (20.5) Therock INSTRUMENT brokethewindow. THEME (20.6) Thewindow THEME broke. (20.7) Thewindow THEME wasbrokenbyJohn. AGENT Theseexamplessuggestthat break has(atleast)thepossiblearguments AGENT , THEME ,and INSTRUMENT .Thesetofthematicroleargumentstakenbyaverbis oftencalledthe thematicgrid , q -grid,or caseframe .Wecanseethatthereare thematicgrid caseframe (amongothers)thefollowingpossibilitiesfortherealizationoftheseargumentsof break : AGENT /Subject, THEME /Object AGENT /Subject, THEME /Object, INSTRUMENT /PP with INSTRUMENT /Subject, THEME /Object THEME /Subject Itturnsoutthatmanyverbsallowtheirthematicrolestoberealizedinvarious syntacticpositions.Forexample,verbslike give canrealizethe THEME and GOAL argumentsintwodifferentways:  4 C HAPTER 20  S EMANTIC R OLE L ABELING (20.8) a. Doris AGENT gavethebook THEME toCary. GOAL b. Doris AGENT gaveCary GOAL thebook. THEME Thesemultipleargumentstructurerealizations(thefactthat break cantake AGENT , INSTRUMENT ,or THEME assubject,and give canrealizeits THEME and GOAL in eitherorder)arecalled verbalternations or diathesisalternations .Thealternation verb alternation weshowedabovefor give ,the dativealternation ,seemstooccurwithparticularse- dative alternation manticclassesofverbs,includingﬁverbsoffuturehavingﬂ( advance , allocate , offer , owe ),ﬁsendverbsﬂ( forward , hand , mail ),ﬁverbsofthrowingﬂ( kick , pass , throw ), andsoon. Levin(1993) listsfor3100Englishverbsthesemanticclassestowhich theybelong(47high-levelclasses,dividedinto193moreclasses)andthe variousalternationsinwhichtheyparticipate.Theselistsofverbclasseshavebeen incorporatedintotheonlineresourceVerbNet (Kipperetal.,2000) ,whichlinkseach verbtobothWordNetandFrameNetentries. 20.3SemanticRoles:ProblemswithThematicRoles Representingmeaningatthethematicrolelevelseemslikeitshouldbeusefulin dealingwithcomplicationslikediathesisalternations.Yetithasprovedquitedif culttocomeupwithastandardsetofroles,andequallydiftoproduceaformal ofroleslike AGENT , THEME ,or INSTRUMENT . Forexample,researchersattemptingtorolesetsoftentheyneedto fragmentarolelike AGENT or THEME intomanyroles. LevinandRappa- portHovav(2005) summarizeanumberofsuchcases,suchasthefactthereseem tobeatleasttwokindsof INSTRUMENTS , intermediary instrumentsthatcanappear assubjectsand enabling instrumentsthatcannot: (20.9) a. Thecookopenedthejarwiththenewgadget. b. Thenewgadgetopenedthejar. (20.10) a. Shellyatetheslicedbananawithafork. b. *Theforkatetheslicedbanana. Inadditiontothefragmentationproblem,therearecasesinwhichwe'dliketo reasonaboutandgeneralizeacrosssemanticroles,butthediscretelistsofroles don'tletusdothis. Finally,ithasproveddiftoformallythethematicroles.Considerthe AGENT role;mostcasesof AGENTS areanimate,volitional,sentient,causal,butany individualnounphrasemightnotexhibitalloftheseproperties. Theseproblemshaveledtoalternative semanticrole modelsthatuseeither semanticrole manyfewerormanymoreroles. Theoftheseoptionsisto generalizedsemanticroles thatabstract overthethematicroles.Forexample, PROTO - AGENT and PROTO - PATIENT pr pr aregeneralizedrolesthatexpressroughlyagent-likeandroughlypatient-likemean- ings.Theserolesarenotbynecessaryandsufconditions,butrather byasetofheuristicfeaturesthataccompanymoreagent-likeormorepatient-like meanings.Thus,themoreanargumentdisplaysagent-likeproperties(beingvoli- tionallyinvolvedintheevent,causinganeventorachangeofstateinanotherpar- ticipant,beingsentientorintentionallyinvolved,moving)thegreaterthelikelihood
Propbank	Framenet	Semantic roles	Semantic role labeling (SRL)
What are word vectors?	Word vectors are one way of representing the identities of words so that they can be grouped and compared to each other for semantic similarity. They do not directly represent the meanings or syntactic identities of words but give very good results for semantic similarity. One creates word vectors by counting how many times words co-occur near to or next to each other, in sentences. There are many different ways of doing this depending on how many neighbors of each word are counted--how large a window around each word contributes to its vector--and whether the distances, to the right or left, make a difference. Then one creates a vector for each word; its dimensions are all the words that could co-occur with it, the values of each dimension being the frequencies of co-occurence. In other words, a word vector is a long list of numbers—one for each word that might co-occur with word so defined. Once word vectors are created it is possible to perform mathematical operations on them and deep learning with them very easily in order to find groups of related words or to judge the semantic similarity between groups of words, such as for topic modeling.	Which of the following is NOT true of word vectors?	One of their most effective applications has been POS-tagging	Word vectors are considered one of the greatest successes of statistical NLP|||Semantic classes can be derived from word-vectors through clustering|||Word vectors not artificially reduced in dimensionality are usually too large to use.	Lecture on word vectors (mainly slides 1-10)||slides||!"##$%& '(()&*(+,-.-/&& 01,&2+34,+5&*+-/4+/(&6,17(88.-/& &&*(734,(&#9&:1,%&;(731,8& &&<.7=+,%&"17=(,&  >1?&%1&?(&,(),(8(-3&3=(&@(+-.-/&10&+&?1,%A& !"!#"#$%&'()*+,%-.()/+% 0%1/23'4.35% B(+-.-/ %67/89:/+%,'(4.3*+;<% ¥!:)/%',/*%:)*:%'9%+/=+/9/3:/,%8;%*%>.+,?%=)+*9/?%/:(@% ¥!:)/%',/*%:)*:%*%=/+9.3%>*3:9%:.%/A=+/99%8;%B9'3C% >.+,9?%9'C39?%/:(@% ¥!:)/%',/*%:)*:%'9%/A=+/99/,%'3%*%>.+D%.E%>+'43C?%*+:?%/:(@%  >1?&31&,(),(8(-3&@(+-.-/&.-&+&71@)43(,A& !"!#"#$%&'()*+,%-.()/+% !%F.GG.3%*39>/+5%H9/%*%:*A.3.G;%I'D/% 7.+,J/: %:)*:%)*9% );=/+3;G9 %6'9K*<%+/I*4.39)'=9%%%%%%%%*3,% %%%%%%%%%%9;3.3;G%9/:9%6C..,<5% L-;39/: 6M=+.(;.3',@3@N#M<?%% -;39/: 6M(*+3'O.+/@3@N#M<?%% -;39/: 6M=I*(/3:*I@3@N#M<?%% -;39/: 6MG*GG*I@3@N#M<?%% -;39/: 6MO/+:/8+*:/@3@N#M<?%% -;39/: 6M().+,*:/@3@N#M<?%% -;39/: 6M*3'G*I@3@N#M<?%% -;39/: 6M.+C*3'9G@3@N#M<?%% -;39/: 6MI'O'3CP:)'3C@3@N#M<?%% -;39/: 6M>).I/@3@N0M<?%% -;39/: 6M.8Q/(:@3@N#M<?%% -;39/: 6M=);9'(*IP/34:;@3@N#M<?%% -;39/: 6M/34:;@3@N#M<R% -5%6 *,Q <%EBII?%C..,%% -5%6 *,Q <%/94G*8I/?%C..,?%).3.+*8I/?%+/9=/(:*8I/%% -5%6 *,Q <%8/3/2('*I?%C..,%% -5%6 *,Q <%C..,?%QB9:?%B=+'C):%% -5%6 *,Q <%*,/=:?%/A=/+:?%C..,?%=+*(4(/,?%% =+.2('/3:?%9D'IIEBI% -5%6 *,Q <%,/*+?%C..,?%3/*+%% -5%6 *,Q <%C..,?%+'C):?%+'=/% S%-5%6 *,O <%>/II?%C..,%% -5%6 *,O <%:).+.BC)I;?%9.B3,I;?%C..,%% -5%63<%C..,?%C..,3/99%% -5%63<%(.GG.,':;?%:+*,/%C..,?%C..,%%  6,1C5(@8&?.3=&3=.8&%.87,(3(&,(),(8(-3+D1-& !"!#"#$%&'()*+,%-.()/+% T%¥!U+/*:%*9%+/9.B+(/%8B:%G'99'3C%3B*3(/9?%/@C@% 8E-1-E@8 5%% *,/=:?%/A=/+:?%C..,?%=+*(4(/,?%=+.2('/3:?%9D'IIEBIV% ¥!W'99'3C%3/>%>.+,9%6'G=.99'8I/%:.%D//=%B=%:.%,*:/<5% >'(D/,?%8*,*99?%3'X;?%(+*(D?%*(/?%>'Y*+,?%C/3'B9?% 3'3Q'*%¥!-B8Q/(4O/% ¥!&/ZB'+/9%)BG*3%I*8.+%:.%(+/*:/%*3,%*,*=:% ¥![*+,%:.%(.G=B:/%*((B+*:/%>.+,%9'G'I*+':;% !% 6,1C5(@8&?.3=&3=.8&%.87,(3(&,(),(8(-3+D1-& \)/%O*9:%G*Q.+':;%.E%+BI/K8*9/,% +-%%9:*494(*I%J]^%>.+D%+/C*+,9% >.+,9%*9%*:.G'(%9;G8.I95% hotel ? conference ? walk %_3%O/(:.+%9=*(/%:/+G9?%:)'9%'9%*%O/(:.+%>':)%.3/%#%*3,%*%I.:%.E%Y/+./9% [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]  1'G/39'.3*I':;5%0N`%69=//()<%a%bN`%6^\c<%a%bNN`%68'C%O.(*8<%a%#!W%6U..CI/%#\<% 7/%(*II%:)'9%*%d .3/K).: e%+/=+/9/3:*4.3@%_:9%=+.8I/G5%   motel [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]  AND    hotel  [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]  =  0 %b% '.83,.C4D1-+5&8.@.5+,.3E&C+8(%&,(),(8(-3+D1-8& f.B%(*3%C/:%*%I.:%.E%O*IB/%8;%+/=+/9/343C%*%>.+,%8;% G/*39%.E%':9%3/'C)8.+9% df.B%9)*II%D3.>%*%>.+,%8;%:)/%(.G=*3;%':%D//=9e% 6g@%&@%h'+:)%#ibj5%##<% k3/%.E%:)/%G.9:%9B((/99EBI%',/*9%.E%G.,/+3%9:*494(*I%J]^%  government debt problems turning into banking crises as has happened in           saying that Europe needs unified banking regulation to replace the hodgepodge  "%\)/9/%>.+,9%>'II%+/=+/9/3:% !"#$%#&' !%%$% >1?&31&@+F(&-(./=C1,8&,(),(8(-3&?1,%8A& !"!#"#$%&'()*+,%-.()/+% j%l39>/+5%7':)%*% (..((B++/3(/ %G*:+'A%m% ¥!0%.=4.395%EBII%,.(BG/3:% O9%>'3,.>9%¥!7.+,%K%,.(BG/3:% (..((B++/3(/ %G*:+'A%>'II%C'O/% C/3/+*I%:.='(9%6*II%9=.+:9%:/+G9%>'II%)*O/%9'G'I*+% /3:+'/9<%I/*,'3C%:.%d]*:/3:%-/G*34(%l3*I;9'9e% ¥!_39:/*,5%7'3,.>%*+.B3,%/*()%>.+,% !%(*=:B+/9%8.:)% 9;3:*(4(%6^k-<%*3,%9/G*34(%'3E.+G*4.3%  :.-%1?&C+8(%& 711774,(-7( &@+3,.G& !"!#"#$%&'()*+,%-.()/+% n%o!7'3,.>%I/3C:)%#%6G.+/%(.GG.35%b%K%#N<% o!-;GG/:+'(%6'++/I/O*3:%>)/:)/+%I/X%.+%+'C):%(.3:/A:<% o!pA*G=I/%(.+=B95%% o!_%I'D/%,//=%I/*+3'3C@%% o!_%I'D/%J]^@%% o!_%/3Q.;%q;'3C@%  :.-%1?&C+8(%& 711774,(-7( &@+3,.G& !"!#"#$%&'()*+,%-.()/+% i%o!pA*G=I/%(.+=B95%% o!_%I'D/%,//=%I/*+3'3C@%% o!_%I'D/%J]^@%% o!_%/3Q.;%q;'3C@% 714-38& H&5.F(& (-I1E& %(()& 5(+,-.-/& 2*6& JE.-/& K&H&N%0%#%N%N%N%N%N%5.F(& 0%N%N%#%N%#%N%N%(-I1E& #%N%N%N%N%N%#%N%%(()& N%#%N%N%#%N%N%N%5(+,-.-/& N%N%N%#%N%N%N%#%2*6& N%#%N%N%N%N%N%#%JE.-/& N%N%#%N%N%N%N%#%K&N%N%N%N%#%#%#%N% 6,1C5(@8&?.3=&8.@)5(& 711774,,(-7( &L(731,8& !"!#"#$%&'()*+,%-.()/+% #N%_3(+/*9/%'3%9'Y/%>':)%O.(*8BI*+;% %r/+;%)'C)%,'G/39'.3*I5%+/ZB'+/%*%I.:%.E%9:.+*C/% %-B89/ZB/3:%(I*99'2(*4.3%G.,/I9%)*O/% 9=*+9':; %'99B/9% %!%W.,/I9%*+/%I/99%+.8B9:%  "154D1-9&*1?&%.@(-8.1-+5&L(731,8& !"!#"#$%&'()*+,%-.()/+% ##%o!_,/*5%9:.+/%dG.9:e%.E%:)/%'G=.+:*3:%'3E.+G*4.3%'3%*% 2A/,?%9G*II%3BG8/+%.E%,'G/39'.395%*%,/39/%O/(:.+% o!H9B*II;%*+.B3,%0b%a%#NNN%,'G/39'.39% o![.>%:.%+/,B(/%:)/%,'G/39'.3*I':;V%  B(3=1%&M9&'.@(-8.1-+5.3E&<(%47D1-&1-&N& !"!#"#$%&'()*+,%-.()/+% #0%-'3CBI*+%r*IB/%1/(.G=.9'4.3%.E% (..((B++/3(/ %G*:+'A% (@%%%Rohde,Gonnerman,PlautModelingWordMeaningUsingLexicalCo-Occurrence r=nnrrXUS SSSS.231rUUU 123VmmVV123........=nXUSmVTVTUUU123Sk0000VmVV123...SSS231...kkknrkFigure1:Thesingularvaluedecompositionofmatrix X.öXisthebestrank kapproximationto X,intermsofleast squares.tropyofthedocumentdistributionofrowvector a.Words thatareevenlydistributedoverdocumentswillhavehigh entropyandthusalowweighting,re ßectingtheintuition thatsuchwordsarelessinteresting. ThecriticalstepoftheLSAalgorithmistocompute thesingularvaluedecomposition(SVD)ofthenormal- izedco-occurrencematrix.AnSVDissimilartoaneigen- valuedecomposition,butcanbecomputedforrectangu- larmatrices.AsshowninFigure1,theSVDisaprod- uctofthreematrices,the Þrst,U,containingorthonormal columnsknownasthe leftsingularvectors ,andthelast, VTcontainingorthonormalrowsknownasthe rightsin- gularvectors ,whilethemiddle, S,isadiagonalmatrix containingthe singularvalues .Theleftandrightsingu- larvectorsareakintoeigenvectorsandthesingularvalues areakintoeigenvaluesandratetheimportanceofthevec- tors.1Thesingularvectorsre ßectprincipalcomponents, oraxesofgreatestvarianceinthedata. IfthematricescomprisingtheSVDarepermutedsuch thatthesingularvaluesareindecreasingorder,theycan betruncatedtoamuchlowerrank, k.Itcanbeshownthat theproductofthesereducedmatricesisthebestrank kap-proximation,intermsofsumsquarederror,totheoriginal matrixX.Thevectorrepresentingword ainthereduced- rankspaceis öUa,the athrowof öU,whilethevectorrepre- sentingdocument bisöVb,the bthrowof öV.Ifanewword, c,oranewdocument, d,isaddedafterthecomputation oftheSVD,theirreduced-dimensionalityvectorscanbe computedasfollows: öUc=XcöVöS!1öVd=XTdöUöS!1ThesimilarityoftwowordsortwodocumentsinLSA isusuallycomputedusingthecosineoftheirreduced- dimensionalityvectors,theformulaforwhichisgivenin 1Infact,ifthematrixissymmetricandpositivesemide Þnite,theleft andrightsingularvectorswillbeidenticalandequivalenttoitseigen- vectorsandthesingularvalueswillbeitseigenvalues. Table3.Itisunclearwhetherthevectorsare Þrstscaled bythesingularvalues, S,beforecomputingthecosine, asimpliedinDeerwester,Dumais,Furnas,Landauer,and Harshman(1990). ComputingtheSVDitselfisnottrivial.Foradense matrixwithdimensions n<m,theSVDcomputation requirestimeproportionalto n2m.Thisisimpractical formatriceswithmorethanafewthousanddimensions. However,LSAco-occurrencematricestendtobequite sparseandtheSVDcomputationismuchfasterforsparse matrices,allowingthemodeltohandlehundredsofthou- sandsofwordsanddocuments.TheLSAsimilarityrat- ingstestedhereweregeneratedusingtheterm-to-term pairwisecomparisoninterfaceavailableontheLSAweb site( http://lsa.colorado.edu ).2Themodelwastrainedon theTouchstoneAppliedScienceAssociates(TASA)Ògen- eralreadingupto ÞrstyearcollegeÓdataset,withthetop 300dimensionsretained. 2.3WordNet-basedmodels WordNetisanetworkconsistingofsynonymsets,repre- sentinglexicalconcepts,linkedtogetherwithvariousrela- tions,suchassynonym,hypernym,andhyponym(Miller etal.,1990).Therehavebeenseveraleffortstobasea measureofsemanticsimilarityontheWordNetdatabase, someofwhicharereviewedinBudanitskyandHirst (2001),Patwardhan,Banerjee,andPedersen(2003),and JarmaszandSzpakowicz(2003).Herewebrie ßysum- marizeeachofthesemethods.Thesimilarityratingsre- portedinSection3weregeneratedusingversion0.06of TedPedersenÕsWordNet::Similaritymodule,alongwith WordNetversion2.0. TheWordNetmethodshaveanadvantageoverHAL, LSA,andCOALSinthattheydistinguishbetweenmul- tiplewordsenses.Thisraisesthequestion,whenjudg- ingthesimilarityofapairofpolysemouswords,of whichsensestouseinthecomparison.Whengiventhe pairthick Ðstout,mosthumansubjectswilljudgethemto bequitesimilarbecause stoutmeansstrongandsturdy, whichmayimplythatsomethingisthick.Butthepair lager Ðstoutisalsolikelytobeconsideredsimilarbecause theydenotetypesofbeer.Inthiscase,theratermaynot evenbeconsciouslyawareoftheadjectivesenseof stout.Consideralso hammerÐsawversus smelledÐsaw.Whether ornotweareawareofit,wetendtoratethesimilarityof apolysemouswordpaironthebasisofthesensesthatare mostsimilartooneanother.Therefore,thesamewasdone withtheWordNetmodels. 2Thedocument-to-documentLSAmodewasalsotestedbuttheterm- to-termmethodprovedslightlybetter. 4'9%:)/%8/9:%+*3D% $'*==+.A'G*4.3%:.% ('?%'3%:/+G9%.E%I/*9:%9ZB*+/9@%% %Rohde,Gonnerman,PlautModelingWordMeaningUsingLexicalCo-Occurrence r=nnrrXUS SSSS.231rUUU 123VmmVV123........=nXUSmVTVTUUU123Sk0000VmVV123...SSS231...kkknrkFigure1:Thesingularvaluedecompositionofmatrix X.öXisthebestrank kapproximationto X,intermsofleast squares.tropyofthedocumentdistributionofrowvector a.Words thatareevenlydistributedoverdocumentswillhavehigh entropyandthusalowweighting,re ßectingtheintuition thatsuchwordsarelessinteresting. ThecriticalstepoftheLSAalgorithmistocompute thesingularvaluedecomposition(SVD)ofthenormal- izedco-occurrencematrix.AnSVDissimilartoaneigen- valuedecomposition,butcanbecomputedforrectangu- larmatrices.AsshowninFigure1,theSVDisaprod- uctofthreematrices,the Þrst,U,containingorthonormal columnsknownasthe leftsingularvectors ,andthelast, VTcontainingorthonormalrowsknownasthe rightsin- gularvectors ,whilethemiddle, S,isadiagonalmatrix containingthe singularvalues .Theleftandrightsingu- larvectorsareakintoeigenvectorsandthesingularvalues areakintoeigenvaluesandratetheimportanceofthevec- tors.1Thesingularvectorsre ßectprincipalcomponents, oraxesofgreatestvarianceinthedata. IfthematricescomprisingtheSVDarepermutedsuch thatthesingularvaluesareindecreasingorder,theycan betruncatedtoamuchlowerrank, k.Itcanbeshownthat theproductofthesereducedmatricesisthebestrank kap-proximation,intermsofsumsquarederror,totheoriginal matrixX.Thevectorrepresentingword ainthereduced- rankspaceis öUa,the athrowof öU,whilethevectorrepre- sentingdocument bisöVb,the bthrowof öV.Ifanewword, c,oranewdocument, d,isaddedafterthecomputation oftheSVD,theirreduced-dimensionalityvectorscanbe computedasfollows: öUc=XcöVöS!1öVd=XTdöUöS!1ThesimilarityoftwowordsortwodocumentsinLSA isusuallycomputedusingthecosineoftheirreduced- dimensionalityvectors,theformulaforwhichisgivenin 1Infact,ifthematrixissymmetricandpositivesemide Þnite,theleft andrightsingularvectorswillbeidenticalandequivalenttoitseigen- vectorsandthesingularvalueswillbeitseigenvalues. Table3.Itisunclearwhetherthevectorsare Þrstscaled bythesingularvalues, S,beforecomputingthecosine, asimpliedinDeerwester,Dumais,Furnas,Landauer,and Harshman(1990). ComputingtheSVDitselfisnottrivial.Foradense matrixwithdimensions n<m,theSVDcomputation requirestimeproportionalto n2m.Thisisimpractical formatriceswithmorethanafewthousanddimensions. However,LSAco-occurrencematricestendtobequite sparseandtheSVDcomputationismuchfasterforsparse matrices,allowingthemodeltohandlehundredsofthou- sandsofwordsanddocuments.TheLSAsimilarityrat- ingstestedhereweregeneratedusingtheterm-to-term pairwisecomparisoninterfaceavailableontheLSAweb site( http://lsa.colorado.edu ).2Themodelwastrainedon theTouchstoneAppliedScienceAssociates(TASA)Ògen- eralreadingupto ÞrstyearcollegeÓdataset,withthetop 300dimensionsretained. 2.3WordNet-basedmodels WordNetisanetworkconsistingofsynonymsets,repre- sentinglexicalconcepts,linkedtogetherwithvariousrela- tions,suchassynonym,hypernym,andhyponym(Miller etal.,1990).Therehavebeenseveraleffortstobasea measureofsemanticsimilarityontheWordNetdatabase, someofwhicharereviewedinBudanitskyandHirst (2001),Patwardhan,Banerjee,andPedersen(2003),and JarmaszandSzpakowicz(2003).Herewebrie ßysum- marizeeachofthesemethods.Thesimilarityratingsre- portedinSection3weregeneratedusingversion0.06of TedPedersenÕsWordNet::Similaritymodule,alongwith WordNetversion2.0. TheWordNetmethodshaveanadvantageoverHAL, LSA,andCOALSinthattheydistinguishbetweenmul- tiplewordsenses.Thisraisesthequestion,whenjudg- ingthesimilarityofapairofpolysemouswords,of whichsensestouseinthecomparison.Whengiventhe pairthick Ðstout,mosthumansubjectswilljudgethemto bequitesimilarbecause stoutmeansstrongandsturdy, whichmayimplythatsomethingisthick.Butthepair lager Ðstoutisalsolikelytobeconsideredsimilarbecause theydenotetypesofbeer.Inthiscase,theratermaynot evenbeconsciouslyawareoftheadjectivesenseof stout.Consideralso hammerÐsawversus smelledÐsaw.Whether ornotweareawareofit,wetendtoratethesimilarityof apolysemouswordpaironthebasisofthesensesthatare mostsimilartooneanother.Therefore,thesamewasdone withtheWordNetmodels. 2Thedocument-to-documentLSAmodewasalsotestedbuttheterm- to-termmethodprovedslightlybetter. 4 ".@)5(&";'&?1,%&L(731,8&.-&6E3=1-& !"!#"#$%&'()*+,%-.()/+% #!%F.+=B95%% _%I'D/%,//=%I/*+3'3C@%_%I'D/%J]^@%_%/3Q.;%q;'3C@% % ".@)5(&";'&?1,%&L(731,8&.-&6E3=1-& !"!#"#$%&'()*+,%-.()/+% #T%F.+=B95%_%I'D/%,//=%I/*+3'3C@%_%I'D/%J]^@%_%/3Q.;%q;'3C@% ^+'343C%2+9:%:>.%(.IBG39%.E%H%(.++/9=.3,'3C%:.%:)/%0%8'CC/9:%9'3CBI*+%O*IB/9%  :1,%&@(+-.-/&.8&%(O-(%&.-&3(,@8&10&L(731,8& ¥!_3%*II%9B89/ZB/3:%G.,/I9?%'3(IB,'3C%,//=%I/*+3'3C%G.,/I9?%*% >.+,%'9%+/=+/9/3:/,%*9%*%,/39/%O/(:.+% %%'')%#&*%+,-+'' s%#b%N@0n$% N@ji0% tN@#jj% tN@#Nj% N@#Ni% tN@bT0% N@!Ti% N@0j#%  >+7F8&31&N& !"!#"#$%&'()*+,%-.()/+% #$%¥!^+.8I/G5%EB3(4.3%>.+,9%6:)/?%)/?%)*9<%*+/%:..% E+/ZB/3:% !%9;3:*A%)*9%:..%GB()%'G=*(: @%-.G/%2A/95%% ¥!G'36 m?: <?%>':)%:u#NN% ¥!_C3.+/%:)/G%*II% ¥!&*G=/,%>'3,.>9%:)*:%(.B3:%(I.9/+%>.+,9%G.+/% ¥!H9/%^/*+9.3%(.++/I*4.39%'39:/*,%.E%(.B3:9?%:)/3%9/:% 3/C*4O/%O*IB/9%:.%N% ¥!vvv% H-3(,(8D-/&8(@+-D7&)+P(,8&(@(,/(&.-&3=(&L(731,8& !"!#"#$%&'()*+,%-.()/+% #j%Rohde,Gonnerman,PlautModelingWordMeaningUsingLexicalCo-Occurrence HEADHANDFACEDOGAMERICACATEYEEUROPEFOOTCHINAFRANCECHICAGOARMFINGERNOSELEGRUSSIAMOUSEAFRICAATLANTAEARSHOULDERASIACOWBULLPUPPYLIONHAWAIIMONTREALTOKYOTOEMOSCOWTOOTHNASHVILLEBRAZILWRISTKITTENANKLETURTLEOYSTERFigure8:Multidimensionalscalingforthreenounclasses. WRISTANKLESHOULDERARMLEGHANDFOOTHEADNOSEFINGERTOEFACEEAREYETOOTHDOGCATPUPPYKITTENCOWMOUSETURTLEOYSTERLIONBULLCHICAGOATLANTAMONTREALNASHVILLETOKYOCHINARUSSIAAFRICAASIAEUROPEAMERICABRAZILMOSCOWFRANCEHAWAIIFigure9:Hierarchicalclusteringforthreenounclassesusingdistancesbasedonvectorcorrelations. 20l3%_G=+.O/,%W.,/I%.E%-/G*34(%-'G'I*+':;%c*9/,%.3%]/A'(*I%F.Kk((B++/3(/%% &.),/%/:%*I@%0NNb% % H-3(,(8D-/&8E-3+7D7&)+P(,8&(@(,/(&.-&3=(&L(731,8& !"!#"#$%&'()*+,%-.()/+% #n%l3%_G=+.O/,%W.,/I%.E%-/G*34(%-'G'I*+':;%c*9/,%.3%]/A'(*I%F.Kk((B++/3(/%% &.),/%/:%*I@%0NNb% %Rohde,Gonnerman,PlautModelingWordMeaningUsingLexicalCo-Occurrence READCALLEDTOLDHEARDASKEDCUTFELTNOTICEDEXPLAINEDKICKEDJUMPEDDETECTEDEMAILEDQUESTIONEDSHOUTEDTASTEDPUNCHEDSHOVEDSTABBEDSMELLEDSENSEDBASHEDTACKLEDDISCERNEDFigure10:Multidimensionalscalingofthreeverbsemanticclasses. TAKESHOWTOOKTAKINGTAKENSPEAKEATCHOOSESPEAKINGGROWGROWINGTHROWSHOWNSHOWINGSHOWEDEATINGCHOSENSPOKECHOSEGROWNGREWSPOKENTHROWNTHROWINGSTEALATETHREWSTOLENSTEALINGCHOOSINGSTOLEEATENFigure11:Multidimensionalscalingofpresent,past,progressive,andpastparticipleformsforeightverbfamilies. 22 H-3(,(8D-/&8(@+-D7&)+P(,8&(@(,/(&.-&3=(&L(731,8& !"!#"#$%&'()*+,%-.()/+% #i%l3%_G=+.O/,%W.,/I%.E%-/G*34(%-'G'I*+':;%c*9/,%.3%]/A'(*I%F.Kk((B++/3(/%% &.),/%/:%*I@%0NNb% %Rohde,Gonnerman,PlautModelingWordMeaningUsingLexicalCo-Occurrence DRIVELEARNDOCTORCLEANDRIVERSTUDENTTEACHTEACHERTREATPRAYPRIESTMARRYSWIMBRIDEJANITORSWIMMERFigure13:Multidimensionalscalingfornounsandtheirassociatedverbs. Table10 The10nearestneighborsandtheirpercentcorrelationsimilaritiesforasetofnouns,undertheCOALS-14Kmodel. gunpointmindmonopolycardboardlipstickleningradfeet 1)46.4handgun32.4points33.5minds39.9monopolies47.4plastic42.9shimmery24.0moscow59.5inches 2)41.1 Þrearms29.2argument24.9consciousness27.8monopolistic37.2foam40.8eyeliner22.7sevastopol57.7foot 3)41.0 Þrearm25.4question23.2thoughts26.5corporations36.7plywood38.8clinique22.7petersburg52.0metres 4)35.3handguns22.3arguments22.4senses25.0government35.6paper38.4mascara20.7novosibirsk45.7legs 5)35.0guns21.5idea22.2subconscious23.2ownership34.8corrugated37.2revlon20.3russia45.4centimeters 6)32.7pistol20.1assertion20.8thinking22.2property32.3boxes35.4lipsticks19.6oblast44.4meters 7)26.3weapon19.5premise20.6perception22.2capitalism31.3wooden35.3gloss19.5minsk40.2inch 8)24.4ri ßes19.3moot20.4emotions21.8capitalist31.0glass34.1shimmer19.2stalingrad38.4shoulders 9)24.2shotgun18.9distinction20.1brain21.6authority30.7fabric33.6blush19.1ussr37.8knees 10)23.6weapons18.7statement19.9psyche21.3subsidies30.5aluminum33.5nars19.0soviet36.9toes Table11 The10nearestneighborsforasetofverbs,accordingtotheCOALS-14Kmodel. needbuyplaychangesendunderstandexplaincreate 1)50.4want53.5buying63.5playing56.9changing55.0sending56.3comprehend53.0understand58.2creating 2)50.2needed52.5sell55.5played55.3changes42.0email53.0explain46.3describe50.6creates 3)42.1needing49.1bought47.6plays48.9changed40.2e-mail49.5understood40.0explaining45.1develop 4)41.2needs41.8purchase37.2players32.2adjust39.8unsubscribe44.8realize39.8comprehend43.3created 5)41.1can40.3purchased35.4player30.2affect37.3mail40.9grasp39.7explained42.6generate 6)39.5able39.7selling33.8game29.5modify35.7please39.1know39.0prove37.8build 7)36.3try38.2sells32.3games28.3different33.3subscribe38.8believe38.2clarify36.4maintain 8)35.4should36.3buys29.0listen27.1alter33.1receive38.5recognize37.1argue36.4produce 9)35.3do34.0sale26.8playable25.6shift32.7submit38.0misunderstand37.0refute35.4integrate 10)34.7necessary31.5cheap25.0beat25.1altering31.5address37.9understands35.9tell35.2implement Table12 The10nearestneighborsforasetofadjectives,accordingtotheCOALS-14Kmodel. highfrightenedredcorrectsimilarfastevilchristian 1)57.5low45.6scared53.7blue59.0incorrect44.9similiar43.1faster24.3sinful48.5catholic 2)51.9higher37.2terri Þed47.8yellow37.7accurate43.2different41.2slow23.4wicked48.1protestant 3)43.4lower33.7confused45.1purple37.5proper40.8same37.8slower23.2vile47.9christians 4)43.2highest33.3frustrated44.9green36.3wrong40.6such28.2rapidly22.5demons47.2orthodox 5)35.9lowest32.6worried43.2white34.1precise37.7speci Þc27.3quicker22.3satan47.1religious 6)31.5increases32.4embarrassed42.8black32.9exact35.6identical26.8quick22.3god46.4christianity 7)30.7increase32.3angry36.8colored30.7erroneous34.6these25.9speeds22.3sinister43.8fundamentalist 8)29.2increasing31.6afraid35.6orange30.6valid34.4unusual25.8quickly22.0immoral43.5jewish 9)28.7increased30.4upset33.5grey30.6inaccurate34.1certain25.5speed21.5hateful43.2evangelical 10)28.3lowering30.3annoyed32.4reddish29.8acceptable32.7various24.3easy21.3sadistic41.2mormon 24 6,1C5(@8&?.3=&";'& !"!#"#$%&'()*+,%-.()/+% 0N%F.G=B:*4.3*I%(.9:%9(*I/9% ZB*,+*4(*II; %%E.+%3%A%G%G*:+'A5% k6.#0<%q.=9%6>)/3%3wG<%% !%c*,%E.+%G'II'.39%.E%>.+,9%.+%,.(BG/3:9% %[*+,%:.%'3(.+=.+*:/%3/>%>.+,9%.+%,.(BG/3:9% 1'x/+/3:%I/*+3'3C%+/C'G/%:)*3%.:)/+%1]%G.,/I9% %% H%(+9&'.,(735E&5(+,-&51?Q%.@(-8.1-+5&?1,%&L(731,8& !"!#"#$%&'()*+,%-.()/+% 0#%¥!kI,%',/*@%&/I/O*3:%E.+%:)'9%I/(:B+/%y%,//=%I/*+3'3C5% ¥!]/*+3'3C%+/=+/9/3:*4.39%8;%8*(DK=+.=*C*43C%/++.+9@% 6&BG/I)*+: %/:%*I@?%#in$<% ¥!l%3/B+*I%=+.8*8'I'94(%I*3CB*C/%G.,/I%6 c/3C'. %/:%*I@?%0NN!<%%% ¥!J]^%6*IG.9:<%E+.G%-(+*:()%6 F.II.8/+: %y%7/9:.3?%0NNn<% ¥!l%+/(/3:?%/O/3%9'G=I/+%*3,%E*9:/+%G.,/I5%% >.+,0O/(%6 W'D.I.O %/:%*I@%0N#!<% !%'3:+.%3.> %% B+.-&H%(+&10&?1,%#L(7& !"!#"#$%&'()*+,%-.()/+% 00%o!_39:/*,%.E%(*=:B+'3C% (..((B++/3(/ %(.B3:9%,'+/(:I;?% o!^+/,'(:%9B++.B3,'3C%>.+,9%.E%/O/+;%>.+,%% o!c.:)%*+/%ZB':/%9'G'I*+?%9//%d /)0123'/)0!")'42-506+'706' 8069':2;62+2#5",0#<' 8;%^/33'3C:.3%/:%*I@%60N#T<%*3,% ]/O;%*3,%U.I,8/+C%60N#T<%S%G.+/%I*:/+% o!h*9:/+%*3,%(*3%/*9'I;%'3(.+=.+*:/%*%3/>%9/3:/3(/" ,.(BG/3:%.+%*,,%*%>.+,%:.%:)/%O.(*8BI*+;%  '(3+.58&10&:1,%#;(7& !"!#"#$%&'()*+,%-.()/+% 0!%¥!^+/,'(:%9B++.B3,'3C%>.+,9%'3%*%>'3,.>%.E%I/3C:)%G%.E% /O/+;%>.+,@% ¥!k8Q/(4O/%EB3(4.35%W*A'G'Y/%:)/%I.C%=+.8*8'I':;%.E% *3;%(.3:/A:%>.+,%C'O/3%:)/%(B++/3:%(/3:/+%>.+,5% ¥!%%¥!7)/+/% !%+/=+/9/3:9%*II%O*+'*8I/9%>/%.=4G'Y/%  '(3+.58&10&:1,%#;(7& !"!#"#$%&'()*+,%-.()/+% 0T%¥!^+/,'(:%9B++.B3,'3C%>.+,9%'3%*%>'3,.>%.E%I/3C:)%G%.E%/O/+;% >.+,% ¥!h.+%%%%%%%%%%%%%%%%%%%%%%:)/%9'G=I/9:%2+9:%E.+GBI*4.3%'9%% ¥!>)/+/%.%'9%:)/%.B:9',/%6.+%.B:=B:<%>.+,%',?%(%'9%:)/%(/3:/+%>.+,% ',?%B%*3,%O%*+/%d(/3:/+e%*3,%d.B:9',/e%O/(:.+9%.E%.%*3,%(% ¥!pO/+;%>.+,%)*9%:>.%O/(:.+9z% ¥!\)'9%'9%/99/34*II;%d,;3*G'(e%I.C'94(%+/C+/99'.3% trainingtime.ThebasicSkip-gramformulationdeÞnes p(wt+j|wt)usingthesoftmaxfunction: p(wO|wI)=exp !v!wO"vwI"#Ww=1exp !v!w"vwI"(2) where vwand v!waretheÒinputÓandÒoutputÓvectorrepresentationsof w,and Wisthenum- berofwordsinthevocabulary.Thisformulationisimpracti calbecausethecostofcomputing !log p(wO|wI)isproportionalto W,whichisoftenlarge( 105Ð107terms). 2.1HierarchicalSoftmax AcomputationallyefÞcientapproximationofthefullsoftm axisthehierarchicalsoftmax.Inthe contextofneuralnetworklanguagemodels,itwasÞrstintro ducedbyMorinandBengio[12].The mainadvantageisthatinsteadofevaluating Woutputnodesintheneuralnetworktoobtainthe probabilitydistribution,itisneededtoevaluateonlyabo utlog 2(W)nodes. Thehierarchicalsoftmaxusesabinarytreerepresentation oftheoutputlayerwiththe Wwordsas itsleavesand,foreachnode,explicitlyrepresentstherel ativeprobabilitiesofitschildnodes.These deÞnearandomwalkthatassignsprobabilitiestowords. Moreprecisely,eachword wcanbereachedbyanappropriatepathfromtherootofthetree .Let n(w,j )bethe j-thnodeonthepathfromtherootto w,andlet L(w)bethelengthofthispath,so n(w,1)=root and n(w,L (w))= w.Inaddition,foranyinnernode n,let ch( n)beanarbitrary Þxedchildof nandlet [[x]]be1if xistrueand-1otherwise.ThenthehierarchicalsoftmaxdeÞn esp(wO|wI)asfollows: p(w|wI)=L(w)#1$j=1!![[n(w,j +1)=ch( n(w,j ))]] áv!n(w,j )"vwI"(3) where !(x)=1 /(1+exp( "x)).ItcanbeveriÞedthat #Ww=1p(w|wI)=1 .Thisimpliesthatthe costofcomputing log p(wO|wI)and !log p(wO|wI)isproportionalto L(wO),whichonaverage isnogreaterthan log W.Also,unlikethestandardsoftmaxformulationoftheSkip- gramwhich assignstworepresentations vwand v!wtoeachword w,thehierarchicalsoftmaxformulationhas onerepresentation vwforeachword wandonerepresentation v!nforeveryinnernode nofthe binarytree. Thestructureofthetreeusedbythehierarchicalsoftmaxha saconsiderableeffectontheperfor- mance.MnihandHintonexploredanumberofmethodsforconst ructingthetreestructureandthe effectonboththetrainingtimeandtheresultingmodelaccu racy[10].Inourworkweuseabinary Huffmantree,asitassignsshortcodestothefrequentwords whichresultsinfasttraining.Ithas beenobservedbeforethatgroupingwordstogetherbytheirf requencyworkswellasaverysimple speeduptechniquefortheneuralnetworkbasedlanguagemod els[5,8]. 2.2NegativeSampling AnalternativetothehierarchicalsoftmaxisNoiseContras tiveEstimation(NCE),whichwasin- troducedbyGutmannandHyvarinen[4]andappliedtolanguag emodelingbyMnihandTeh[11]. NCEpositsthatagoodmodelshouldbeabletodifferentiated atafromnoisebymeansoflogistic regression.ThisissimilartohingelossusedbyColloberta ndWeston[2]whotrainedthemodels byrankingthedataabovenoise. WhileNCEcanbeshowntoapproximatelymaximizethelogprob abilityofthesoftmax,theSkip- grammodelisonlyconcernedwithlearninghigh-qualityvec torrepresentations,sowearefreeto simplifyNCEaslongasthevectorrepresentationsretainth eirquality.WedeÞneNegativesampling (NEG)bytheobjective log !(v!wO"vwI)+k%i=1Ewi$Pn(w)&log !("v!wi"vwI)'(4) 3 !183RSCI(7DL(&04-7D1-8& !"!#"#$%&'()*+,%-.()/+% 0b%7/%>'II%.=4G'Y/%6G*A'G'Y/%.+%G'3'G'Y/<%% .B+%.8Q/(4O/"(.9:%EB3(4.39% %h.+%3.>5%G'3'G'Y/% !%C+*,'/3:%,/9(/3: %%&/E+/9)/+%>':)%:+'O'*I%/A*G=I/5%6E+.G%7'D'=/,'*<% h'3,%*%I.(*I%G'3'GBG%.E%:)/%EB3(4.3%% 76=<s=Tt!=!v0?%>':)%,/+'O*4O/% 7M6=<sT =!ti=0@%%%%% '(,.L+D1-8&10&/,+%.(-3& !"!#"#$%&'()*+,%-.()/+% 0$%¥!7)':/8.*+,%69//%O',/.%'E%;.B{+/%3.:%'3%(I*99%|<% ¥!\)/%8*9'(%]/C.%='/(/% ¥!H9/EBI%8*9'(95% ¥!_E%'3%,.B8:5%>+':/%.B:%>':)%'3,'(/9% ¥!F)*'3%+BI/z%_E% >%s% 76*<%*3,% *%s% &6=<?%'@/@%;sE6C6A<<?%:)/35%  !=+.-&<45(& !"!#"#$%&'()*+,%-.()/+% 0j%¥!F)*'3%+BI/z%_E% >%s% 76*<%*3,% *%s% &6=<?%'@/@%;sE6C6A<<?%:)/35% ¥!-'G=I/%/A*G=I/5%%  H-3(,+7DL(&:=.3(C1+,%&"(88.1-T& !"!#"#$%&'()*+,%-.()/+% 0n%]/:{9%,/+'O/%C+*,'/3:%:.C/:)/+% h.+%.3/%/A*G=I/%>'3,.>%*3,%.3/%/A*G=I/%.B:9',/%>.+,5%  U)),1G.@+D1-89& 6"(3 &M&!"!#"#$%&'()*+,%-.()/+% 0i%¥!7':)%I*+C/%O.(*8BI*+'/9%:)'9%.8Q/(4O/%EB3(4.3%'9%3.:% 9(*I*8I/%*3,%>.BI,%:+*'3%:..%9I.>I;z% !%7);V %¥!_,/*5%*==+.A'G*:/%:)/%3.+G*I'Y*4.3%.+%% ¥!1/23/%3/C*4O/%=+/,'(4.3%:)*:%.3I;%9*G=I/9%*%E/>% >.+,9%:)*:%,.%3.:%*==/*+%'3%:)/%(.3:/A:% ¥!-'G'I*+%:.%E.(B9'3C%.3%G.9:I;%=.9'4O/%(.++/I*4.39% ¥!f.B%>'II%,/+'O/%*3,%'G=I/G/3:%:)'9%'3% ^9/: %#z% *.-(+,&<(5+D1-8=.)8&.-&?1,%#L(7& \)/9/%+/=+/9/3:*4.39%*+/% 126>'&009' *:%/3(.,'3C%,'G/39'.39%.E% 9'G'I*+':;z% ¥!l3*I.C'/9%:/943C%,'G/39'.39%.E%9'G'I*+':;%(*3%8/%9.IO/,%ZB':/% >/II%QB9:%8;%,.'3C%O/(:.+%9B8:+*(4.3%'3%:)/%/G8/,,'3C%9=*(/% -;3:*(4(*II;% ¥!=";;)2't%=";;)2+ '}%=-"6 't%=-"6+ '}%=7".%)> 't%=7".%)%2+ ''¥!-'G'I*+I;%E.+%O/+8%*3,%*,Q/(4O/%G.+=).I.C'(*I%E.+G9% -/G*34(*II;%6 -/G/O*I %0N#0%:*9D%0<% ¥!=+?%65't%=-)05?%#& '}%=-?"%6 't%=7*6#%5*62 ''¥!=$%#& 't%=."# '}%=@*22# 't%=A0."# ''!N% !14-3&C+8(%& L8&%.,(73&),(%.7D1-& !"!#"#$%&'()*+,%-.()/+% !#%LSA, HAL  (Lund & Burgess) , COALS  (Rohde et al) , "Hellinger-PCA  (Lebret  &  Collobert )%¥!Fast training %¥!EfÞcient usage of statistics %¥!Primarily used to capture word  similarity %¥!Disproportionate importance  given to large counts %¥!NNLM, HLBL, RNN, Skip- gram/CBOW,  (Bengio  et al; Collobert  & Weston; Huang et al;  Mnih  & Hinton;  Mikolov  et al; Mnih  &  Kavukcuoglu )%¥!Scales with corpus size %¥!InefÞcient usage of statistics %¥!Can capture complex patterns  "beyond word similarity  %¥!Generate improved performance  "on other tasks % !1@C.-.-/&3=(&C(83&10&C13=&?1,5%89& V51;( &!"!#"#$%&'()*+,%-.()/+% !0%o!h*9:%:+*'3'3C% o!-(*I*8I/%:.%)BC/%(.+=.+*% o!U..,%=/+E.+G*3(/%/O/3%>':)%9G*II%(.+=B9?%*3,%9G*II% O/(:.+9% % V51L(&,(84538& !"!#"#$%&'()*+,%-.()/+% !!%#@%E+.C9% 0@%:.*,% !@% I':.+'* %T@% I/=:.,*(:;I',*/ %b@% +*3* %$@%I'Y*+,% j@% /I/B:)/+.,*(:;IB9 %I':.+'*% I/=:.,*(:;I',*/% +*3*% /I/B:)/+.,*(:;IB9 %J/*+/9:%>.+,9%:. %%E+.C 5% D'3C %G*3 %>.G*3 %7.+,%l3*I.C'/9 %\/9:%E.+%I'3/*+%+/I*4.39)'=9?%/A*G'3/,%8;% W'D.I.O %/:%*I@%60N#T< %*58%55%(5V% G*3 %>.G*3 %L%N@0N%N@0N%R %L%N@$N%N@!N%R %D'3C %L%N@!N%N@jN%R %L%N@jN%N@nN%R %K%v%v%ZB//3 %ZB//3 %G*35>.G*3%55%D'3C5V %*58%55%(5V%  V51L(&;.84+5.W+D1-8& !"!#"#$%&'()*+,%-.()/+% !b% V51L(&;.84+5.W+D1-89&!1@)+-E&Q&!XS& !"!#"#$%&'()*+,%-.()/+% !$% V51L(&;.84+5.W+D1-89&"4)(,5+DL(8& !"!#"#$%&'()*+,%-.()/+% !j% L%%%%%%%%%%%%R% :1,%&(@C(%%.-/&@+3,.G& ¥!_3'4*I'Y/%G.9:%>.+,%O/(:.+9%.E%EB:B+/%G.,/I9%>':)%.B+%d=+/K :+*'3/,e%/G8/,,'3C%G*:+'A% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%~r~%%%B%%s%%%%%%%%% %%%%%%%%%%%%%%%S%%%%%%%%%%#%%%%%%%%**+,O*+D%%%%*%%%%%%%%*:%%%S% ¥!lI9.%(*II/,%*%I..DKB=%:*8I/% ¥!F.3(/=:B*II;%;.B%C/:%*%>.+,{9%O/(:.+%8;%I/X%GBI4=I;'3C%*% .3/K).:%O/(:.+% 2%6.E%I/3C:)%~r~<%8;% B5%%%%% =%s% B2' !n% U%L+-3+/(8&10&51?&%.@(-8.1-+5&?1,%&L(731,8& !i%!i%7)*:%'9%:)/%G*Q.+%8/3/2:%.E%,//=%I/*+3/,%>.+,%O/(:.+9V% l8'I':;%:.%*I9.%=+.=*C*:/% +-E&'3E.+G*4.3%'3:.%:)/G% O'*%3/B+*I%3/:>.+D9%63/A:%I/(:B+/<@% %C'-#%%%%%%%%%%%-0%%%%%%%%%%%%-!%%%%=#'''''''''=0%%%%%%%%%%%%%%%%=!'''''''''''%v#% '"#'''''''''"0'P(c|d,!)=e!!f(c,d)e!!f(!c,d)!c" U%L+-3+/(8&10&51?&%.@(-8.1-+5&?1,%&L(731,8& TN%¥!7.+,%O/(:.+9%>'II%E.+G%:)/%8*9'9%E.+%*II%9B89/ZB/3:% I/(:B+/9@% ¥!lII%.B+%9/G*34(%+/=+/9/3:*4.39%>'II%8/%O/(:.+9z% ¥!J/A:%I/(:B+/5% ¥!-.G/%G.+/%,/:*'I9%*8.B:%>.+,%O/(:.+9% ¥!^+/,'(:%I*8/I9%E.+%>.+,9%'3%(.3:/A:%E.+%9.IO'3C%I.:9%.E% ,'x/+/3:%:*9D9%
Word vectors	Lexical semantics	Machine learning	Word embeddings	Bag-of-words	Text classification
What is a 'Parse Tree'?	A Parse Tree is a computational technique where every word is attached to and categorized as a particular part of speech. This tree also reveals how sentences of a particular language can be constructed.	Which parts of speech assist in creating a prepositional phrase? 	Preposition and article and Noun	Preposition and article and verb|||Preposition and article and adverb	NLP Crash Course clip||youtube||N/A
Natural Language Processing (NLP)	Syntax	Branches of linguistics	Computer science	Part-of-speech (POS) tagging
What is the difference between Skinner's behaviorist theory and Chomsky's Universal Grammar theory of language acquisition?	According to Skinner, children acquire language by imitating sounds and making associations. Chomsky argues against this, saying that if the mechanism of language acquisition was dependent solely on language input, children would never acquire the tools needed to understand language. According to him, there exist innate, biological grammatical categories, such as a noun category and a verb category (Universal Grammar), which make it easier for children to develop language and for adults to process it.	Of the branches of linguistics, which one deals with how language is acquired and developed?	Psycholinguistics	Neurolinguistics|||Macrolinguistics|||Sociolinguistics	Language Acquisition Theory from Simply Psychology||document||N/A
Psycholinguistics	B.F. Skinner	Noam Chomsky	Chomsky's Universal Grammar	Language acquisition
What are Grice's Maxims?	Grice's Maxims describe unspoken rules for conversation, which are known altogether as "the cooperative principle." They are the maxims of quality, quantity, manner, and relevance (or 'relation'). The maxim of quality says, try to say things that are true and supported by evidence. The maxim of quantity says, try to give as much information as needed, and no more. The maxim of manner says try to be as clear, brief, and orderly as you can. The maxim of relevance, says try to say only things that are relevant. These rules are used not only by being followed; they are also used, very often, by breaking them, such as to create humor, or to imply unspoken messages. This is called conversational implicature.	Which of the following is not an example of violating one of Grice's maxims?	Person A: How are you? Person B: A little tired and anxious, but mostly okay.	Person A: How are you? Person B: Well, my neighbor didn't wake me up by coming home drunk last night.|||Person A: How are you? Person B: Did you know that there are more dwarf planets in our solar system than just Pluto?|||Person A: How are you? Person B: Fine. (said in an angry voice).	Grice's Maxims: "Do the Right Thing"||publication||Grice'sMaxims:\DotheRightThing" RobertE.Frederking CenterforMachineTranslation CarnegieMellonUniversity ref@cs.cmu.edu Abstract Grice'smaximsarehopelesslyvague,andinfactharm- ful,becausetheyformamisleadingtaxonomy.While hiscooperativeprinciplemaybeusefulatahighlevel oftheoreticalanalysis,ittooisvague,andshouldnot bedirectlyimplementedincomputationalnaturallan- guagesystems.Answersaresuggestedtoanumberof thissymposium'stopicsbasedonthisposition.Ex- amplesarepresentedtoshowthatthemaximsare toovagueandtoogeneral,andthattheyarenotre- allyusedbycomputationalsystemsthatclaimtobe basedonthem.Thehistoricaloriginsofthemaxims inKant'sphilosophyarerevealed.Acomparisonis madewithRelevanceTheory,whichseemstoprovide abetterapproachtothesamephenomena.Iconclude bysuggestingthatitmaybetooearlyinthehistoryof computationallinguisticstoexpecttosuchbroad principles. Introduction IwillargueinthispositionpaperthatGrice'smaxims arehopelesslyvague,andthatwhilehiscooperative principlemaybeusefulatahighleveloftheoretical analysis,itshouldnotbedirectlyimplementedincom- putationalnaturallanguagesystems. Fromthispointofview,thereareclearanswersto severalofthequestionsthissymposiumwilladdress:  Isthenotionofconversationalimplicature stilluseful?WhatroleifanydoGrice'smax- imsandCooperativePrinciplestillplayin computationalandformalapproaches? The notionofconversationalimplicature,andtheCoop- erativePrinciple,havebeenusefulandimportant tosomeresearchersinthinkingabouthowlanguage worksinrealuse.Buthoweverusefultheyarefor guidingaresearcher'sthinking,theyarenotusefulas anactualpartofanimplementation.Themaxims, ontheotherhand,playnousefulrolewhatsoeverin anycomputationalorformalapproaches,evenata theoreticallevel.Theyareinfactharmful,because theyformamisleadingtaxonomy.  ButisRelevanceawnotion? No. Liketheothermaxims,Grice'sRelevanceisabroad, generalstatementthatisclearlytrueatsomelevel, butisfartoovaguetobeuseddirectlyincompu- tationalsystems.Ibelieveitiscurrentlyanopen questionwhethersomeotherapproachtorelevance (suchasRelevanceTheory,discussedbelow)maybe amenabletoprecise  Whatdistinguishesconversationalimplica- turesfromotherdefeasibleinferencesindis- course(e.g.,defaultinferencesintextunder- standing)? Iwouldclaimthattheonlythingthat distinguishesconversationalimplicaturesasaclassis thefactthattheycanbeseenasexamplesofGrice's principles.Inotherwords,thecategoriesheuses havenopredictiveorexplanatorypower.Thisis not tosaythatcertainsubclasses(suchasscalar implicatures)donothaveusefuldistinguishingfea- tures;onlythattheGriceanlevelofdescriptionis misleading.  Mostmodelshavefocusedonsingleclassesof conversationalimplicature.Whatproblems wouldariseinintegratingthem? Itispointless toconsiderintegratingtclassesofconversa- tionalimplicature,basedonGrice'staxonomy.His taxonomyisnotofatlyconcretenatureto befruitfullyappliedtorealimplementations.There are tissuesinintegratingttypes ofinferencemechanismsinconversation,butGrice's categoriesareirrelevanttotheseissues. Inshort,IclaimthatGrice'sMaximsaresimilarto themaxim\DotheRightThing,"whichanycorrectly workingnaturallanguagesystemcanbesaidtoimple- ment.  Grice'smaximsconsideredharmful Amisleadingtaxonomy SeveralresearchershavetriedtoimplementGrice's maximsinsomefashion(forexample,[Gazdar79, Hirschberg85]).Thisisnotpossibletododirectly, duetothevaguenessofthemaxims,sotheytypi- callyhaveimplementedsomethingmorereasonable, andthenclaimeditwas\Gricean".Moreoften,re- searchershavetriedtouseGrice'smaximstodescribe particularsystemsorphenomena(forexample,[Dale andReiter95,Joshietal.84,Passonneau95]).Be- causethemaximshavetheformofataxonomy,they leadresearcherstothinkthatthemaximstaxonomize thespaceofconversationalimplicaturesinsomeuse- fulfashion.Butusingthemaximseveninthiswayis counter-productive,becausetheyaremuchtoovague, andoftenoverlapwhenappliedtoactualexamplesof conversationalimplicature.Theytendtoleadtocon- fusionmorethanenlightenment. Forexample,itispointlesstodiscuss(asLevinson does[Levinson83,p.109])whetheraparticularphe- nomenonsuchasironyisbasedontheofRele- vanceorQuality.Ironyisaphenomenonthatquite comfortablyintobothnotions.Itbothatonce, andperhapsManner,too.Thedesiretodecidewhich maximironyisbasedonthefalseimpressionthat thereissomekindoftbetweenim- plicaturesthatintoonecategoryandthosethat intheother. Themaximsnotonlydividediscoursephenomenaup badly;theyalsogroupthemtogetherbadly.Scalarand clausalquantityimplicatures[Gazdar79,Hirschberg 85]areagoodexample.Thesehavebothbeende- scribedassubtypesofQuantityimplicatures.But theredoesnotseemtometobeanygoodreasonto believethatthesearetwosubclassesofthesamephe- nomenon. Clausalimplicaturetypicallyoccurswhenanembed- dedpropositionisneithernordeniedbythe fullutterance.Sotheutteranceof\IfJohnseesme thenhewilltellMargaret"implicatesthatthespeaker doesnotknowwhetherJohnwillseehim.Thestan- dardexplanationofthisisthat,basedontheCooper- ativePrinciple,ifthespeakerknewwhetherthe clauseweretrueorfalse,heshouldhavesaidso. Scalarimplicature,ontheotherhand,isbasedon theexistanceofsetsoftermsthathavesomesalient partialorderingindegreeofinformativeness.Sothe utteranceof\Paulatesomeoftheeggs"implicates thatthespeakerdoesnotknowthatPaulateallofthe eggs.Again,thestandardexplanationofthisrelieson Quantity,thatthespeakershouldhavesaidsoifhe knew. Clearly,theseexplanationsaresimilarincharacter; unfortunately,aswewillseebelow,speakersoftenpro- videmoreorlessinformationthanisnecessary,sothe generalizationmadebythemaximisnotvalid. Theotherwaythesephenomenacouldberelated isifthedetailed,spformalmechanismsforhan- dlingthetwophenomenaaresimilar.Infact,the formalmechanismsproposedtohandlethem Hirschbergdescribesthemastphenomenabe- forehermechanismforscalarimplicature,and Gazdarstates[Gazdar79,p.59]thathismecha- nismforhandlingscalarimplicaturesdoesnotgener- ateclausalimplicatures,justifyinghisdevelopmentof aseparateformalmechanismforclausalimplicatures. Attheveryleast,norigorousconnectiontothemaxim isestablished,anditseemscleartomethattherere- allyisn'tone.So,whileeachphenomenonappearsto bewinitsownright,theredoesnotseem tobeanyclearsimilarityinthewaytheyareactually processed. Thereisasubtlebutimportantcluetothegen- esisofsuchanunhelpfultaxonomyinGrice'sorigi- nalarticle[Grice75].Gricesaysthathiscategories are\echoingKant"(p.45).Thisclearlyrefersto Kant'stheoryofcategories,whichdeclara- tivestatementsalongfourdimensions: Quantity , Qual- ity , Relation ,and Modality .Thismakesitclearwhy Grice'staxonomydoesnotthediscoursephenomena itwassupposedtodescribe;ithasbeenborrowedasa wholefromapre-linguistic,philosophical ofstatements!Taxonomiesfromonedomainsimply cannotbetransferredwholesaletoanotherandretain anyusefulness.Itisinterestingthatthisattribution hasneverbeenquoted,tomyknowledge. Tryingtomakeuseofamisleading taxonomy Despitetheirvagueness,themaximsareclearlytrue insomesense.This,coupledwiththeirvagueness, hasallowednumerousresearcherstoreadintothem allsortsofsptrueinterpretations,ratherthan treatingthemas maxims ,asGrice'snameforthem suggests 1 .Asintheexamplesabove,whenweexam- inewhatactuallyexistsinspsystemsthatare claimedtooneormoreofthemaxims,we muchmorespmechanismsthatapplytomuch morespphenomena,andonlybearaveryten- uousconnectiontothemaxims.Theclarityofthese individualphenomenaandrules,despiteanyremain- ingcontroversies,isinsharpcontrasttothehazeof confusionsurroundingthemaxims.Giventhis,and 1 AlthoughGriceapparentlydidintendforthemtobe appliedrigorously.  thefactthatresearchersoftenpointoutmajorprob- lemswiththemaxims,itistounderstandthe widespread,seeminglywillfulrefusaltorealizethatthe maximssimplyarenotcorrect. Asoneexampleofbeing\softonGrice",Hirschberg [Hirschberg85]Qualityverynarrowly,and indicatesthatQuantity,Manner,andRelevancecan- notreallybeprecisely.Yetshethengoeson towritelogicalformulaecontainingthemaxims,asif theywererigorouslyThebasicproblemis vagueness;Grice'smaximsareloadedwithtermsthat aresuchas\asinformativeasrequired". IfonewantstobekindtoGrice,thisallowsahuge amountofleewayforreinterpretation. AnotherexampleisLevinson'sdiscussion[Levinson 83]ofassymetric\and".Thefactthat\and"canbe usedtomean\andthen",andthatthisisnotalexical ambiguity,seemstometohavebeenclearlyestablished atthispoint.Butthisfacthardlymakesthe\Beor- derly"submaximofManneragenerallyusefulcompu- tationalrule.Thereareinfactcontextsinwhichitis quiteacceptabletodescribeeventsoutoforder.Ifone istellingastoryandsays\JohnsayshelikesMary,and Philwalksoutthedoor,"thereisaclearimplicationof sequentiality.However,ifonesaysinstead\Johnsays helikesMary,andPhilsayshelikesMary,"thereisno implicationofsequentiality.Thesequentialityseemsto beprovidedbycomplex(andcurrentlynotwellunder- stood)phenomenainvolvingrealworldknowledgeand sequencesoftenseandaspect,notbysomehigh-level principleoforderliness.Whatisneeded,hereandin general,isacarefulinvestigationofspphenom- ena,notageneralpronouncementofaprinciplethat issometimestrue. Asanevenbetterexampleofthetendencytoapolo- gizeforGrice,thereistheDaleandReiterdiscussionof thegenerationofreferringexpressions[DaleandReiter 95,section2.4].Theyattributethetendencyofspeak- erstogeneratetheshortestuniquereferringexpression tothemaximofQuantity.Theygiveatypicalexam- pleofobeyingQuantity:aspeakerwhosays\Lookat thepitbull"ratherthan\Lookatthedog"implicates thatthetypeofdogisimportant,perhapsbecause itismoredangerous.Unfortunately,asitturnsout, speakersdo not generateminimalreferringexpressions. Hereisasimilarexample(similaratthelevelofthe maxims)thatdoes not obeyQuantity:Supposethere isaroomcontainingonlyalligators.Englishspeakers wouldnormallyrefertothelargestoneas\thelargest alligator"ratherthan\thelargestanimal"(oreven better,\thelargestthing").Theydothissimplybe- cause\alligator"istheunmarkedlevelofdescription thatEnglishnormallyuses.Whydoesthisnotgen- erateaconversationalimplicature?Accordingtothe maximofQuantity,aspeakergeneratingclearlysuper- informationshouldcausethehearertoproduce implicatures,asinthepreviousexample.Thisphe- nomenonclearlyviolatesanydirectinterpretationof Quantity. ThetrulysurprisingthingisthatDaleandReiter discussthistotalfailureofQuantityinthepaper,and yetdonotdescribeitasafailure.Theyrefertothis asonetypeof\lexicalpreference"[DaleandReiter95, quoting[Reiter91]].Inaddition,insection3.2,they mentionthatspeakersgenerateotherredundantinfor- mationaswell.Accordingtothemaxims,thisshould alsocauseimplicatures;butitdoesnot(exceptinthose caseswhereitdoes...)Furthermore,thealgorithmde- scribedinthispaperdoesthe conventional thing,that is,whatpeopleseemtodo,accordingtopsychologi- calexperiments.Thisseemstometobeexactlythe rightapproachtodesigningnounphrasegeneratorsat thecurrenttime,butitisentirelynon-Gricean,despite thetitle,anddespitetheirclaimsthattheyareusing \moreprecise"versionsofhismaxims. ItseemstomethatDaleandReitershouldhavegone ontoconcludethatthemaximofQuantityissimply wrong,oratleastremoved\GriceanMaxims"from thetitle.Later,theysuggestthattheGriceanmaxims maybe\approximations"toageneralprincipleof\if aspeakeruttersanunexpectedutterance,thehearer maytrytoinferareasonforthespeaker'sfailureto usetheexpectedutterance"(section3.3).Butthis cannot beacaseofapproximationto,or\simpleinter- pretations"of,Grice'smaximofQuantity,sincetheir principledoesnotreferto Quantity atall.Whatthey areactuallysuggestingareimplicaturesbasedonvi- olating conventions ,asopposedtoimplicaturesbased onanyhigh-levelprinciple.Which,again,Ibelieveis probablycorrect,exceptthatthemaximsshouldbe leftoutofit. Previousargumentsalongtheselines Kiefer[Kiefer79]makessimilarargumentsagainst Grice'smaxims,aswellasadditionalargumentsthat seemwell-foundedtome.Unfortunately,thefactthat hecombinedthisgeneralattackwithspcriticisms ofGazdar'swork[Gazdar79]allowedGazdar'sreply [Gazdar80]tofocusnarrowlyonthetechnicallinguis- ticissuesofhisownwork,ratherthanthebroadcrit- icismofGrice'soriginaltheory.Thesamesituation occurswithCohen'sattack[Cohen71]andGazdar's replytoit[Gazdar79].Disturbingly,Levinson'sbook [Levinson83,p.122]givestheimpressionthatthese replieshavesuccessfullyrespondedtothegeneralat- tacksonGrice,whichisnotthecaseatall.Additional  clearcriticismsoftheGriceanapproachcanbefound inworksbySadock[Sadock78]andbyWilsonand Sperber[WilsonandSperber81,SperberandWilson 86,WilsonandSperber88]. Tryingtoclearthingsup TheCooperativePrincipleasanimplicit constraint Asindicatedabove,Grice'sCooperativePrincipleex- plainsmanyimplicaturesthatoccurindiscourse,but itisfartoobroad.Thatis,onecanviewlotsoftrue factsassubsumedbyit,butonecannotstartwith theCooperativePrincipleorthemaximsandproduce usefulinferences,becausetheyalsopredictthingsthat donotoccur,suchasthe\alligator"exampleabove. NotethatIamcriticizingtheGricean-levelmaxims here,andnotthemuchmorespsystemssuchas clausalorscalarimplicature. AhelpfulwaytothinkabouttheCooperativePrin- cipleisasan implicit constraint,asopposedtoan ex- plicit one.BythisImeanthattherearetruecon- straintsthatarenotexplicitlyimplementedbythesys- temstheydescribe,butwhichthesystemsimplicitly obey.Forexample,themoonobeysKepler'sLawsof PlantetaryMotion,butnoonebelievesthatthemoon thinksaboutKepler'sLawsanddecideshowtomove basedonthem 2 .Forcomputationallinguistics,explicit constraintsareconstraintsthatcanbedirectlyimple- mentedbyprogramsthatcorrectlytransducebetween languageandtheinformationconveyedbylanguage. Thereareimplicationswheneverpeopledosome- thingunusual.Butitappearstome,ashintedat above,thattherearespclassesof conventional- ized implicaturesthatpeopleuse.Notethatthisis tfrom conventionalimplicature inthatthese classesofimplicaturesarenotattachedtosplex- icalitems,butratheroccurincertainsituations.Asin the\pitbull"exampleabove,oneclassmightbewhen aspeakerfailstousethenormallexicalpreferencefor anobject.Otherpossibleclassesmightbeclausalim- plicaturesandthevariousscalarimplicatures. Manyandperhapsallofthespconvention- alizedimplicaturesthatpeopleusedoobeyGrice's broad,implicitconstraint.Butitispointlesstoput hisconstraintexplicitlyintoacomputationalsystemif italsopredictsimplicatureswheretheydonotoccur. Intheterminologyofconstraints,unlikeKepler'sLaws, theCooperativePrincipleisa loose constraint,which isexactlysayingthatitpermitssomethingsthatdo nothappen. 2 Somepeoplethinkthatthemoonreallydoescompute thelawofgravity,whichiswhyIrefertoKepler'sLaws here. Ifthisviewiscorrect,thenpartoflearningtouse aparticularlanguageislearningsprulesabout whentomakeanimplicatureandwhennotto,orpos- siblylearningtorecognizecategoriesofexpectedbe- havior,andmakinginferenceswhenexpectationsare violated.OnecanviewthesystemdescribedbyGreen andCarberry[GreenandCarberry94]asasystemof thistype,wheretheirprecompileddiscourseoperators wouldbetheendproductofthelearningofconven- tionalizedimplicatureusage. Thisraisesanintriguingpossibility:thattheCoop- erativePrinciplecouldbeusedaspartofalanguage learningsystem,toconstrainthespaceofpossibleim- plicatureslearned.Inordertobesuccessful,induc- tivelearningrequiresasmanyconstraintsandbiasesas possible.TheCooperativePrinciple,orpossiblyRel- evanceTheory(seebelow),couldbeusedasonesuch constraint.Thelearningsystem'shypotheseswouldbe constrainedbytheassumptionthattheotherspeaker's behaviorwascooperative(orrelevant). Furthertaxonomicconsiderations Ihavedescribedabovetheinternaltaxonomicprob- lemswithGrice'sscheme,duetoitsmaximsbeing vagueandoverlapping.Inaddition,thereisanex- ternaltaxonomicproblem,inthatitdoesnotseemto bepossibletodistinguishconversationalimplicature fromothernon-logical,non-conventional,constraint- basedinferences.BasedonGrice'sderivationofhis tests[Grice75],theyinfactoughttoapplytoany suchconstraints.Thatis,hederiveshistestsfromthe factthatconversationalimplicaturesarenotentail- ments,thattheyarenotbasedonthesurfaceforms ofutterances,thattheyarenotpartofconventional meaning,andthattheyarecontextdependent.Ifthis istrue,thentheyfundamentallycannotdistinguish conversationalimplicaturefromotherdefeasibleinfer- ences.ThismeansthatGriceanimplicaturesareonly distinguishablebythefactthattheycanbeseentobe derivedfromcooperativity,whichisnotveryinterest- ing.Amongothers,Sadock[Sadock78]haspreviously madesimilarpoints. Evenworse,WilsonandSperber[WilsonandSper- ber81]makeafairlycompellingargumentthatdis- ambiguationandreferenceresolutioncanalsobeseen asrelyingontheCooperativePrinciple.Tousetheir example,ifsomeonelisteningtoastudentplayingvio- linsays\Johnplayswell,"thenormalinterpretationis thatheplaystheviolinwell.Supposesomeonesaysin- stead\Johnplayswell{butnottheviolin."Itis culttoseehowtheinstancefrom\obeying themaxims"withregardtodisambiguation,orhow thesecondfromtheusualtypesof  andcancellation.ThustheCooperativePrincipleap- pearstobesobroadthatitevencoversphenomena thathavenothingtodowithwhatisnormallyunder- stoodasimplicature.Finally,evenLevinson[Levinson 83,p.132]saysthatspeakers\maywell"makeuseof inferencesbasedonanyconstraints. Givenallthis,whatisthecorrectwaytobuilda taxonomy?Forataxonomytobeusefulforourcom- putationalpurposes,itshouldbebasedonfunctional classesthatcorrespondtoexplicitlyusedinformation processingconstraintsandmechanisms.Thesecon- straintsmusthave operational : basedonsimpleprimitivesthatcanbeimplementedin hardware.Asanexampleofatleastabetterattempt, WilsonandSperber[WilsonandSperber81]divide pragmaticimplicaturesintotwosubclasses,directand indirect,basedonwhethernewassumptionsareneeded tointerpretanutteranceasrelatingtotheprevious conversation.Thisisamuchclearerapproachtothe versusdistinctionmadeby Grice. TheCooperativePrincipleversus RelevanceTheory Ihavedescribedabovehowconversationalimplicature istodistinguishfromotherinferences,and howGrice'smaximsseemtooverlapinconfusingways. Thatsaid,itdoesseemthattheCooperativePrinciple isrealinsomewaybeyondapossibleroleinlearning, andthatinferencesthatseemtobederivedfromitdo occurinlanguage. SperberandWilsonhaveproducedapromisingal- ternativeapproachtothiswholeareaintheirwork onRelevanceTheory(RT)[SperberandWilson86]. Theystartfromtheposition[WilsonandSperber81] thatRelevancedoesnotfollowfromtheCooperative Principle,oranyothersociologicalprinciple.Itjust arisesfromthenatureofcommunication:aspeaker demandsresourcesfromahearer,creatinganimpli- cationthatwhatthespeakerissayingisworthwhile forthehearertoattendto.Relevanceresultsfrom havingalargeenoughonthehearer'scognitive environmentwithasmallenoughprocessing Thecallforparticipationtothisworkshopmen- tionedthepossibilitythatRelevancewasthekey Griceanmaxim.Remarkablyenough,ifonere-reads Grice'sCooperativePrincipleinthiscontext,itseems tobeessentiallydescribingRelevance,inbothGrice's andSperberandWilson'ssenses: Makeyourconversationalcontributionsuchas isrequired,atthestageatwhichitoccurs,bythe acceptedpurposeordirectionofthetalkexchange inwhichyouareengaged. (AlthoughSperberandWilsonarguethattheirnotion ofrelevanceinnotimplyinganyagreementona commonpurpose,oranyknowledgeofacceptednorms [SperberandWilson86,p.161-163].) RTcanthusbeviewedasaclaimthatabetterver- sionofRelevanceisindeedtheonlymaxim.Whether ornotitstandsupasawhole,RTinanyeventseem tohaveamuchclearerofRelevance,havea consistent,clearlyworked-outtheory,andhaveavoided thesortsofproblemscausedbyGrice'smaxims.The maindrawbacktoRT(atleastinits1986form)is thatthecrucialconceptsofcognitiveofutter- ancesandprocessinginunderstandingutter- ancesbothbelongtoanunspdetailedcogni- tivetheory.Thisdependencyoncognitivemodelling isunavoidable,butuntilsomeexperimentsaredone combiningRTandasuitablecomputationalcognitive model,itishardtojudgethevalidityofthetheory. Intheirbook,SperberandWilsondemonstrateina numberofplacesacertainamountofnaiveteregarding computation,soIsuspectcomputationalimplementa- tionswillhavetocomefromotherresearchers.The onlyattemptatacomputationalimplementationof RelevanceTheorythathascometomyattentionasof thiswritingis[Poznanski92] 3 ,whichIhavenotyet obtainedacopyof. Conclusion:let'swaitawhile Ihavearguedthatwhilemuchgoodworkisself- describedas\Gricean",itbearsonlyalooseconnec- tiontohismaxims.Thisisnoaccident,sinceGrice's maximstaxonomizeconversationalimplicaturesinun- fruitfulways,andleadtoconfusionifonetriesto takethemseriously.AsIhaveindicated,mineisby nomeansthecriticismoftheGriceanapproach, butpreviousattackshaveapparentlybeendisregarded, withoutbeingrefuted. IfGrice'stheoryisunusable,whatshouldtakeits place?WhileIbelievethattheworkofSperberand Wilsoncouldformamuchmorepromisingbasisfor generalizationthanGrice'smaxims,Isuspectthatit maysimplybetooearlyinthehistoryofcomputa- tionallinguisticsforbroad,deeptheoriestobeformu- lated. Ifwelookatthehistoryofeveryone'sfavoritescience (physics),weseeaninterestingpattern.Keplerformu- latedhisLawsofPlantetaryMotionmorethan50years beforeNewtondevelopedtheuniversaltheoryofgrav- ity.Keplerdidnotunderstandgravity;hebelieved inexoticPythagoreantheoriesthathavesincebeen discredited.Similarly,whenMichelsonandMorleyex- perimentallydemonstratedthatthespeedoflightwas 3 ThankstoRobynCarstonforthereference.  constantinallframesofreference,theyhadnoexpla- nation.Theybelievedthatlighttravelledthroughthe ether.Einsteincamealongalmost20yearslaterwith thespecialtheoryofrelativitytoexplainwhatwas happening.Inbothcases(andmanyothers),correct, detailedmathematicaldescriptionsofspphenom- enaprecededtheformationofcorrectgeneraltheories. Similarly,Ibelieveweshouldcarefullyandcon- cretelydescribe,andcomputationallysolve,manyspe- phenomena,continuingalongthelinesofJoshi [Joshietal.84],DaleandReiter[DaleandReiter 95],Hirschberg[Hirschberg85],Green[GreenandCar- berry94],Passonneau[Passonneau95],and 87],butwithoutappeallingtoGrice.Only afterwehavealargebodyofwell-understoodcompu- tationaldiscoursesystemsshouldwetrytogeneralize. References L.Cohen1971.Thelogicalparticlesofnaturallan- guage.In PragmaticsofNaturalLanguage .Y.Bar- Hillel(ed.),Dordrecht:Reidel. R.DaleandE.Reiter1995.ComputationalInter- pretationsoftheGriceanMaximsinthegeneration ofreferringexpressions. AppliedAIntelligence Journal ,9.Toappear. G.Gazdar1979. Pragmatics:Implicature,Presuppo- sitionandLogicalForm. NewYork:AcademicPress. G.Gazdar1980.ReplytoKiefer. LinguisticaeInves- tigationes ,3,pp.375-377. N.GreenandS.Carberry1994.Ahybridreasoning modelforindirectanswers.In Proceedingsofthe32nd AnnualMeetingoftheACL ,LasCruces,NM. J.Hirschberg1985. ATheoryofScalarQuantityIm- plicature .PhDthesis,UniversityofPennsylvania. A.Joshi,B.Webber,R.Weischedel1984.Livingup toexpectations:computingexpertresponses.In Pro- ceedingsofAAAI-84 . F.Kiefer1979.Whatdoconversationalmaximsex- plain? LinguisticaeInvestigationes ,3,pp.57-74. S.Levinson1983. Pragmatics. CambridgeUniversity Press,Cambridge. R.Passonneau1995.IntegratingGriceanandAtten- tionalConstraints.In ProceedingsofIJCAI-95 . V.Poznanski1992.Arelevance-basedutterancepro- cessingsystem.UniversityofCambridgeComputer Laboratorytechnicalreportno.246. E.Reiter1991.Anewmodeloflexicalchoicefor nouns. ComputationalIntelligence ,7(4):240-251. R.1987.ScalarImplicatureandthe Given/NewDistinction.Unpublishedpaperfromthe 1987PennLinguisticsColloquium. J.Sadock1978.Ontestingforconversationalimplica- ture.In SyntaxandSemantics9:Pragmatics ,P.Cole (ed.),NewYork:AcademicPress. D.SperberandD.Wilson1986. Relevance:communi- cationandcognition ,HarvardUniversityPress. D.WilsonandD.Sperber1981.OnGrice'stheory ofconversation.In ConversationandDiscourse ,P. Werth(ed.),NewYork:St.Martin'sPress. D.WilsonandD.Sperber1988.Representationand relevance.In MentalRepresentations ,R.Kempson (ed.),CambridgeUniversityPress.
Grice's Maxims	Paul Grice	Pragmatics	Implicature	Cooperative Principle
Describe the general steps involved in creating a classifier using supervised learning, in NLTK.	The first step would be to gather 1,000s of tweets from 100s of Twitter users and classify those Twitter users according to their personalities. To do it properly, you would hire a psychologist to make the classifications. Then you would divide the labeled data into three sets – the initial training set, development set, and test set. Ideally, these sets would be the same size. Then you would write a program in Python to extract the features of each Twitter user’s collection of tweets; these features could include many things, such as the use of certain words, the lengths of words and sentences, the diversity of the vocabulary, or the numbers of emojis of various types. Then you would train an NLTK classifier on these labeled “featuresets” – just the ones belonging to the training set. After training, you would test that classifier on the featuresets of the development set, analyze its performance using certain functions (“methods”) of NLTK classifiers, and modify the features extracted in order to improve the performance. After this developmental stage, one would test the resulting classifier on the test set in order to ascertain its accuracy on new data.	Which of the following is least likely to be an obstacle for this project?	Difficulty getting consistent and meaningful-looking results from this method in general.	Difficulty obtaining enough data for free|||Difficulty getting the data annotated affordably and correctly|||Difficulty extracting the best features for the job	NONE
Supervised learning	Data analytics	Machine learning	Text classification	Classifiers
What system of code does the following Python expression exploit and what are its results? Describe what the expression finds in detail, and name the NLP task that it carries out.	This is a Regex expression. The results will be a list of all of the words in the text consisting of either purely alpha-numeric sequences, or with one hyphen, slash, or apostrophe inside the sequence, and separately, any quotation marks, or sequences of one or more dashes, periods, parentheses, or brackets, singly or in combination, or any items consisting of any non-white space character, optionally followed by a letter. It is an expression intended to word-tokenize the text.	Which of the following English items would be correctly tokenized by the expression above?	23-skiddoo	Mr. Magoo|||$20,000|||#ILoveNLP	Introduction to Regex||publication||ˇ ˆˇ˙     ˛  "  ' ˛    )$ ˛ * ˆ ˛ ,   ˛  $    ˛ˆ'     - ($ $ "ˆ$   % )  $  $  ˆ    $    ˛ˆ  ˛ˆ˛     ˇ0ˇ       ˚  ˘  ,  ˛ ˘˘˘˘ˆ˙  ˆ˙ ' / ˘      ˝ "    ˝ˆ    ˚(    ˆ( ˘$ ˜˘˚˜  ˜ ˆ     !( ˘  ˇ  "#    ,  ˛( "#(ˆ$   /$    1   %%˘! &&˘! '( )'  *+˘!$  ,, %%˘!,&&˘!,'(,)'"˙ #    ˛     /     ,
Regular expressions	Python	Tokenization	Context-free grammars
What is a probabilistic context free grammar (PCFG)?	A context free grammar (CFG) is a grammar consisting of "terminals" (words and phrases), non-terminals (phrasal categories), a starting symbol (such as "S" for "sentence") and "production rules," such as "S -> NP + VP" (a sentence can be a noun phrase and a verb phrase), "NP -> ADJP + NN" (a noun phrase can be an adjective phrase plus a noun" or "NN -> 'boy'."  It is context free, because the application of the rules does not depend on context. A probabilistic context free grammar (PCFG) is a CFG in which the rules, and therefore the parses produced by them have probabilities.  One derives the production rules and their probabilities from a corpus of parsed sentences, and one can use a PCFG as the grammar for a probabilistic parser.	Which of the following is NOT true of CFGs or PCFGs?	Automatically generated CFG's are boiled down to the fewest rules possible that produce the data.	You have not finished parsing until all non-terminals have been replaced by terminals.|||Natural languages are not the only systems for which you can write CFGs.|||The probability of a parse is the product of the probabilities of the rules use to construct it.	Probabilistic Context Grammars||document||N/A|||Context-Free Grammars webpage section||image||N/A
Context-free grammars	Probabilistic context-free grammars	Parsing	Natural Language Processing (NLP)
What is the corpus callosum?	The corpus callosum is a thick band of nerves and fibres that can be found between the two hemispheres. It allows for communication to take place between the hemispheres.	The corpus callosum is the largest collection of:	White matter	Grey matter|||Sensory neurons|||Linguistics fibres	Corpus Callosum clip||youtube||N/A
Neurolinguistics	Psycholinguistics	Neuroanatomy	Cerebral callosum	Cerebral hemispheres
Show that words for men and women in English represent a sexist ideology.	Some of the word pairs that could be used to prove this are: slut / gigolo, master / mistress, lord / lady, and bachelor / spinster. In most of these word pairs the female word is negative and defines women in terms of their sexual, economic, and legal dependence on men, while the male word is positive and defined in terms of social and economic power. While a "lady" is defined in terms of polite behavior, not power. There are many other such examples.	Which of the following does not definitively demonstrate sexism in English?	There is no special female form of the word "doctor," like "doctoress."	Traditionally, a person of unknown gender has been referred to as "man" or "he."|||There are more pejorative terms for women than for men, according to research.|||There are the titles Mrs / Miss versus only Mr for men.	Gender and the Dichotomic Representations in the Linguistic Imaginary||publication||Ioana BO TENARU Œ Gender and the Dichotomic Representations  in the Linguistic Imaginary     74  Sexism has known various approaches, being related  to the superior  position of one particular gender (that is masculin e) to the other one. For  instance, Suzanne Pharr attempted to define sexism,  pointing out that it is  connected with ﬁan enforced belief in male dominanc e and controlﬂ [Pharr,  1988: 8] that undermines the position of women in s ociety. This opinion is also  shared by Graddol and Swann, who argue that sexism  resides in the  ﬁdiscrimination against women or men because of the ir sex.ﬂ [Graddol &  Swann, 1989: 96]   This social reality is reflected also at the lingui stic level, leading to what  sociolinguists call  sexist languages . As it is reflected in the inquiries in the field,   a sexist language is a a language that shows favour itism towards one sex,  discriminating thus the other one. This opinion is  also shared by Gamble and  Gamble, who state that: ﬁSexist language empowers t he members of one sex at  the expense of members of the other sex, promoting  the continuance of status  differentials based on sex.ﬂ [Gamble & Gamble, 2015 : 65] The main tendency  of languages is to favour men to the great detrimen t of women, who are placed  on an inferior position. This second place occupied  by women™s speech is a  result of the manner in which they are perceived in  society, which assigns men  and women with stereotyped patterns of behaviour, p erceived by Frank and  Treichler as: ﬁlinguistic usage shapes and reinforc es selective cognitive  tendencies, usually those in conformity with widely  accepted cultural practices  and beliefs.ﬂ [Francine & Treichler, 1989: 9]   The bias towards men is definitely present in certa in languages, among  which English can also be spotted. Taking into acco unt this premise, the main  cases of sexism in English will be highlighted, giv ing suggestive examples that  endorse this reality of an unsymmetrical representa tion of men and women  within the language system.    English and its sexist features     At a lexical and syntactic level, researchers [Guim ei, 2010: 332-335]  have argued that the use of generic terms such as ﬁ manﬂ or of generic pronouns  such as ﬁheﬂ (and its forms in different cases: G Œ  his, D-Ac Œ him) to refer to  situations or aspects that regard both sexes (mascu line and feminine) is an  eloquent proof of English™s sexist tendency.  Example : Is  man  thinking about the consequences of global warming?  Œ  in this example, it is visible how the word ﬁmanﬂ r efers to humankind. Therefore,  women are no longer visible within the language, me n representing the norm.  Example : Every person must be aware of the present dangers  for the  environment and  he  should fight to avoid them. Œ this is a suggestive  example of  how the pronoun ﬁheﬂ is used in a context that conc ern both sexes. In terms of  gender, the ﬁpersonﬂ can be either a man or a woman , but, by using the  masculine third person pronoun as an anaphor, the p ossibility of a woman   Ioana BO TENARU Œ Gender and the Dichotomic Representations  in the Linguistic Imaginary     75  subject is excluded. The use of plural forms ﬁthey/ their/themﬂ has been  perceived as a solution for grammarians. Taking int o consideration this point of  view, the previous example would become:  Example : Every person must be aware of the present dangers  for the  environment and  they  should fight to avoid them.  What is more, sexism in English is emphasized throu gh the manner in  which derivation [Guimei, 2010: 332-335] functions.  In most cases, feminine  gender noun are formed by adding a suffix to the ma sculine gender noun. This  affiliation of women to men, the dependence of a ma sculine form in order to be  created has been interpreted by linguists as an obv ious pattern of sexism, as  Baron Dennis argues: ﬁThe masculine gender is the p rimary unmarked gender  (–) the use of an additional suffix to signal femal eness is seen as conveying the  message that women are deviant, abnormal and not im portant.ﬁ [Baron, 1986:  41] There are many examples of such cases in Englis h, the following list of  situations revealing only a small part of the entir e amount of such pairs:  actor Œ  actress ,  poet Œ poetess ,  waiter Œ waitress, prince Œ princess ,  steward Œ  stewardess ,  author Œ authoress ,  hero Œ heroine ,  bachelor Œ bachelorette ,  usher Œ  usherette  etc. Derivation does not underline only this depen dence on men, but it  often leads to placing women on an inferior positio n or to assigning the feminine  term a negative meaning. This would be the case of  pairs like:  governor Œ  governess, host Œ hostess  etc. Analyzing the first example, the pejorative l abel  assigned to women is obvious. While the masculine t erm ﬁgovernorﬂ refers to ﬁa  person in charge of a particular political unitﬂ [ Cambridge Advanced Learner™s  Dictionary , 2005: 553], its feminine equivalent is defined as  ﬁa woman who  lives with a family and teaches their children at h ome.ﬂ [ Cambridge Advanced  Learner™s Dictionary , 2005: 553]  It is not only in the case of derived forms that th e feminine equivalents  of masculine terms have negative connotations and t his tendency has been  explained by Romaine Suzanne in terms of status dis similarities in society:  ﬁBecause the word ‚woman™ does not share equal stat us with ‚man™ terms  referring to women have undergone a kind of semanti c downgrading or  pejoration.ﬂ [Romaine, 1999: 93] In this respect, w ord-pairs like  master Œ  mistress  are eloquent. The pejorative connotation assigned t o the feminine term  is explicit: while ‚master™ denotes ﬁa person who h as control over or  responsibility for someone or something, or who is  the most important or  influential person in a situation or organizationﬂ  [ Cambridge Advanced  Learner™s Dictionary , 2005: 780], a ﬁmistressﬂ, its feminine correspond ent is  definitely a pattern of derogation, being defined a s ﬁa woman who is having a  sexual relationship with a married man.ﬂ [ Cambridge Advanced Learner™s  Dictionary , 2005: 810] Another suggestive example is  bachelor-spinster , the  discrimination that lies behind being approached by  Romaine Suzanne, who  argues that: ﬁ ‚spinster™ and ‚bachelor™ both refer  to unmarried adults, but the  female terms has negative overtones to it (–) a spi nster is also unmarried but   Ioana BO TENARU Œ Gender and the Dichotomic Representations  in the Linguistic Imaginary     76  she is more than that: she is beyond the expecting  marrying age and therefore  seen as rejected and undesirable.ﬂ [Romaine, 1999:  92]   Furthermore, Turner and West identified ﬁthe wealth  of negative terms  for womenﬂ [West & Turner, 2010: 140] in English in  comparison with the  lower number of pejorative labels for men. Most of  them ( bitch, whore, chick,  etc.) have emerged due to their constant use by men  who place themselves on a  superior position, set in contrast with women. Henc e, the sexual objectification  of women which results from the use of the feminine  labels is explicit. There are  cases when even the pejorative labels assigned to m en introduce feminine terms  in their structure, affecting once again the image  of women, as Romaine  Suzanne claims: ﬁSome of the more common derogatory  terms applied to men,  such as bastard and son of a bitch, actually degrad e women in their role of  mothers.ﬂ [Romaine, 1994: 107]  In addition, the existence of ﬁmale-oriented terms  which denote titles or  positionsﬂ [Guimei, 2010: 332-335] is another sign  of sexism in English.  Linguists have argued that the existence of words l ike  businessman, chairman,  salesman, postman, policeman, fireman, craftsman, s pokesman, etc. are eloquent  proofs in this respect. The use of neutral form ins tead of these ones is considered  by linguists a solution for eliminating the bias to wards men. Consequently, the  use of terms like  business person, chairperson, salesperson, post wor ker, police  officer ,  firefighter, craftworker, speaker or spokesperson  etc. is taken into  account by Gamble and Gamble as a means of avoiding  the discrimination of  women: ﬁTo challenge such sexist practices, in lieu  of using man-linked words,  we are starting the transition to the use of gender -neutral terms.ﬂ [Gamble &  Gamble, 2015: 67]  Another argument invoked by linguists placed among  those which support  the idea of sexist language is  the stereotypical association of sexes with certain   fields of interest/occupations  [Guimei, 2010: 332-335], in spite of the fact that  the  terms which are used can denote both sexes. While h igher-status occupations such  as  lawyer, judge, engineer, doctor, surgeon, professor  tend to be assigned to male  figures, lower status positions are attached to wom en:  teacher, nurse, secretary,  babysitter  etc. Hence, stereotypical beliefs associate men wi th occupations and  positions which point out the idea of power, of dom inance, being assumed that each  sex is suitable just for certain types of occupatio n. Linguists have underlined that a  woman who accedes to the previously mentioned posit ions attributed to men will be  referred to as  woman lawyer, woman judge, woman engineer, woman do ctor,  woman surgeon, woman professor , being gender marked. Nevertheless, even this  tendency to add a gender marker so as to illustrate  that a woman is in a position of  power illustrates their discrimination in the lingu istic system: women are not  supposed to have access to high-status positions du e to their inferior condition. This  dissimilarity in terms of professions has its roots  in the manner in which men and  women are perceived in society. They are assigned c ertain stereotypical attitudes,  roles or responsibilities which determine their att achment to one or another   Ioana BO TENARU Œ Gender and the Dichotomic Representations  in the Linguistic Imaginary     77  particular type of profession. Being associated wit h the idea of power, men are  linked with high-status positions. On the other han d, women are associated with the  ideas of empathy, cooperation, support and patience  and this is the main reason why  they tend to by attached to professions that requir e such features. Once again,  language goes hand in hand with the social reality,  becoming a mirror of social  injustice and emphasizing the prejudice against wom en.  Another noticeable sexist pattern of English is rel ated to the manner in  which women and men are called (their titles) or  addressed  [Romaine, 1994: 108- 111]:  Mr. vs Mrs/Miss . Men are the ones who continue the name tradition  of a  family, while their wives are supposed to change th eir last name after marriage.  This is also a case of discrimination, because the  woman is defined by having  recourse to the man, to his last name:  Mrs Taylor, Mrs Smith  etc. By adopting the  last name of the husband, the subordinate position  of women has been  emphasized. Women reach an identity through marriag e, through the mediation of  a male figure. When a person is addressed  Mr Thompson , for instance, it means  that he is a man, an adult who has attained the sta tus of  Mr , but when a woman is  addressed  Mrs Thompson , her status of wife is brought to the surface.   As we managed to see in these situations identified  by linguists, sexism is  undoubtedly present in English at a lexical, syntac tic and semantic level.  Language becomes thus a proof of the social derogat ion of women and it certainly  does not represent women and men in an equal manner . The attempts to adapt the  existent male-oriented forms in order to avoid disc rimination do not prove to be  always successful, their results being perceived on ly at a formal level. The  discrimination of women is not annihilated because  it is part of what happens in  nowadays society. Language™s sexist tendencies are  just an expression of the  stereotyped society in which individuals live, wher e distinct types of approaching  men and women within the language and distinct type s of approaching their own  language emerge. In the subsequent section of this  paper, an inventory of the main  stereotypical linguistic patterns assigned to each  sex will be outlined.    Stereotypes of men and women in interaction     The approach of differences between the patterns of   linguistic behaviour  of men and women have dominated the inquiries of sc holars in the field, leading  to the emergence of stereotypes, which are defined  as: ﬁfixed idea that people  have about what someone or something is like, espec ially an idea that is wrong.ﬂ  [Cambridge Advanced Learner™s Dictionary , 2005: 1268] As it was highlighted in  the subchapter which presented the accounts on gend er differences, linguists  formulated opinions with regard to the dissimilarit ies between the speech of the  sexes. The stereotypical conversational styles iden tified by linguists are definitely  influenced by more complex issues than the innate d istinction between the sexes  and here should be included: the position on the so cial ladder of the participants in  an interaction (men or women), their profession, th eir age etc.
Gender linguistics	Stereotypes	Semantics
What does micro-linguistics consist of?	Micro-linguistics is comprised of phonetics, phonology, morphology, syntax, pragmatics, and semantics.	What is NOT true of micro-linguistics?	It studies the connection between language and culture.	It adopts a narrower view than macro-linguistics.|||It studies speech sounds.|||It is concerned with the study of meaning.	Branches of Linguistics presentation||slides||BRANCHES OF  LINGUISTICS  LINGUISTICS Linguistics is the scientific study of natural language . Someone who engages in this study is called a linguist .  BRANCHES OF LINGUISTICS  Language in general and language in particular can be studied from different points of view  The field of linguistics as a whole can be divided into several subfields according to the point of view that is adopted  FIRST DISTINCTION GENERAL LINGUISTICS  Studying language in  general  Supplies the concepts  and categories in terms  of which particular  languages are to be  analysed DESCRIPTIVE LINGUISTICS  Studying particular  languages  Provides the data  which confirm or refute  the propositions and  theories put forward in  general linguistics  SECOND DISTINCTION Diachronic (Historical)  Linguistics  Traces the historical  development of the  language and records  the changes that have  taken place in it between  successive points in time:   to historical  Of particular interest to  linguists throughout the  nineteenth century Synchronic Linguistics  Non - historical: presents  an account of the  language as it is at  some particular point in  time  THIRD DISTINCTION Theoretical Linguistics  Studies language and  languages with a view to  constructing a theory of  their structure and functions  and without regard to any  practical applications that  the investigation of  language and languages  might have  Goal: formulation of a  satisfactory theory of the  structure of language in  general Applied Linguistics  Application of the  concepts and findings  of linguistics to a variety  of practical tasks ,  including language  teaching  Concerned with both  the general and  descriptive branches of  the subject  FOURTH DISTINCTION Micro linguistics  Adopts the narrower  view  Concerned solely with  the structures of the  language system in  itself and for itself Macro linguistics  Adopts the broader view  Concerned with the way  languages are acquired,  stored in the brain and  used for various functions;  interdependence of  language and culture;  physiological and  psychological  mechanisms involved in  language behaviour  FOURTH DISCTINCTION (CONTD.) Micro Linguistics  Phonetics  Phonology  Morphology  Syntax  Semantics  Pragmatics Macro linguistics  Psycholinguistics  Sociolinguistics  Neurolinguistics  Discourse Analysis  Computational  Linguistics  Applied Linguistics   MICROLINGUISTICS  Phonetics is the scientific study of speech sounds . It studies how speech sounds are articulated, transmitted, and received .  Phonology is the study of how speech sounds function in a language, it studies the ways speech sounds are organized . It can be seen as the functional phonetics of a particular language .  Morphology is the study of the formation of words . It is a branch of linguistics which breaks words into morphemes . It can be considered as the grammar of words as syntax is the grammar of sentences .  MICROLINGUISTICS  Syntax deals with the combination of words into phrases, clauses and sentences . It is the grammar of sentence construction .  Semantics is a branch of linguistics which is concerned with the study of meaning in all its formal aspects . Words have several types of meaning  Pragmatics can be defined as the study of language in use in context .  MACROLINGUISTICS  Sociolinguistics studies the relations between language and society : how social factors influence the structure and use of language .  Psycholinguistics is the study of language and mind : the mental structures and processes which are involved in the acquisition, comprehension and production of language .  Neurolinguistics is the study of language processing and language representation in the brain . It typically studies the disturbances of language comprehension and production caused by the damage of certain areas of the brain .  MACROLINGUISTICS  Discourse analysis, or text linguistics is the study of the relationship between language and the contexts in which language is used . It deals with how sentences in spoken and written language form larger meaningful units .  Computational linguistics is an approach to linguistics which employs mathematical techniques, often with the help of a computer .  Applied linguistics is primarily concerned with the application of linguistic theories, methods and findings to the elucidation of language problems which have arisen in other areas of experience  MACROLINGUISTICS  Forensic linguistics, legal linguistics, or language and the law, is the application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure . It is a branch of applied linguistics .  There are principally three areas of application for linguists working in forensic contexts :  understanding language of the written law,  understanding language use in forensic and judicial processes, and  the provision of linguistic evidence
Branches of linguistics	Microlinguistics	Phonetics	Phonology	Morphology	Syntax	Pragmatics	Semantics
Why is it impossible to reconstruct the first language, or even prove that there was one?	It is impossible to reconstruct a first language because language is too old, records of language do not go back far enough, and languages change too much, too quickly to reconstruct them far back enough. To be specific, it is believed that fully developed human languages probably existed at least 50,000 years ago, when there is first a lot of evidence for culture, such as paintings, ritual graves, and other meaningful symbolic artifacts. And language might go back as far as 250,000 years when anatomically modern humans first existed. But records of language only go back a little more than 5,000 years to Egyptian, Sumerian, and Hittite. And languages change so fast that if they're not written down they can change completely within a few hundred or, at most about ten thousand, years.	Which of the following claims about language origins is most provable / disprovable?	The Indo-European languages originated near Turkey 6-8,000 years ago.	Language first developed to facilitate hunting during the stone age.|||Human beings had languages 50,000 years ago.|||All languages are descended from one mother language.	NONE
Language origins	Language reconstruction	Language evolution	Writing	Historical linguistics	Comparative linguistics	Indo-European languages
Imagine you are building a text classifier for tweets. What unique challenges may tweets pose and what are some strategies to deal with those challenges?	Tweets have a slightly different syntax than normal text. For example, #hashtags and @mentions are commonly used elements of tweets. While it may be important to keep the information that a text is included with a hashtag, case folding may be especially important to make sure #ThisIsAHashtag and #thisisahashtag are both recognized as the same feature. Regarding mentions, a specific user that is mentioned may be insignificant for a given task, but the fact that a mention is included may be important, and thus maybe a feature should be added for generic mentions, where "@user1232" and "@user999" are both considered as the same feature.	True or false: Since tweets can only contain up to 240 characters of text, this limited data makes building text classifiers around tweets impossible, or near impossible, for most tasks.	False	True	Sentiment Analysis Presentation||slides||!"#$%&"#$';.Q"#%R)$%.#'B,,6", ¥Y)#-%=&$"%Q@KG%#*3%RKG%+#;H04 ¥@=&$$);%+#;H L04%Z*#+)'<%"#'"%$#6'[ ¥J#4&$#-&9#$&5*%Z4;)');2)%85;% =5;3'%&*%#--%:#4'[ ¥E"5*)%*0+1);'<%3#$)' ¥e+5$&:5*' ¥h')80-%:53)F ¥J";&'$54");%E5$$'%')*$&+)*$%$5H)*&9); ¥P;)*3#*%ImJ5**5;%$=&$$);%$5H)*&9); BN[<>]? # optional hat/brow [:;=8]                      # eyes[\-o\*\']?  # optional nose [\)\]\(\[dDpP /\:\}\{@\|\\]  # mouth       |                            #### reverse orientation [\)\]\(\[dDpP /\:\}\{@\|\\]  # mouth [\-o\*\']?  # optional nose [:;=8]                      #  eyes[<>]? # optional hat/brow E5$$'%)+5$&:5*'|||Speech and Language Processing: Regular Expressions, Text Normalization, Edit Distance||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 2 RegularExpressions,Text Normalization,EditDistance User:Iamunhappy. ELIZA:DOYOUTHINKCOMINGHEREWILLHELPYOUNOTTOBEUNHAPPY User:Ineedsomehelp,thatmuchseemscertain. ELIZA:WHATWOULDITMEANTOYOUIFYOUGOTSOMEHELP User:PerhapsIcouldlearntogetalongwithmymother. ELIZA:TELLMEMOREABOUTYOURFAMILY User:Mymothertakescareofme. ELIZA:WHOELSEINYOUFAMILYTAKESCAREOFYOU User:Myfather. ELIZA:YOURFATHER User:Youarelikemyfatherinsomeways. Weizenbaum(1966) Thedialogueaboveisfrom ELIZA ,anearlynaturallanguageprocessingsystem ELIZA thatcouldcarryonalimitedconversationwithauserbyimitatingtheresponsesof aRogerianpsychotherapist (Weizenbaum,1966) .ELIZAisasurprisinglysimple programthatusespatternmatchingtorecognizephraseslikeﬁIneedXﬂandtranslate themintosuitableoutputslikeﬁWhatwoulditmeantoyouifyougotX?ﬂ.This simpletechniquesucceedsinthisdomainbecauseELIZAdoesn'tactuallyneedto know anythingtomimicaRogerianpsychotherapist.AsWeizenbaumnotes,thisis oneofthefewdialoguegenreswherelistenerscanactasiftheyknownothingofthe world.Eliza'smimicryofhumanconversationwasremarkablysuccessful:many peoplewhointeractedwithELIZAcametobelievethatitreally understood them andtheirproblems,manycontinuedtobelieveinELIZA'sabilitiesevenafterthe program'soperationwasexplainedtothem (Weizenbaum,1976) ,andeventoday such chatbots areafundiversion. chatbots Ofcoursemodernconversationalagentsaremuchmorethanadiversion;they cananswerquestions,bookorrestaurants,functionsforwhichtheyrely onamuchmoresophisticatedunderstandingoftheuser'sintent,aswewillseein Chapter26.Nonetheless,thesimplepattern-basedmethodsthatpoweredELIZA andotherchatbotsplayacrucialroleinnaturallanguageprocessing. We'llbeginwiththemostimportanttoolfordescribingtextpatterns:the regular expression .Regularexpressionscanbeusedtospecifystringswemightwantto extractfromadocument,fromtransformingﬁIneedXﬂinElizaabove,to stringslike $ 199 or $ 24.99 forextractingtablesofpricesfromadocument. We'llthenturntoasetoftaskscollectivelycalled textnormalization ,inwhich text normalization regularexpressionsplayanimportantpart.Normalizingtextmeansconvertingit toamoreconvenient,standardform.Forexample,mostofwhatwearegoingto dowithlanguagereliesonseparatingoutor tokenizing wordsfromrunning text,thetaskof tokenization .Englishwordsareoftenseparatedfromeachother tokenization bywhitespace,butwhitespaceisnotalwayssuf NewYork and rock'n'roll aresometimestreatedaslargewordsdespitethefactthattheycontainspaces,while sometimeswe'llneedtoseparate I'm intothetwowords I and am .Forprocessing tweetsortextswe'llneedtotokenize emoticons like :) or hashtags like #nlproc .  2 C HAPTER 2  R EGULAR E XPRESSIONS ,T EXT N ORMALIZATION ,E DIT D ISTANCE Somelanguages,likeJapanese,don'thavespacesbetweenwords,sowordtokeniza- tionbecomesmoredif Anotherpartoftextnormalizationis lemmatization ,thetaskofdetermining lemmatization thattwowordshavethesameroot,despitetheirsurfacedifferences.Forexample, thewords sang , sung ,and sings areformsoftheverb sing .Theword sing isthe common lemma ofthesewords,anda lemmatizer mapsfromalloftheseto sing . Lemmatizationisessentialforprocessingmorphologicallycomplexlanguageslike Arabic. Stemming referstoasimplerversionoflemmatizationinwhichwemainly stemming juststripsufesfromtheendoftheword.Textnormalizationalsoincludes sen- tencesegmentation :breakingupatextintoindividualsentences,usingcueslike sentence segmentation periodsorexclamationpoints. Finally,we'llneedtocomparewordsandotherstrings.We'llintroduceametric called editdistance thatmeasureshowsimilartwostringsarebasedonthenumber ofedits(insertions,deletions,substitutions)ittakestochangeonestringintothe other.Editdistanceisanalgorithmwithapplicationsthroughoutlanguageprocess- ing,fromspellingcorrectiontospeechrecognitiontocoreferenceresolution. 2.1RegularExpressions Oneoftheunsungsuccessesinstandardizationincomputersciencehasbeenthe regularexpression ( RE ),alanguageforspecifyingtextsearchstrings.Thisprac- regular expression ticallanguageisusedineverycomputerlanguage,wordprocessor,andtextpro- cessingtoolsliketheUnixtoolsgreporEmacs.Formally,aregularexpressionis analgebraicnotationforcharacterizingasetofstrings.Theyareparticularlyuse- fulforsearchingintexts,whenwehavea pattern tosearchforanda corpus of corpus textstosearchthrough.Aregularexpressionsearchfunctionwillsearchthroughthe corpus,returningalltextsthatmatchthepattern.Thecorpuscanbeasingledocu- mentoracollection.Forexample,theUnixcommand-linetool grep takesaregular expressionandreturnseverylineoftheinputdocumentthatmatchestheexpression. Asearchcanbedesignedtoreturneverymatchonaline,iftherearemorethan one,orjustthematch.Inthefollowingexampleswegenerallyunderlinethe exactpartofthepatternthatmatchestheregularexpressionandshowonlythe match.We'llshowregularexpressionsdelimitedbyslashesbutnotethatslashesare not partoftheregularexpressions. Regularexpressionscomeinmanyvariants.We'llbedescribing extendedregu- larexpressions ;differentregularexpressionparsersmayonlyrecognizesubsetsof these,ortreatsomeexpressionsslightlydifferently.Usinganonlineregularexpres- siontesterisahandywaytotestoutyourexpressionsandexplorethesevariations. 2.1.1BasicRegularExpressionPatterns Thesimplestkindofregularexpressionisasequenceofsimplecharacters.Tosearch for woodchuck ,wetype /woodchuck/ .Theexpression /Buttercup/ matchesany stringcontainingthesubstring Buttercup ; grep withthatexpressionwouldreturnthe line I'mcalledlittleButtercup .Thesearchstringcanconsistofasinglecharacter (like /!/ )orasequenceofcharacters(like /urgl/ ). Regularexpressionsare casesensitive ;lowercase /s/ isdistinctfromupper case /S/ ( /s/ matchesalowercase s butnotanuppercase S ).Thismeansthat thepattern /woodchucks/ willnotmatchthestring Woodchucks .Wecansolvethis
Text classification	Case folding	Normalization	Natural Language Processing (NLP)
What is the difference between surface and deep structure?	Surface structure in language consists of the words and sounds a speaker produces and a listener perceives (or writer and reader). It is the form that is heard or seen. Ex. "My car was broken into last night." We could say "Someone broke into my car last night", whereby the surface structure changes (from passive to active voice), but the information remains the same on the level of deep structure - the sentence's basic meaning.	Which grammar type explains changes in surface structure and corresponding changes in deep structure?	Transformational grammar	Reference grammar|||Performance grammar|||Generative grammar	NONE
Surface structure	Deep structure	Grammar	Syntax	Generative grammar
What is CALL (computer-assisted language learning) and why is it used?	These are computer programs to help people learn a foreign language. They are used when the learners don't have access to a human teacher and/or settings in which the language is spoken naturally.	What does a CALL program need to be successful?	AV technology combining features to correct spelling and read text	Audio-visual input|||Artificial intelligence|||An integrated dictionary with pronunciation	Computer-Assisted Audiovisual Language Learning clip||youtube||N/A
Computer-assisted language learning	Language acquisition	Foreign languages	Language learning technologies
What is a morpheme?	Morphemes are the smallest units of a language with consistent meanings. They may be words, or parts of words, for example, there are three morphemes in "re-educat-ion." Like, "educate," the forms of morphemes may change a little in context. The tricky part is that not every syllable with a meaning is a morpheme in every word: in "person," "son" and "per" are not morphemes because they do not have those meanings; the word "person" is one morpheme.  Morphemes that can stand alone, like "bug" and "person" are "free morphemes." Morphemes that cannot, like "-ness" and "re-" are "bound morphemes." While free morphemes are always words, bound morphemes may be affixes (prefixes or suffixes, inflectional or derivational) or bound stems (also called roots or bases), such as the "-scribe" in prescribe, inscribe, and describe.	Which of the following shows "undependable" broken into its smallest morphemes?	un-de-pend-able	un-depend-able|||un-de-pend-a-ble|||unde-pend-able	"What are morphemes?" from Rochester Institute of Technology website||document||N/A
Morphology	Word structures	Morphemes	Affixes	Morphemic analysis
What is the difference between a stem word and a base word?	A stem is any morpheme, to which you can add a syntactical affix, while a base is any form to which you can add affixes of any kind. For example, "please" is a base - you can add a suffix to it to form "pleasure", but "displease" is a stem because it's a verb.	How are stems different from roots and bases?	A stem always has affixation, roots and bases can exist on their own.	A stem can be the same as a root but a base cannot.|||Roots and bases are the same.|||Stems are bases with affixation, while roots are always on their own.	NONE
Stem words	Base words	Morphemes	Morphology	Affixes
Examine the training set provided in Figure A. Build a Naive Bayes classifier to classify the following sentence: "it was not funny rather it was terrible"	The word "rather" does not appear in training, so it can be ignored as an unknown word. Prepending "NOT_" to tokens to deal with negation gives us the following sentence: "it was NOT_funny it was terrible". Since "NOT_funny" does not occur in training, it can be ignored as an unknown word. That means we need to determine the probabilities of "it", "was", and "terrible". Using add-one (Laplace) smoothing and a standard (non-binary) Naive Bayes, our probabilities are as follows:  $$ \hat P(“it”| +) = {{count(“it”, +) + 1} \over {\sum _{w \in V} count(w, +) + |V|}} = {3 + 1 \over 25 + 27} = 0.0769  $$ $$ \hat P(“it”| -) = {{count(“it”, -) + 1} \over {\sum _{w \in V} count(w, -) + |V|}} = {2 + 1 \over 13 + 27} = 0.0750  $$ $$ \hat P(“was”| +) = {{count(“was”, +) + 1} \over {\sum _{w \in V} count(w, +) + |V|}} = {2 + 1 \over 25 + 27} = 0.0385  $$ $$ \hat P(“was”| -) = {{count(“was”, -) + 1} \over {\sum _{w \in V} count(w, -) + |V|}} = {2 + 1 \over 13 + 27} = 0.0750  $$ $$ \hat P(“terrible”| +) = {{count(“terrible”, +) + 1} \over {\sum _{w \in V} count(w, +) + |V|}} = {0 + 1 \over 25 + 27} = 0.0192  $$ $$ \hat P(“terrible”| -) = {{count(“terrible”, -) + 1} \over {\sum _{w \in V} count(w, -) + |V|}} = {1 + 1 \over 13 + 27} = 0.05  $$  The P(c) for our two classes are as follows:  $$ P(+) = {4 \over 6} = {2 \over 3} = 0.667 $$ $$ P(-) = {2 \over 6} = {1 \over 3} = 0.333 $$  The label class is decided as follows for S = "it was it was terrible" (the initial sentence after removing unknown tokens):  $$ P(+)P(S|+) = 0.667 \times 0.0769 \times 0.0385 \times 0.0769 \times 0.0385 \times 0.0192 = 0.00000011 $$ $$ P(-)P(S|-) = 0.333 \times 0.0750 \times 0.0750 \times 0.0750 \times 0.0750 \times 0.05 = 0.00000053 $$  As such, the predicted label for the given sentence with this model would be -.  NOTE: Your solution may be different depending on your specific parameters of your Naive Bayes.	Given the sentence S = "it was too good" and the training data provided in Figure A, what are some important considerations for a Naive Bayes model?	Both A and B	Converting words to their negated equivalent when building our model.|||Applying add-one (Laplace) smoothing, or another smoothing technique, when building our model.|||Excluding "it" as a common word when building our model.|||Both A and C	Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||4.3  W ORKEDEXAMPLE 7 4.3Workedexample Let'swalkthroughanexampleoftrainingandtestingnaiveBayeswithadd-one smoothing.We'lluseasentimentanalysisdomainwiththetwoclassespositive (+)andnegative(-),andtakethefollowingminiaturetrainingandtestdocuments fromactualmoviereviews. Cat Documents Training - justplainboring - entirelypredictableandlacksenergy - nosurprisesandveryfewlaughs + verypowerful + themostfunofthesummer Test ? predictablewithnofun Theprior P ( c ) forthetwoclassesiscomputedviaEq. 4.11 as N c N doc : P (  )= 3 5 P (+)= 2 5 Theword with doesn'toccurinthetrainingset,sowedropitcompletely(as mentionedabove,wedon'tuseunknownwordmodelsfornaiveBayes).Thelike- lihoodsfromthetrainingsetfortheremainingthreewordsﬁpredictableﬂ,ﬁnoﬂ,and ﬁfunﬂ,areasfollows,fromEq. 4.14 (computingtheprobabilitiesfortheremainder ofthewordsinthetrainingsetisleftasanexerciseforthereader): P ( ﬁpredictableﬂ j )= 1 + 1 14 + 20 P ( ﬁpredictableﬂ j +)= 0 + 1 9 + 20 P ( ﬁnoﬂ j )= 1 + 1 14 + 20 P ( ﬁnoﬂ j +)= 0 + 1 9 + 20 P ( ﬁfunﬂ j )= 0 + 1 14 + 20 P ( ﬁfunﬂ j +)= 1 + 1 9 + 20 ForthetestsentenceS=ﬁpredictablewithnofunﬂ,afterremovingtheword`with', thechosenclass,viaEq. 4.9 ,isthereforecomputedasfollows: P (  ) P ( S j )= 3 5  2  2  1 34 3 = 6 : 1  10  5 P (+) P ( S j +)= 2 5  1  1  2 29 3 = 3 : 2  10  5 Themodelthuspredictstheclass negative forthetestsentence. 4.4OptimizingforSentimentAnalysis WhilestandardnaiveBayestextcanworkwellforsentimentanalysis, somesmallchangesaregenerallyemployedthatimproveperformance. First,forsentimenttionandanumberofothertexttasks, whetherawordoccursornotseemstomattermorethanitsfrequency.Thusit oftenimprovesperformancetoclipthewordcountsineachdocumentat1(see theendofthechapterforpointerstotheseresults).Thisvariantiscalled binary
Add-one (Laplace)	Naive Bayes	Smoothing	Text classification	Natural Language Processing (NLP)	Sentiment analysis
What is the difference between theoretical and applied linguistics?	Theoretical linguists study a language to develop theories of its structure and functions. They are not concerned with practical applications. Applied linguistics deals with application of findings and concepts to practical tasks.	What is an example of theoretical linguistics?	Syntax / parse trees	Language teaching|||Search engines|||Forensic science	Branches of Linguistics presentation||slides||THIRD DISTINCTION Theoretical Linguistics  Studies language and  languages with a view to  constructing a theory of  their structure and functions  and without regard to any  practical applications that  the investigation of  language and languages  might have  Goal: formulation of a  satisfactory theory of the  structure of language in  general Applied Linguistics  Application of the  concepts and findings  of linguistics to a variety  of practical tasks ,  including language  teaching  Concerned with both  the general and  descriptive branches of  the subject|||Human Language Sentences - Basic Parse Trees, X-Bar Theory & Ambiguity||youtube||N/A
Syntax trees	Branches of linguistics	Theoretical linguistics	Applied linguistics
What is overfitting and what is underfitting?	Overfitting and underfitting are terms used to describe problems with a machine learning model occurring while training the model. Overfitting means the model was trained too similarly to the training set and will not generalize as well to new points. Underfitting is the opposite, where the model was trained too generally, and is also too general to new points. Overfitted models generally performs really well against a training set, but less well against a testing set. Underfitted models perform poorly against a training set and poorly against a testing set. An example of underfitting is applying a linear function to fit a curvy dataset. An example of overfitting is applying a logistical function to a linear dataset. Figure A shows a good visualization.	How can you tell that a model may be overfitting its training data?	Both B and C.	You can't.|||Plotting a graph and examining.|||Seeing if the model performs significantly better on its training data opposed to its testing data.|||None of the above.	Overfitting, Underfitting, and Model Capacity||youtube||N/A
Overfitting	Underfitting	Machine learning	Machine learning (ML) overview	Supervised learning	Logistic regression
Is applied linguistics part of diachronic or synchronic linguistics?	It is part of synchronic linguistics, which studies language as is at a particular point in time. Applied linguistics looks at practical implications like language teaching and translation based on the current state of the language. Diachronic linguistics studies the development of language over time.	Which of the following definitions ISN'T valid for applied linguistics?	A branch of linguistics that establishes the structure and function of modern language.	A branch of linguistics offering practical insight into how communication and language contribute to human interaction.|||A branch of linguistics taking language theories as the basis to identify challenging issues or problems involving language in different contexts.|||A branch of linguistics analyzing language problems to draw conclusions and insight useful for people in those contexts.	"What is Applied Linguistics?" presentation from University Utara Malaysia||slides||What is Applied Linguistics 1  2 Awang Had Salleh School of Graduate Studies University Utara Malaysia, Malaysia mostafa.shalaby 1970 @gmail.com 11 / 2 / 2018 9 / 3 / 2018 IN GOD WE TRUST Moustafa Mohammad Shalabi MA. Applied Linguistics PhD. Scholar Corpus Linguistics  What is applied Linguistics   It is commonly known as the branch of linguistics concerned with practical applications of language studies, for example language teaching, translation, and speech therapy . 3  The term 'applied linguistics' refers to a broad range of activities which involve solving some language - related problem or addressing some language - related concern .  What is applied Linguistics  4 It appears as though applied linguistics, at least in North America, was first officially recognized as an independent course at the University of Michigan in 1946 . In those early days, the term was used both in the United States and in Great Britain to refer to applying a so - called 'scientific approach' to teaching foreign languages, including English for non - native speakers .  What is applied Linguistics  5 Early work to improve the quality of foreign language teaching by Professors Charles Fries (University of Michigan) and Robert Lado ( University of Michigan, then Georgetown University) helped to bring definition to the field as did the 1948 publication of a new journal, Language Learning : A Quarterly Journal of Applied Linguistics  What is applied Linguistics  6 Professor Anne Burns  Professor  of  TESOL, BA  (Hons), Diploma in  Adult TESOL, PhD,  MEd  Applied linguistics is notoriously hard to define. What sets it apart  from other areas of linguistics? How has it evolved over the years?  What do applied linguists do?   We asked ten leading and up - and - coming academics to give   us  their answer to the question:   Below  are their responses.  Take a look at  them  As an applied linguist,  primarily interested in offering people practical and illuminating insights into how language and communication contribute fundamentally to interaction between people .  What is applied Linguistics  7  Of course, several commentators have offered definitions of applied linguistics in recent decades , including Crystal ( 1980 : 20 ) , Richards et al , ( 1985 : 29 ) , Brumfit ( 1995 : 27 ) and Rampton ( 1997 : 11 ) .  For me, applied linguistics means taking language and language theories as the basis from which to elucidate how communication is actually carried out in real life, to identify problematic or challenging issues involving language in many different contexts, and to analyse them in order to draw out practical insights and implications that are useful for the people in those contexts .  What is applied Linguistics  8  Professor Richards' many successful publications include the Interchange series, Approaches and Methods in Language Teaching, and Curriculum Development in Language Teaching . Jack C. Richards  Professor Jack C. Richards is an internationally            recognized authority on English - language  acquisition, teacher  training, and materials design.   A well - known lecturer and consultant, he has taught at universities in the United States, China, Singapore, New Zealand, Canada, Indonesia, and Brazil .  What is applied Linguistics  9  More seriously, looking back at the term  applied linguistics  , it first emerged as an attempt to provide a theoretical basis for the activities of language teaching (witness Pit Corder  s book on the subject from 1973 ) .  A wit once described an applied linguist as  someone with a degree in linguistics who was  unable to get a job in a linguistics department.  Later, it became an umbrella term for a variety of  disciplines  which  focus on language issues in such fields as law,  speech  pathology , language planning, and forensic science.   What is applied Linguistics  10  Some years ago, many graduate programs in language teaching were labelled as programs in applied linguistics .  In the meantime, language teaching has evolved its own theoretical foundations, and these include second language acquisition, teacher cognition, pedagogical grammar, and so on, and there is a declining interest in viewing   as having any relevance to language teaching .  What is applied Linguistics  11  Today they are generally called programs in TESOL . Many specialists in language teaching, such as myself, don  t call themselves  applied linguists  . We are what we are  specialists in language teaching, and we don  t see that adding the label  applied linguistics  to our field adds any further understanding to what we do .    is of  course, something they need to  decide for themselves.  What is applied Linguistics  12  BA (Hons) Philosophy; MA Applied Linguistics;  PhD Applied Linguistics; Cambridge DELTA Dr Philip Durrant  Applied linguistics is any attempt to work with language in a critical and reflective way, with some ultimate practical goal in mind .  This includes(amongst other things ) : deliberately trying to learn (or teach) a foreign language or to develop your ability in your native language ; overcoming a language impairment ; translating from one language to another ; editing a piece of writing in a linguistically thoughtful way .  It also includes  doing any  research or developing any ideas or  tools which aim to help  people do  these sorts of things.  What is applied Linguistics  13  Professor of  Psycholinguistics,  University of Nottingham Zoltán Dörnyei    (AL) is one of several academic disciplines focusing on how language is acquired and used in the modern world .  It is a somewhat eclectic field that accommodates diverse theoretical approaches, and its interdisciplinary scope includes linguistic, psychological and educational topics . Although the field  s original focus was the study of foreign/second languages, this has been extended to cover first language issues, and nowadays many scholars would consider sociolinguistics and pragmatics to be part of the AL rubric .  Recently , AL conferences and journals have reflected the growing influence of psychology - based approaches, which in turn is a reflection of the increasing prevalence of cognitive (neuro)science in the study of human mental functions .  What is applied Linguistics  14  In my discipline (I am a Germanist), applied linguistics is perceived almost exclusively as research into the teaching and learning of the foreign - language , often resulting in the production of teaching materials . Professor Wini Davies Reader in German, Aberystwyth University  However, a broader definition (e . g . Dick Hudson  see references and below) sees applied linguistics as concerned with providing theoretical and empirical foundations for investigating and solving language - related problems in the   .  This definition would be relevant to some of my research  interests; for example, the problems facing speakers of non - standard dialects at schools in Germany.  What is applied Linguistics  15 Emeritus  Professor of Linguistics, University College London  Applied linguistics (AL) provides the theoretical  and descriptive foundations for the investigation and solution of language - related problems, especially those of language education (first - language, second - language and foreign language teaching and learning), but also problems of translation and interpretation, lexicography, forensic linguistics and (perhaps) clinical linguistics  Professor Richard Hudson  16  The main distinguishing characteristic of AL is its concern with professional activities whose aim is to solve  real -  language - based problems, which means that research touches on a particularly wide range of issues - psychological, pedagogical, social, political and economic as well as linguistic . As a consequence, AL research tends to be interdisciplinary . What is applied Linguistics   17  It is generally agreed that in spite of its name AL is not simply the  application  of research done in linguistics . On the one hand, AL has to look beyond linguistics for relevant research and theory, so AL research often involves the synthesis of research from a variety of disciplines, including linguistics . On the other hand, AL has been responsible for the development of original research in a number of areas of linguistics - e . g . bilingualism, literacy, genre .  Beyond this agreement, there is at least as much disagreement within AL as within linguistics about fundamental issues of theory and method, which leads (among other things ) to differences of opinion about the relationships between the two disciplines . What is applied Linguistics   18 What is applied Linguistics  Professor Andy Kirkpatrick  Professor, School of Humanities,  Languages Science Griffith  University  One way I can answer this broad question is by considering the Applied Linguistic issues that currently interest me, namely how languages interact and what differences we might expect when the languages concerned are not related to each other  19 What is applied Linguistics   For example, the Hong Kong language policy seeks to develop people who are trilingual in Cantonese, Putonghua and English . What specific linguistic difficulties will such learners face and how can we help them overcome them? What does it mean to be multilingual? Can we describe a multilingual model from which we could derive useful linguistic benchmarks for the language classroom?  20 What is applied Linguistics  Dr. Dawn Knight  Research Associate, University of Nottingham  Applied linguistics draws on a range of disciplines, including linguistics . In consequence, applied linguistics has applications in several areas of language study, including language learning and teaching, the psychology of language processing, discourse analysis, stylistics, corpus analysis, literacy studies and language planning and policies .  Applied linguistics is a discipline which explores the relations between theory and practice in language with particular reference to issues of language use .  It embraces contexts in which people use and learn languages and is a platform for systematically addressing problems involving the use of language and communication in real - world situations .  21 What is applied Linguistics   Applied linguistics is a broadly interdisciplinary field concerned with promoting our understanding of the role language plays in human life . At its centre are theoretical and empirical investigations of real - world issues in which language plays a leading role . Applied linguistics focuses on the relationship between theory and practice, using the insights gained from the theory - practice interface for solving language - related problems in a principled way . Juliane House  Professor of Foreign Language Teaching, Universität Hamburg  22 What is applied Linguistics   Applied linguistics is not  linguistics applied  , because it deals with many more issues than purely linguistic ones, and because disciplines such as psychology, sociology, ethnography, anthropology, educational research , communication and media studies also inform applied linguistic research . The result is a broad spectrum of themes in applied linguistics such as first, second and foreign language learning and teaching, bilingualism and multilingualism, discourse analysis, translation and interpreting, language policy and language planning, research methodology , language testing, stylistics, literature, rhetoric, literacy and other areas in which language - related decisions need to be taken .  23 What is applied Linguistics  Susan Hunston  Head of Department of English, University of Birmingham  The real - world concerns include language learning and teaching but also other issues such as professional communication, literacies, translation practices, language and legal or health issues, and many more . Applied linguistics is practically - oriented, but it is also theory driven and interdisciplinary . Models of how languages are learned and stored, for example, are  applied linguistics  , as are descriptions of individual language varieties that prioritise actual and contextualised language use .  One answer to this question is that it is the study of language in order to address real - world concerns . Another is that it is the study of language, and language - related topics, in specified situations .   References  Corder, P. ( 1973 )  Introducing Applied Linguistics,  Harmondsworth:  Penguin.  Crystal, D. ( 1980 )  A First Dictionary of Linguistics and Phonetics ,  London: André Deutsch.  Brumfit, C. ( 1995 )   Teacher professionalism and research  , in: Cook,  G. & Seidlhofer, B. (eds.) ( 1995 )  Principles and Practice in Applied Linguistics , Oxford: Oxford  University Press, pp 27 - 42 .  Hudson, R.  Applied Linguistics , available online at:  http://www.phon.ucl.ac.uk/home/dick/AL.html  Rampton, B. ( 1997 )   Retuning in applied linguistics?  ,  International  Journal of Applied Linguistics,  7 ( 1 ):  3 - 25 .  Richards, J.C., Platt, J. & Weber, H. ( 1985 )  Longman Dictionary of  Applied Linguistics,  London: Longman. 24
Applied linguistics	Synchronic linguistics	Branches of linguistics
Figure A is a hand-drawn panel used from which famous linguistics experiment? What were the findings from this experiment?	The figure is from the "wug" test, a 1958 experiment executed by psycholinguist Jean Berko Gleason. This experiment tested a child's comprehension of basic grammar, including understanding the different plural allomorphs, verb conjugation, and the possessive, by asking them to fill in the blanks of sentences involving fabricated words (such as "wug"). The major finding of this experiment was that even very young children are able to internalize aspects of the linguistic system and generalize grammar rules to apply to new words in different contexts.	How do the majority of children answer the question, "What would you call a very tiny wug?"	Baby wug	Wuggy|||Wuggette	"Jean Berko Gleason and Wugs"||youtube||N/A|||Clip: What is the wug test?||youtube||N/A
Wug test
What are some critiques of Chomsky's Universal Grammar Theory?	Chomsky argues humans are innately predisposed to acquire language because of a language acquisition faculty humankind shares. One popular argument against this theory is that grammar rules vary from language to language – for example, we add an auxiliary verb to make a question in English, but not in Italian, word order has one set of rules for English and another for German, etc.	Which of the following terminology is not related to Chomsky's UG theory?	Language learning motivation	Generative grammar|||The 'innateness hypothesis'|||Subjacency	Linguistics and Language Learning clip||youtube||N/A
Noam Chomsky	Chomsky's Universal Grammar	Language acquisition
How does a Logistic Regression learn its weights \(w\) and bias \(b\)?	After selecting features (either based on linguistic intuitions or using unsupervised learning techniques such as principal component analysis), we must train our Logistic Regression model to learn the optimal weights \(w\) and \(b\) to make sure our predictions for each training example \( \hat y \) are as close as possible to the true value \(y\). First, we should initialize our weights to either 0 or random values close to 0. This will allow us to make initial predictions. From here, as we iterate through our training examples, we will apply a cost function that measures how accurate our prediction \( \hat y \) is to the actual value \( y \). A common loss function used for Logistic Regression problems is cross-entropy loss. After this function returns, we will use an optimization function, such as gradient descent, to update our weights to minimize our loss function. Typically, we will continue to iterate through our training examples until we have hit a pre-set number of epochs (for instance, iterate through each training example 30 times to update our weights and bias).	Which of the following determines how "right" our prediction is?	A and B	Cost (or loss) function|||Cross-entropy loss function|||Optimization algorithm|||Gradient descent	Speech and Language Processing: Logistic Regression||publication||6 C HAPTER 5  L OGISTIC R EGRESSION Formanytasks(especiallywhenfeaturevaluescanreferencewords) we'llneedlargenumbersoffeatures.Oftenthesearecreatedautomaticallyvia fea- turetemplates ,abstractoffeatures.Forexampleabigramtemplate feature templates forperioddisambiguationmightcreateafeatureforeverypairofwordsthatoccurs beforeaperiodinthetrainingset.Thusthefeaturespaceissparse,sinceweonly havetocreateafeatureifthatn-gramexistsinthatpositioninthetrainingset.The featureisgenerallycreatedasahashfromthestringdescriptions.Auserdescription ofafeatureas,ﬁbigram(Americanbreakfast)ﬂishashedintoauniqueinteger i that becomesthefeaturenumber f i . Inordertoavoidtheextensivehumaneffortoffeaturedesign,recentresearchin NLPhasfocusedon representationlearning :waystolearnfeaturesautomatically inanunsupervisedwayfromtheinput.We'llintroducemethodsforrepresentation learninginChapter6andChapter7. Choosinga Logisticregressionhasanumberofadvantagesovernaive Bayes.NaiveBayeshasoverlystrongconditionalindependenceassumptions.Con- sidertwofeatureswhicharestronglycorrelated;infact,imaginethatwejustaddthe samefeature f 1 twice.NaiveBayeswilltreatbothcopiesof f 1 asiftheyweresep- arate,multiplyingthembothin,overestimatingtheevidence.Bycontrast,logistic regressionismuchmorerobusttocorrelatedfeatures;iftwofeatures f 1 and f 2 are perfectlycorrelated,regressionwillsimplyassignpartoftheweightto w 1 andpart to w 2 .Thuswhentherearemanycorrelatedfeatures,logisticregressionwillassign amoreaccurateprobabilitythannaiveBayes.Sologisticregressiongenerallyworks betteronlargerdocumentsordatasetsandisacommondefault. Despitethelessaccurateprobabilities,naiveBayesstilloftenmakesthecorrect decision.Furthermore,naiveBayescanworkextremelywell(some- timesevenbetterthanlogisticregression)onverysmalldatasets (NgandJordan, 2002) orshortdocuments (WangandManning,2012) .Furthermore,naiveBayesis easytoimplementandveryfasttotrain(there'snooptimizationstep).Soit'sstilla reasonableapproachtouseinsomesituations. 5.2LearninginLogisticRegression Howaretheparametersofthemodel,theweights w andbias b ,learned?Logistic regressionisaninstanceofsupervisedcationinwhichweknowthecorrect label y (either0or1)foreachobservation x .WhatthesystemproducesviaEq. 5.5 is‹ y ,thesystem'sestimateofthetrue y .Wewanttolearnparameters(meaning w and b )thatmake‹ y foreachtrainingobservationascloseaspossibletothetrue y . Thisrequires2componentsthatweforeshadowedintheintroductiontothe chapter.Theisametricforhowclosethecurrentlabel(‹ y )istothetruegold label y .Ratherthanmeasuresimilarity,weusuallytalkabouttheoppositeofthis: the distance betweenthesystemoutputandthegoldoutput,andwecallthisdistance the loss functionorthe costfunction .Inthenextsectionwe'llintroducetheloss loss functionthatiscommonlyusedforlogisticregressionandalsoforneuralnetworks, the cross-entropyloss . Thesecondthingweneedisanoptimizationalgorithmforiterativelyupdating theweightssoastominimizethislossfunction.Thestandardalgorithmforthisis gradientdescent ;we'llintroducethe stochasticgradientdescent algorithminthe followingsection.  5.3  T HECROSS - ENTROPYLOSSFUNCTION 7 5.3Thecross-entropylossfunction Weneedalossfunctionthatexpresses,foranobservation x ,howclosethe output(‹ y = s ( w  x + b ) )istothecorrectoutput( y ,whichis0or1).We'llcallthis: L ( ‹ y ; y )= Howmuch‹ y differsfromthetrue y (5.7) Wedothisviaalossfunctionthatprefersthecorrectclasslabelsofthetrain- ingexamplestobe morelikely .Thisiscalled conditionalmaximumlikelihood estimation :wechoosetheparameters w ; b that maximizethelogprobabilityof thetrue y labelsinthetrainingdata giventheobservations x .Theresultingloss functionisthenegativeloglikelihoodloss,generallycalledthe cross-entropyloss . cropy loss Let'sderivethislossfunction,appliedtoasingleobservation x .We'dliketo learnweightsthatmaximizetheprobabilityofthecorrectlabel p ( y j x ) .Sincethere areonlytwodiscreteoutcomes(1or0),thisisaBernoullidistribution,andwecan expresstheprobability p ( y j x ) thatourproducesforoneobservationas thefollowing(keepinginmindthatify=1,Eq. 5.8 esto‹ y ;ify=0,Eq. 5.8 to1  ‹ y ): p ( y j x )= ‹ y y ( 1  ‹ y ) 1  y (5.8) Nowwetakethelogofbothsides.Thiswillturnouttobehandymathematically, anddoesn'thurtus;whatevervaluesmaximizeaprobabilitywillalsomaximizethe logoftheprobability: log p ( y j x )= log  ‹ y y ( 1  ‹ y ) 1  y  = y log‹ y +( 1  y ) log ( 1  ‹ y ) (5.9) Eq. 5.9 describesaloglikelihoodthatshouldbemaximized.Inordertoturnthis intolossfunction(somethingthatweneedtominimize),we'lljustthesignon Eq. 5.9 .Theresultisthecross-entropyloss L CE : L CE ( ‹ y ; y )=  log p ( y j x )=  [ y log‹ y +( 1  y ) log ( 1  ‹ y )] (5.10) Finally,wecanplugintheof‹ y = s ( w  x + b ) : L CE ( w ; b )=  [ y log s ( w  x + b )+( 1  y ) log ( 1  s ( w  x + b ))] (5.11) Let'sseeifthislossfunctiondoestherightthingforourexamplefromFig. 5.2 .We wantthelosstobesmallerifthemodel'sestimateisclosetocorrect,andbiggerif themodelisconfused.Solet'ssupposethecorrectgoldlabelforthesentiment exampleinFig. 5.2 ispositive,i.e., y = 1.Inthiscaseourmodelisdoingwell,since fromEq. 5.6 itindeedgavetheexampleahigherprobabilityofbeingpositive(.69) thannegative(.31).Ifweplug s ( w  x + b )= : 69and y = 1intoEq. 5.11 ,theright sideoftheequationdropsout,leadingtothefollowingloss: L CE ( w ; b )=  [ y log s ( w  x + b )+( 1  y ) log ( 1  s ( w  x + b ))] =  [ log s ( w  x + b )] =  log ( : 69 ) = : 37  8 C HAPTER 5  L OGISTIC R EGRESSION Bycontrast,let'spretendinsteadthattheexampleinFig. 5.2 wasactuallynegative, i.e. y = 0(perhapsthereviewerwentontosayﬁButbottomline,themovieis terrible!Ibegyounottoseeit!ﬂ).Inthiscaseourmodelisconfusedandwe'dwant thelosstobehigher.Nowifweplug y = 0and1  s ( w  x + b )= : 31fromEq. 5.6 intoEq. 5.11 ,theleftsideoftheequationdropsout: L CE ( w ; b )=  [ y log s ( w  x + b )+( 1  y ) log ( 1  s ( w  x + b ))] =  [ log ( 1  s ( w  x + b ))] =  log ( : 31 ) = 1 : 17 Sureenough,thelossforthe(.37)islessthanthelossforthesecond (1.17). Whydoesminimizingthisnegativelogprobabilitydowhatwewant?Aper- fectwouldassignprobability1tothecorrectoutcome(y=1ory=0)and probability0totheincorrectoutcome.Thatmeansthehigher‹ y (thecloseritisto 1),thebetterthethelower‹ y is(thecloseritisto0),theworsetheclas- .Thenegativelogofthisprobabilityisaconvenientlossmetricsinceitgoes from0(negativelogof1,noloss)to(negativelogof0,loss).This lossfunctionalsoensuresthatastheprobabilityofthecorrectanswerismaximized, theprobabilityoftheincorrectanswerisminimized;sincethetwosumtoone,any increaseintheprobabilityofthecorrectansweriscomingattheexpenseofthein- correctanswer.It'scalledthecross-entropyloss,becauseEq. 5.9 isalsotheformula forthe cross-entropy betweenthetrueprobabilitydistribution y andourestimated distribution‹ y . Nowweknowwhatwewanttominimize;inthenextsection,we'llseehowto theminimum. 5.4GradientDescent Ourgoalwithgradientdescentistotheoptimalweights:minimizetheloss functionwe'veforthemodel.InEq. 5.12 below,we'llexplicitlyrepresent thefactthatthelossfunction L isparameterizedbytheweights,whichwe'llreferto inmachinelearningingeneralas q (inthecaseoflogisticregression q = w ; b ): ‹ q = argmin q 1 m m X i = 1 L CE ( y ( i ) ; x ( i ) ; q ) (5.12) Howshallwetheminimumofthis(orany)lossfunction?Gradientdescent isamethodthataminimumofafunctionbyoutinwhichdirection (inthespaceoftheparameters q )thefunction'sslopeisrisingthemoststeeply, andmovingintheoppositedirection.Theintuitionisthatifyouarehikingina canyonandtryingtodescendmostquicklydowntotheriveratthebottom,youmight lookaroundyourself360degrees,thedirectionwherethegroundisslopingthe steepest,andwalkdownhillinthatdirection. Forlogisticregression,thislossfunctionisconveniently convex .Aconvexfunc- convex tionhasjustoneminimum;therearenolocalminimatogetstuckin,sogradient descentstartingfromanypointisguaranteedtotheminimum.(Bycontrast,  5.4  G RADIENT D ESCENT 9 thelossformulti-layerneuralnetworksisnon-convex,andgradientdescentmay getstuckinlocalminimaforneuralnetworktrainingandnevertheglobalopti- mum.) Althoughthealgorithm(andtheconceptofgradient)aredesignedfordirection vectors ,let'sconsideravisualizationofthecasewheretheparameterofour systemisjustasinglescalar w ,showninFig. 5.3 . Givenarandominitializationof w atsomevalue w 1 ,andassumingtheloss function L happenedtohavetheshapeinFig. 5.3 ,weneedthealgorithmtotellus whetheratthenextiterationweshouldmoveleft(making w 2 smallerthan w 1 )or right(making w 2 biggerthan w 1 )toreachtheminimum. Figure5.3 Thestepiniterativelytheminimumofthislossfunction,bymoving w inthereversedirectionfromtheslopeofthefunction.Sincetheslopeisnegative,weneed tomove w inapositivedirection,totheright.Heresuperscriptsareusedforlearningsteps, so w 1 meanstheinitialvalueof w (whichis0), w 2 atthesecondstep,andsoon. Thegradientdescentalgorithmanswersthisquestionbythe gradient gradient ofthelossfunctionatthecurrentpointandmovingintheoppositedirection.The gradientofafunctionofmanyvariablesisavectorpointinginthedirectionofthe greatestincreaseinafunction.Thegradientisamulti-variablegeneralizationofthe slope,soforafunctionofonevariableliketheoneinFig. 5.3 ,wecaninformally thinkofthegradientastheslope.ThedottedlineinFig. 5.3 showstheslopeofthis hypotheticallossfunctionatpoint w = w 1 .Youcanseethattheslopeofthisdotted lineisnegative.Thustotheminimum,gradientdescenttellsustogointhe oppositedirection:moving w inapositivedirection. Themagnitudeoftheamounttomoveingradientdescentisthevalueoftheslope d dw f ( x ; w ) weightedbya learningrate h .Ahigher(faster)learningratemeansthat learningrate weshouldmove w moreoneachstep.Thechangewemakeinourparameteristhe learningratetimesthegradient(ortheslope,inoursingle-variableexample): w t + 1 = w t  h d dw f ( x ; w ) (5.13) Nowlet'sextendtheintuitionfromafunctionofonescalarvariable w tomany variables,becausewedon'tjustwanttomoveleftorright,wewanttoknowwhere intheN-dimensionalspace(ofthe N parametersthatmakeup q )weshouldmove. The gradient isjustsuchavector;itexpressesthedirectionalcomponentsofthe sharpestslopealongeachofthose N dimensions.Ifwe'rejustimaginingtwoweight dimensions(sayforoneweight w andonebias b ),thegradientmightbeavectorwith twoorthogonalcomponents,eachofwhichtellsushowmuchthegroundslopesin the w dimensionandinthe b dimension.Fig. 5.4 showsavisualization:  10 C HAPTER 5  L OGISTIC R EGRESSION Figure5.4 Visualizationofthegradientvectorintwodimensions w and b . Inanactuallogisticregression,theparametervector w ismuchlongerthan1or 2,sincetheinputfeaturevector x canbequitelong,andweneedaweight w i for each x i .Foreachdimension/variable w i in w (plusthebias b ),thegradientwillhave acomponentthattellsustheslopewithrespecttothatvariable.Essentiallywe're asking:ﬁHowmuchwouldasmallchangeinthatvariable w i thetotalloss function L ?ﬂ Ineachdimension w i ,weexpresstheslopeasapartialderivative ¶ ¶ w i oftheloss function.Thegradientisthenasavectorofthesepartials.We'llrepresent‹ y as f ( x ; q ) tomakethedependenceon q moreobvious: Ñ q L ( f ( x ; q ) ; y ))= 2 6 6 6 6 4 ¶ ¶ w 1 L ( f ( x ; q ) ; y ) ¶ ¶ w 2 L ( f ( x ; q ) ; y ) . . . ¶ ¶ w n L ( f ( x ; q ) ; y ) 3 7 7 7 7 5 (5.14) Theequationforupdating q basedonthegradientisthus q t + 1 = q t  h Ñ L ( f ( x ; q ) ; y ) (5.15) 5.4.1TheGradientforLogisticRegression Inordertoupdate q ,weneedaforthegradient Ñ L ( f ( x ; q ) ; y ) .Recallthat forlogisticregression,thecross-entropylossfunctionis: L CE ( w ; b )=  [ y log s ( w  x + b )+( 1  y ) log ( 1  s ( w  x + b ))] (5.16) Itturnsoutthatthederivativeofthisfunctionforoneobservationvector x is Eq. 5.17 (theinterestedreadercanseeSection 5.8 forthederivationofthisequation): ¶ L CE ( w ; b ) ¶ w j =[ s ( w  x + b )  y ] x j (5.17) NoteinEq. 5.17 thatthegradientwithrespecttoasingleweight w j representsa veryintuitivevalue:thedifferencebetweenthetrue y andourestimated‹ y = s ( w  x + b ) forthatobservation,multipliedbythecorrespondinginputvalue x j .  5.4  G RADIENT D ESCENT 11 5.4.2TheStochasticGradientDescentAlgorithm Stochasticgradientdescentisanonlinealgorithmthatminimizesthelossfunction bycomputingitsgradientaftereachtrainingexample,andnudging q intheright direction(theoppositedirectionofthegradient).Fig. 5.5 showsthealgorithm. function S TOCHASTIC G RADIENT D ESCENT ( L () , f () , x , y ) returns q #where:Listhelossfunction #fisafunctionparameterizedby q #xisthesetoftraininginputs x ( 1 ) ; x ( 2 ) ;:::; x ( n ) #yisthesetoftrainingoutputs(labels) y ( 1 ) ; y ( 2 ) ;:::; y ( n ) q   0 repeat tildone#seecaption Foreachtrainingtuple ( x ( i ) ; y ( i ) ) (inrandomorder) 1.Optional(forreporting):#Howarewedoingonthistuple? Compute‹ y ( i ) = f ( x ( i ) ; q ) #Whatisourestimatedoutput‹ y ? Computetheloss L ( ‹ y ( i ) ; y ( i ) ) #Howfaroffis‹ y ( i ) ) fromthetrueoutput y ( i ) ? 2. g   Ñ q L ( f ( x ( i ) ; q ) ; y ( i ) ) #Howshouldwemove q tomaximizeloss? 3. q   q  h g #Gotheotherwayinstead return q Figure5.5 Thestochasticgradientdescentalgorithm.Step1(computingtheloss)isused toreporthowwellwearedoingonthecurrenttuple.Thealgorithmcanterminatewhenit converges(orwhenthegradient < e ),orwhenprogresshalts(forexamplewhentheloss startsgoinguponaheld-outset). Thelearningrate h isaparameterthatmustbeadjusted.Ifit'stoohigh,the learnerwilltakestepsthataretoolarge,overshootingtheminimumofthelossfunc- tion.Ifit'stoolow,thelearnerwilltakestepsthataretoosmall,andtaketoolongto gettotheminimum.Itiscommontobeginthelearningrateatahighervalue,and thenslowlydecreaseit,sothatitisafunctionoftheiteration k oftraining;youwill sometimesseethenotation h k tomeanthevalueofthelearningrateatiteration k . 5.4.3Workingthroughanexample Let'swalkthoughasinglestepofthegradientdescentalgorithm.We'lluseasim- versionoftheexampleinFig. 5.2 asitseesasingleobservation x ,whose correctvalueis y = 1(thisisapositivereview),andwithonlytwofeatures: x 1 = 3(countofpositivelexiconwords) x 2 = 2(countofnegativelexiconwords) Let'sassumetheinitialweightsandbiasin q 0 areallsetto0,andtheinitiallearning rate h is0.1: w 1 = w 2 = b = 0 h = 0 : 1 Thesingleupdatesteprequiresthatwecomputethegradient,multipliedbythe learningrate q t + 1 = q t  h Ñ q L ( f ( x ( i ) ; q ) ; y ( i ) )
Logistic regression	Natural Language Processing (NLP)	Supervised learning	Machine learning	Cost function (loss function)	Optimization algorithms
What are the four major morphological types of languages?	The four types are: (1) isolating or analytic (2) agglutinating, (3) fusional, (4) polysynthetic or incorporating. Types 2-4 are all considered "synthetic" or "concatenative" among some theorists. Isolating languages use no inflectional morphology; their words never change form. Chinese is an example. Agglutinating languages pile on multiple inflectional morphemes, each carrying one piece of grammatical information. Swahili, Turkish, and Japanese are examples. Fusional languages attach only one inflectional morpheme to a word at a time, but that morpheme "carries" several pieces of grammatical information (such as tense, number, and person) by changing one part of the word, such as the ending. French and Russian are examples. Polysynthetic languages incorporate multiple inflectional morphemes, and even other words in the sentence into the structure of the verb. Eskimo and many other Native American languages qualify.	Which of the following describes the morphological type of English?	Mainly isolating, but also fusional	Mainly agglutinating|||Mainly fusional|||Mainly fusional, but also agglutinating	Morphological Typology Lecture Presentation||slides||Morphological Typology Ling 100   Introduction to Linguistic Science Guest Lecture   Jonathan Manker 26 February 2016        Typological Map of Consonant Inventory Sizes                  What is a  word?                                                                   Phonological Evidence for the word  Some languages have  vowel harmony that applies to entire words --- for example, in Turkish all the vowels in most words must be all front  vowels or all back vowels.  We never find vowel harmony occurring  over entire sentences.  /el - ler -  - PLR - gen.   vs.   /at - lar -   - PLR - gen.   phonotactic considerations:  certain  sequences of sounds cannot occur within syllables, but may be  permissible over word boundaries (e.g. [ dzm      Positional mobility  --- word form as a whole can be moved.    Eg .  I love plums, Plums I love.  vs.  I love dehumidifiers, de I love  humidifiers. (!)  Uninterruptability --- extraneous material cannot be introduced into  the middle of a word - form.  Eg .  A dehumidifier, a, well, dehumidifier, a de, well,  humidifer (!)  Internal stability  --- fixed order of morphemes within word forms   Dehumidifiers,  ifyshumiderde (!)                                                                        Synthetic Languages  Synthetic languages allow affixation such that words may (though are  not required to) include two or more morphemes.  These languages  have  bound morphemes , meaning they must be attached to another  word (whereas analytic languages only have free morphemes).  Synthetic languages include three subcategories: agglutinative,  fusional , and polysynthetic.      Agglutinative languages have words which may consist of more than  one, and possibly many, morphemes.  The key characteristic separating agglutinative languages from other  synthetic languages is that morphemes within words are easily parsed   1:many word to morpheme ratio; 1:1 morpheme to meaning   agglutinative languages.                                                                                                                                    Fusional Languages  Latin fusion:  [ re:ksisti   There are four pieces of grammatical information and four morphs,        Polysynthetic languages often display a high degree of affixation (high number of  morphemes per word) and fusion of morphemes, like agglutinative and  fusional languages.  Additionally, however, polysynthetic languages may have words with  multiple  stems in a single word (which are not compounds).  This may be achieved by  incorporating the subject and object nouns into complex verb forms.  For example:  anin -  -  - te - n   ( Sora )  he - catch - fish - nonpast - do  -   This is called  noun incorporation                                                                                 Classifying languages into morphological  types  Ask yourself the following questions:  1) How many morphemes can occur in a single word?  If the answer is one, or usually one, the language is analytical.  Otherwise,  it is probably synthetic.   A language with a few might be  fusional ,  agglutinative, or polysynthetic; A language with many is probably  agglutinative or polysynthetic (since  fusional morphemes may contain  multiple bits of grammatical information).     2) If the language allows affixation, are the morphemes easy to  divide?  Is each piece of grammatical information contained in a  single morpheme (and the reverse)?                                                               Ancient Greek - fusional !   the suffix contains information about person, number, tense, mood,  and voice  Practice   categorize the language                                         Aztecan  - Agglutinative!  Morphemes each contain one bit of information and are easily  divisible                                           Han ( Athabascan )           Han   polysynthetic!  (noun incorporation.  Also a high degree of  fusion)  (Although interestingly enough, while verbs tend to be polysynthetic,  nouns are almost analytical and can only have a single possessive  prefix --- no case, number, gender, definiteness, etc. is indicated).                 (modern day languages)
Morphology	Typology	Fusional	Agglutinating	Polysynthetic
Would theoretical linguistics be the subject of a macrolinguistics or microlinguistics overview?	Microlinguistics, because theoretical linguistics is concerned with the structure of language in and of itself.	Which of the following DOES NOT fall in the scope of theoretical linguistics?	Speech synthesis	Grammatical categories|||Transformational generative grammar|||Making the alphabet represent phonemes more accurately	Branches of Linguistics presentation||slides||BRANCHES OF  LINGUISTICS  LINGUISTICS Linguistics is the scientific study of natural language . Someone who engages in this study is called a linguist .  BRANCHES OF LINGUISTICS  Language in general and language in particular can be studied from different points of view  The field of linguistics as a whole can be divided into several subfields according to the point of view that is adopted  FIRST DISTINCTION GENERAL LINGUISTICS  Studying language in  general  Supplies the concepts  and categories in terms  of which particular  languages are to be  analysed DESCRIPTIVE LINGUISTICS  Studying particular  languages  Provides the data  which confirm or refute  the propositions and  theories put forward in  general linguistics  SECOND DISTINCTION Diachronic (Historical)  Linguistics  Traces the historical  development of the  language and records  the changes that have  taken place in it between  successive points in time:   to historical  Of particular interest to  linguists throughout the  nineteenth century Synchronic Linguistics  Non - historical: presents  an account of the  language as it is at  some particular point in  time  THIRD DISTINCTION Theoretical Linguistics  Studies language and  languages with a view to  constructing a theory of  their structure and functions  and without regard to any  practical applications that  the investigation of  language and languages  might have  Goal: formulation of a  satisfactory theory of the  structure of language in  general Applied Linguistics  Application of the  concepts and findings  of linguistics to a variety  of practical tasks ,  including language  teaching  Concerned with both  the general and  descriptive branches of  the subject  FOURTH DISTINCTION Micro linguistics  Adopts the narrower  view  Concerned solely with  the structures of the  language system in  itself and for itself Macro linguistics  Adopts the broader view  Concerned with the way  languages are acquired,  stored in the brain and  used for various functions;  interdependence of  language and culture;  physiological and  psychological  mechanisms involved in  language behaviour  FOURTH DISCTINCTION (CONTD.) Micro Linguistics  Phonetics  Phonology  Morphology  Syntax  Semantics  Pragmatics Macro linguistics  Psycholinguistics  Sociolinguistics  Neurolinguistics  Discourse Analysis  Computational  Linguistics  Applied Linguistics   MICROLINGUISTICS  Phonetics is the scientific study of speech sounds . It studies how speech sounds are articulated, transmitted, and received .  Phonology is the study of how speech sounds function in a language, it studies the ways speech sounds are organized . It can be seen as the functional phonetics of a particular language .  Morphology is the study of the formation of words . It is a branch of linguistics which breaks words into morphemes . It can be considered as the grammar of words as syntax is the grammar of sentences .  MICROLINGUISTICS  Syntax deals with the combination of words into phrases, clauses and sentences . It is the grammar of sentence construction .  Semantics is a branch of linguistics which is concerned with the study of meaning in all its formal aspects . Words have several types of meaning  Pragmatics can be defined as the study of language in use in context .  MACROLINGUISTICS  Sociolinguistics studies the relations between language and society : how social factors influence the structure and use of language .  Psycholinguistics is the study of language and mind : the mental structures and processes which are involved in the acquisition, comprehension and production of language .  Neurolinguistics is the study of language processing and language representation in the brain . It typically studies the disturbances of language comprehension and production caused by the damage of certain areas of the brain .  MACROLINGUISTICS  Discourse analysis, or text linguistics is the study of the relationship between language and the contexts in which language is used . It deals with how sentences in spoken and written language form larger meaningful units .  Computational linguistics is an approach to linguistics which employs mathematical techniques, often with the help of a computer .  Applied linguistics is primarily concerned with the application of linguistic theories, methods and findings to the elucidation of language problems which have arisen in other areas of experience  MACROLINGUISTICS  Forensic linguistics, legal linguistics, or language and the law, is the application of linguistic knowledge, methods and insights to the forensic context of law, language, crime investigation, trial, and judicial procedure . It is a branch of applied linguistics .  There are principally three areas of application for linguists working in forensic contexts :  understanding language of the written law,  understanding language use in forensic and judicial processes, and  the provision of linguistic evidence
Branches of linguistics	Microlinguistics	Theoretical linguistics
What is tokenization in NLP?	"Tokenization" in NLP refers to the operation of breaking any chunk of language into smaller meaningful or grammatical units.  It most often refers to breaking sentences into words, but there is also sentence tokenization (breaking text into sentences) and breaking text into phrases. It can be difficult for many reasons. One usually wants to eliminate punctuation, except that punctuation needs to be kept in as part of contractions, hyphenated words, and abbreviations. In some data, especially speech, words may run together, or interrupted.	Which of the following is NOT considered a problem for word-tokenization?	It is difficult to tell whether an "s" on the end of a word is an inflectional ending or part of the stem.	Some sequences of words separated by spaces in English are considered single words.|||All of the parts of Chinese words are considered whole words on their own.|||URLs, email addresses, and other computerese containing punctuation.	Tokenization webpage from Stanford NLP||document||N/A
Natural Language Processing (NLP)	Tokenization	Tokens	Word-tokenize	Segmenting	Data preparation
Discuss challenges with POS-tagging the following sentence properly (by hand) using Penn Treebank tags: “One cannot look to reading for answers not well-known.”	One: Is this a pronoun (PRP) or cardinal number (CD)?  Cannot: This word needs to be split into “can” and “not” in order to tag properly (as MD and RB) look to: Is this a phrasal verb or verb pklus preposition? Is the “to” a PRT, IN, or TO? Reading: is this a VBG or NN? well-known: is this an adjective, JJ, or adverb RB plus past-participle VBN?	Which of the following is most correct annotation according to both PTB rules and traditional linguistic conceptions of the parts of speech?	 One / PRP, can / MD, not / RB, look / VB, to / RP, one / PRP, ‘s / POS, reading / VBG, for / IN, answers / NNS, not / RB, known / VBN	One / PRP, can / MD, not / RB, look / VBZ, to / RP, one / PRP, ‘s / POS, reading / VBG, for / IN, answers / NNS, not / RB, known / VBN|||One / PRP, can / MD, not / RB, look / VB, to / TO, one / PRP, ‘s / POS, reading / NN, for / IN, answers / NNS, not / RB, known / JJ|||One / PRP, can / MD, not / RP, look / VBP, to / IN, one / PRP, ‘s / POS, reading / VBG, for / IN, answers / NNS, not / RB, known / VBN	Chapter Sections on basic POS tags||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 8 Part-of-SpeechTagging DionysiusThraxofAlexandria( c. 100 B . C .),orperhapssomeoneelse(itwasalong timeago),wroteagrammaticalsketchofGreek(aﬁ techn ¯ e ﬂ)thatsummarizedthe linguisticknowledgeofhisday.Thisworkisthesourceofanastonishingproportion ofmodernlinguisticvocabulary,includingwordslike syntax , diphthong , clitic ,and analogy .Alsoincludedareadescriptionofeight partsofspeech :noun,verb, partsofspeech pronoun,preposition,adverb,conjunction,participle,andarticle.Althoughearlier scholars(includingAristotleaswellastheStoics)hadtheirownlistsofpartsof speech,itwasThrax'ssetofeightthatbecamethebasisforpracticallyallsubsequent part-of-speechdescriptionsofmostEuropeanlanguagesforthenext2000years. SchoolhouseRockwasaseriesofpopularanimatededucationaltelevisionclips fromthe1970s.ItsGrammarRocksequenceincludedsongsaboutexactly8parts ofspeech,includingthelategreatBobDorough's ConjunctionJunction : ConjunctionJunction,what'syourfunction? Hookingupwordsandphrasesandclauses... Althoughthelistof8wasslightlyfromThrax'soriginal,theastonishing durabilityofthepartsofspeechthroughtwomillenniaisanindicatorofboththe importanceandthetransparencyoftheirroleinhumanlanguage. 1 Partsofspeech(alsoknownas POS , wordclasses ,or syntacticcategories )are POS usefulbecausetheyrevealalotaboutawordanditsneighbors.Knowingwhether awordisa noun ora verb tellsusaboutlikelyneighboringwords(nounsarepre- cededbydeterminersandadjectives,verbsbynouns)andsyntacticstructure(nouns aregenerallypartofnounphrases),makingpart-of-speechtaggingakeyaspectof parsing(Chapter13).Partsofspeechareusefulfeaturesforlabeling namedentities likepeopleororganizationsin informationextraction (Chapter18),orforcorefer- enceresolution(Chapter22).Aword'spartofspeechcanevenplayaroleinspeech recognitionorsynthesis,e.g.,theword content ispronounced CONtent whenitisa nounand conTENT whenitisanadjective. Thischapterintroducespartsofspeech,andthenintroducestwoalgorithmsfor part-of-speechtagging ,thetaskofassigningpartsofspeechtowords.Oneis generativeŠHiddenMarkovModel(HMM)ŠandoneisdiscriminativeŠtheMax- imumEntropyMarkovModel(MEMM).Chapter9thenintroducesathirdalgorithm basedontherecurrentneuralnetwork(RNN).Allthreehaveroughlyequalperfor- mancebut,aswe'llsee,havedifferenttradeoffs. 8.1(Mostly)EnglishWordClasses Untilnowwehavebeenusingpart-of-speechtermslike noun and verb ratherfreely. Inthissectionwegiveamorecompleteoftheseandotherclasses.While wordclassesdohavesemantictendenciesŠadjectives,forexample,oftendescribe 1 Nonetheless,eightisn'tverymanyand,aswe'llsee,recenttagsetshavemore.  2 C HAPTER 8  P ART - OF -S PEECH T AGGING properties andnouns people Špartsofspeecharetraditionallyinsteadbased onsyntacticandmorphologicalfunction,groupingwordsthathavesimilarneighbor- ingwords(their distributional properties)ortakesimilarafes(theirmorpholog- icalproperties). Partsofspeechcanbedividedintotwobroadsupercategories: closedclass types closedclass and openclass types.Closedclassesarethosewithrelativelyedmembership, openclass suchasprepositionsŠnewprepositionsarerarelycoined.Bycontrast,nounsand verbsareopenclassesŠnewnounsandverbslike iPhone or tofax arecontinually beingcreatedorborrowed.Anygivenspeakerorcorpusmayhavedifferentopen classwords,butallspeakersofalanguage,andsuflargecorpora,likely sharethesetofclosedclasswords.Closedclasswordsaregenerally functionwords functionword like of , it , and ,or you ,whichtendtobeveryshort,occurfrequently,andoftenhave structuringusesingrammar. Fourmajoropenclassesoccurinthelanguagesoftheworld: nouns , verbs , adjectives ,and adverbs .Englishhasallfour,althoughnoteverylanguagedoes. Thesyntacticclass noun includesthewordsformostpeople,places,orthings,but noun othersaswell.Nounsincludeconcretetermslike ship and chair ,abstractionslike bandwidth and relationship ,andverb-liketermslike pacing asin Hispacingtoand frobecamequiteannoying .WhatanouninEnglish,then,arethingslikeits abilitytooccurwithdeterminers( agoat,itsbandwidth,Plato'sRepublic ),totake possessives( IBM'sannualrevenue ),andformostbutnotallnounstooccurinthe pluralform( goats,abaci ). Openclassnounsfallintotwoclasses. Propernouns ,like Regina , Colorado , propernoun and IBM ,arenamesofpersonsorentities.InEnglish,theygenerallyaren't precededbyarticles(e.g., thebookisupstairs ,but Reginaisupstairs ).Inwritten English,propernounsareusuallycapitalized.Theotherclass, commonnouns ,are commonnoun dividedinmanylanguages,includingEnglish,into countnouns and massnouns . countnoun massnoun Countnounsallowgrammaticalenumeration,occurringinboththesingularandplu- ral( goat/goats,relationship/relationships )andtheycanbecounted( onegoat,two goats ).Massnounsareusedwhensomethingisconceptualizedasahomogeneous group.Sowordslike snow,salt ,and communism arenotcounted(i.e., *twosnows or *twocommunisms ).Massnounscanalsoappearwithoutarticleswheresingular countnounscannot( Snowiswhite butnot *Goatiswhite ). Verbs refertoactionsandprocesses,includingmainverbslike draw , provide , verb and go .Englishverbshave(non-third-person-sg( eat ),third-person-sg ( eats ),progressive( eating ),pastparticiple( eaten )).Whilemanyresearchersbelieve thatallhumanlanguageshavethecategoriesofnounandverb,othershaveargued thatsomelanguages,suchasRiauIndonesianandTongan,don'tevenmakethis distinction( Broschart1997 ; Evans2000 ; Gil2000 ). ThethirdopenclassEnglishformis adjectives ,aclassthatincludesmanyterms adjective forpropertiesorqualities.Mostlanguageshaveadjectivesfortheconceptsofcolor ( white , black ),age( old , young ),andvalue( good , bad ),buttherearelanguages withoutadjectives.InKorean,forexample,thewordscorrespondingtoEnglish adjectivesactasasubclassofverbs,sowhatisinEnglishanadjectiveﬁbeautifulﬂ actsinKoreanlikeaverbmeaningﬁtobebeautifulﬂ. Theopenclassform, adverbs ,isratherahodge-podgeinbothformand adverb meaning.Inthefollowingalltheitalicizedwordsareadverbs: Actually ,Iran homeextremelyquicklyyesterday Whatcoherencetheclasshassemanticallymaybesolelythateachofthese wordscanbeviewedasmodifyingsomething(oftenverbs,hencethenameﬁad-  8.1  (M OSTLY )E NGLISH W ORD C LASSES 3 verbﬂ,butalsootheradverbsandentireverbphrases). Directionaladverbs or loca- tiveadverbs ( home , here , downhill )specifythedirectionorlocationofsomeaction; locative degreeadverbs ( extremely , very , somewhat )specifytheextentofsomeaction,pro- degree cess,orproperty; manneradverbs ( slowly , slinkily , delicately )describethemanner manner ofsomeactionorprocess;and temporaladverbs describethetimethatsomeac- temporal tionoreventtookplace( yesterday , Monday ).Becauseoftheheterogeneousnature ofthisclass,someadverbs(e.g.,temporaladverbslike Monday )aretaggedinsome taggingschemesasnouns. Theclosedclassesdiffermorefromlanguagetolanguagethandotheopen classes.SomeoftheimportantclosedclassesinEnglishinclude: prepositions: on,under,over,near,by,at,from,to,with particles: up,down,on,off,in,out,at,by determiners: a,an,the conjunctions: and,but,or,as,if,when pronouns: she,who,I,others auxiliaryverbs: can,may,should,are numerals: one,two,three,second,third Prepositions occurbeforenounphrases.Semanticallytheyoftenindicatespatial preposition ortemporalrelations,whetherliteral( onit , beforethen , bythehouse )ormetaphor- ical( ontime , withgusto , besideherself ),butoftenindicateotherrelationsaswell, likemarkingtheagentin Hamletwaswrittenby Shakespeare .A particle resembles particle aprepositionoranadverbandisusedincombinationwithaverb.Particlesoften haveextendedmeaningsthataren'tquitethesameastheprepositionstheyresemble, asintheparticle over in sheturnedthepaperover . Averbandaparticlethatactasasinglesyntacticand/orsemanticunitare calleda phrasalverb .Themeaningofphrasalverbsisoftenproblematically non- phrasalverb compositional Šnotpredictablefromthedistinctmeaningsoftheverbandthepar- ticle.Thus, turndown meanssomethinglike`reject', ruleout `eliminate', out `discover',and goon `continue'. Aclosedclassthatoccurswithnouns,oftenmarkingthebeginningofanoun phrase,isthe determiner .Onesmallsubtypeofdeterminersisthe article :English determiner article hasthreearticles: a , an ,and the .Otherdeterminersinclude this and that ( thischap- ter , thatpage ). A and an markanounphraseaswhile the canmarkit asisadiscourseproperty(Chapter23).Articlesarequitefre- quentinEnglish;indeed, the isthemostfrequentlyoccurringwordinmostcorpora ofwrittenEnglish,and a and an aregenerallyrightbehind. Conjunctions jointwophrases,clauses,orsentences.Coordinatingconjunc- conjunctions tionslike and , or ,and but jointwoelementsofequalstatus.Subordinatingconjunc- tionsareusedwhenoneoftheelementshassomeembeddedstatus.Forexample, that in ﬁIthoughtthatyoumightlikesomemilkﬂ isasubordinatingconjunction thatlinksthemainclause Ithought withthesubordinateclause youmightlikesome milk .Thisclauseiscalledsubordinatebecausethisentireclauseistheﬁcontentﬂof themainverb thought .Subordinatingconjunctionslike that whichlinkaverbtoits argumentinthiswayarealsocalled complementizers . complementizer Pronouns areformsthatoftenactasakindofshorthandforreferringtosome pronoun nounphraseorentityorevent. Personalpronouns refertopersonsorentities( you , personal she , I , it , me ,etc.). Possessivepronouns areformsofpersonalpronounsthatin- possessive dicateeitheractualpossessionormoreoftenjustanabstractrelationbetweenthe personandsomeobject( my,your,his,her,its,one's,our,their ). Wh-pronouns wh ( what,who,whom,whoever )areusedincertainquestionforms,ormayalsoactas  4 C HAPTER 8  P ART - OF -S PEECH T AGGING complementizers( Frida,whomarriedDiego... ). AclosedclasssubtypeofEnglishverbsarethe auxiliary verbs.Cross-linguist- auxiliary ically,auxiliariesmarksemanticfeaturesofamainverb:whetheranactiontakes placeinthepresent,past,orfuture(tense),whetheritiscompleted(aspect),whether itisnegated(polarity),andwhetheranactionisnecessary,possible,suggested,or desired(mood).Englishauxiliariesincludethe copula verb be ,thetwoverbs do and copula have ,alongwiththeirforms,aswellasaclassof modalverbs . Be iscalled modal acopulabecauseitconnectssubjectswithcertainkindsofpredicatenominalsand adjectives( Heis aduck ).Theverb have canmarktheperfecttenses( Ihave gone , I had gone ),and be isusedaspartofthepassive( Wewere robbed )orprogressive( We are leaving )constructions.Modalsareusedtomarkthemoodassociatedwiththe eventdepictedbythemainverb: can indicatesabilityorpossibility, may permission orpossibility, must necessity.Thereisalsoamodaluseof have (e.g., Ihave togo ). Englishalsohasmanywordsofmoreorlessuniquefunction,including inter- jections ( oh,hey,alas,uh,um ), negatives ( no,not ), politenessmarkers ( please, interjection negative thankyou ), greetings ( hello,goodbye ),andtheexistential there ( there aretwoon thetable )amongothers.Theseclassesmaybedistinguishedorlumpedtogetheras interjectionsoradverbsdependingonthepurposeofthelabeling. 8.2ThePennTreebankPart-of-SpeechTagset AnimportanttagsetforEnglishisthe45-tagPennTreebanktagset (Marcusetal., 1993) ,showninFig. 8.1 ,whichhasbeenusedtolabelmanycorpora.Insuch labelings,partsofspeecharegenerallyrepresentedbyplacingthetagaftereach word,delimitedbyaslash: TagDescriptionExampleTagDescriptionExampleTagDescriptionExample CC coordinating conjunction and,but,or PDT predeterminer all,both VBP verbnon-3sg present eat CD cardinalnumber one,two POS possessiveending 's VBZ verb3sgpres eats DT determiner a,the PRP personalpronoun I,you,he WDT wh-determ. which,that EX existential`there' there PRP$ possess.pronoun your,one's WP wh-pronoun what,who FW foreignword meaculpa RB adverb quickly WP$ wh-possess. whose IN preposition/ subordin-conj of,in,by RBR comparative adverb faster WRB wh-adverb how,where JJ adjective yellow RBS superlatv.adverb fastest $ dollarsign $ JJR comparativeadj bigger RP particle up,off # poundsign # JJS superlativeadj wildest SYM symbol + , % , & ﬁ leftquote `orﬁ LS listitemmarker 1,2,One TO ﬁtoﬂ to ﬂ rightquote 'orﬂ MD modal can,should UH interjection ah,oops ( leftparen [,(, f , < NN singormassnoun llama VB verbbaseform eat ) rightparen ],), g , > NNS noun,plural llamas VBD verbpasttense ate , comma , NNP propernoun,sing. IBM VBG verbgerund eating . sent-endpunc .!? NNPS propernoun,plu. Carolinas VBN verbpastpart. eaten : sent-midpunc :;...Œ- Figure8.1 PennTreebankpart-of-speechtags(includingpunctuation). (8.1) The/DTgrand/JJjury/NNcommented/VBDon/INa/DTnumber/NNof/IN other/JJtopics/NNS./. (8.2) There/EX are/VBP70/CDchildren/NNS there/RB
Penn Treebank	Part-of-speech (POS) tagging
What is the dot product between the following vectors, \(a · b\): \(a = [1, 7, -5], b = [-3, 3, -1]\)	$$ [1, 7, -5] · [-3, 3, -1] = \sum_{i=1}^{n}w_{i}x_{I} = (1 \times -3) + (7 \times 3) + (-5 \times -1) = -3 + 21 + 5 = 23  $$	What does the dot product accomplish with regards to logistic regression?	A valid way to apply weights to feature vectors.	A valid way to assess the similarity between an input and an output label.|||A valid way to compress features into one numeric value.|||The dot product is not used in logistic regression.	Speech and Language Processing: Logistic Regression||publication||5.1  C LASSIFICATION : THESIGMOID 3 isﬁpositivesentimentﬂversusﬁnegativesentimentﬂ,thefeaturesrepresentcounts ofwordsinadocument,and P ( y = 1 j x ) istheprobabilitythatthedocumenthas positivesentiment,whileand P ( y = 0 j x ) istheprobabilitythatthedocumenthas negativesentiment. Logisticregressionsolvesthistaskbylearning,fromatrainingset,avectorof weights anda biasterm .Eachweight w i isarealnumber,andisassociatedwithone oftheinputfeatures x i .Theweight w i representshowimportantthatinputfeatureis tothedecision,andcanbepositive(meaningthefeatureisassociated withtheclass)ornegative(meaningthefeatureisnotassociatedwiththeclass). Thuswemightexpectinasentimenttasktheword awesome tohaveahighpositive weight,and abysmal tohaveaverynegativeweight.The biasterm ,alsocalledthe biasterm intercept ,isanotherrealnumberthat'saddedtotheweightedinputs. intercept TomakeadecisiononatestinstanceŠafterwe'velearnedtheweightsin trainingŠthemultiplieseach x i byitsweight w i ,sumsuptheweighted features,andaddsthebiasterm b .Theresultingsinglenumber z expressesthe weightedsumoftheevidencefortheclass. z =   n X i = 1 w i x i ! + b (5.2) Intherestofthebookwe'llrepresentsuchsumsusingthe dotproduct notationfrom dotproduct linearalgebra.Thedotproductoftwovectors a and b ,writtenas a  b isthesumof theproductsofthecorrespondingelementsofeachvector.Thusthefollowingisan equivalentformationtoEq. 5.2 : z = w  x + b (5.3) ButnotethatnothinginEq. 5.3 forces z tobealegalprobability,thatis,tolie between0and1.Infact,sinceweightsarereal-valued,theoutputmightevenbe negative; z rangesfrom  ¥ to ¥ . Figure5.1 Thesigmoidfunction y = 1 1 + e  z takesarealvalueandmapsittotherange [ 0 ; 1 ] . Itisnearlylineararound0butoutliervaluesgetsquashedtoward0or1. Tocreateaprobability,we'llpass z throughthe sigmoid function, s ( z ) .The sigmoid sigmoidfunction(namedbecauseitlookslikean s )isalsocalledthe logisticfunc- tion ,andgiveslogisticregressionitsname.Thesigmoidhasthefollowingequation, logistic function showngraphicallyinFig. 5.1 : y = s ( z )= 1 1 + e  z (5.4) Thesigmoidhasanumberofadvantages;ittakesareal-valuednumberandmaps itintotherange [ 0 ; 1 ] ,whichisjustwhatwewantforaprobability.Becauseitis
Logistic regression	Natural Language Processing (NLP)	Dot product
What are Naive Bayes classifiers?	Naive Bayes classifiers are generative classifiers, which means they build a model of how a class could generate some input data and return the most likely class to have generated the observation (Jurafsky 2019). The idea of Naive Bayes is to multiply the probabilities of each word in an input document appearing as a given class, and selecting the class that has the highest score (known as "argmax").	Consider the following model: Positive: 'I': 0.2, 'won': 0.1, 'the': 0.05, 'game': 0.1 | Negative: 'I': 0.1, 'won': 0.05, 'the': 0.05, 'game': 0.2. How would a Naive Bayes classifier label the following sentence with this model: I won the game.	Positive	Negative|||Not enough information provided.	Naive Bayes presentation||slides||0(12"3(4")%(./%H(.A?(A"%>-/"'*.A ¥;%k :#L%6#& /-%&&.).#9&$/%0$8&#$%06$&(9+$()$)#%+89# ¥>VdB$#7%.-$%DD9#&&B$D./+.(0%9.#&B$0#+<(9'$)#%+89#& ¥O8+$.)B$%&$.0$+"#$G9#:.(8&$&-.D#& ¥M#$8&#$ -.'4 <(9D$)#%+89#&$ ¥<#$8&#$ ('' ()$+"#$<(9D&$.0$+"#$+#*+$i0(+$%$&8L&#+j ¥!"#0$ ¥;%k:#L%6#& "%&$%0$.7G(9+%0+$&.7.-%9.+6$+($-%0N8%N#$ 7(D#-.0N? KJ S(,6%,'())%T%(%?.*A<(8%'(.A?(A"%8-/"' ¥F&&.N0.0N$#%/"$<(9D5$si<(9D$m$/j ¥F&&.N0.0N$#%/"$&#0+#0/#5$si &m/ je#si<(9Dm/ jW?1 ^W?1 -(:# W?W1 +".& W?WP )80 W?1 ).-7 T^-(:# +".& )80 ).-7 W?1 W?1 ?WP W?W1 W?1 ,-%&&$ #:; si&$m$ G(& j$e$W?WWWWWWP$ @#/?1K?H?1  0(12"3(4")%()%(%H(.A?(A"%>-/"' ¥M"./"$/-%&&$%&&.N0&$+"#$".N"#9$G9(L%L.-.+6$+($&a W?1 ^W?1 -(:# W?W1 +".& W?WP )80 W?1 ).-7 C(D#-$ G(& C(D#-$ 0#N ).-7 -(:# +".& )80 ^W?1 W?1 W?W1 W?WP W?1 W?1 W?WW1 W?W1 W?WWP W?H si&mG(& j$$ t$$si &m0#N jW?H ^W?WW1 -(:# W?W1 +".& W?WWP )80 W?1 ).-7 Sec.13.2.1|||Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||4.1  N AIVE B AYES C LASSIFIERS 3 4.1NaiveBayes Inthissectionweintroducethe multinomialnaiveBayes ,socalledbe- naiveBayes  causeitisaBayesianthatmakesasimplifying(naive)assumptionabout howthefeaturesinteract. TheintuitionoftheisshowninFig. 4.1 .Werepresentatextdocument asifitwerea bag-of-words ,thatis,anunorderedsetofwordswiththeirposition ords ignored,keepingonlytheirfrequencyinthedocument.Intheexampleinthe insteadofrepresentingthewordorderinallthephraseslikeﬁIlovethismovieﬂand ﬁIwouldrecommenditﬂ,wesimplynotethattheword I occurred5timesinthe entireexcerpt,theword it 6times,thewords love , recommend ,and movie once,and soon. Figure4.1 IntuitionofthemultinomialnaiveBayesappliedtoamoviereview.Thepositionofthe wordsisignored(the bagofwords assumption)andwemakeuseofthefrequencyofeachword. NaiveBayesisaprobabilistic,meaningthatforadocument d ,outof allclasses c 2 C theclassireturnstheclass‹ c whichhasthemaximumposterior probabilitygiventhedocument.InEq. 4.1 weusethehatnotation ‹ tomeanﬁour ‹ estimateofthecorrectclassﬂ. ‹ c = argmax c 2 C P ( c j d ) (4.1) Thisideaof Bayesianinference hasbeenknownsincetheworkof Bayes(1763) , Bayesian inference andwasappliedtotextby MostellerandWallace(1964) .The intuitionofBayesianistouseBayes'ruletotransformEq. 4.1 into otherprobabilitiesthathavesomeusefulproperties.Bayes'ruleispresentedin Eq. 4.2 ;itgivesusawaytobreakdownanyconditionalprobability P ( x j y ) into threeotherprobabilities: P ( x j y )= P ( y j x ) P ( x ) P ( y ) (4.2)  4 C HAPTER 4  N AIVE B AYESAND S ENTIMENT C LASSIFICATION WecanthensubstituteEq. 4.2 intoEq. 4.1 togetEq. 4.3 : ‹ c = argmax c 2 C P ( c j d )= argmax c 2 C P ( d j c ) P ( c ) P ( d ) (4.3) WecanconvenientlysimplifyEq. 4.3 bydroppingthedenominator P ( d ) .This ispossiblebecausewewillbecomputing P ( d j c ) P ( c ) P ( d ) foreachpossibleclass.But P ( d ) doesn'tchangeforeachclass;wearealwaysaskingaboutthemostlikelyclassfor thesamedocument d ,whichmusthavethesameprobability P ( d ) .Thus,wecan choosetheclassthatmaximizesthissimplerformula: ‹ c = argmax c 2 C P ( c j d )= argmax c 2 C P ( d j c ) P ( c ) (4.4) Wethuscomputethemostprobableclass‹ c givensomedocument d bychoosing theclasswhichhasthehighestproductoftwoprobabilities:the priorprobability prior probability oftheclass P ( c ) andthe likelihood ofthedocument P ( d j c ) : likelihood ‹ c = argmax c 2 C likelihood z }| { P ( d j c ) prior z }| { P ( c ) (4.5) Withoutlossofgeneralization,wecanrepresentadocument d asasetoffeatures f 1 ; f 2 ;:::; f n : ‹ c = argmax c 2 C likelihood z }| { P ( f 1 ; f 2 ;::::; f n j c ) prior z }| { P ( c ) (4.6) Unfortunately,Eq. 4.6 isstilltoohardtocomputedirectly:withoutsomesim- plifyingassumptions,estimatingtheprobabilityofeverypossiblecombinationof features(forexample,everypossiblesetofwordsandpositions)wouldrequirehuge numbersofparametersandimpossiblylargetrainingsets.NaiveBayes thereforemaketwosimplifyingassumptions. Theisthe bagofwords assumptiondiscussedintuitivelyabove:weassume positiondoesn'tmatter,andthatthewordﬁloveﬂhasthesameeffecton whetheritoccursasthe1st,20th,orlastwordinthedocument.Thusweassume thatthefeatures f 1 ; f 2 ;:::; f n onlyencodewordidentityandnotposition. Thesecondiscommonlycalledthe naiveBayesassumption :thisisthecondi- naiveBayes assumption tionalindependenceassumptionthattheprobabilities P ( f i j c ) areindependentgiven theclass c andhencecanbe`naively'multipliedasfollows: P ( f 1 ; f 2 ;::::; f n j c )= P ( f 1 j c )  P ( f 2 j c )  :::  P ( f n j c ) (4.7) TheequationfortheclasschosenbyanaiveBayesisthus: c NB = argmax c 2 C P ( c ) Y f 2 F P ( f j c ) (4.8) ToapplythenaiveBayestotext,weneedtoconsiderwordpositions,by simplywalkinganindexthrougheverywordpositioninthedocument: positions   allwordpositionsintestdocument c NB = argmax c 2 C P ( c ) Y i 2 positions P ( w i j c ) (4.9)  4.2  T RAININGTHE N AIVE B AYES C LASSIFIER 5 NaiveBayescalculations,likecalculationsforlanguagemodeling,aredoneinlog space,toavoidwandincreasespeed.ThusEq. 4.9 isgenerallyinstead expressedas c NB = argmax c 2 C log P ( c )+ X i 2 positions log P ( w i j c ) (4.10) Byconsideringfeaturesinlogspace,Eq. 4.10 computesthepredictedclassasalin- earfunctionofinputfeatures.thatusealinearcombinationoftheinputs tomakeadecisionŠlikenaiveBayesandalsologisticregressionŠ arecalled linear . linear  4.2TrainingtheNaiveBayes Howcanwelearntheprobabilities P ( c ) and P ( f i j c ) ?Let'sconsiderthemax- imumlikelihoodestimate.We'llsimplyusethefrequenciesinthedata.Forthe documentprior P ( c ) weaskwhatpercentageofthedocumentsinourtrainingset areineachclass c .Let N c bethenumberofdocumentsinourtrainingdatawith class c and N doc bethetotalnumberofdocuments.Then: ‹ P ( c )= N c N doc (4.11) Tolearntheprobability P ( f i j c ) ,we'llassumeafeatureisjusttheexistenceofaword inthedocument'sbagofwords,andsowe'llwant P ( w i j c ) ,whichwecomputeas thefractionoftimestheword w i appearsamongallwordsinalldocumentsoftopic c .Weconcatenatealldocumentswithcategory c intoonebigﬁcategory c ﬂtext. Thenweusethefrequencyof w i inthisconcatenateddocumenttogiveamaximum likelihoodestimateoftheprobability: ‹ P ( w i j c )= count ( w i ; c ) P w 2 V count ( w ; c ) (4.12) HerethevocabularyVconsistsoftheunionofallthewordtypesinallclasses,not justthewordsinoneclass c . Thereisaproblem,however,withmaximumlikelihoodtraining.Imaginewe aretryingtoestimatethelikelihoodofthewordﬁfantasticﬂgivenclass positive ,but supposetherearenotrainingdocumentsthatbothcontainthewordﬁfantasticﬂand areclasas positive .Perhapsthewordﬁfantasticﬂhappenstooccur(sarcasti- cally?)intheclass negative .Insuchacasetheprobabilityforthisfeaturewillbe zero: ‹ P ( ﬁfantasticﬂ j positive )= count ( ﬁfantasticﬂ ; positive ) P w 2 V count ( w ; positive ) = 0 (4.13) ButsincenaiveBayesnaivelymultipliesallthefeaturelikelihoodstogether,zero probabilitiesinthelikelihoodtermforanyclasswillcausetheprobabilityofthe classtobezero,nomattertheotherevidence! Thesimplestsolutionistheadd-one(Laplace)smoothingintroducedinChap- ter3.WhileLaplacesmoothingisusuallyreplacedbymoresophisticatedsmoothing
Naive Bayes	Natural Language Processing (NLP)	Text classification
Define the following speech errors: Addition, Swapping and Shifting	Addition is when a new and undesired unit or phoneme appears in the sentence Swapping  is when two entire words exchange positions in the sentence Shifting is when a unit or a segment is relocated and shifted to a different position within the sentence	Which of the speech error examples shows a 'shift'	saying 'she jump ins' instead of 'she jumps in'	saying 'she in jumps' instead of 'she jumps in'|||saying 'she munderstands it' instead of 'she understands it'	Speech Errors||slides||SPEECH  ERRORS :   SLIP OF  TOUNGE IN  PSYCHOLINGUISTIC       Serendre Shutter in 2004 stated that it is a which is happening entirely below the level of consciousness, so we're not aware of doing anything except when we hear ourselves saying something funny, and its all happening at such lighting speed that   Victoria Fromkin Said that  of the tongue are often the result of a sound being carried over from one word to the next  . Although the slips are mostly treated as errors of articulation, it has been suggested that they may result from  of the  as it tries to organize linguistic messages .   One of researcher named Yule and he come out his conception and he believes that when brain and tongue deny to work in accordance slips of tongue occur . Since our whole linguistics knowledge is stored in our mind he calls it the  SLIPS OF BRAIN  . He suggests that our  -  system is organized on the basis of some phonological information and that some words in the store are more easily retrieved than others .    The scientific study of speech  errors, c ommonly called Slips of the tongue or tongue - slips c an provided useful clues to the processes;  o f language production :  t hey can tell us where a speaker stop to  think.    Type Description Examples 1 Substitution A unit of the sentence contains  an intruder The queer old dean instead of ( the dear old queen) 2 Deletion A unit is omitted in the  utterance  was there) 3 Perseveration An    earlier    segment  reappears  in  a  latter  one Pulled a tantrum (pulled a pantrum) 4 Addition A new unit  is added The   optional   number   (the  moptiona l number)   Spooner had a nervous tendency to  sometimes transpose initial letters or half - syllables in speech. This tendency became  known, circa 1885,  as Spoonerism and the  sometimes hilarious transpositions became  known as Spoonerisms - Dr. Spooner's  occasional transpositions created a  reputation and started a fad. Students began  devising  transpositional puns, and  attributing them to him. His famous speech lapses are thought to  have resulted from the difficulty he may  have had reading. Spooner was an albino  and as such, suffered from defective  eyesight.  5 Swapping Two words are exchanged To  let  the  cat  out  of  the   bag (to   let   the  house out of the cat 6 Shifting A segment or unit is relocated  somewhere else in the  utterance She decides to hit it . ( she decide to hits it) . 7 Anticipation A later segment is used to  replace an earlier one Reading   list   (leading  list) 8 Blending Where  more  than  one  item  is  untended,  two  items       are       fused  together Person/people (perple) 9 Malapropism Inappropriate word  selection The  two  cars  collide  (collude) 10 Spoonerism Taken  from the Rev.  W.  Spooner  noted  for  puns and word plays Drink  is  the  curse  of  the    working    classes  (work  is  the  curse  of  drinking classes)  HOW TO OVERCOME ?   Practise and talk slow down Get enough sleep.
Applied linguistics	Key terms in linguistics	Speech errors	Psycholinguistics
What is WordNet?	WordNet is a digital resource made available for free from Princeton University. It is a database containing thousands of English nouns, verbs, adjective, and adverbs, and their morphological information stored in sets of synonyms (synsets). The synsets are inter-related in a network of lexical semantic relationships—mainly in terms of hyponomy (the superordinate-subordinate relationship), i.e. general to specific category relationships,such as communicate-speak-lecture. Some other relationships are also stored for some words, such as meronymy (part-whole), and antynomy for adjectives. Its simplest uses are as a thesaurus and to judge similarity of meaning between words. It is often used in topic modeling and word sense disambiguation. And for morphological analysis.	Which of the following is / are NOT part of WordNet?	derivational morphology	pertainyms|||semantic roles of nouns with verbs||| example sentences	What is WordNet?||publication||ˇ ˜˙˜%& & &   '  ˜ˇˇ (˜˝ +˜ ˇˇ&˜˜˜ ˜ ˜ˆ  %˜& $$&ˇ˜ !ˆ, ˜˚ˆ1+ 1! ˜ %$˜ˆ  %˜ +˝ 6% ˚ ˆ+ˇ˜ (8 ˇ˜%˜ ˚ˇ%# :˜&˜!ˆ  ;˝$&&6˝6 48 &ˆ˙˚& ˇ8 ˆ <7 =˝˜6$4#6$4& %˜7˙˜ˇ˜˚! ˜$˛ ˜=!ˆ< ˇ$46$46$46ˆ +=˜ˆ1 ˜˜ˇ ˜˜18$ &ˆ˜˜˜1 ˜˜ˇ ˜˜!ˆ %&˝˝ˆ!˛ˇ ˆ! ˆ˙˝˛˘ %˛ˇ,˜ˇ:"% & :"ˆ:"ˇ˜˜1˜ ˜˜˜ˇ˜8! ˜ 8˝ˇ:@+%3:˚6 4+B˘%66"A%ˆ ˇ )˙˜˝ˆ A˝
Semantic relationships	Word sense disambiguation	Wordnet	Lexical semantics	Semantics	Word meanings
What is meant if the cosine of two words is 1?	The words are the same. For instance, if the word 'king' is represented in 3-D space as the word embedding [25, 10, 3], then the cosine similarity between [25, 10, 3] and [25, 10, 3] is 1.	Words that have a cosine similarity of 0 are considered to be _______.	orthogonal	antonyms|||the same|||one-hot vectors|||parallel	NONE
Natural Language Processing (NLP) basics	Word vectors	Word embeddings
What is Psycholinguistics?	Psycholinguistics is a field of study that incorporates theories and concepts from psychology and linguistics. It investigates the relationship between the mind and language. It looks at the various processes involved when the human brain perceives and produces written and spoken language discourse.	In which time period did Psycholinguistics first appear?	1950s	1930s|||1980s|||2000s	Psycholinguistics||slides||PSYCHOLINGUISTICS  Definition  Psycholinguistics is a branch of study which  combines the disciplines of psychology and  linguistics. It is concerned with the relationship  between the human mind and the language as it  examines the processes that occur in brain while  producing and perceiving both written and  spoken discourse.   Psycholinguistics as a separate branch of study  emerged in the late  1950 s and  1960 s as a result  of  Chomskyan revolution .  The three primary processes investigated in  psycholinguistics  Language  Comprehension  Language  Production  Language Acquisition  Language Comprehension  Understanding what other people say and write (i.e., language  comprehension) is more complicated than it might at first  appear. Comprehending language involves a variety of  capacities, skills, processes, knowledge, and dispositions that  are used to derive meaning from spoken, written, and signed  language. Comprehension is mainly thought to occur in the   temporal lobe. Language comprehension is a complex process  that occurs easily and effortlessly by humans.  It develops  along with the brain and is able to be enhanced with the use  of gesture.  Though it is unknown exactly how early  comprehension is fully developed in children, gestures are  undoubtedly useful for understanding the language around  us.   Language Production  language production is the production of spoken or written  language. It describes all of the stages between having a  concept, and translating that concept into linguistic form.   Stages of production The basic loop occurring in the creation of language consists of the  following stages:  Intended message  Encode message into linguistic form  Encode linguistic form into speech [motor system]  Sound goes from speaker's mouth to hearer's ear [auditory  system]  Speech is decoded into linguistic form  Linguistic form is decoded into meaning  Language Acquisition  Language acquisition is the process by which humans  acquire the capacity to perceive and comprehend  language, as well as to produce and use words and  sentences to communicate. Language acquisition  usually refers to first - language acquisition, which  studies infants' acquisition of their native language. This  is distinguished from second - language  acquisition, which deals with the acquisition (in both  children and adults) of additional languages. Language  acquisition is just one strand of psycholinguistics which  is all about how people learn to speak and the mental  processes involved.   Central themes in psycholinguistics 1 ) What  knowledge of language is needed for us   to  use language ?  Tacit ( implicit ) knowledge vs. Explicit  knowledge .  T acit : knowledge of how to perform something, but not  aware of full  rules  explicit: knowledge of the processes of mechanisms in  performing that thing  2)  What cognitive processes are involved in  the ordinary use of language ?  How do we understand a lecture, read a book, hold a  conversation?  Cognitive processes:  perception, memory, thinking, learning  When  is  psycholinguistics studied?  Psycholinguistic research started as far back as  Plato, who was interested in human knowledge  and language, however, it became a concern in  linguistics during the second half of the  nineteenth century with linguists looking at  language acquisition.  In  1960 , Charles  Hockett published a list of  'design features of human language', where he  identified  13  different features, that the language  we use to communicate as humans, is  characterised  by.   In the late twentieth century, Willem  Levelt did a great deal of study on what he called  'the mental lexicon'. His work has become  more prominent in recent years, especially  his research into speech production.  Since  the 1990s, the advances in brain  scanning and mapping have provided new  information for psycholinguistics, meaning  we can now see brain activity relating to  word processing, comprehension.  Where is  psycholinguistics studied?  Psycholinguistic research is not limited  to a particular area in the world, but  there is more evidence of  psycholinguistic study in the Western  world due to advanced science and  technology.   THANK YOU
Psycholinguistics Theory	Applied linguistics	Branches of linguistics	Key terms in linguistics
How do languages change?	Languages change in every way, normally, if not interfered with, and continuously. Unless artificially held to a standard, the sounds, morphology, syntax, vocabulary, and meanings of a language will change due to both 'internal' and 'external’ forces. Internal forces can be structural, so for example, sounds may change just to become easier to say, or easier to distinguish from each other, or to simplify the grammar. Or, also internally, the language changes because each generation learns a slightly different language, re-interpreting some of the grammar and word meanings. Or they are taught things in school about their language that were not correct but eventually become part of the language because they are taught. Or changes in culture and technology introduce new words. "External" language change forces mean "language contact"; languages borrow words, sounds, and even grammatical constructions from each other. Languages can also eradicate each other through conquest and colonialism.	Which of the following is NOT true of language change?	Languages can degenerate when people forget their correct rules for generations.	Languages tend to change almost completely within ten thousand years if not held still.|||Social and economic disparities between groups can cause language change.|||Languages can change for poetic reasons.	Types of Language Change (University of Pennsylvania webpage)||document||Linguistics 001     Lecture 22    Language  ChangeTypes of Language Change Language is always changing. We've seen that language changes across  space and across social group. Language also varies across time. Generation by generation, pronunciations evolve, new words are borrowed  orinvented, the meaning of old words drifts, and morphology develops  or decays. The rate of change varies, but whether the changes are faster  or slower, they build up until the "mother tongue" becomes arbitrarily  distant and different. After a thousand years, the original and new languages  will not be mutually intelligible. After ten thousand years, the relationship  will be essentially indistinguishable from chance relationships between  historically unrelated languages. In isolated subpopulations speaking the same language, most changes will  not beshared. As a result, such subgroups will drift apart linguistically,  and eventually will not be able to understand one another. In the modern world, language change is often socially problematic. Long  before divergent dialects lose mutual intelligibility completely, they  begin to show difficulties and inefficiencies in communication, especially  under noisy or stressful conditions. Also, as people observe language  change, they usually react negatively, feeling that the language has "gone  down hill". You never seem to hear older people commenting that the language  of their children or grandchildren's generation has improved compared  to the language of their own youth. Here is a puzzle: language change is functionally disadvantageous, in  that ithinders communication, and it is also negatively evaluated by  socially dominant groups. Nevertheless is is a universal fact of human  history. How and why does language change?There are many different routes to language change. Changes can take  originate in language learning, or through  language contact , social differentiation , andnatural processes in usage .Language learning: Language is transformed as it is transmitted  from one generation to the next. Each individual must re-create a grammar  and lexicon based on input received from parents, older siblings and other  members of the speech community. The experience of each individual is  different, and the process of linguistic replication is imperfect, so  that the result is variable across individuals. However, a bias in the  learning process -- for instance, towards regularization -- will cause  systematic drift, generation by generation. In addition, random differences  may spread and become 'fixed', especially in small populations. Language contact : Migration, conquest and trade bring speakers  of onelanguage into contact with speakers of another language. Some individuals  willbecome fully bilingual as children, while others learn a second language  more or home  schedule  homework  less well as adults. In such contact situations, languages often  borrow words, sounds, constructions and so on. Social differentiation . Social groups adopt distinctive norms  of dress, adornment, gesture and so forth; language is part of the package.  Linguistic distinctiveness can be achieved through vocabulary (slang or  jargon), pronunciation (usually via exaggeration of some variants already  available in the environment), morphological processes, syntactic constructions,  and so on. Natural processes in usage . Rapid or casual speech naturally produces processes such as  assimilation , dissimilation , syncope  and apocope . Through repetition, particular cases may become conventionalized,  and therefore produced even in slower or more careful speech. Word meaning  change in a similar way, through conventionalization of processes like  metaphor  andmetonymy .Some linguists distinguish between  internal  and external  sources of language change, with "internal" sources of change  being those that occur within a single languistic community, and contact  phenomena being the main examples of an external source of change. The analogy with evolution via natural selection Darwin himself, in developing the concept of evolution of species via  natural selection, made an analogy to the evolution of languages. For  the analogy to hold, we need a pool of individuals with variable traits,  a process of replication creating new individuals whose traits depend  on those of their "parents", and a set of environmental processes  that result in differential success in replication for different traits. We can cast each of the just-listed types of language change in such  aframework. For example, in child language acquisition, different grammatical  ordifferent lexical patterns may be more or less easily learnable, resulting  in betterreplication for grammatical or lexical variants that are "fitter"  in this sense. There are some key differences between grammars/lexicons and genotypes.  Forone thing, linguistic traits can be acquired throughout one's life  from many different sources, although intitial acquisition and (to a lesser  extent) adolescence seem to be crucial stages. Acquired (linguistic) traits  can also be passed on to others. One consequence is that linguistic history  need not have the form of a tree , with languages splitting but  never rejoining, whereas genetic evolution is largely constrained to have  a tree-like form (despite the possibility of transfer of genetic material  across species boundaries by viral infection and so on). However, as a  practical matter, the assumption that linguistic history is a sort of tree structure has been found to be a good working approximation. In particular, the basic sound structure and morphology of languages  usually seems to "descend" via a tree-structured graph of inheritance,  with regular, lawful relationships between the patterns of "parent"  and "child" languages. Types of Change  Sound change All aspects of language change, and a great deal is know about general mechanisms and historical details of changes at all levels of linguistic  analysis. However, a special and conspicuous success has been achieved  in modeling changes in phonological systems, traditionally called  sound change. In the cases where we have access to several historical stages  -- for instance, the development of the modern Romance Languages from  Latin -- these sound changes are remarkably regular. Techniques developed  in such cases permit us to reconstruct the sound system -- and some of  the vocabulary -- of unattested parent languages from information about  daughter languages. In some cases, an old sound becomes a new sound across the board. Such  achange occurred in Hawai'ian, in that all the " t" sounds in an  older form of the language became " k"s: at the time Europeans encountered  Hawai'ian, there were no "t"s in it at all, though the closely  related languages Tahitian, Samoan, Tongan and Maori all have " t"s.Another  unconditioned  sound change that occurred between Middle  and Early Modern English (around Shakespeare's time) is known as the Great  Vowel Shift. At that time, there was a length distinction in the English  vowels, and the Great Vowel Shift altered the position of  all the  long vowels, in a giant rotation. The nucleus of the two high vowels (front "long i" /i:/, and the back  "long u" /u:/) started to drop, and the high position was retained only  in the offglide. Eventually, the original /i:/ became /ai/ - so a "long  i" vowel in Modern English is now pronounced /ai/ as in a word like 'bite':  /bait/. Similarly, the "long u" found its nucleus dropping all the way  to /au/: the earlier 'house' /hu:s/ became /haus/. All the other long  vowels rotated, the mid vowels /e:/ and /o:/ rising to fill the spots vacated by the former /i:/ and /u:/ respectively, and so on. That is why  the modern pronouns 'he' and 'she' are written with /e/ (reflecting the  old pronunciation) but pronounced as /i/. In the following chart, the  words are located where their vowel used to be pronounced  -- where they are pronounced today is indicated by the arrows. In other cases, a sound change may be "conditioned" so as to  apply in certain kinds of environments and not in others. For example,  it's very common for tongue-tip ("coronal") consonants to become  palatal when they are followed by high front vowels. The residue of this  process can be seen in English pairs like divide/division, fuse/fusion,  submit/submission, oppress/oppression.
Language change	Sociolinguistics	Historical linguistics	Diachronic linguistics
Explain the difference between Productive and Receptive Bilingualism?	Productive bilingualism is when a bilingual speaker and produce and understand speech in more than one language, this is different to receptive bilingualism where a person is able to comprehend and understand two languages well, however their production in both languages may be limited.	Mary grew up in a household where her mother spoke Italian and her father spoke English. Mary can understand both English and Italian yet she is only able to speak English. What type of bilingualism is this an example of?	Receptive Bilingualism	Productive Bilingualism	Bilingualism||slides||ˇˆˆ ˆˇ     ˛ˇ  ˘     ˛˚ ˚ ˛!ˇ   #˛ ˆ!$ ˛ ˆˆ     ˚$ˆ ˆ!%˚ %" "&$ ˆ!%˚ %" ˛  &$ˆ  ˆ ˆˆ %!%   ˜ !" )*$  ˆ+*$,˚-(˚-.-/ %0ˆ 12ˆ ˆ˛ 3$2 ˆˆ ˛& ˆ  ˘ ,4 ˆ/ ˆ  ˆ˙    5˛   & ˙ & ˇˇ    ˆ˙ $ˆ ˆˇ   ˆ  ˛ $˛  $   ˆ˙ ˆˇ   ˛  !  '! ˆˇ ˆˇˆ  ˛    !    ˆ˙ 3ˆ & + %˛˛'   ) (+& ˘ ) /01,  ˆ˙ ˘7 ' 7 'ˆ  ˆ    ˚˙ ˘ˇˇ  ˘ˇˇ ˛$˛&86ˆ ˚-9./  &˛ˆˇ ˆ ,  . + !ˆˆ ˆ ,:$ˆ% :; ˆˆˇ ,$;  ˙ ˘ ˇˆ  ˛ %ˆ˚-9 ˘ˆ ˇˆ   ˆ7   ˆˆˆˇ ˛ ˆˇ ˆ ˆ     ˆ ˆ ˘ ˆˆ  6ˆ ˆˇ ˆˆˇ ˇ  ˆˆ ˆ$   ˛ˇˇ      2$'& ˇ ˇˆ ˆ @ ,1 ˆ"CC./  ˆ ,"CC(/ ˆ ˛ˆ &˛˛ˇ˛ ˆ˛ &,$ˆ ;"CC4/        ˙ ˘! ˆ 1 D2ˆ ˆ     ˇ445 ˛˚ ˚˘!˘ ˇ   ,   /˘ˆˆ ˆ   " ˘!    ,   /!         6      6       6      6     6      ˛  E<  &;1:,˚--.5+$ˇ ˇ$ˇ2ˇ  ˇ  *,˚--H/˘ˆ&ˇ% ,"=+ˆ3˜ ˆ<,˚--˚ˇ6'$ˇ ˜ˇ + 3˜ :,"CC45& I&0    >,˚-94˜ %ˆ   ˝ 1 % 1˘"CC9/ˆ * 1 --H/ 1 ˚ ˆ J F%&ˆ2 #$; J˚-9./˙     J&˚-94/ˇˇ˘% % ˇ ˇ˝˛ F<%ˇˆ 2 ˆJ,˚-94/'=ˇ˘= %ˆ ˛  F<%ˇ&ˆˇ ˜˛=Jˆ5,˚-99/ ' =˘ˆˇ&  !"ˇˇ ˜J:˛ˇˆ% ,˚-9./ %˙ˇ=ˇ#    ˝ $ˇ*%0%==,˚-9./ˇ %˙ˇ2ˇˇ 5#ˇ ˜   ˇ 0,"C˚"/%˘ #˛<˚H"C˚"ˆ ˇˇ'E"C˚"CHEˇ' '''' ˝˛˚˙ 2Fˇ˘ ˚-H4'"CCC ,J˜"CC-/  ˆˆ&)0˜ˇˇ' #Jˇ,J#$˘ "CC./  %˜ˇ,˘ˆ*ˆK *CCH/ ˘#*0,0>/ ˘ ˆ,˚--C/  ˚ˆˆ!ˆˆ ˆˆˇ * ,ˆ/ 'ˆ$  ˇ ˜$ ˚C/ % 1ˆ* % #%2ˆ/  2ˇ˝ D,F Jˇ%$ˆˆ
Bilingualism	Applied linguistics	Language Production	Psycholinguistics	Key terms in linguistics
What are morphemes?	Morphemes are minimal linguistic unit that have a function, form, and meaning. It was traditionally believed that "words" were the smallest functional and structural units of any language. However, words can be broken down further into meaningful parts. For example, "disagree" can be broken up into two morphemes "agree" and "dis". "Disagree" is then a morphologically complex word. So is "cleaned" - the morpheme clean and past tense suffix -ed. -ed has no meaning on its own, only with a root word ex. play - played, stay - stayed etc.	What is the difference between a word and a morpheme?	A word is the smallest content that can convey meaning, while a morpheme is the smallest component of a word that can convey such.	A word has meaning, while a morpheme does not.|||There isn't always a difference.|||A word is made up of morphemes.	Words and Morphemes chapter of Essentials of Linguistics||document||HomeReadSign inESSENTIALS OF LINGUISTICSCONTENTS Search in book ÉChapter 6: Word Forms 6.1 WORDS AND MORPHEMESWhatÕs a word?  It seems almost silly to ask such a simple question, but if you think about it, the question doesnÕt have an obvious answer. A famous linguist named Ferdinand de Saussure said that a word is like a coin because it has two sides to it that can never be separated. One side of this metaphorical coin is the form of a word: the sounds (or letters) that combine to make the spoken or written word. The other side of the coin is the  meaning  of the word: the image or concept we have in our mind when we use the word. So a word is something that IncreaseFont Size links a given form with a given meaning. Linguists have also noticed that words behave in a way that other elements of mental grammar donÕt because words are  free. What does it mean for a word to be free? One observation that leads us to say that words are free is that they can appear in  isolation , on their own. In ordinary conversation, we donÕt often utter just a single word, but there are plenty of contexts in which a single word is indeed an entire utterance. Here are some examples: What are you doing?  Cooking. What are you cooking?  Soup. How does it taste?  Delicious. Can I have some?  No. Each of those single words is perfectly grammatical standing in isolation as the answer to a question. Another reason we say that words are free is that theyÕre  moveable : they can occupy a whole variety of di ! erent positions in a sentence.  Look at these examples: Penny is making  soup .Soup  is delicious. I love to eat  soup  when itÕs cold outside. The word  soup  can appear as the last word in a sentence, as the  " rst word, or in the middle of a sentence.  ItÕs free to be moved around.  The other important observation we can make about words is that theyÕre inseparable : We canÕt break them up by putting other pieces inside them. For example, in the sentence, Penny cooked some  carrots .The word  carrot  has a bit of information added to the end of it to show that thereÕs more than one carrot. But that bit of information canÕt go just anywhere:  it canÕt interrupt the word  carrot :*Penny cooked some car-s-rot. This might seem like a trivial observation Ð of course, you canÕt break words up into bits! Ð but if we look at a word thatÕs a little more complex than  carrots  we see that itÕs an important insight.  What about: Penny bought two  vegetable peelers .ThatÕs  " ne, but itÕs totally impossible to say: *Penny bought two  vegetables peeler .even though she probably uses the peeler to peel multiple vegetables.  ItÕs not that a plural  -s canÕt go on the end of the word  vegetable ; itÕs that the word vegetable peeler  is a single word (even though we spell it with a space between the two parts of it).  And because itÕs a single word, itÕs inseparable, so we canÕt add anything else into the middle of it. So weÕve seen that  a word is a free form that has a meaning .  But youÕve probably already noticed that there are other forms that have meaning and some of them seem to be smaller than whole words.  A morpheme is the smallest form that has meaning . Some morphemes are  free: they can appear in  isolation. (This means that some words are also morphemes.)  But some morphemes can only ever appear when theyÕre attached to something else; these are called  bound morphemes. LetÕs go back to that simple sentence, Penny cooked some carrots. ItÕs quite straightforward to say that this sentence has four words in it. We can make the observations we just discussed above to check for isolation, moveability, and inseparability to provide evidence that each of  Penny, cooked , some , andcarrots  is a word. But there are more than four units of meaning in the sentence. Penny cook-ed some carrot-s. The word  cooked  is made up of the word  cook  plus another small form that tells us that the cooking happened in the past.  And the word  carrots  is made up of carrot  plus a bit that tells us that thereÕs more than one carrot. That little bit thatÕs spelled  Ðed  (and pronounced a few di ! erent ways depending on the environment) has a consistent meaning in English: past tense.  We can easily think of several other examples where that form has that meaning, like  walked, baked, cleaned, kicked, kissed. This Ðed  unit appears consistently in this form and consistently has this meaning, but it never appears in isolation: itÕs always attached at the end of a word. ItÕs a  bound morpheme. For example, if someone tells you, ÒI need you to walk the dog,Ó itÕs not grammatical to answer Ò- edÓ to indicate that you already walked the dog. Likewise, the bit thatÕs spelled  Ðs  or  Ðes  (and pronounced a few di ! erent ways) has a consistent meaning in many di ! erent words, like  carrots, bananas, books, skates, cars, dishes , and many others.  Like  Ðed , it is not free: it canÕt appear in isolation.  ItÕs a  bound morpheme too.  LICENSEEssentials of Linguistics by Catherine Anderson is licensed under a CreativeIf a word is made up of just one morpheme, like  banana, swim, hungry , then we say that itÕs  morphologically simple , or  monomorphemic.But many words have more than one morpheme in them: theyÕre morphologically complex  or  polymorphemic. In English, polymorphemic words are usually made up of a  root plus one or more  a! xes . The root morpheme is the single morpheme that determines the core meaning of the word. In most cases in English, the root is a morpheme that could be free. The a # xes are bound morphemes. English has a # xes that attach to the end of a root; these are called su!xes, like in  book s, teaching, happier, hope ful , singer. And English also has a # xes that attach to the beginning of a word, called  pre"xes, likein unzip, reheat, disagree,  impossible .Some languages have bound morphemes that go into the middle of a word; these are called  in"xes. Here are some examples from Tagalog (a language with about 24 million speakers, most of them in the Philippines). [takbuh] run [tumakbuh] ran [lakad]walk[lumakad]walked[bili]buy[bumili]bought [kain]eat[kumain]ateIt might seem like the existence of in " xes is a problem for our claim above that words are inseparable. But languages that allow in " xation do so in a systematic way Ñ the in " x canÕt be dropped just anywhere in the word. In Tagalog, the position of the in " x depends on the organization of the syllables in the word.  Essentials of Linguistics by Catherine Anderson is licensed under a CreativeCommons Attribution-ShareAlike 4.0 International License, except whereotherwise noted.Powered by PressbooksGuides and Tutorials |Contact Previous: Chapter 6: Word Forms Next: 6.2 Allomorphs
Morphemes	Morphology
Identify at least three ambiguities of different kinds in the following sentence, explain their interpretations, and classify each as lexical or structural, and what kind of structural: “They served the wealthy women and men at the meeting.”	Here are the three most obvious different types of ambiguity; others are possible:  (1) a structural ambiguity of coordination: [wealthy women] and [men] OR [noble [women and men]]? (2) a lexical ambiguity of "served": acted as servants to them, or served them as food? (further interpretations of served ARE possible) (3) structural ambiguity of attachment: does "at the meeting" modify the VP headed by "served" (performed the service while at the conference) or does "does at the meeting" modify the NP, meaning that it was the men and women who attended the meeting who were served, but not necessarily while at the meeting.  NB: (1) and (3) combine, such that they can also be interpreted as serving the "wealthy women" (who were not necessarily at the meeting) and the men who were at the meeting, among other possible interpretations!	Which of the following does not describe the meaning of some grammatically acceptable parse of the following sentence?: “Time flies like an arrow.”	All of these represent valid parses.	It commands us to time an arrow, as if with a stopwatch.|||It informs us of the nature of time.|||It informs Time of the nature of flies.|||It commands flies.	Chapter Section on ambiguity in parsing||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 13 ConstituencyParsing OnemorningIshotanelephantinmypajamas. HowhegotintomypajamasIdon'tknow. GrouchoMarx, AnimalCrackers ,1930 Syntacticparsingisthetaskofrecognizingasentenceandassigningasyntactic structuretoit.Thischapterfocusesonthestructuresassignedbycontext-freegram- marsofthekinddescribedinChapter12.Sincetheyarebasedonapurelydeclar- ativeformalism,context-freegrammarsdon'tspecify how theparsetreeforagiven sentenceshouldbecomputed.Wethereforeneedtospecifyalgorithmsthatemploy thesegrammarstoefproducecorrecttrees. Parsetreesaredirectlyusefulinapplicationssuchas grammarchecking in word-processingsystems:asentencethatcannotbeparsedmayhavegrammatical errors(oratleastbehardtoread).Moretypically,however,parsetreesserveasan importantintermediatestageofrepresentationfor semanticanalysis (asweshowin Chapter17)andthusplayanimportantroleinapplicationslike questionanswering and informationextraction .Forexample,toanswerthequestion WhatbookswerewrittenbyBritishwomenauthorsbefore1800? we'llneedtoknowthatthesubjectofthesentencewas whatbooks andthattheby- adjunctwas Britishwomenauthors tohelpusoutthattheuserwantsalistof books(andnotalistofauthors). Beforepresentinganyalgorithms,webeginbydiscussinghowtheambiguity arisesagaininthiscontextandtheproblemsitpresents.Thesectionthatfol- lowsthenpresentstheCocke-Kasami-Younger(CKY)algorithm( Kasami1965 , Younger1967 ),thestandarddynamicprogrammingapproachtosyntacticparsing. Recallthatwe'vealreadyseenapplicationsofdynamicprogrammingalgorithmsin theMinimum-Edit-DistanceandViterbialgorithmsofearlierchapters.Finally,we discuss partialparsingmethods ,foruseinsituationsinwhichasyntac- ticanalysisofaninputmaybesuf 13.1Ambiguity Ambiguityisperhapsthemostseriousproblemfacedbysyntacticparsers.Chap- ter8introducedthenotionsof part-of-speechambiguity and part-of-speechdis- ambiguation .Here,weintroduceanewkindofambiguity,called structuralambi- guity ,whicharisesfrommanycommonlyusedrulesinphrase-structuregrammars. structural ambiguity Toillustratetheissuesassociatedwithstructuralambiguity,we'llmakeuseofanew toygrammar L 1 ,showninFigure 13.1 ,whichconsistsofthe L 0 grammarfromthe lastchapteraugmentedwithafewadditionalrules. Structuralambiguityoccurswhenthegrammarcanassignmorethanoneparse toasentence.GrouchoMarx'swell-knownlineasCaptainSpauldingin Animal  2 C HAPTER 13  C ONSTITUENCY P ARSING Grammar Lexicon S ! NPVP Det ! that j this j the j a S ! AuxNPVP Noun ! book j  j meal j money S ! VP Verb ! book j include j prefer NP ! Pronoun Pronoun ! I j she j me NP ! Proper-Noun Proper-Noun ! Houston j NWA NP ! DetNominal Aux ! does Nominal ! Noun Preposition ! from j to j on j near j through Nominal ! NominalNoun Nominal ! NominalPP VP ! Verb VP ! VerbNP VP ! VerbNPPP VP ! VerbPP VP ! VPPP PP ! PrepositionNP Figure13.1 The L 1 miniatureEnglishgrammarandlexicon. S VP NP Nominal PP inmypajamas Nominal Noun elephant Det an Verb shot NP Pronoun I S VP PP inmypajamas VP NP Nominal Noun elephant Det an Verb shot NP Pronoun I Figure13.2 Twoparsetreesforanambiguoussentence.Theparseontheleftcorrespondstothehumorous readinginwhichtheelephantisinthepajamas,theparseontherightcorrespondstothereadinginwhich CaptainSpauldingdidtheshootinginhispajamas. Crackers isambiguousbecausethephrase inmypajamas canbepartofthe NP headedby elephant orapartoftheverbphraseheadedby shot .Figure 13.2 illus- tratesthesetwoanalysesofMarx'slineusingrulesfrom L 1 . Structuralambiguity,appropriatelyenough,comesinmanyforms.Twocommon kindsofambiguityare attachmentambiguity and coordinationambiguity . Asentencehasan attachmentambiguity ifaparticularconstituentcanbeat- attachment ambiguity tachedtotheparsetreeatmorethanoneplace.TheGrouchoMarxsentenceis anexampleof PP -attachmentambiguity.Variouskindsofadverbialphrasesare alsosubjecttothiskindofambiguity.Forinstance,inthefollowingexamplethe gerundive- VPtoParis canbepartofagerundivesentencewhosesubjectis theEiffelTower oritcanbeanadjunctmodifyingthe VP headedby saw : (13.1) WesawtheEiffelTowertoParis.  13.2  CKYP ARSING :AD YNAMIC P ROGRAMMING A PPROACH 3 In coordinationambiguity differentsetsofphrasescanbeconjoinedbyacon- coordination ambiguity junctionlike and .Forexample,thephrase oldmenandwomen canbebracketedas [old[menandwomen]] ,referringto oldmen and oldwomen ,oras [oldmen]and [women] ,inwhichcaseitisonlythemenwhoareold. Theseambiguitiescombineincomplexwaysinrealsentences.Aprogramthat summarizedthenews,forexample,wouldneedtobeabletoparsesentenceslike thefollowingfromtheBrowncorpus: (13.2) PresidentKennedytodaypushedasideotherWhiteHousebusinessto devoteallhistimeandattentiontoworkingontheBerlincrisisaddresshe willdelivertomorrownighttotheAmericanpeopleovernationwide televisionandradio. Thissentencehasanumberofambiguities,althoughsincetheyaresemantically unreasonable,itrequiresacarefulreadingtoseethem.Thelastnounphrasecouldbe parsed [nationwide[televisionandradio]] or [[nationwidetelevision]andradio] . Thedirectobjectof pushedaside shouldbe otherWhiteHousebusiness butcould alsobethebizarrephrase [otherWhiteHousebusinesstodevoteallhistimeand attentiontoworking] (i.e.,astructurelike Kennedyaf[hisintentiontopropose anewbudgettoaddressthe ).Thenthephrase ontheBerlincrisisaddresshe willdelivertomorrownighttotheAmericanpeople couldbeanadjunctmodifying theverb pushed .A PP like overnationwidetelevisionandradio couldbeattached toanyofthehigher VP sor NP s(e.g.,itcouldmodify people or night ). Thefactthattherearemanygrammaticallycorrectbutsemanticallyunreason- ableparsesfornaturallyoccurringsentencesisanirksomeproblemthataffectsall parsers.Ultimately,mostnaturallanguageprocessingsystemsneedtobeableto chooseasinglecorrectparsefromthemultitudeofpossibleparsesthroughaprocess of syntacticdisambiguation .Effectivedisambiguationalgorithmsrequirestatisti- syntactic disambiguation cal,semantic,andcontextualknowledgesourcesthatvaryinhowwelltheycanbe integratedintoparsingalgorithms. Fortunately,theCKYalgorithmpresentedinthenextsectionisdesignedtoef cientlyhandlestructuralambiguitiesofthekindwe'vebeendiscussing.Andaswe'll seeinChapter14,therearestraightforwardwaystointegratestatisticaltechniques intothebasicCKYframeworktoproducehighlyaccurateparsers. 13.2CKYParsing:ADynamicProgrammingApproach Theprevioussectionintroducedsomeoftheproblemsassociatedwithambiguous grammars.Fortunately, dynamicprogramming providesapowerfulframeworkfor addressingtheseproblems,justasitdidwiththeMinimumEditDistance,Viterbi, andForwardalgorithms.Recallthatdynamicprogrammingapproachessystemati- callyintablesofsolutionstosub-problems.Whencomplete,thetablescontain thesolutiontoallthesub-problemsneededtosolvetheproblemasawhole.In thecaseofsyntacticparsing,thesesub-problemsrepresentparsetreesforallthe constituentsdetectedintheinput. Thedynamicprogrammingadvantagearisesfromthecontext-freenatureofour grammarrulesŠonceaconstituenthasbeendiscoveredinasegmentoftheinput wecanrecorditspresenceandmakeitavailableforuseinanysubsequentderivation thatmightrequireit.Thisprovidesbothtimeandstorageefsincesubtrees canbelookedupinatable,notreanalyzed.ThissectionpresentstheCocke-Kasami-
Constituency parsing	Syntactic ambiguity	Ambiguity	Parsing
What is a key similarity between Naive Bayes and Logistic Regression?	Both Naive Bayes and Logistic Regression are probabilistic machine learning classifiers, which means they output a probabilistic distribution of the likelihood an input is each class. Both Naive Bayes and Logistic Regression train models over a training corpus to then be able to take a feature representation of the input and output probabilities of each class. (Note: a classification function such as the sigmoid function may be needed or probability distributions may need to be normalized to get a probability as a percentage likelihood.)	A probabilistic machine learning classifier takes in input, converts the input to ________, and outputs ________.	a feature representation; a probability distribution	a numeric value; a probability distribution|||a feature representation; a prediction label|||a numeric value; a prediction label|||word embeddings; a prediction label	Speech and Language Processing: Logistic Regression||publication||2 C HAPTER 5  L OGISTIC R EGRESSION Moreformally,recallthatthenaiveBayesassignsaclass c toadocument d not bydirectlycomputing P ( c j d ) butbycomputingalikelihoodandaprior ‹ c = argmax c 2 C likelihood z }| { P ( d j c ) prior z }| { P ( c ) (5.1) A generativemodel likenaiveBayesmakesuseofthis likelihood term,which generative model expresseshowtogeneratethefeaturesofadocument ifweknewitwasofclassc . Bycontrasta discriminativemodel inthistextcategorizationscenarioattempts discriminative model to directly compute P ( c j d ) .Perhapsitwilllearntoassignahighweighttodocument featuresthatdirectlyimproveitsabilityto discriminate betweenpossibleclasses, evenifitcouldn'tgenerateanexampleofoneoftheclasses. Componentsofaprobabilisticmachinelearning LikenaiveBayes, logisticregressionisaprobabilisticthatmakesuseofsupervisedmachine learning.Machinelearningrequireatrainingcorpusof M input/output pairs ( x ( i ) ; y ( i ) ) .(We'llusesuperscriptsinparenthesestorefertoindividualinstances inthetrainingsetŠforsentimentcleachinstancemightbeanindividual documenttobeAmachinelearningsystemforthenhas fourcomponents: 1. A featurerepresentation oftheinput.Foreachinputobservation x ( i ) ,this willbeavectoroffeatures [ x 1 ; x 2 ;:::; x n ] .Wewillgenerallyrefertofeature i forinput x ( j ) as x ( j ) i ,sometimesas x i ,butwewillalsoseethe notation f i , f i ( x ) ,or,formulticlass f i ( c ; x ) . 2. Afunctionthatcomputes‹ y ,theestimatedclass,via p ( y j x ) .In thenextsectionwewillintroducethe sigmoid and softmax toolsfor cation. 3. Anobjectivefunctionforlearning,usuallyinvolvingminimizingerroron trainingexamples.Wewillintroducethe cross-entropylossfunction 4. Analgorithmforoptimizingtheobjectivefunction.Weintroducethe stochas- ticgradientdescent algorithm. Logisticregressionhastwophases: training: wetrainthesystemtheweights w and b )usingstochastic gradientdescentandthecross-entropyloss. test: Givenatestexample x wecompute p ( y j x ) andreturnthehigherprobability label y = 1or y = 0. 5.1thesigmoid Thegoalofbinarylogisticregressionistotrainathatcanmakeabinary decisionabouttheclassofanewinputobservation.Hereweintroducethe sigmoid thatwillhelpusmakethisdecision. Considerasingleinputobservation x ,whichwewillrepresentbyavectorof features [ x 1 ; x 2 ;:::; x n ] (we'llshowsamplefeaturesinthenextsubsection).Theclas- output y canbe1(meaningtheobservationisamemberoftheclass)or0 (theobservationisnotamemberoftheclass).Wewanttoknowtheprobability P ( y = 1 j x ) thatthisobservationisamemberoftheclass.Soperhapsthedecision
Logistic regression	Probabilistic language model	Text classification	Natural Language Processing (NLP)	Naive Bayes
How are Naive Bayes classifiers trained?	When training a Naive Bayes classifier, we want to build a table of probabilities as our model. For instance, consider the task of sentiment analysis with three labels: 'Positive', 'Negative', and 'Neutral'. In our training data, the word "disappointing" may appear 100 times out of 10,000 total words with the class 'Negative', 5 times out of 5,000 total words with the class 'Neutral', and 2 times out of 10,000 total words with the class 'Positive'. Then, in testing, when we run into the word "disappointing," we use the probabilities we built in training to determine a score of likelihood for each of the classes (which can be normalized to show a likelihood percentage for each class), as follows:$$ \hat P(“disappointing”| `Negative`) = {count(“disappointing”, `Negative`) \over \sum_{w \in V} count(w, `Negative`)} = {100 \over 10,000 } = 0.01$$$$ \hat P(“disappointing” | `Neutral`) = {count(“disappointing”, `Neutral`) \over \sum_{w \in V} count(w, `Neutral`)} = {5 \over 5,000 } = 0.001$$$$ \hat P(“disappointing” | `Positive`) = {count(“disappointing”, `Positive`) \over \sum_{w \in V} count(w, `Positive`)} = {2 \over 10,000 } = 0.0002$$Smoothing can be applied to help fix the problem of if certain words appear in the training data, but not as every class (for instance, a word "smart" could appear in 'Positive' and 'Neutral' examples, but not in 'Negative', and thus would cause a probability of 0 for \(\hat P(“smart”| `Negative`)\).	Consider a Naive Bayes classifier trained with the following data without smoothing: '+': "good good good good okay." | '-': "bad bad bad okay." What issue(s) arise when the model sees the following sentence in testing: "bad good good good okay amazing."	"good" only appears as '+' and "bad" only appears as '-'. Thus, the model considers the probability of the sentence to be 0 for both '+' and '-'.	"amazing" does not occur in the training data, and thus the probability of the sentence is considered to be 0 for both '+' and '-'.|||No issues arise––the model will ignore "bad" and "good" since they do not appear in the training data as both classes, and "amazing" since it does not appear in the training data as either class. The model then will find that \( \hat P(“okay”, `-`) > \hat P(“okay”, `+`) \).|||Both A and B.	Naive Bayes presentation||slides||!"#$%&'())*+*,($*-.% (./%0( 12"%3(4") ;%k:#O%6#&5$ d#%90.0N  H"(<.*.A%$6"% >?'$*.-8*('%0( 12"3(4")%>-/"' ¥R.9&+$%++#7G+5$7%*.787$-.'#-."((D$#&+.7%+#& ¥&.7G-6$8&#$+"#$)9#o8#0/.#&$.0$+"#$D%+% Sec.13.3 öP(wi|cj)=count(wi,cj)count(w,cj)w!V"öP(cj)=doccount(C=cj)Ndoc ¥,9#%+#$ 7#N% 4D(/87#0+$)(9$+(G./$ 8L6$/(0/%+#0%+.0N$%--$D(/&$.0$ +".&$+(G./ ¥>&#$)9#o8#0/6$()$ 9.0$7#N% 4D(/87#0+ @(<(8"$"<%")$*8($*-. )9%/+.(0$()$+.7#&$ <(9D$ 97%GG#%9&$ %7(0N$%--$<(9D&$ .0$D(/87#0+&$()$+(G./$ )8öP(wi|cj)=count(wi,cj)count(w,cj)w!V" @<-B'"8%;*$6%>(#*8?8%H*P"'*6--/ ¥M"%+$.)$<#$"%:#$&##0$0($+9%.0.0N$D(/87#0+&$<.+"$+"#$<(9D$ !"#$"%$&' %0D$/-%&&.).#D$.0$+"#$ +(G./$ 7-)*$*2" i$()*+% ,)-. a¥p#9($ G9(L%L.-.+.#&$/%00(+$L#$/(0D.+.(0#D$%<%6B$0($7%++#9$ +"#$(+"#9$#:.D#0/#q öP("fantastic" positive) = count("fantastic", positive)count(w,positivew!V") = 0cMAP=argmaxcöP(c)öP(xi|c)i!Sec.13.3  H(7'(,"%M(// GQN%)8--$6*.A%+-<%0( 12"3(4") öP(wi|c)=count(wi,c)+1count(w,c)+1()w!V"=count(wi,c)+1count(w,cw!V")#$%%&'(( + VöP(wi|c)=count(wi,c)count(w,c)()w!V" >?'$*.-8*('%0(12"%3(4")F%H"(<.*.A ¥,%-/8-%+#$ 5i)8j+#97& ¥R(9$#%/"$ )8.0$ 'D(&:); 8"%--$D(/&$<.+"$$/-%&&$e )8P(wk|cj)!nk+!n+!|Vocabulary|P(cj)!|docsj||total # documents|¥,%-/8-%+#$ 5i9<m)8j+#97& ¥=>6% 8"&.0N-#$D(/$/(0+%.0.0N$%--$ &:); 8¥R(9 #%/"$<(9D$ 9<.0$ ?:)@A$B@CD "<"r$()$(//899#0/#&$()$ 9<.0$ =>6% 8¥R9(7$+9%.0.0N$/(9G8&B$#*+9%/+$ Vocabulary|||Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||4.2  T RAININGTHE N AIVE B AYES C LASSIFIER 5 NaiveBayescalculations,likecalculationsforlanguagemodeling,aredoneinlog space,toavoidwandincreasespeed.ThusEq. 4.9 isgenerallyinstead expressedas c NB = argmax c 2 C log P ( c )+ X i 2 positions log P ( w i j c ) (4.10) Byconsideringfeaturesinlogspace,Eq. 4.10 computesthepredictedclassasalin- earfunctionofinputfeatures.thatusealinearcombinationoftheinputs tomakeadecisionŠlikenaiveBayesandalsologisticregressionŠ arecalled linear . linear  4.2TrainingtheNaiveBayes Howcanwelearntheprobabilities P ( c ) and P ( f i j c ) ?Let'sconsiderthemax- imumlikelihoodestimate.We'llsimplyusethefrequenciesinthedata.Forthe documentprior P ( c ) weaskwhatpercentageofthedocumentsinourtrainingset areineachclass c .Let N c bethenumberofdocumentsinourtrainingdatawith class c and N doc bethetotalnumberofdocuments.Then: ‹ P ( c )= N c N doc (4.11) Tolearntheprobability P ( f i j c ) ,we'llassumeafeatureisjusttheexistenceofaword inthedocument'sbagofwords,andsowe'llwant P ( w i j c ) ,whichwecomputeas thefractionoftimestheword w i appearsamongallwordsinalldocumentsoftopic c .Weconcatenatealldocumentswithcategory c intoonebigﬁcategory c ﬂtext. Thenweusethefrequencyof w i inthisconcatenateddocumenttogiveamaximum likelihoodestimateoftheprobability: ‹ P ( w i j c )= count ( w i ; c ) P w 2 V count ( w ; c ) (4.12) HerethevocabularyVconsistsoftheunionofallthewordtypesinallclasses,not justthewordsinoneclass c . Thereisaproblem,however,withmaximumlikelihoodtraining.Imaginewe aretryingtoestimatethelikelihoodofthewordﬁfantasticﬂgivenclass positive ,but supposetherearenotrainingdocumentsthatbothcontainthewordﬁfantasticﬂand areclasas positive .Perhapsthewordﬁfantasticﬂhappenstooccur(sarcasti- cally?)intheclass negative .Insuchacasetheprobabilityforthisfeaturewillbe zero: ‹ P ( ﬁfantasticﬂ j positive )= count ( ﬁfantasticﬂ ; positive ) P w 2 V count ( w ; positive ) = 0 (4.13) ButsincenaiveBayesnaivelymultipliesallthefeaturelikelihoodstogether,zero probabilitiesinthelikelihoodtermforanyclasswillcausetheprobabilityofthe classtobezero,nomattertheotherevidence! Thesimplestsolutionistheadd-one(Laplace)smoothingintroducedinChap- ter3.WhileLaplacesmoothingisusuallyreplacedbymoresophisticatedsmoothing  6 C HAPTER 4  N AIVE B AYESAND S ENTIMENT C LASSIFICATION algorithmsinlanguagemodeling,itiscommonlyusedinnaiveBayestextcatego- rization: ‹ P ( w i j c )= count ( w i ; c )+ 1 P w 2 V ( count ( w ; c )+ 1 ) = count ( w i ; c )+ 1  P w 2 V count ( w ; c )  + j V j (4.14) NoteonceagainthatitiscrucialthatthevocabularyVconsistsoftheunionofallthe wordtypesinallclasses,notjustthewordsinoneclass c (trytoconvinceyourself whythismustbetrue;seetheexerciseattheendofthechapter). Whatdowedoaboutwordsthatoccurinourtestdatabutarenotinourvocab- ularyatallbecausetheydidnotoccurinanytrainingdocumentinanyclass?The solutionforsuch unknownwords istoignorethemŠremovethemfromthetest unknownword documentandnotincludeanyprobabilityforthematall. Finally,somesystemschoosetocompletelyignoreanotherclassofwords: stop words ,veryfrequentwordslike the and a .Thiscanbedonebysortingthevocabu- stopwords larybyfrequencyinthetrainingset,andthetop10Œ100vocabularyentries asstopwords,oralternativelybyusingoneofthemanystopwordlist availableonline.Theneveryinstanceofthesestopwordsaresimplyremovedfrom bothtrainingandtestdocumentsasiftheyhadneveroccurred.Inmosttextclassi- applications,however,usingastopwordlistdoesn'timproveperformance, andsoitismorecommontomakeuseoftheentirevocabularyandnotuseastop wordlist. Fig. 4.2 showsthealgorithm. function T RAIN N AIVE B AYES (D,C) returns log P ( c ) andlog P ( w j c ) foreach class c 2 C #Calculate P ( c ) terms N doc =numberofdocumentsinD N c =numberofdocumentsfromDinclassc logprior [c]   log N c N doc V   vocabularyofD bigdoc [ c ]   append (d) for d 2 D with class c foreach word w inV#Calculate P ( w j c ) terms count(w,c)   #ofoccurrencesof w in bigdoc [ c ] loglikelihood [w,c]   log count ( w ; c )+ 1 P w 0 inV ( count ( w 0 ; c )+ 1 ) return logprior , loglikelihood , V function T EST N AIVE B AYES ( testdoc , logprior , loglikelihood ,C,V) returns best c foreach class c 2 C sum [ c ]   logprior [ c ] foreach position i in testdoc word   testdoc[i] if word 2 V sum [ c ]   sum [ c ]+ loglikelihood [ word , c ] return argmax c sum [ c ] Figure4.2 ThenaiveBayesalgorithm,usingadd-1smoothing.Touseadd- a smoothing instead,changethe + 1to + a forloglikelihoodcountsintraining.
Naive Bayes	Natural Language Processing (NLP)	Statistical Natural Language Processing (NLP)	Generative classifiers	Text classification	Sentiment analysis	Probabilistic language model
What's the difference between supervised and unsupervised machine learning techniques?	Supervised learning maps an output to an input (either a label or number), while unsupervised learning aims to identify patterns in a dataset without knowing outputs to predict. Supervised learning trains over a set of known input-output data to then form a model, such as a neural network or a perceptron, to classify new points, while unsupervised learning analyzes data, such as clustering similar data points that a human may be able to then identify a label for the clustered groups.	A machine learning model that predicts a price to sell a house based on various features such as its size in square feet, bedroom count, and neighborhood quality is an example of which type of machine learning technique?	Supervised learning	Unsupervised learning	Natural Language Processing (NLP) Presentation||slides||0G-B)JK+BI#J+T#UH+G-B)JK+BI#"BE)HKHF !#+*-.H%;-BE$k(+$O&(J$,=-$>@A->$,($*.-B%/, !F-?.-;;%(&E$G+)-.%/@>$.-;+>, !'>@;;%0%/@,%(&E$L%;,%&/,$?.(+*$.-;+>, !1&;+*-.H%;-BE$k(+$B(&`,$O&(J$,=-$>@A->$,($*.-B%/, !'>+;,-.%&?E$#%)%>@.$?.(+*; !2&@><;%;
Statistical Natural Language Processing (NLP)	Machine learning	Machine learning (ML) overview	Supervised learning	Unsupervised learning
How are unknown words commonly handled by Naive Bayes classifiers in testing?	Naive Bayes classifiers will typically ignore unknown words in testing. For example, if a sentence "that city has the most beautiful sunsets every evening" is seen in testing, and "sunsets" was not seen in training, the model would treat the sentence as "that city has the most beautiful every evening".	Consider a sentiment analysis Naive Bayes classifier with 3 labels: 'Positive', 'Neutral', and 'Negative'. The word "tremendously" appears once in training with the 'Positive' label, but never as 'Neutral' or 'Negative'. How should a sentence with this word be handled in testing?	It should be handled normally, since smoothing should handle the issue of the word not appearing in all classes.	It should automatically classified as 'Positive', since "tremendously" must always have a positive label.|||It should be ignored as an unknown word, since it did not occur as every class in training.	Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||6 C HAPTER 4  N AIVE B AYESAND S ENTIMENT C LASSIFICATION algorithmsinlanguagemodeling,itiscommonlyusedinnaiveBayestextcatego- rization: ‹ P ( w i j c )= count ( w i ; c )+ 1 P w 2 V ( count ( w ; c )+ 1 ) = count ( w i ; c )+ 1  P w 2 V count ( w ; c )  + j V j (4.14) NoteonceagainthatitiscrucialthatthevocabularyVconsistsoftheunionofallthe wordtypesinallclasses,notjustthewordsinoneclass c (trytoconvinceyourself whythismustbetrue;seetheexerciseattheendofthechapter). Whatdowedoaboutwordsthatoccurinourtestdatabutarenotinourvocab- ularyatallbecausetheydidnotoccurinanytrainingdocumentinanyclass?The solutionforsuch unknownwords istoignorethemŠremovethemfromthetest unknownword documentandnotincludeanyprobabilityforthematall. Finally,somesystemschoosetocompletelyignoreanotherclassofwords: stop words ,veryfrequentwordslike the and a .Thiscanbedonebysortingthevocabu- stopwords larybyfrequencyinthetrainingset,andthetop10Œ100vocabularyentries asstopwords,oralternativelybyusingoneofthemanystopwordlist availableonline.Theneveryinstanceofthesestopwordsaresimplyremovedfrom bothtrainingandtestdocumentsasiftheyhadneveroccurred.Inmosttextclassi- applications,however,usingastopwordlistdoesn'timproveperformance, andsoitismorecommontomakeuseoftheentirevocabularyandnotuseastop wordlist. Fig. 4.2 showsthealgorithm. function T RAIN N AIVE B AYES (D,C) returns log P ( c ) andlog P ( w j c ) foreach class c 2 C #Calculate P ( c ) terms N doc =numberofdocumentsinD N c =numberofdocumentsfromDinclassc logprior [c]   log N c N doc V   vocabularyofD bigdoc [ c ]   append (d) for d 2 D with class c foreach word w inV#Calculate P ( w j c ) terms count(w,c)   #ofoccurrencesof w in bigdoc [ c ] loglikelihood [w,c]   log count ( w ; c )+ 1 P w 0 inV ( count ( w 0 ; c )+ 1 ) return logprior , loglikelihood , V function T EST N AIVE B AYES ( testdoc , logprior , loglikelihood ,C,V) returns best c foreach class c 2 C sum [ c ]   logprior [ c ] foreach position i in testdoc word   testdoc[i] if word 2 V sum [ c ]   sum [ c ]+ loglikelihood [ word , c ] return argmax c sum [ c ] Figure4.2 ThenaiveBayesalgorithm,usingadd-1smoothing.Touseadd- a smoothing instead,changethe + 1to + a forloglikelihoodcountsintraining.  4.3  W ORKEDEXAMPLE 7 4.3Workedexample Let'swalkthroughanexampleoftrainingandtestingnaiveBayeswithadd-one smoothing.We'lluseasentimentanalysisdomainwiththetwoclassespositive (+)andnegative(-),andtakethefollowingminiaturetrainingandtestdocuments fromactualmoviereviews. Cat Documents Training - justplainboring - entirelypredictableandlacksenergy - nosurprisesandveryfewlaughs + verypowerful + themostfunofthesummer Test ? predictablewithnofun Theprior P ( c ) forthetwoclassesiscomputedviaEq. 4.11 as N c N doc : P (  )= 3 5 P (+)= 2 5 Theword with doesn'toccurinthetrainingset,sowedropitcompletely(as mentionedabove,wedon'tuseunknownwordmodelsfornaiveBayes).Thelike- lihoodsfromthetrainingsetfortheremainingthreewordsﬁpredictableﬂ,ﬁnoﬂ,and ﬁfunﬂ,areasfollows,fromEq. 4.14 (computingtheprobabilitiesfortheremainder ofthewordsinthetrainingsetisleftasanexerciseforthereader): P ( ﬁpredictableﬂ j )= 1 + 1 14 + 20 P ( ﬁpredictableﬂ j +)= 0 + 1 9 + 20 P ( ﬁnoﬂ j )= 1 + 1 14 + 20 P ( ﬁnoﬂ j +)= 0 + 1 9 + 20 P ( ﬁfunﬂ j )= 0 + 1 14 + 20 P ( ﬁfunﬂ j +)= 1 + 1 9 + 20 ForthetestsentenceS=ﬁpredictablewithnofunﬂ,afterremovingtheword`with', thechosenclass,viaEq. 4.9 ,isthereforecomputedasfollows: P (  ) P ( S j )= 3 5  2  2  1 34 3 = 6 : 1  10  5 P (+) P ( S j +)= 2 5  1  1  2 29 3 = 3 : 2  10  5 Themodelthuspredictstheclass negative forthetestsentence. 4.4OptimizingforSentimentAnalysis WhilestandardnaiveBayestextcanworkwellforsentimentanalysis, somesmallchangesaregenerallyemployedthatimproveperformance. First,forsentimenttionandanumberofothertexttasks, whetherawordoccursornotseemstomattermorethanitsfrequency.Thusit oftenimprovesperformancetoclipthewordcountsineachdocumentat1(see theendofthechapterforpointerstotheseresults).Thisvariantiscalled binary
Naive Bayes	Natural Language Processing (NLP)	Text classification	Unknown tokens	Sentiment analysis	Smoothing	Add-one (Laplace)
Who was Eliza and what was 'her' primary purpose?	Eliza was a ''chatbot that was created in the mid 1960's at MIT University. The program was able to identify basic syntactic rules in order to understand and generate content. The primary purpose was for her to act as a therapist for users to talk to.	Was Eliza perfectly programmed?	No	Yes	NLP Crash Course clip||youtube||N/A
Computer science	Chatbot	Natural Language Processing (NLP)	Branches of linguistics
Describe the difference between building a sentiment analysis classifier that classifies a text as positive or negative vs. one that classifies a text on a scale between 1-5 in terms of likability.	A positive vs. negative sentiment analysis classifier is a binary classifier, and thus only requires one classifier (which would return the answer to the question, "Is this positive?"). A 1-5 sentiment analysis classifier could be a multiclass classifier with five specific labels ("1", "2", "3", "4", or "5"), which would each return a probability for that number and the classifier would select the label with the highest probability. Alternatively, a 1-5 sentiment analysis classifier could be treated as a regression problem and return the closest whole number between 1-5 as returned by the model.	What type of text classifier would be used to determine the sentiment of text as either positive, neutral, or negative?	Multinomial classifier	Binary classifier|||Multilabel classifier	Sentiment Analysis Presentation||slides||!"#$%&"#$'(#)*+,%, ¥(&+4-)'$%$#'HF ¥>'%$")%#$$&$03)%58%$"&'%$)A$%45'&$&2)%5;%*)6#$&2)/ ¥K5;)%:5+4-)AF ¥]#*H%$")%#$$&$03)%58%$"&'%$)A$%8;5+%N%$5%b ¥,32#*:)3F ¥Y)$):$%$")%$#;6)$<%'50;:)<%5;%:5+4-)A%#$$&$03)%$.4)'|||Speech and Language Processing: Naive Bayes and Sentiment Classification||publication||4.7  E VALUATION :P RECISION ,R ECALL ,F- MEASURE 13 4.7.1Morethantwoclasses Uptonowwehavebeenassumingtextclassitaskswithonlytwoclasses. Butlotsoftasksinlanguageprocessinghavemorethantwoclasses. Forsentimentanalysiswegenerallyhave3classes(positive,negative,neutral)and evenmoreclassesarecommonfortaskslikepart-of-speechtagging,wordsense disambiguation,semanticrolelabeling,emotiondetection,andsoon. Therearetwokindsofmulti-classtasks.In any-of or multi-label   ,eachdocumentoritemcanbeassignedmorethanonelabel.Wecan solve any-of bybuildingseparatebinaryclassiforeachclass c , trainedonpositiveexampleslabeled c andnegativeexamplesnotlabeled c .Given atestdocumentoritem d ,theneachmakestheirdecisionindependently, andwemayassignmultiplelabelsto d . Morecommoninlanguageprocessingis one-of or multinomial ,  multinomial  inwhichtheclassesaremutuallyexclusiveandeachdocumentoritemappearsin exactlyoneclass.Hereweagainbuildaseparatebinarytrainedonpositive examplesfrom c andnegativeexamplesfromallotherclasses.Nowgivenatest documentoritem d ,werunalltheandchoosethelabelfromthe withthehighestscore.Considerthesampleconfusionmatrixforahypothetical3- way one-of emailcategorizationdecision(urgent,normal,spam)showninFig. 4.5 . Figure4.5 Confusionmatrixforathree-classcategorizationtask,showingforeachpairof classes ( c 1 ; c 2 ) ,howmanydocumentsfrom c 1 were(in)correctlyassignedto c 2 Thematrixshows,forexample,thatthesystemmistakenlylabeled1spamdoc- umentasurgent,andwehaveshownhowtocomputeadistinctprecisionandrecall valueforeachclass.Inordertoderiveasinglemetricthattellsushowwellthe systemisdoing,wecancombinethesevaluesintwoways.In macroaveraging ,we macroaveraging computetheperformanceforeachclass,andthenaverageoverclasses.In microav- eraging ,wecollectthedecisionsforallclassesintoasinglecontingencytable,and microaveraging thencomputeprecisionandrecallfromthattable.Fig. 4.6 showsthecontingency tableforeachclassseparately,andshowsthecomputationofmicroaveragedand macroaveragedprecision. Astheshows,amicroaverageisdominatedbythemorefrequentclass(in thiscasespam),sincethecountsarepooled.Themacroaveragebetterthe statisticsofthesmallerclasses,andsoismoreappropriatewhenperformanceonall theclassesisequallyimportant.
Text classification	Multinomial classification (one-of classification)	Multiclass classification	Sentiment analysis	Natural Language Processing (NLP)	Binary classification
In linguistic terminology, what is a morph?	A morph is a word segment that represents one morpheme (the smallest unit of language that has meaning) in writing or sound.	What is the difference between allomorphs and morphs?	A morph is a string of phonemes that cannot be broken down into smaller components with a grammatical or lexical function, while an allomorph is a morph with a unique set of lexical or grammatical features.	They both describe sounds produced of morphemes.|||A morph has a unique set of grammatical features.|||Allomorphs cannot be broken down into smaller parts that have a grammatical function.	Morph Meaning clip||youtube||N/A
Branches of linguistics	Linguistic terminology	Morphs	Allomorphs	Morphemes
Define typological classification in languages.	Typological classification in language means categorizing a language according to type of word order (syntactically, ex. S-V-O), lexically and semantically (word type and meaning), or phonetically (ex. a tonal language). Examples of classification include classification by use of color terms, complexity of syllables, and how many vowels there are in a language.	What is / are not an example / s of a useful typological classification?	Neither A nor B are examples of useful typological classifications.	The word for a canine animal is "dog" in English and in Mbabaram, a language spoken in Australia.|||People in tightly-knit communities speak faster than people in communities with more distant relationships.|||A and B are both examples of useful typological classifications.	"The Classification of Languages" clip||youtube||N/A
Typology	Language typology	Typological classifications
How could language acquisition be thought of as an instinctual process?	Steven Pinker makes the argument that language acquisition is an instinctual process because it has existed in all human cultures in history, is acquired by children without formal instruction, and humans can speak without awareness of the underlying logic of language. The "wug" test, an experiment created by Jean Berko Gleason in 1958, provides further evidence of language being instinctual, as this experiment determined that even very young children have internalized systemic aspects of the linguistic system that enable them to produce plurals, past tenses, possessives, and other forms of made-up words.	True or false: Writing acquisition is also an instinctual process.	False	True	Steven Pinker Lecture Clip||youtube||N/A
Language acquisition
Describe the difference between the “sense” and “referent” of a word.	The referent of a word is the “thing” in the world that the word is supposed to point to; the referent of the phrase “the morning star”, or the referent of the phrase “the evening star,” is the actual planet Venus, out there. In contrast, the sense of a word is the concept it conveys, such as “the star one sees in the morning” or “the star one sees in the evening,” therefore not only are referents and senses different, but two words or phrases with the same referent can have different senses.	According to Frege's theory of sense and reference, which is true?	The word "Odysseus" in the Odyssey has no referent.	The word "Odysseus" in Homer's Odyssey refers to the real individual Odysseus.|||The word "Odysseus" in the Odyssey refers to the idea we have of such a man.|||The referent of the word "Odysseus" is a fictional being.	On Sense and Reference||publication||˝   !ˆ       ˛  ˇ ˜   ˇ + ,+ ,  .ˇ˝ ˘ ˛˛ ˛ + ,+ ,            ˆ+ ,+  +  +  + , ˇ˜   ˜ ˝   ˜˜       ˜, ,. ˘(    ˆ  ˜   , ,   ˆ    ˛ ˇ 2       ˇ   ˘ ˆ ˜˛    ˜ ˇ  78 ˜       ˜    ˛            ˇˆ˜     ˇ/     +˝, ++˝,,,ˇ                      79    ˆ ˆ ˇ  ˜    ˝˜          ˇ      ˇ ˇˆ  ˜    )     ˇˆ  $  ˜ /  ˜      7> ˆ  ˜    ˇ˜    0$˝      ˜    ˜˛ ˜  ˜ )ˇ"    ˜     (      ˝˜.      # ˜  ˘ ˆ @A    ,   ˜ ˛        (   ˇ       ˜     ˜ˇ˜      ˘    ˛  ˘˜  ˇ ˜˜     0ˆ ˜  @!   ˇˆ ˜          /  ˇ          ˘  ˘         ˆ  ˆ˜      B    ˆ ˆ   @2  ˇˆ  7      #˜C   +C    C ˜+C ,ˆ ˇ  $     ˝˛˜˜˜      ˛ +   ˇ,  ˛ ˙     /  ˜    ˝  @7    ˆ  ˆ      ,  ˝   ˛     ˘    ˜   ˇ   B     ˜    B   ˇ ˜˘      @  @@   ˜  ˜ˇˇˇ˜,         ˛    +4(     ˜    ˜   #  $         ˇˇˇ,ˇˇˇ,+ˆ ˇˇˇ, ˜,  , ˜ C     ˆ+˝0˜, -!.˝-2.˝  @C B   +4           ,˛   , $  ˝˜,,˘ ˇ ˝      )    ˜,              #  ˜  @H  ˘  ˇ       +˘   ˜    $        ,   ,   , ,      .  @8                     Aˇ˝ - .     ( ˆ    Bˇ $    ˜     @9        @A˜,  @ˇ,  )     Aˇ,    ˇ˜     ˜˜  ˛     ˜  ˛ ˇ   ˇ     ˆ  A   ˇ '˜   @>   ˇˆ !A˜! A       ˜˘ ˜   ˆ ˆ#+˝      ˛       '˜  '˜            ˝ CA   ˇˆ    $    ˇ , ,˛     ˘!A  !Aˇ,      ! 2 ˛    ˇ         $ ˛      C!   ˇ     ˘ˇ   ˜  ˛           ˇ ˜            ˜      B  C2         B  ˘ ˆ   ˛ ˜˛ ˛       ˘         ˛  ˛     ˇ#/    @C   C7  B    ˇ    ˘   ˇ# -!.  -2.  ˜       (      (   ˜,˘ -!.$ -2.˜  -7.     C@   ˘ ˛     ˜ˇ ˜ ˇ      ˜ ˜    ˛ ˆ     -!. $ -2.     -˛.   CC          ˛   + ,+ ,   ˘ ˜+ ,+ ˜,+ , + ˇ,+ ,+ ,˜+ ,+ ˇ, ˛ ˜˘    !   !98>.ˇ 2+I J-  % KDˆF!9>2G˜!>2B2AC.ˇ 7+I J-  % KDˆF!9>2G˜!>2B2AC.ˇ @˝ J CB CH
Semantics	Truth-conditions	Gottlob Frege	Grammatical clauses
In what way do word embeddings encode “relational” semantic properties of words?	Relational meaning consists of semantic relations between words, such as the fact that “king” and “queen” have the same meaning except for their genders. Similarity and difference are kinds of relations. But vectors can encode other more unique relations. Another example could be that the relation between “doctor” and “medicine” is similar to the relation between “teacher” and “education.” Or that the relation between "chow mein" and "China" is similar to the relation between "taco" and "Mexico." Word-vectors / embeddings automatically encode these relations to a large degree, and the relations can be recovered through vector addition, subtraction, and comparison. E.g. If one subtracts the vector for “female” from the vector for “queen” and adds the vector “male,” one should get back a vector very close to the model’s vector for “king."	What word below should be most similar in meaning to the vector derived by subtracting the vector for “teacher” from the vector for “education” and adding the vector for “supervisor” to the result?	training	 business|||employee||| qualifications	Chapter Section on Embeddings and Relational meaning||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 6 VectorSemanticsandEmbed- dings TheasphaltthatLosAngelesisfamousforoccursmainlyonitsfreeways.Butinthe middleofthecityisanotherpatchofasphalt,theLaBreatarpits,andthisasphalt preservesmillionsoffossilbonesfromthelastoftheIceAgesofthePleistocene Epoch.Oneofthesefossilsisthe Smilodon ,orsabre-toothedtiger,instantlyrec- ognizablebyitslongcanines.Fivemillionyearsagoorso,acompletelydifferent sabre-toothtigercalled Thylacosmilus lived inArgentinaandotherpartsofSouthAmer- ica.Thylacosmiluswasamarsupialwhereas Smilodonwasaplacentalmammal,butThy- lacosmilushadthesamelonguppercanines and,likeSmilodon,hadaprotectivebone onthelowerjaw.Thesimilarityof thesetwomammalsisoneofmanyexamples ofparallelorconvergentevolution,inwhichparticularcontextsorenvironments leadtotheevolutionofverysimilarstructuresindifferentspecies (Gould,1980) . Theroleofcontextisalsoimportantinthesimilarityofalessbiologicalkind oforganism:theword.Wordsthatoccurin similarcontexts tendtohave similar meanings .Thislinkbetweensimilarityinhowwordsaredistributedandsimilarity inwhattheymeaniscalledthe distributionalhypothesis .Thehypothesiswas distributional hypothesis formulatedinthe1950sbylinguistslike Joos(1950) , Harris(1954) ,and Firth (1957) ,whonoticedthatwordswhicharesynonyms(like oculist and eye-doctor ) tendedtooccurinthesameenvironment(e.g.,nearwordslike eye or examined ) withtheamountofmeaningdifferencebetweentwowordsﬁcorrespondingroughly totheamountofdifferenceintheirenvironmentsﬂ (Harris,1954,157) . Inthischapterweintroduce vectorsemantics ,whichinstantiatesthislinguistic vector semantics hypothesisbylearningrepresentationsofthemeaningofwords,called embeddings , embeddings directlyfromtheirdistributionsintexts.Theserepresentationsareusedinevery naturallanguageprocessingapplicationthatmakesuseofmeaning,andunderliethe morepowerful contextualizedwordrepresentations like ELMo and BERT that wewillintroduceinChapter10. Thesewordrepresentationsarealsotheexampleinthisbookof repre- sentationlearning ,automaticallylearningusefulrepresentationsoftheinputtext. representation learning Findingsuch self-supervised waystolearnrepresentationsoftheinput,insteadof creatingrepresentationsbyhandvia featureengineering ,isanimportantfocusof NLPresearch (Bengioetal.,2013) . We'llbegin,however,byintroducingsomebasicprinciplesofwordmeaning, whichwillmotivatethevectorsemanticmodelsofthischapteraswellasextensions thatwe'llreturntoinChapter19,Chapter20,andChapter21.  2 C HAPTER 6  V ECTOR S EMANTICSAND E MBEDDINGS 6.1LexicalSemantics Howshouldwerepresentthemeaningofaword?IntheN-grammodelswesaw inChapter3,andinmanytraditionalNLPapplications,ouronlyrepresentationof awordisasastringofletters,orperhapsasanindexinavocabularylist.This representationisnotthatdifferentfromatraditioninphilosophy,perhapsyou've seenitinintroductorylogicclasses,inwhichthemeaningofwordsisrepresented byjustspellingthewordwithsmallcapitalletters;representingthemeaningof ﬁdogﬂas DOG ,andﬁcatﬂas CAT ). Representingthemeaningofawordbycapitalizingitisaprettyunsatisfactory model.Youmighthaveseentheoldphilosophyjoke: Q:What'sthemeaningoflife? A: LIFE Surelywecandobetterthanthis!Afterall,we'llwantamodelofwordmeaning todoallsortsofthingsforus.Itshouldtellusthatsomewordshavesimilarmean- ings( cat issimilarto dog ),otherwordsareantonyms( cold istheoppositeof hot ).It shouldknowthatsomewordshavepositiveconnotations( happy )whileothershave negativeconnotations( sad ).Itshouldrepresentthefactthatthemeaningsof buy , sell ,and pay offerdifferingperspectivesonthesameunderlyingpurchasingevent (IfIbuysomethingfromyou,you'veprobablysoldittome,andIlikelypaidyou). Moregenerally,amodelofwordmeaningshouldallowustodrawusefulinfer- encesthatwillhelpussolvemeaning-relatedtaskslikequestion-answering,sum- marization,detectingparaphrasesorplagiarism,anddialogue. Inthissectionwesummarizesomeofthesedesiderata,drawingonresultsinthe linguisticstudyofwordmeaning,whichiscalled lexicalsemantics ;we'llreturnto lexical semantics andexpandonthislistinChapter19. LemmasandSenses Let'sstartbylookingathowoneword(we'llchoose mouse ) mightbeinadictionary: 1 mouse(N) 1.anyofnumeroussmallrodents... 2.ahand-operateddevicethatcontrolsacursor... Heretheform mouse isthe lemma ,alsocalledthe citationform .Theform lemma citationform mouse wouldalsobethelemmafortheword mice ;dictionariesdon'thaveseparate forformslike mice .Similarly sing isthelemmafor sing , sang , sung .Inmanylanguagestheveformisusedasthelemmafortheverb,so Spanish dormir ﬁtosleepﬂisthelemmafor duermes ﬁyousleepﬂ.Theforms sung or carpets or sing or duermes arecalled wordforms . wordform Astheexampleaboveshows,eachlemmacanhavemultiplemeanings;the lemma mouse canrefertotherodentorthecursorcontroldevice.Wecalleach oftheseaspectsofthemeaningof mouse a wordsense .Thefactthatlemmascan be polysemous (havemultiplesenses)canmakeinterpretationdif(issomeone whotypesﬁmouseinfoﬂintoasearchenginelookingforapetoratool?).Chapter19 willdiscusstheproblemofpolysemy,andintroduce wordsensedisambiguation , thetaskofdeterminingwhichsenseofawordisbeingusedinaparticularcontext. Synonymy Oneimportantcomponentofwordmeaningistherelationshipbe- tweenwordsenses.Forexamplewhenonewordhasasensewhosemeaningis 1 ThisexampleshortenedfromtheonlinedictionaryWordNet,discussedinChapter19.
Word embeddings	Word vectors	Relational meanings	Lexical semantics
What is the difference between derivational and inflectional morphemes?	Inflectional morphemes attach to words (or change words) in order to express grammatical information such as verb tense, person, and aspect, and noun number and case, such as the suffixes -ed, -ing, and -s in English. Derivational morphemes change words into new words either by changing their grammatical category or their meaning. For example, the derivational suffix -ly changes adjectives into adverbs, while the derivational prefix re- adds the meaning "again" to words in English.	Which of the following is NOT true about inflectional or derivational morphemes?	Inflectional morphemes are always suffixes or prefixes	Derivational morphemes attach to the word-stem before inflectional morphemes|||Inflectional morphemes are usually more productive than derivational morphemes|||There are fewer inflectional than derivational morphemes in English	Morphology UPenn webpage||document||N/A
Morphology	Morphemes	Inflection
What is the semantic difference between the following two sets of verb usages (the one difference that  applies to all the sentences in each set), and how can that difference be represented for sentences? (simply describe a general solution that applies to all sentences).	The difference is that all of the verb-usages in Set A describe “states” while all the usages in Set B describe “activities” (or “events” in some schools of semantics). One can represent this aspect of the sentence using an event-variable “e”to represent the entire "event" (or situation) and predicating State() or Activity() of it, like so: State(e), or Activity(e).	What is the tense and aspect of the sentence, “We have not fully understood temporal logic.”?	present perfect	past perfect|||stative|||activity	Chapter section on verb Aspect||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 16 LogicalRepresentationsof SentenceMeaning I SHMAEL : Surelyallthisisnotwithoutmeaning. HermanMelville, MobyDick Inthischapterweintroducetheideathatthemeaningoflinguisticexpressionscan becapturedinformalstructurescalled meaningrepresentations .Considertasks meaning representations thatrequiresomeformofsemanticprocessing,likelearningtouseanewpieceof softwarebyreadingthemanual,decidingwhattoorderatarestaurantbyreading amenu,orfollowingarecipe.Accomplishingthesetasksrequiresrepresentations thatlinkthelinguisticelementstothenecessarynon-linguistic knowledgeofthe world .Readingamenuanddecidingwhattoorder,givingadviceaboutwhereto gotodinner,followingarecipe,andgeneratingnewrecipesallrequireknowledge aboutfoodanditspreparation,whatpeopleliketoeat,andwhatrestaurantsarelike. Learningtouseapieceofsoftwarebyreadingamanual,orgivingadviceonusing software,requiresknowledgeaboutthesoftwareandsimilarapps,computers,and usersingeneral. Inthischapter,weassumethatlinguisticexpressionshavemeaningrepresenta- tionsthataremadeupofthe samekindofstuff thatisusedtorepresentthiskindof everydaycommon-senseknowledgeoftheworld.Theprocesswherebysuchrepre- sentationsarecreatedandassignedtolinguisticinputsiscalled semanticparsing or semantic parsing semanticanalysis ,andtheentireenterpriseofdesigningmeaningrepresentations andassociatedsemanticparsersisreferredtoas computationalsemantics . computational semantics 9 e ; yHaving ( e ) ^ Haver ( e ; Speaker ) ^ HadThing ( e ; y ) ^ Car ( y ) Figure16.1 Alistofsymbols,twodirectedgraphs,andarecordstructure:asamplerof meaningrepresentationsfor Ihaveacar. ConsiderFig. 16.1 ,whichshowsexamplemeaningrepresentationsforthesen- tence Ihaveacar usingfourcommonlyusedmeaningrepresentationlanguages. Thetoprowillustratesasentencein First-OrderLogic ,coveredindetailinSec- tion 16.3 ;thedirectedgraphanditscorrespondingtextualformisanexampleof an AbstractMeaningRepresentation(AMR) form (Banarescuetal.,2013) ,and ontherightisa frame-based or  representation,discussedinSection 16.5 andagaininChapter18.  2 C HAPTER 16  L OGICAL R EPRESENTATIONSOF S ENTENCE M EANING Whiletherearenon-trivialdifferencesamongtheseapproaches,theyallshare thenotionthatameaningrepresentationconsistsofstructurescomposedfroma setofsymbols,orrepresentationalvocabulary.Whenappropriatelyarranged,these symbolstructuresaretakento correspond toobjects,propertiesofobjects,andrela- tionsamongobjectsinsomestateofaffairsbeingrepresentedorreasonedabout.In thiscase,allfourrepresentationsmakeuseofsymbolscorrespondingtothespeaker, acar,andarelationdenotingthepossessionofonebytheother. Importantly,theserepresentationscanbeviewedfromatleasttwodistinctper- spectivesinalloftheseapproaches:asrepresentationsofthemeaningofthepar- ticularlinguisticinput Ihaveacar ,andasrepresentationsofthestateofaffairsin someworld.Itisthisdualperspectivethatallowstheserepresentationstobeused tolinklinguisticinputstotheworldandtoourknowledgeofit. Inthenextsectionswegivesomebackground:ourdesiderataforameaning representationlanguageandsomeguaranteesthattheserepresentationswillactually dowhatweneedthemtodoŠprovideacorrespondencetothestateofaffairsbeing represented.InSection 16.3 weintroduceFirst-OrderLogic,historicallytheprimary techniqueforinvestigatingnaturallanguagesemantics,andseeinSection 16.4 how itcanbeusedtocapturethesemanticsofeventsandstatesinEnglish.Chapter17 thenintroducestechniquesfor semanticparsing :generatingtheseformalmeaning representationsgivenlinguisticinputs. 16.1ComputationalDesiderataforRepresentations Let'sconsiderwhymeaningrepresentationsareneededandwhattheyshoulddofor us.Tofocusthisdiscussion,let'sconsiderasystemthatgivesrestaurantadviceto touristsbasedonaknowledgebase. V Considerthefollowingsimplequestion: (16.1) DoesMaharaniservevegetarianfood? Toanswerthisquestion,wehavetoknowwhatit'sasking,andknowwhetherwhat it'saskingistrueofMahariniornot. v isasystem'sabilitytocompare v thestateofaffairsdescribedbyarepresentationtothestateofaffairsinsomeworld asmodeledinaknowledgebase.Forexamplewe'llneedsomesortofrepresentation like Serves ( Maharani ; VegetarianFood ) ,whichasystemcancanmatchagainstits knowledgebaseoffactsaboutparticularrestaurants,andifitarepresentation matchingthisproposition,itcanansweryes.Otherwise,itmusteithersay No ifits knowledgeoflocalrestaurantsiscomplete,orsaythatitdoesn'tknowifitknows itsknowledgeisincomplete. UnambiguousRepresentations Semantics,likealltheotherdomainswehavestudied,issubjecttoambiguity.Words andsentenceshavedifferentmeaningrepresentationsindifferentcontexts.Consider thefollowingexample: (16.2) Iwannaeatsomeplacethat'scloseto ICSI . Thissentencecaneithermeanthatthespeakerwantstoeat at somenearbylocation, orunderaGodzilla-as-speakerinterpretation,thespeakermaywanttodevoursome  16.1  C OMPUTATIONAL D ESIDERATAFOR R EPRESENTATIONS 3 nearbylocation.Thesentenceisambiguous;asinglelinguisticexpressioncanhave oneoftwomeanings.Butour meaningrepresentations itselfcannotbeambiguous. Therepresentationofaninput'smeaningshouldbefreefromanyambiguity,sothat thethesystemcanreasonoverarepresentationthatmeanseitheronethingorthe otherinordertodecidehowtoanswer. Aconceptcloselyrelatedtoambiguityis vagueness :inwhichameaningrepre- vagueness sentationleavessomepartsofthemeaningVaguenessdoesnotgive risetomultiplerepresentations.Considerthefollowingrequest: (16.3) IwanttoeatItalianfood. While Italianfood mayprovideenoughinformationtoproviderecommendations,it isnevertheless vague astowhattheuserreallywantstoeat.Avaguerepresentation ofthemeaningofthisphrasemaybeappropriateforsomepurposes,whileamore representationmaybeneededforotherpurposes. CanonicalForm Thedoctrineof canonicalform saysthatdistinctinputsthatmeanthesamething canonicalform shouldhavethesamemeaningrepresentation.Thisapproachgreatlyrea- soning,sincesystemsneedonlydealwithasinglemeaningrepresentationfora potentiallywiderangeofexpressions. Considerthefollowingalternativewaysofexpressing( 16.1 ): (16.4) DoesMaharanihavevegetariandishes? (16.5) DotheyhavevegetarianfoodatMaharani? (16.6) ArevegetariandishesservedatMaharani? (16.7) DoesMaharaniservevegetarianfare? Despitethefactthesealternativesusedifferentwordsandsyntax,wewantthem tomaptoasinglecanonicalmeaningrepresentations.Iftheywerealldifferent, assumingthesystem'sknowledgebasecontainsonlyasinglerepresentationofthis fact,mostoftherepresentationswouldn'tmatch.Wecould,ofcourse,storeall possiblealternativerepresentationsofthesamefactintheknowledgebase,butdoing sowouldleadtoenormousdifinkeepingtheknowledgebaseconsistent. Canonicalformdoescomplicatethetaskofsemanticparsing.Oursystemmust concludethat vegetarianfare , vegetariandishes ,and vegetarianfood refertothe samething,that having and serving areequivalenthere,andthatalltheseparse structuresstillleadtothesamemeaningrepresentation.Orconsiderthispairof examples: (16.8) Maharaniservesvegetariandishes. (16.9) VegetariandishesareservedbyMaharani. Despitethedifferentplacementoftheargumentsto serve ,asystemmuststillassign Maharani and vegetariandishes tothesamerolesinthetwoexamplesbydraw- ingongrammaticalknowledge,suchastherelationshipbetweenactiveandpassive sentenceconstructions. InferenceandVariables Whataboutmorecomplexrequestssuchas: (16.10) CanvegetarianseatatMaharani? Thisrequestresultsinthesameanswerastheothersnotbecausetheymeanthesame thing,butbecausethereisacommon-senseconnectionbetweenwhatvegetarianseat  4 C HAPTER 16  L OGICAL R EPRESENTATIONSOF S ENTENCE M EANING andwhatvegetarianrestaurantsserve.Thisisafactabouttheworld.We'llneedto connectthemeaningrepresentationofthisrequestwiththisfactabouttheworldina knowledgebase.Asystemmustbeabletouse inference Štodrawvalidconclusions inference basedonthemeaningrepresentationofinputsanditsbackgroundknowledge.It mustbepossibleforthesystemtodrawconclusionsaboutthetruthofpropositions thatarenotexplicitlyrepresentedintheknowledgebasebutthatarenevertheless logicallyderivablefromthepropositionsthatarepresent. Nowconsiderthefollowingsomewhatmorecomplexrequest: (16.11) I'dliketoarestaurantwhereIcangetvegetarianfood. Thisrequestdoesnotmakereferencetoanyparticularrestaurant;theuserwantsin- formationaboutanunknownrestaurantthatservesvegetarianfood.Sincenorestau- rantsarenamed,simplematchingisnotgoingtowork.Answeringthisrequest requirestheuseof variables ,usingsomerepresentationlikethefollowing: variables Serves ( x ; VegetarianFood ) (16.12) Matchingsucceedsonlyifthevariable x canbereplacedbysomeobjectinthe knowledgebaseinsuchawaythattheentirepropositionwillthenmatch.Thecon- ceptthatissubstitutedforthevariablecanthenbeusedtotheuser'srequest. Itiscriticalforanymeaningrepresentationlanguagetobeabletohandlethesekinds ofreferences. Expressiveness Finally,ameaningrepresentationschememustbeexpressiveenoughtohandlea widerangeofsubjectmatter,ideallyanysensiblenaturallanguageutterance.Al- thoughthisisprobablytoomuchtoexpectfromanysinglerepresentationalsystem, First-OrderLogic,asdescribedinSection 16.3 ,isexpressiveenoughtohandlequite alotofwhatneedstoberepresented. 16.2Model-TheoreticSemantics Whatisitaboutaboutmeaningrepresentationlanguagesthatallowsthemto thesedesiderata,bridgingthegapfromformalrepresentationstorepresentationsthat tellussomethingaboutsomestateofaffairsintheworld? Theanswerisa model .Amodelisaformalconstructthatstandsforthepartic- model ularstateofaffairsintheworld.Expressionsinameaningrepresentationlanguage canbemappedtoelementsofthemodel,likeobjects,propertiesofobjects,and relationsamongobjects.Ifthemodelaccuratelycapturesthefactswe'reinterested in,thenaconsistentmappingbetweenthemeaningrepresentationandthemodel providesthebridgebetweenmeaningrepresentationandworld.Modelsprovidea surprisinglysimpleandpowerfulwaytogroundtheexpressionsinmeaningrepre- sentationlanguages. First,someterminology.Thevocabularyofameaningrepresentationconsistsof twoparts:thenon-logicalvocabularyandthelogicalvocabulary.The non-logical vocabulary consistsoftheopen-endedsetofnamesfortheobjects,properties,and  vocabulary relationsthatmakeuptheworldwe'retryingtorepresent.Theseappearinvarious schemesaspredicates,nodes,labelsonlinks,orlabelsinslotsinframes,The log- icalvocabulary consistsoftheclosedsetofsymbols,operators,links, logical vocabulary
Temporal logic	Sentence semantics
What is constraint-based grammar?	Constraint-based grammar is grammar that does not allow any merges, transformations, omissions, or movements that can result in a correct sentence.	Why are constraint-based grammars considered to be part of microlinguistics?	Because syntax, a component of micro-linguistics, studies the combination of words in phrases or sentences, which is limited on constraint-based grammar.	Because grammar is a component of micro-linguistics.|||Because grammar is the same as syntax.|||Because microlinguistics deals with linguistic constraints	2 MINUTE Language Theories: Constraint-based Grammar clip||youtube||N/A
Syntax	Microlinguistics	Constraint-based grammars	Grammar
What does "autonomous syntax" mean?	This term means that syntax is best explained in isolation from other linguistic subsystems, function and usage. It is the view that syntax exists separately from semantic, phonological, or pragmatic information and that no syntactic rule can make reference to such information.	Which of the following linguistic subsystems DO NOT study the effect of social factors on language: sociolinguistics, discourse analysis, pragmatics, or morphology?	Morphology	Sociolinguistics|||Discourse analysis|||Pragmatics	"The influence of social, cultural, and natural factors on language structure: An overview"||publication||This is the  final  draft  of the book chapter:  Busser, Rik De. 2015. The influence of social, cultural, and natural factors on language  structure: An overview. In Rik De Busser &  Randy J. LaPolla (eds.), Language Structure  and Environment: Social, Cultural, and Natural Factors, 1 Œ28. Amsterdam: John  Benjamins.   DOI: 10.1075/clscc.6.01bus.   Book URL:  https://ben jamins.com/#catalog/books/clscc.6/main  It does not reflect  the final page numbering or  any changes made during the  proofing process.  For an offprint of the  published  version of this article, please  contact the author.   This article is unde r copyright. Please contact the publisher for any questions about  reuse or redistribution.    CHAPTER  1 The influence of s ocial , cultural, and natural  factors on language structure:  An overview  Rik De Busser  National Chengchi University  This book is an  attempt to give a n overview of how language interacts with its  environment, or better, how actual linguistic structure is formed, changed and  influenced by different aspects of the human environment. The focus is mainly on  effects of the extra -linguistic e nvironment on the actual grammatical structure of  languages ; we will leave influences on other linguistic subsystems such as phonology,  the lexicon, and discourse structure to the efforts of other researchers.   The underlying assumption of this entire volu me is that linguistic structure is not  only shaped by how speakers interact with each other and with the world they live in,   but also by external forces that are outside the control of individual speakers or  speech communities. One might call it natural se lection in grammar, were it not for  the fact that it is not entirely clear whether biological and linguistic change operate  along the same real -world principles, or whether any correspondences are much more  superficial.    2 1. Introduction  The general idea set o ut  in this book is  that language structure is influenced by  the  environment  in which it is used . This idea  is not original in itself and, to some,  might appear trivial. Indeed, as  Gumperz & Levinson  (1996:1)  courageously  remark at the very outset of an edited volume:   Every student of language or society should be familiar with the essential  idea of linguistic relativity, the idea that culture,  through  language,  affects the way we think, especially perhaps ou r classification of the  experienced world.  Putting aside directionality (language influencing culture or vice versa),  two  things  are worth pointing out.   1.1.  Non -autonomous  syntax  First, if the idea of the extra -linguistic environment shaping linguistic struct ure  were self -evident, one would expect it to have become more popular in linguistics.  Instead,  we find the following categorical statement in a work on generative  phonology:  There is no correlation whatsoever between phonological structure (or  for that ma tter, any matter of linguistic structure) and the environment.  [–] Studying the structure of a language reveals absolutely nothing  about either the people who speak it or the physical environment in  which they live.  (Kaye, 1989, p. 48)  The idea  is  also  dia metrically opposed to what since the 1950s  has  been  an  influential  tenet in linguistic theory , especially in the generative tradition, namely  the autonomy of syntax.  There are different interpretations of  what  this concept  exactly means , but all imply that  syntax is best explained in isolation from  other  linguistic subsystems,  function  and  usage .1 While this usually does not negate the  importance of semantics or pragmatics in the understanding of language in general,  it does imply that ﬁ a formal grammar can  in principle be selected  [–]  on the basis  of a preliminary analysis of data in terms of formal primitives excluding the core   notions of semantics ﬂ (Chomsky, 1977, p. 42) . In other words, meaning, actual  language use  and the extra -linguistic context  are  in consequential for a n understanding of the  grammatical structure of language.  As  Croft  (1995, pp. 490 Œ491)  points out, the autonomy of syntax is often seen  as a consequence of that ﬁundeniable fact of all languagesﬂ, the Saussurian concept                                                               1 See e.g. the Autonomous syntax Principle in  (Radford, 2009, p. 31) : ﬁNo syntactic rule can  make reference to pragmatic, phonological, or semantic informationﬂ    3 of the  arbitrariness of the sign, which does indeed imply at the very least a certain  degree of disconnect between linguistic form and its function within a non -linguistic context. However, the evolution of arbitrary form -function combinations  does not exclude th e existence of direct environmental pressure,  either  in language  or in other communicative systems.  Alarm calls in various monkey species all  evolved in response to acute danger in the immediate environment, but their exact   vocalisations are to a large ext ent random (for instance, there is no iconic  relationship between the sound structure and the predator indicated).  One of the reasons why the idea of autonomous syntax is so attractive is  undoubtedly because it prevents theoretical models from becoming too  complicated:  Chomsky  (2002, pp. 52 Œ53)  implies as much in saying that it is unreasonable to  demand from a grammar that it accurately represents language use in context,   because this would lead ﬁinto a maze of more and more elaborate and complex   analytic p rocedures that will fail to provide answers for many important questions  about the nature of linguistic structure.ﬂ This might explain why even in  frameworks formulated in opposition to generative linguistic theory the non -linguistic context is often large ly excluded from grammatical description and  interpretation, not necessarily by axiomatic fiat, but certainly as a pervasive  working assumption.   A good example is  Croft  (2003) , a well -known and in many ways excellent  introductory work to linguistic typolo gy, which describes typology in opposition to  generative theories of language as a functional approach to language, that is, a  linguistic approach that espouses ﬁthe view that linguistic structure should be  explained primarily in terms of linguistic functi onﬂ  (Croft  2003, p. 2) .  Croft  (2003,  pp. 13 Œ14)  recognizes the importance of semantic and pragmatic factors in  determining cross -linguistically valid grammatical categories, but extra -linguistic  categories are not discussed at all, and the book focuses str ongly on structural  explanations of cross -linguistically valid grammatical patterns. There is nothing  inherently wrong with this (any theory needs to limit its subject matter in certain  ways); it merely illustrates that autonomous approaches to syntactic s tructure are  not a phenomenon exclusive to generative grammar. Even in reactions against the  idea of the autonomy of syntax, such as  Anderson  (2006) , the term is usually  interpreted in its original, narrow sense, namely the absence of theoretically   relevan t interactions between grammatical structure  and semantics  or pragmatics .  The extra -linguistic environment itself is of no real concern in his discussion.   The studies in this  book  show  that such views  are increasingly untenable: a n ever -increasing mountai n of evidence suggests that there are  plenty of  complex  interactions between language  and  its  environment , and that in certain cases these  interactions have a measurable influence on the development of grammatical  structures. One of the first and foremost  goals of this volume is to illustrate that  it  does not make sense to investigate the structure of a language in an artificially  imposed isolation from the environmental factors  that  have a significant influence   4 on  its development and  evolution.  In other wo rds, we will provide evidence here  that grammar, and language in general, is non -autonomous.  1.2.  Linguistic relativity  Secondly, the idea at the basis of this volume is compatible with that of linguistic  relativity, but there are important differences. The con cept of linguistic relativity  was originally formulated by Whorf, but probably most eloquently expressed by  Edward Sapir, who stated that ﬁlanguage does not exist apart from culture, that is,  from the socially inherited assemblage of practices and beliefs  that determines the  texture of our livesﬂ  (Sapir, 1921, p.  221) .   Many interpretations exist about the exact nature and  scope of linguistic  relativity  Gumperz & Levinson  (1996c ) and  Lucy  (1997)  both provide excellent  overviews of the historical  development, and diverse interpretations of the concept).  Sidestepping a theoretical quagmire, we will here assume a weak interpretation of   linguistic relativity, which implies  that culture exerts an influence  on but does not  fully determine linguistic str ucture ,  and further assume that the interaction between  culture and language is bidirectional.  It is debatable whether either of these  assumptions was made by Whorf, and especially the latter would be contentious to   at least a portion of linguists and anth ropologists that are presently working on  linguistic relativity.  Lucy  (1997, p. 294) , for instance, states that ﬁ[l]anguage embodies an  interpretation of reality and language can influence thought about that realityﬂ and  that ﬁ[l]inguistic relativity propo sals emphasize a distinctive role for language  structure in interpreting experience and influencing thought.ﬂ In contrast, the  contributions to this volume are not interested in how language influences our  experience of reality, but rather the opposite, ho w external reality leads to certain  grammatical features.  One could argue that some circularity is implied in linguistic  relativity and that when language influences our perception of reality, this perceived  reality in return suggests or implies certain re strictions on specific grammatical  patterns. However, this is an observational implication and has little to do with real -world causality . We are here not merely interested  in observed  correlations  between  grammatical structure and reality without regard t o causal direction; our aim is to  investigate how external real -world factors  can  trigger or influence  the  development of certain grammatical features in a language.  During the last decades, the idea of linguistic relativity, with some  modifications, was r ediscovered by a number of linguistic subfields that study the  interaction between culture, language and cognition, such as sociolinguistics,  ecolinguistics and ethnosyntax (see  Related fields  below).  Gumperz & Levinson  (1996b , p.  9) also relate linguistic  relativity to a broad interpretation of Peirce™s  concept of indexicality as the relationship between sign, communicative    5 participations, and the communicative context. 2 This is what makes linguistic  relativity relevant to understanding functional approach es to language, which in all  their variety all start from the assumption ﬁthat the communicative situation  motivates, constrains, explains, or otherwise det ermines grammatical structureﬂ  (Nichols, 1984, p. 97) . Often, this communicative situation is interp reted narrowly  in terms of communicative intent: linguistic structures are explained in terms of the  needs or desires of speakers to communicate certain pieces of information in certain  situations.  Though obviously relevant to the topic of this book, both  interpretations of the  extra -linguistic context are much more restrictive than how we defined it above. In  the case of the original Sapir -Whorf hypothesis and its modern incarnations, this  context is assumed to be human culture; in modern functional theori es of language,  it is the communicative setting and its influence on communicative intent. Both are  subsumed in our definition of the extra -linguistic context, but our basic assumption  will be that every single element in the extra -linguistic environment,  be it cognitive,  social, cultural, biological or physical, should be treated as a potential factor of  influence on the structure of languages. Some of these factors might be consciously  observed or even constructed by the speakers of these languages as com municative  goals or as part of the socio -cultural setting, but others might be imperceptible to  the language users, for instance because they exert their influence on evolutionary   time scales or in an indirect fashion.  2. Related fields  In this section we wil l first set out to what extent the general idea behind this book,  that grammatical structure is directly influenced by the extra -linguistic environment,  is similar to or different from existing approaches to linguistics. Although the  modern study of langua ge in its environmental context goes back at least to  Sapir 's (1912)  article, it was especially in the last half century that a  number of  sub fields  in  linguistics, both small and not so small,   have arisen that  study  various  interaction s of language and  the  non -linguistic  environment. In terms of their research subjects,  the boundaries between these disciplines are not always equally clear, but each   started from its own distinct source and has its own unique point -of-view of how  language should be analyse d.                                                              2 The traditional interpretation of  indexicality is narrow and only applies to deixis. On the  other hand,  Enfield  (2004, pp. 10 Œ11) talks of social indexicality as a creative force  enforcing social identity and leading to grammatical innovation.   6 2.1.  Functional grammar  The foundations for functional linguistics were laid in the Prague School of  linguistics in the first half of the twentieth century.  As  Nichols  (1984)  points out,  there is considera ble variety in how the concept  function  is actually interpreted, but   in general, functional (or functionalist) theories of language seek to explain the  development and use of linguistic phenomena in terms of their socio -cultural and  discursive function.   It is obvious that in such frameworks,  any adequate explanation of grammatical  structure needs to take into account the influence of the extra -linguistic context,  although interpretations of the exact nature and scope of this context may vary. For  instance,  Dik (1 987)  has an instrumentalist vie w on language as a tool ﬁ in the  establishment of complex patterns of social interaction ﬂ (Dik, 1987, p. 83) , realised  through the interaction of syntax, semantics and pragmatics that can be encoded in  a formal -logical model.  On the other hand, in Halliday ™s systemic -functional  grammar, linguistic structure arises from environmentally imposed constraints on a   speaker™s creative potential and takes the form of  ﬁ systematic relations  [–]  between semantic system networks and behaviour patterns on the one hand  and   between semantic networks and the lexicogrammar on the other ﬂ (Davidse, 1987,  pp. 47 Œ49) . The two examples illustrate that in practice, many contemporary functionalist  theories tend to focus on the interplay between semantic and pragmatic function on  the one hand and grammatical structure on the other, abstracting away from the  actual interaction between linguistic and non -linguistic information networks in  favour of a system -internal, purely linguistic interpretation. As mentioned before,  such approach es are generally compatible with the assumptions at the basis of the  present volume, and some contributors to this volume would identify themselves as   functionalist linguists.   However, relationships between language and the communicative context only  form  a small subset of the relationships between grammatical structure and the non -linguistic environment that we are interested in here. While generally compatible  with the general tenets of functional linguistics, the contributions in this volume  tend to emp hasize the interactions between grammatical structure and the outside  world, rather than intra -linguistic relationships between syntax and pragmatics (see  LaPolla, this volume, for an in -depth discussion).  2.2.  Sociolinguistics  Though relatively young  Œ the ter m itself goes back  to the work in the early  1960s  (see Hymes, 1974, p. 193)  Œ sociolinguistics is a relatively mature field, both  in terms of the variety of its research matter and the amount of research published.  Sociolinguistics studies the interactions  between language and society. What this    7 exactly pertains is a subject of healthy debate. For  Hymes  (1974, pp. 195 Œ197) ,  there are ﬁthree main orientationsﬂ: (1) attitudinal studies, investigating social  attitudes toward certain linguistic phenomena (e.g.  work on language  standardization); (2) variational studies, investigating the influence of the social   context on language variation (e.g. dialect; and (3) functional studies, investigating  the social functions of linguistic expressions (e.g. discourse anal ysis). It is mainly  the second strand that is of interest to us here, to the extent that it focuses on  socially conditioned variation of grammar (rather than, for instance, phonology, a  popular subject in some early studies). The influence of geo -social an d societal  factors on grammatical variation (dialectal or sociolectal) is discussed i n the  contributions in S ection 2 of this book.  Some principles of sociolinguistics are of general relevance to the work  presented in this volume, whether it is explicitly  sociolinguistic in nature or not. All  sociolinguists have an explicit interest in actual language use in context rather than  the structuralist insistence on a fundamental separation  between competence and  performance .  Labov  (1972, p. xiii)  goes as far as t o say that it is simply not possible  to investigate language outside its social context, a sentiment we would like to  support unequivocally. It makes as little sense to study the abstract formal structure  of a language in isolation from its context of use  or from its developmental pathway  as it does to do so with an abstract Mondri aan painting. A formal analysis of such  artwork might record in excruciating detail the dimensions, position, and colour of  each square, the materials of which it is made, and transformation rules that allow  us to mathematically derive this particular paint ing from its predecessors, but this  would be utterly meaningless. The work only gets meaning in its historical   background (the abstract movement that evolved out of expressionism), the artistic  evolution of the painter (from impressionism over cubism to st rict non -representationalism), and its intended meaning in a particular social -artistic context  (an expression of the abstract beauty of the laws of the universe; see  Gombrich,  2006, p. 451, fig. 381 ). Sociolinguistics has traditionally also had a strong i nterest in empirically  grounded research, not uncommonly with an experimental component. This type of  research can be quantitative or qualitative, but in both cases it tends to derive results  from verifiable and falsifiable data sets, unlike  formalist  theo ries of language,  which traditionally are focused more on introspection. 3 A similar concern about the  nature of evidence is reflected in most if not all contributions to this volume.                                                               3 Even rece ntly, w hen  Featherston  (2007, p. 272)  pleaded for an increased use of empirical  data in generative syntax and remarked that ﬁ a significant number of linguists are still, in  spite of all the warnings to the contrary, using as the basis of their work what we  might call  linguist™s  judgements ﬂ,  Fanselow  (2007, p. 354)  was able to answer that there are no sound  methodological reasons that ﬁ would require the exclusion of linguists™  judgments from  syntax research. ﬂ  8 2.3.  Ecolinguistics  Einar  Haugen, credited in  Fill & Mühlhäusler  (2001)  with f ounding ecolinguistics,  defined the field as ﬁthe study of interactions between any given language and its  environmentﬂ  (Haugen, 2001, p. 57)  and defines this environment very specifically  as human society. In this light, it is not entirely clear whether e colinguistics should  be considered a linguistic subdiscipline in its own right, or rather a particular  attitude towards sociolinguistics.   Another problematic aspect of ecolinguistics is that the exact boundaries of its  research subject are not entirely cl ear. Nicholas Ostler, whose popular monograph  on the influence of empire building on the development of languages  (Ostler, 2006) ,  a topic squarely within the scope of ecolinguistics, expresses his reservations rather  directly in a review of  Fill & Mühlhäus ler  (2001) ™s overview volume:  ﬁEcolinguistics is not a discipline, and hardly even a subject, despite the bold claim  of the editorsﬂ and ﬁcannot be seen as any sort of probative or empirical scienceﬂ  because it is internally inconsistent. A more charitable  interpretation would be that  ecolinguistics is a highly diversified field with a small number general topical   trends. In his seminal article, Haugen put great importance on language variation  and contact, multilingualism, and standardization. This instiga ted an avalanche of  research into dialect variation, writing systems, pidgins and creoles, and the like.  Especially in variational linguistics and creole studies, the influence of the non -linguistic environment on language development is still very salient , and from time  to time the term  ecology  stic ks up its head in this context  (e.g. Mufwene, 2001;  Ansaldo, 2009) .   The work of other linguists and anthropologists followed a rather different path  and reinterpreted  environment  and  ecology  in a different, mor e literal fashion. They  gradually focused more on the relationship between biological and linguistic   diversity  (see e.g. Maffi, 2005) . Not uncommonly, ecolinguistics develops an  ideological undertone, with a focus on raising awareness of and preventing   lin guistic and cultural extinction  (Haugen, 2001, p. 60) . For instance,  Maffi  (2005,  p. 601)  believes ecolinguistics should have ﬁa focus on the relationships between  linguistic, cultural, and biological diversity, their global overlapping distributions,   and  the common threats they are facing.ﬂ   What these research strands have in common is that (1) they all investigate the  relationship between language  Œ or languages  Œ and its environment, however that  might be defined exactly, (2) rather than investigating l inguistic properties  per se ,  they rather tend to focus on the global structure and diversity of languages, and (3)  they do this from a linguistic, anthropological and/or philosophical point -of-view.  The first characteristic also underlies the research in t his volume. With regard to the  second and third, our interests diverge. All of the contributions in this volume are  firmly interested in the grammatical structure of language, and are strongly inspired  by observation of language use or empirical linguistic  research. We have aimed at a    9 balance between theory and practical studies, and our contributors are as much  interested in the development of grammatical micro -structure under pressure of the  environment (e.g. particular grammatical categories such as evid entiality; see  Michael,  this volume ), as we are in environmental influence on languages in their  entirety (Trudgill,  this volume ; Nichols,  this volume ). 2.4.  Ethnosyntax  Ethnosyntax is an approach to grammatical analysis influenced by linguistic  anthropology. I t is defined in  Enfield  (2004b:3)  as ﬁ the study of connections  between the cultural knowledge, attitudes, and practices of speakers, and the   morphosyntactic resources they employ in speech .ﬂ He goes on to explain that the  field has been interpreted narrowly as the study of direct influences of culture on   linguistic structure, but that other linguists also subsume the study of general  pragmatic effects such as typicality under its objects of  study. This focus on cultural  praxis sets the field apart from sociolinguistics, which tends to have an interest in  relations between language and social factors outside the speaker™s control (such as  gender, social class, population size, etc.).   Wierzbi cka  (1979) , who coined the term, uses it most definitely in a narrow  sense, and takes the idea as a starting point for her work on Natural Semantic   Metalanguage, a semantic framework, aimed at constructing a coherent and  formalised set of cross -linguistica lly valid semantic primes  (see e.g. Wierzbicka,  1996) . Interestingly, this interest in formalisation and semantic universals sets her  apart from ensuing ethnosyntacticians  who envisaged the field, with its strong  relativist tendencies, as an antidote to fo rmalist and universalist theories of  language and, as a result, viewed her work with some suspicion.  This anti -universalist agenda is clear in more polemical works associated with  the ethnosyntax program, such as  Everett  (2005) . In his description of  Pirah ã, a  language isolate spoken in the Brazilian Amazon, he notes the absence of linguistic  features often considered to be basic to any human language, such as a counting  system, colour terms, and  Œ most controversially  Œ grammatical embedding and  recursion,  two pillars of formal linguistic theory. He connects this to the general  world view of the  Pirahã  people, and argues this implies that cross -linguistically,  ﬁsome of the components of so -called core grammar are subject to cultural  constraintsﬂ  (Everett, 2 005, p. 622) . His article invoked a strong  Œ and sometimes  even emotional  Œ response, not in the least because it attacked some of the basic  assumptions of formal theories of languages (see  Pullum, 2012  for a popular  discussion of this poisonous dispute).  Most notably,  Nevins, Pesetsky, & Rodrigues  (2009)  set out to refute all or most of Everett™s claims and state that ﬁthere is no  evidence from  Pirahã  for the particular causal relation between culture and  grammatical structureﬂ  (Nevins et al., 2009, p. 355 ). While they do not deny that   10 culture exerts an influence on linguistic structure, it is clear that they would rather  minimize its role in grammatical theory.   In general, most work in this relatively small field has been altogether less  controversial. Ty pical research topics include kinship systems  (e.g. Evans, 2003) ,  gendered language  (Chafe, 2004) , and deictic systems  (Levinson, 1996) . One of the  more fascinating peripheral interests of ethnosyntax is in how our description of   grammatical structure migh t be influenced by our academic training and cultural  background  (Enfield, 2004b, p. 12) . This is the subject of  Easton &  Stebbin™s  contribut ion to this volume, who discuss  how preconceptions instilled by linguistic  tradition might have a profound effect o n how we conceptualize the linguistic  structure of languages that do not belong to that same tradition.  The general assumptions and goals of ethnosyntax, as they are set out in  Enfield  (2004b, p. 12) , are fully compatible with the research presented here,  and some of  the contributions in this volume squarely take an ethnosyntactic view on  grammatical analysis (see Burridge , this volume  and  Easton &  Stebbins,  this  volume) . One point of difference is that we assume it to be likely that certain extra -linguisti c factors beyond the realm of culture exert a direct influence on  grammatical structure without the mediation of cultural praxis. An obvious example  is the contribution of Nichols,  this volume , who describes how geographical  altitude influences the diversi fication of language   It will be clear that most of the research fields mentioned above focus primarily  on the interaction between languages and socio -cultural reality in which they are  spoken.  To a degree, our journey  in  this book will lead us through  thi s familiar  terrain  Œ it is after all meant to function as a broad overview. However, as we have  mentioned repeatedly above, it is our explicit intention to go beyond the usual  suspects and investigate less obvious connections between language and  environme ntal parameters. As mentioned in the previous paragraph, we also do not  take it as a given that interaction between grammar and the extra -linguistic  environment is necessarily mediated through culture. This intermediary function of  culture is probably comm on, but it likely not universal and has to be established for  each interaction between language and the environment.  The next section gives a  broad classification of the types of  environmental parameters which existing  research has identified as being  potential factors of influence on grammatical  structure.  3. Relevant environmental parameters  One of the goals of this volume is to  catalogue  Œ however tentatively  Œ the different  environmental parameters that are relevant  as potential  influences  on grammatic al  structures. Within the confines of a single monograph, it would be impossible to  give a complete overview, so we will here  list  major  categories of extra -linguistic  factors that  have been reported to directly influence  linguistic structure in general,    11 and grammar in particular. We will indicate in which part of this volume they are  discussed.   These categories are just intended as convenient conceptual tools for  classification and clarification; they were not inspired by any methodological  consideration,  and have no theoretical or diagnostic value. They often intersect with  more than one of the linguistic subdisciplines discussed in the previous section, and  they are not necessarily mutually exclusive.  However, they do illustrate the great  variety of fac tors that are believed to be somehow involved in the shaping of  grammatical structure. Another note of caution: not all of the research presented  below has met with equally wide acceptance from the research community; we will  indicate controversies when we  are aware of them (ignoring those that pertain to the  deep -rooted formalist -functionalist divide in linguistics). We still think it is useful  to mention such studies as they represent valuable hypotheses about plausible   connections between language and th e environment.  3.1.  Cultural factors  Under culture, we here understand cultural praxis, that is, the set of conscious  behaviours and beliefs that are associated with the expression of a coherent social  identity of a specific community. A preoccupation with the  connection between  culture and grammar has been a constant in at least part of the linguistic and  anthropological community ever since the work of Humboldt, Boas, Sapir and  Whorf  (Boas, 1938, pp. 122 Œ145; Sapir, 1912, 1921; Whorf, 1940, amongst other  publi cations) . However, the actual study of cultural influences on the formation of  grammar, or specific grammatical features has never been part of a coherent  research field, but rather existed at the fringes of mainly functional, cognitive, and  descriptive ap proaches to linguistics, at least until the coming of ethnosyntax at the  end of the 1970s (see above).  An extreme form of the influence of culture on language is  language  construction  or manipulation  as a deliberate cultural activity. Modern, relatively  we ll -known examples  of language construction  include the creation of international  languages such as Esperanto and  Volapük  and the development of fictional  languages  in literature or film  (see relevant chapters in Adams, 2011) . Deliberate  language  manipulati on is also important in secret languages.  Storch (2011)  gives a  linguistic and anthropological account  of  such languages in  Nigeria, Uganda and  the African diaspora. These languages are  typically related to  secret  ritual  knowledge  or the conservation of es tablish social boundaries (e.g. between males  and females, insiders and outsiders, etc.). Their creation often  involve s the  deliberate manipulation of  its  lexicon,  phonological system  and  Œ less commonly  Œ morphosyntax . Storch makes  a number of  interesting observations  about restrictions  on what is manipulated in  such languages, for instance that ﬁ[n]noun classes are   more easily changed than is verbal morphologyﬂ  (2011, p. 67) .  12 The influence of kinship distinctions on linguistic structure, sometimes termed  kintax , has been a fruitful subject of study for almost half a century now. Australian  languages have been a particularly productive source for such studies, starting with  Hale  (1966)  on kinship terminology in  Lardil , an Australian language. Research  mainly focuses on unusual kinship and moiety systems, their associated   classificatory system (e.g.  McConvell, 1985 ; see also  Evans, 2003, pp. 17 Œ20) and  pronominal paradigms  (Schebe ck, 1973) ,  their diachronic development,  and the  syntactic re strictions they impose . A useful overview and assessment on this  phenomenon in Australian languages from a linguistic point -of-view is given in  Evans  (2003) . Research on other cultural factors us ually lacked the thematic unity of kinship  research. Gender -based distinctions have often been linked to grammatical  developments in various languages and to language change in general  (see e.g.   Labov, 1972, p. 303) .  Chafe  (2004)  describes how in all North ern Iroquoian  languages obligatory pronominal prefixes distinguish three genders in the singular,  but neutralise the feminine -neuter distinction in dual and plural forms, with the  exception of Mohawk where female referents can be referred to by both femini ne  and neutral forms. The former indicates female elegance, the latter has been  interpreted as derogatory and conveys certain unfeminine characteristics (for  instance, when referring to uncommonly large or rude women). The masculine  forms also distinguish  more grammatical categories than the female forms. Chafe  traces this skewed paradigm back to a historical gender imbalance in Iroquoian  society, in which ﬁmen were conspicuous, often even flamboyant, and invested with  decision -making powers, whereas women  stayed in the backgroundﬂ  (Chafe, 2004,  p. 105) . On a macroscopic level, the monumental work by  Gal  (1978)  sets out how  language shift from Hungarian to German in the Austrian village of Oberwart is  among other social factors strongly correlated to the gen der and that especially  younger women are vectors for linguistic change. She concludes that abstract social   factors, such as the social network in which these women live, have no significant  impact in their tendency to switch languages. What does make a di fference is ﬁthat  in their stated attitudes and their marriage choices the women evaluate peasant life  more negatively than the men and reject the social identity of peasant wifeﬂ  (Gal,  1978, p. 14) . Their social aspirations are reflected symbolically in t heir linguistic  choices.  Other researchers have pointed out the influence of religious and abstract belief  systems on grammar.  Burridge  (2004)  convincingly argues that the religious belief  system of Pennsylvania German speakers influenced, among other things, the   degrammaticalisation of the modal verb  wotte  ‚wish™ into a fully lexical verb. More  controversially,  Everett  (2005, 2009)  attributes t he lack of grammatical number, a  complex tense system, and any form of embedding to cultural beliefs that only  value immediate experience.    13 In this book, the influence of cultural factors on grammar is discussed in Part 1.  Building on her previous research  (see above  Burridge  (2004) , Kate Burridge  discusses the influence of the belief in the supernatural on the evolution of  grammatical properties in two distinct Germanic cultures and different time periods:  the development of modal systems in Pennsylvanian G erman, the language of  modern -day Anabaptist groups in North America, and the expression of experiencer  constructions in Anglo -Saxon and early Dutch. Uri Tadmor relates the development  of the pronominal system in Onya Darat (Austronesian) to a complex kin   relationships resulting from a social life revolving around traditional long houses.   Finally, Lev  Michael  describes the grammaticalisation of quotative evidentiality  Nati (Arawak, Peru) under influence of ethical values permeating Nanti society.   3.2.  Social  factors  Social factors have to do with the social structure and general organisation of  society; they are typically outside the conscious control of individual members of  that society, in contrast to cultural factors, which are the result of beliefs and  rit ualised behaviours. The distinction between social and cultural influences on  language is to some extent artificial. For instance, in  Gal  (1978) , we considered  gender to be a cultural construct, because it is evident from Gal™s exposition that   there is a s trong component of free choice involved in the interpretation of gender -based roles in society. On the other hand, in many societies biological sex and its   associated social status is not something individuals have much control over. Gal™s  study also point s out that linguistic change is often influenced by a complex  interaction of social and cultural factors: gender, age, social status, a rural -urban  dichotomy, the wider historical and political context, etc. Despite various areas of  vagueness and complex i nteractions, there are a  considerable  number of clear -cut  cases where social factors outside the conscious control of individuals, or social  groups, exert an influence on the linguistic development. Dialectal variation, for  instance, does not typically ari se from deliberate choices made by the respective  communities, but is also influenced by rather abstract phenomena such as the   degree of isolation of the community  (Schreier, 2009) , and community size,  structure, and density  (Trudgill, 2011) . The influence  of various social factors on language development and  grammatical structure has been well -documented in sociolinguistics, dialectology  and  Œ to a lesser extent  Œ linguistic anthropology, both for well -known Western  languages and for smaller languages in o ther parts of the world. Typical factors  under research include regional variation, age, gender, ethnicity, social stratification,  genres and specific social settings, community structure, power relationships, the   acceptance and perception of specific soci al groups, and interactions between  societies. These phenomena are elaborately described in most sociolinguistic   14 handbooks  (Coulmas, 1998; Hudson, 1980; Trudgill, 2001; Wardhaugh, 2006 , and  many others) and we will here forego an in -depth overview.   Langua ge standardization is a cultural phenomenon (by our definition) in as far  as it contains a prescriptive component  (e.g. Milroy & Milroy, 1985) .  However  Haugen (1 966)  also points out that, in addition to the importance of societal  acceptance of prescriptive  norms, the development of certain speech varieties into  standard languages depends on the socio -economic and political development of  their associated heartland. The relationship between empire building and global  language development is the subject of  Os tler  (2006) ; it is unthinkable that these  processes would have no influence on the development of  certain grammatical  phenomena.  One area in which the influence of social factors on grammatical structure is  uncontroversial is the case of cultural contact,  which if sustained seems to lead   almost unavoidably to language contact. Whereas traditional research on language  contact tended to focus primarily on lexical borrowings, there is now ample  evidence that the borrowing of grammatical structures and template s through  language contact is extremely common and that almost anything grammatical can   be borrowed, from conjunctors and pron ominal paradigms to word order  (see  Matras & Sakel, 2007 for an overview and examples) . On an abstract level, we have  a decent und erstanding of the parameters involved in language conta ct and their  effect on grammar  (Aikhenvald, 2006  provides a discussion). The diversity of the  concrete social factors involved in this process means that it will take some time  before we get a truly co mprehensive view. Aikhenvald only cursorily mentions  population size, degree of urbanisation, marriage habits, trade, warfare, lifestyle and   occupation, division of labour, and religion.  One specific type of language contact has received a disproportionate  amount of  attention in the literature. Sustained contact between two or more linguistic groups  without a common language for communication  can lead  to the creation of pidgins  and  Œ if intergenerational transmission sets in  Œ creoles .  Siegel  (2009)  describes the  development of an extremely complex linguistic ecosystem on the multi -cultural  and multi -lingual plantations on Fiji, where alongside Fijian, English and  Melanesian Pidgin English, and various Indian and Austronesian languages, two  distinct  pidgins developed, Plantation Pidgin Fijian and Plantation Pidgin  Hindustani. One other well -documented cause of language contact is migration.  Clyne & al., this volume gives an overview of how this process works in the highly  multicultural context of Melb ourne Australia.  Certain global societal properties have also been postulated to influence the  grammatical structure of languages spoken in that society.  Trudgill  (2011)  Œ see  also  his contribution to  this volume  Œ argues that a major factor of grammatical  change is the relative stability of a society, its social density, and its degree of   isolation. He hypothesizes that small, relatively stable societies with a relatively  high social density are disproportionately more likely to develop complex    15 morphosynta ctic feature systems. Based on computer simulations,  Wichmann,  Stauffer, Schulze, & Holman  (2008)  somewhat controversially postulate that an  increase in population size will tend to slow down linguistic change. Similarly,  Nettle  (2012, p. 1835)  reports a ( relatively weak) correlation between population  size and the linguistic complexity of languages: ﬁLanguages of small communities   tend to have smaller phonological inventories, longer words and greater   morphological complexity than languages spoken in large r communities.ﬂ The  studies of both Wichmann and Nettle are based on quantitative analyses of data in   the  World Atlas of Language Structures  (Dryer & Haspelmath, 2011) , and have  been met with considerable criticism  Œ sometimes in healthy, sometimes in  unhe althy doses  Œ mainly related to their methodology and the granularity of the  WALS data. They both attempt to explain the development of global linguistic   properties in terms of quantifiable evolutionary processes (see also  Natural factors  below).  The influ ence of social factors on grammatical structure is discussed in Part 2 of  this book. In an extension of previous research, Trudgill postulates a relationship  between closely interconnected small -scale societies and the development of  complex and rare gramm atical features, and its implications for historical linguistics.  Clyne & al. give an overview of linguistic variation and change in migrant  commu nities in Melbourne, Australia . 3.3.  Geographical factors  The influence of geography and the geophysical environmen t on various aspects of  grammar has been a prolific subject of study, its universally perceived importance  surpassing linguistic ideologies and traditional divisions between syntax, semantics  and pragmatics. This is undoubtedly so because spatial perceptio n and reasoning is  central to our interaction with the world and is therefore prominently present in the  grammar of most languages of the world.  Traditional grammatical research on the relationship between the spatial  environment and linguistic structure h as been mainly directed towards spatial deixis.  No grammar of a language is complete without a thorough description of its  demonstrative paradigm and an overview of its directional verbs. It is impossible to  give even an incomplete overview of all the work  done in this field, nor would this  be particularly useful here: work on deixis is typically only concerned with the   grammatical behaviour of deictic categories and more often than not barely  acknowledges the external spatial reality that led to the creati on of these categories  (see  Langacker, 1982  for an example).  The role of the geographical context in the development of grammatical  categories expressing space and motion has come under systematic investigation  only recently. In the last decade, much inter esting research is connected to or  inspired by the language and space research program at the  Max Planck Institute for   16 Psycholinguistics  (Levinson, 1996, 2003; Levinson & Wilkins, 2006; Burenhult &  Levinson, 2008) . Typical topics are the conceptualisation  of space relative to  culturally determined coordinate systems (so -called frames of reference;  Levinson,  2003; Levinson & Wilkins, 2006; Senft, 2006; O™Meara & Pérez Báez, 2011)  and  the culture -specific categoriz ation of landscape terminology  (Burenhult & L evinson,  2008) . A third strand of research does not focus on the expression of space through  linguistic categories, but on the influence, either direct or mediated through culture,  of geographical features on linguistic diversity, complexity, and the globa l structure  of languages.  Nichols  (1992)  discusses possible relationships between various  properties of geographical areas and grammatical properties of languages or  language groups. She introduces the notions of spread zones and residual zones as  two geop hysical types that have a distinctive influence on language diversification.  The former are areas of swift linguistic spread, often dominated by one or a few  languages or language families; the latter tend to be conservative, highly diverse  areas of great  linguistic complexity. Her strongest claim is that a global east -west  asymmetry exists in the distribution of grammatical features such as   head/dependent marking, plurality neutralisation and the presence of an  inclusive/exclusive distinction, correspondin g to the initial spread of human  language from Africa via Europe into Asia  (Nichols, 1992, pp. 254 Œ259) . Enfield  (2005)  describes the areal spread of linguistic features, across language  family boundaries, through Mainland Southeast Asia. For instance, languages in  this area tend to be isolating, often have an elaborate class of expressive nominal  and verbal compounds form ed through alliteration or rhyme, and tend to have  classifiers, politeness distinctions in pronominal paradigms, and pragmatic  sentence -final particles. Areal features are the result of gradual diffusion through  contact (see  Social factors  above), a proces s that is influenced by social as well as  geographical factors.  A number of abstract features of the geophysical environment have been  postulated to have an influence of some sort on linguistic development. Very often,  such hypotheses have been a matter of  contention. This is not the case for  geographical distance, which has been recognized as an uncontroversial   determinant of linguistic diversity. Geographical contiguity tends to result in  language contact, and has traditionally been associated with the de velopment of  progressive variants, an exchange of linguistic features, and decrease in diversity.   On the other hand, geographical isolation is typically linked to linguistic  conservativeness and language diversification. However, as  Schreier  (2009, p. 696)  points out, linguistic isolation is hardly ever a purely geographical factor but ﬁa  multi -faceted phenomenon with regional, social and sociopsychological  dimensions.ﬂ For instance, many languages of Micronesia are part of large dialect   chains spanning ent ire small -island chains stretching out over areas of several  hundreds of kilometres, a situation that could only arise in a society with highly    17 developed seafaring technology.  Marck  (1986)  discovered that these chains were  only kept unbroken when the dista nce between two islands was at most a day™s  voyage, because this allowed the inhabitants to maintain ﬁpatterns of social  interaction with that island's inhabitants that resulted in maintenance of mutual  linguistic intelligibility between the two population s.ﬂ  In recent years, an increasing body of quantitative research has related linguistic  diversity (and sometimes complexity) to climatic zones.  Mace & Pagel  (1995)  find  that linguistic diversity follows a latitudinal gradient, with language diversity  incre asing towards the equator and, additionally, that ecological and linguistic  diversity are statistically correlated independent of latitude (see  Natural factors  below).  Nettle  (1996, 1998)  comes to very similar conclusions and attributes this  phenomenon to  the fact that, particularly in pre -modern societies, tropical and  subtropical climates are more conducive to year -round agriculture and as such  allow for the sustained existence of smaller social groups that for their survival do  not need to engage with ex ternal groups, for instance through trade. This in turn  leads to increased linguistic diversity. More controversially,  Laitin, Moortgat, &  Robinson  (2012)  claim that ﬁlinguistic diversity should be more persistent to the  degree that a geographic area is or iented more north -south than east -west.ﬂ The  underlying presumption is that, as with the spread of animals and plant species, it is  easier for languages and other cultural constructs to spread within the same climatic  band.  Part 3  of this book  contains thr ee contribution s discussing the  various  influence s of geographical factors on grammatical structure.  Palmer™s contribution  is an  excellent example of areal linguistic research in the Levinsonian tradition. He   redefines the concept of absolute frame of refe rence and argues that it is based on  topographical features of the extra -linguistic environment. Frowein describes an  unusual system of directional words in Siar, an Oceanic language spoken in New  Ireland,  that  is based on the general topography of the phy sical environment.  Finally,  Nichols  extends her definition of spread zones (linguistic areas that  facilitate the rapid expansion of languages at the expense of others; see discussion  above) by incorporating altitude and geographical delineation (open vs. c losed  areas).   3.4.  Natural factors  Especially in recent years, there has been an increased interest in the influence of   the natural environment on linguistic  Œ and more broadly cultural  Œ development.  Much of this interest appears to be inspired by a perceived  analogy between  biological and linguistic evolution, the rationale being that language is an evolved   property of the human species and that therefore language change is subject to the  same or similar adaptive pressures as biological evolution. This idea g oes back to  Darwin, who in  The Descent of Man  remarked that ﬁ[t]he survival or preservation   18 of certain favoured words in the struggle for existence is natural selectionﬂ  (Darwin,  1871, p. 61) .4 It is not exactly clear how far this correspondence between na tural  and cultural evolution should go (although it is clear that languages cannot really be  equated to living organisms). We will sidestep this thorny issue here, as we are  mainly interested in the effect of specific natural factors on grammatical  develop ment.  The search for connections between biological and linguistic diversity has often  focused on large -scale relationships between linguistic, cultural and ecological  diversity.  Apart from adherents to the ecolinguisti cs program  (see Fill &  Mühlhäusler, 2 001; Maffi, 2005 ; and also  Ecolinguistics  above), this subject has in  recent years received quite some attention from biogeographers  (see Cox & Moore,  1993 for an introduction to the field) , who through quantitative analysis or  simulation try to establish correlations between particular natural or ecological  indicators and linguistic diversity. Invariably, they are looking for direct  correlations: it is generally assumed that cultural and li nguistic diversity increase  with ecological diversity. Often, the actual subject of study is human cultural  diversity, and linguistic diversity is used as a proxy  (see e.g. Nettle, 2009) .5 A  number of studies have found quantitative evidence corroborating  this claim  (Gorenflo, Romaine, Mittermeier, & Walker -Painemilla, 2012; Mace & Pagel, 1995;  Manne, 2003) . However, not all studies find an equally  strong correlation. For  instance,  Moore et al.  (2002, p. 1645)  state that ﬁthe form of the relationships  betwe en species richness and language richness and environmental factors differs,  and it is unlikely that comparable mechanisms underpin the similar patterns of  species and language richness.ﬂ  Nevertheless , they do find strong evidence for  influences of various  natural environmental factors on both linguistic and biological  diversity.  Axelsen & Manrubia  (2014)  compare the influence of various  environmental and geographical factors on linguistic diversity in different  continental regions. Their data indicates tha t  only river density and landscape  roughness  consistently correlate to l inguistic diversity  globally; other  variables  are  only significant within certain regions. For instance, altitude only meaningfully   predicts linguistic diversity in the Americas, and a verage temperature only in Africa  and Asia, but not in Europe or the Americas.  Research like this has remained  somewhat peripheral to the linguistic enterprise, and tends to be met by varying  degrees of hostility in general linguistic circles. To our knowl edge, there are few, if  any, linguistic analyses of the direct influence (i.e. not mediated by culture) of   natural phenomena on the development of specific grammatical features.                                                               4 Darwin actually refers back to Max M üller, who  translated August Schleicher™s words in a  most poetic fashion: ﬁA struggle for life is going on amongst the words and grammatical  forms in each languageﬂ  (Müller, 1870, p. 257) . 5 This is problematic, as it is not certain that a straightforward correlation between linguistic  diversity and other cultural factors exists; see  (Moore et al., 2002) .   19 Throughout the years, a number of specific environmental factors have been  lin ked to linguistic diversity and linguistic change.  In extreme cases, cataclysmic  events  such as floods, earthquakes or volcanic eruptions  can wipe o ut entire  language communities.  Crystal  (2000, p. 71)  gives an account  how in 1998 the  destruction of three  ethnic communities on  the north coast of Papua New Guinea  by  an earthquake meant the disappearance of their three languages. Such events are  relatively rare, but their impact on linguistic diversity on a local level is absolute.  There is an obvious correla tion between geography and geology on the one hand,  and biological factors on the other. In the previous section on geographical factors,  a number of studies established a link between latitudinal bands and linguistic  diversity  (Mace & Pagel, 1995; Nettle,  1996, 1998) . In fact, this correlation does  not between linguistic diversity and geographical features by themselves, but   between linguistic diversity and the influence of various geographically climatic   factors on the natural ecosystem. As such, it is ev idence for the adaptation of  language to its natural environment.   Some research attempts to explain correlations between cultural -linguistic and  biological diversity in terms of environmental productivity. Focussing on cultural  diversity, but implying a c orrelation between ethnic and linguistic groups, in Latin  America,  Duin & Wilcox  (1994)  investigate the influence the instrumental utility of  ecological regions (i.e. their suitability for agricultural and other economic  activities) has on cultural diversi ty. They conclude that ﬁa strong argument can be  made that traditional use of biological resources, even with a cautious mix of   conventional land and resource use, could maintain and even enhance biodiversity.ﬂ  From a historical angle, it has been asserted  that the development of agriculture and  its subsequent population expansion is responsible for the spread and current  distribution of many present -day language families  (Bellwood, 2001; Diamond &  Bellwood, 2003) . This causal relationship has been most famously claimed for the  Austronesian  (e.g. Bellwood, 1984, 1995)  and the Indo -European  (Renfrew, 1987)  language families. The farming/language hypothesis has repeatedly been called into   doubt  (e.g. by Oppenheimer &  Richards, 2001; Donohue & Denham, 2010) .  Kemp  et al.  (2010)  compares the distribution of mitochondrial and Y -chromosomal DNA  to that of the Uto -Aztecan language family. They find no significant relationship  between the spread of populations and the Uto -Az tecan languages.  The results of  Hammarström  (2010)  are more ambiguous. He compares language family size and  geographical spread to the subsistence type of their associated population groups.   He finds at best a weak influence of the development of farming  on the size of  language families, although this might be due to confounding geographical or other   factors. He proposes that, alternatively, the relationship could have been the inverse:  the spread of population groups (and therefore languages) with a large  average  family size might have influenced the spread of agriculture.  More exotically, the prevalence of pathogens has been proposed as an influence  on linguistic diversity. A relationship between low pathogen stress and an increase   20 in cultural diversity or the formation of large -scale empires has been  postulated  in the anthropological literature  (e.g. Cashdan, 2001) .  Fincher & Thornhill  (2008)  postulate that ﬁhuman language richness across countries positively correlates with  parasit e richnessﬂ  (Fincher & Thornhill, 2008, p. 1293) , because the presence of  potentially dangerous disease in communities increases the chance of ostracism and   therefore social division.  On a more abstract level, various researchers have attempted to model li nguistic  change mathematically as a process similar to natural selection.  A t ypical example  is  Atkinson, Meade, Vendetti, Greenhill, & Pagel  (2008) , who argue that language  evolution is not gradual, but consists of rapid bursts followed by relatively long   periods of stagnation (see also  Dixon, 1997 , and  Silva & de Oliveira, 2008 , who  mathematically model language spread as a process of biological colonisation.   Lupyan & Dale  (2010)  and  Dale & Lupyan  (2012)  differ in their approach, in that  they model the dev elopment of morphosyntactic complexity, rather than global  linguistic diversity, in evolutionary terms as adaptations of languages to their  environment. This type of approach merges the distinction between natural and  socio -cultural influences on language.  It also builds a bridge between the studies in  human biogeography above and linguistic typological research, which focusses on  the distribution of fine -grained grammatical features across languages and language  groups. Their contribution to this volume pr esents a model for conceptualizing  linguistic differentiation as an evolutionary process driving by complex interactions  of language users and their environment.  3.5.  Human  biology   An obvious extra -linguistic influence on linguistic development is human biolog y in its widest sense. The idea that human language is restricted by various aspects of  human physiology will not sound controversial to any linguist. The extent and exact  nature of such restrictions has been, and still is, debated vigorously. This is not  a matter we will dwell upon in this book, since we are mainly interested in influences  external to the human individual.  We will shortly discuss research correlating genetics and linguistics. Since these  studies investigate plausible relations between envi ronmentally induced human  migration patterns and the resulting ethnic diversity on one hand and linguistic  differentiation on the other they are often relevant to the previous two subjections.  For instance, we mentioned above  Kemp et al.  (2010) , who use DN A analysis to  reconstruct ethnic diversity, which they then compare to the diversification of the   Uto -Aztecan language family.  Barbujani & Sokal  (1990)  report that sharp genetic  differences between adjacent populations in Europe tend to coincide either wit h substantial geographical obstacles or with boundaries between language families or   individual languages. They conclude that ﬁlanguage barriers may oppose the  process of population admixtureﬂ  (Barbujani & Sokal, 1990, p. 1818) . In all    21 likelihood, this is  then a linguistic influence on genetic diversity, rather than the  other way around.  Ward, Redd, Valencia, Frazier, & Pääbo  (1993)  compare genetic  and linguistic diffusion in three Native American tribes. Contrary to expectation,   they find that there is no  correspondence between the two factors, either in  terms of  the speed of change or the order of phylogenetic splits. This suggests that   ﬁlinguistic diversity is generated in a fundamentally different way from genetic   diversityﬂ  (Ward et al., 1993, p. 10667) . Finally,  Lansing et al.  (2007)  conclude that  on the relatively small island of Sumbawa in Indonesia, there is a clear correlation   between linguistic and Y -chromosomal differentiation, suggesting that both evolved  in tandem. It is evident from these examp les that the exact relationship between   genetic and linguistic diversity is far from clear.   3.6.  Meta -perception of language   Finally, one often -ignored influence on linguistic structure is the very act of  conducting linguistic research and, more broadly, our  perception of language. This  can affect both the structure of language itself and our perception of that structure.   The former happens when prescriptively oriented linguistic research, or societal  pressure, leads to the introduction of certain grammatical,  lexical, or phonetic  restrictions. For instance, at the time of writing, a London school introduced a  prohibition on the use of slang words and constructions, including contractions such  as  innit  ‚isn™t it™ and the informal forms of the English copula  you /we woz  ‚you/we  were™ because such terms are deemed inappropriate in formal settings  (Fishwick,  2013) . Enfield  (2004, p. 11)  remarks that ﬁgrammatical description is constrained by  culture -specific assumptions, objectives, expectations, superstitions, and  taboos.ﬂ  Some constraints might be consciously introduced, as when a grammarian selects  what they should include in and exclude from a grammar based on its target  audience  (Mithun, 2006) . This is not a problem, as long as one is aware of the fact  that some  form of selection has taken place. Other constraints are introduced by our  specific cultural and theoretical viewpoints and our education. This type of bias is  ubiquitous  in our conceptualisation of linguistic structure, and is much more  difficult to dete ct because almost ubiquitously ignored or underestimated.  Mithun  (2006, p. 285)  hints at it when she says:   A potential danger in over -inclusiveness is that of shaping the description  of a little -known language in terms of the structures currently recogni zed  in better -known languages.  LaPolla  (2001, p. 236)  points out that in their descriptions of Tibeto -Burman  languages Indian linguists tend to focus more on complex paradigms and verb  forms, while Chinese linguists often end up with tonal systems in their  descriptions.   22 These biases reflect the fact that the former are typically trained in Sanskrit, a  language with a complex morphology but without tone, and the latter in Chinese, a  tonal language with strong isolating tendencies.  Diller  (2004, p. 32)  points  out a  similar problem for theoretical analysis when he remarks that most native speaker   linguists have received a Western education and tend to analyse their languages in  terms of Western linguistic frameworks.   In the present volume, this problem is discu ssed by  Easton &  Stebbins. They  give examples of how perception of language is determined by the social values,   interests and intensions of the observer and introduce a conceptual framework for  thinking about the unavoidable conceptual biases that are intr oduced when  languages are being described, analysed, or even just talked about.   This concludes our overview of different factors that have been asserted to exert  an influence on linguistic structure. We hope we struck a good balance between  influences tha t are generally recognized to exist by the linguistic community and  those that are more exotic and less accepted, but might point towards interesting  avenues for future research. This book provides a number of case studies that  illustrate all except one of  the main categories discussed above. We aimed at a  balance between more general, theoretically oriented chapters and very specific  examples of how specific environmental factors influence specific languages.   Beckner et al.  (2009)  is one of the  increasing  number of  publications that in  recent times referred to language as a complex adaptive system.  Although he never  uses this specific term,  Keller  (1994)  has essentially the same idea, conceiving  languages as ﬁ emerged systems of social rules ﬂ (p. 43)  in which change is driven by  evolutionary change  (see also LaPolla, this volume).  If these scholars  are correct in  assuming  that language is an adaptive system , it has to be adaptive to something.  This book certainly makes no claim  at being in any way com prehensive; it is meant  as an invitation to join a n exciting new direction in linguistics, which considers  language and linguistic structure as a complex system that functions in  Œ and can  only be understood in terms of  Œ a larger context of use. If we hop e the reader will  remember one thing about this book, it is that not only is language not an   autonomous system, but its interactions with its environment are more varied and  complex than we had previously assumed.  Finally, w e would like to dedicate this  wo rk to  the late  Michael Clyne, a leading  light of the  Australian  sociolinguistics scene , known for his  research on i mmigrant  languages in Australia, but probably even more for his  friendly, compassionate   nature . At the workshop that inspired this volume, he  gave one of his last  talks. His  conference notes have been turned into  a chapter of this book  by  Yvette Slaughter,  John Hajek and Doris Schupbach . We hope it is a worthy memento to his life and   work.    23 Bibliography  Adams, M. (Ed.). (2011).  From Elvish to Kl ingon: Exploring Invented Languages . Oxford:  Oxford University Press.  Aikhenvald, A. Y. (2006). Grammars in contact: a cross -linguistic perspective. In A. Y.  Aikhenvald & R. M. W. Dixon (Eds.),  Grammars in Contact: A Cross -Linguistic  Typology  (pp. 1 Œ66). O xford: Oxford University Press.  Anderson, J. M. (2006). The non -autonomy of syntax.  Folia Linguistica , 39(3-4), 223 Œ250. Ansaldo, U. (2009).  Contact Languages: Ecology and Evolution in Asia . Cambrid ge:  Cambridge University Press.  Atkinson, Q. D., Meade, A. , Vendetti, C., Greenhill, S. J., & Pagel, M. (2008). Languages  evolve in punctual bursts.  Science , 319(5863), 588.  Axelsen, J. B., & Manrubia, S. (2014). River density and landscape roughness are universal  determinants of linguistic diversity.  Proceedings  of the Royal Society B: Biological  Sciences , 281(1784), 20133029.  Barbujani, G., & Sokal, R. R. (1990). Zones of sharp genetic change in Europe are also  linguistic boundaries.  Proceedings of the National Academy of Sciences , 87(5), 1816 Œ1819. Beckner, C.,  Blythe, R., Bybee, J., Christiansen, M. H., Croft, W., Ellis, N. C., –  Schoenemann, T. (2009). Language is a complex adaptive system: Position paper. In N.  C. Ellis & D. Larsen -Freeman (Eds.),  Language as a Complex Adaptive System  (pp. 1 Œ26). Ann Arbor, M I: Language Learning Research Club, University of Michigan.  Bellwood, P. (1984). A hypothesis for Austronesian origins.  Asian Perspectives. A Journal of  Archaeology and Prehistory of Asia and the Pacific , 16(1), 107 Œ117. Bellwood, P. (1995). Austronesian prehistory in Southeast Asia: Homeland, expansion and  transformation. In P. Bellwood, J. J. Fox, & D. Tryon (Eds.),  The Austronesians:  Historical and Comparative Perspectives  (pp. 96 Œ111). Canberra: Department of  Anthropol ogy, Australian National University.  Bellwood, P. (2001). Archaeology and the historical  determinants of punctuation in  language -family origins. In A. Y. Aikhenvald & R. M. W. Dixon (Eds.),  Areal Diffusion  and Genetic Inheritance: Problems in Comparative  Linguistics  (pp. 27 Œ43). Oxford:  Oxford University Press.  Boas, F. (1938).  General Anthropology . Boston: D. C. Heath And Company.  Burenhult, N., & Levinson, S. C. (2008). Language and landscape: a cross -linguistic  perspective.  Language Sciences , 30(2-3),  135Œ150.  Burridge, K. (2004). Changes within Pennsylvania German grammar as enactments of  Anabaptist world view. In N. J. Enfield (Ed.),  Ethnosyntax: Explorations in Grammar  and Culture  (pp. 207 Œ230). O xford: Oxford University Press.  Cashdan, E. (2001). Et hnic diversity and its environmental determinants: effects of climate,  pathogens, and habitat diversity.  American Anthropologist , 103(4), 968 Œ991.  Chafe, W. L. (2004). Masculine and feminine in the Northern Iroquoian languages. In N. J.  Enfield (Ed.),  Ethn osyntax: Explorations in Grammar and Culture . Oxford: Oxford  University Press.  Chomsky, N. (1977). Questions of form and interpretation. In  Essays on Form and  Interpretation  (pp. 25 Œ59). Amsterdam: Elsevier.  Chomsky, N. (2002).  Syntactic structures . Berlin : Mouton de Gruyter.   24 Coulmas, F. (Ed.). (1998). The Handbook of Sociolinguistics. In  The Handbook of  Sociolinguistics . Oxford: Blackwell Publishing.  Cox, C. B., & Moore, P. D. (1993).  Biogeography: An Ecological and Evolutionary  Approach, Fifth Edition . Ox ford: Blackwell.  Croft, W. (1995). Autonomy and functionalist linguistics.  Language , 71(3), 490 Œ532.  Croft, W. (2003).  Typology and Universals. Second Edition . Cambridge: Camb ridge  University Press.  Crystal, D. (2000).  Language Death . Cambridge: Cambridge  University Press.  Dale, R., & Lupyan, G. (2012). Understanding the origins of morphological diversity: The  linguistic niche hypothesis.  Advances in Complex Systems , 15(3), 1150017 Œ1Œ1150017 Œ16. Darwin, C. (1871).  The Descent of Man, and Selection in Relati on to Sex, Vol. 1  (Vol. 1).  London: John Murray.  Davidse, K. (1987). M.A.K. Halliday™s functional grammar and the Prague school. In R.  Dirven & V. Fried (Eds.),  Functionalism in Linguistics  (pp. 39 Œ79). Amsterdam: John  Benjamins.  Diamond, J., & Bellwood, P . (2003). Farmers and their languages: The first expansions.  Science , 300(5619), 597 Œ603.  Dik, S. C. (1987). Some principles of functional grammar. In R. Dirven & V. Fried (Eds.),  Functionalism in Linguistics  (pp. 81 Œ100). Amsterdam: John Benjamins.  Diller , A. V. N. (2004). Synta ctic enquiry as a cultural activity. In N. J. Enfield (Ed.),  Ethnosyntax: Explorations in Grammar and Culture  (pp. 31 Œ51). Oxford: Oxford  University Press.  Dixon, R. M. W. (1997).  The Rise and Fall of Languages . Cambridge: Cambridge  University  Press.  Donohue, M., & Denham, T. (2010). Farming and language in Island Southeast Asia:  Reframing Austronesian history.  Current Anthropology , 51(2), 223 Œ256. Dryer, M. S., & Haspelmath, M. (Eds.). (2011).  The World Atlas of Language Structures  Online . Munich: Max Planck Digital Library. Retrieved from http://wals.info  Duin, B. A., & Wilcox, K. N. (1994). Indigenous cultural and biological diversity:  Overlapping values of Latin American ecoregions.  Cultural Survival Quarterly , 18(4),  49Œ53. Enfie ld, N. J. (2004). Ethnosyntax: Introduction. In N. J. Enfield (Ed.),  Ethnosyntax:  Explorations in Grammar and Culture  (pp. 3 Œ30). O xford: Oxford University Press.  Enfield, N. J. (2005). Areal linguistics and Mainland Southeast Asia.  Annual Review of  Anthro pology , 34(1), 181 Œ206.  Evans, N. (2003). Context, culture, and structuration in the languages of Australia.  Annual  Review of Anthropology , 32(1), 13 Œ40. Everett, D. L. (2005). Cultural constraints on grammar and cognition in Pirahã: Another look  at the de sign features of human language.  Current Anthropology , 46(4), 621 Œ646.  Everett, D. L. (2009). Pirahã culture and grammar: A response to some criticisms.  Language , 85(2), 1 Œ38.  Fanselow, G. (2007). Carrots  Œ perfect as vegetables, but please not as a main dish.  Theoretical Linguistics , 33(3), 353 Œ367. Featherston, S. (2007). Data in generative grammar: The stick and the carrot.  Theoretical  Linguistics , 33(3), 269 Œ318.    25 Fill, A., &  Mühlhäusler, P. (Eds.). (2001).  Ecolinguistics Reader: Language, Ecology and  Environment . London: Continuum.  Fincher, C. L., & Thornhill, R. (2008). A parasite -driven wedge: infectious diseases may  explain language and other biodiversity.  Oikos , 117 (9), 1 289Œ1297.  Fishwick, C. (2013, October 15). London school bans pupils from using ﬁinnitﬂ, ﬁlikeﬂ, and  ﬁbare.ﬂ  The Guardian . Retrieved from http://www.theguardian.com/uk -news/2013/oct/15/london -school -bans -pupils -slang -innit  Gal, S. (1978). Peasant men can™t  get wives: Language change and sex roles in a bilingual  community.  Language in Society , 7(1), 1 Œ16.  Gombrich, E. H. (2006).  The Story of Art . London: Phaidon.  Gorenflo, L. J., Romaine, S., Mittermeier, R. A., & Walker -Painemilla, K. (2012). Co -occurrence  of linguistic and biological diversity in biodiversity hotspots and high  biodiversity wilderness areas.  Proceedings of the National Academy of Sciences . Gumperz, J. J., & Levinson, S. C. (1996a). Introduction to part I. In J. J. Gumperz & S. C.  Levinson (E ds.),  Rethinking Linguistic Relativity  (pp. 21 Œ36). Cambridge: Cambridge  University Press.  Gumperz, J. J., & Levinson, S. C. (Eds.). (1996b). Introduction: Linguistic relativity re -examined. In  Rethinking Linguistic Relativity  (pp. 1 Œ18). Cambridge: Cambri dge  University Press.  Gumperz, J. J., & Levinson, S. C. (Eds.). (1996c).  Rethinking Linguistic Relativity . Cambridge: Cambridge University Press.  Hale, K. L. (1966). Kinship reflections in syntax: Some Australian languages.  Word , 22(1-3),  318Œ324. Hammarst röm, H. (2010). A full -scale test of the language farming dispersal hypothesis.  Diachronica , 27(2), 197 Œ213.  Haugen, E. (1966). Dialect, Language, Nation.  American Anthropologist , 68(4), 922 Œ935. Haugen, E. (2001). The ecology of language. In A. Fill & P.  Mühlhäusler (Eds.),  Ecolinguistics Reader: Language, Ecology and Environment  (pp. 57 Œ66). London:  Continuum.  Hudson, R. A. (1980).  Sociolinguistics . Cambridge: Cambridge University Press.  Hymes, D. (1974).  Foundations in Sociolinguistics: An Ethnographic A pproach . Philadelphia, PA: University of Philadelphia Press.  Kaye, J. (1989).  Phonology: a cognitive view . Hillsdale, N.J: L. Erlbaum Associates.  Keller, R. (1994).  On Language Change: The Invisible Hand in Language . (B. Nerlich,  Trans.). London: Routledge . Kemp, B. M., Gonzalez -Oliver, A., Malhi, R. S., Monroe, C., Schroeder, K. B., McDonough,  J., – Smith, D. G. (2010). Evaluating the Farming/Language Dispersal Hypothesis with  genetic variation exhibited by populations in the Southwest and Mesoamerica.  Pro ceedings of the National Academy of Sciences , 107(15), 6759 Œ6764. Labov, W. (1972).  Sociolinguistic Patterns . Philadelphia, PA: Un iversity of Pennsylvania  Press.  Laitin, D. D., Moortgat, J., & Robinson, A. L. (2012). Geographic axes and the persistence of  cultural diversity.  Proceedings of the National Academy of Sciences , 201205338.  Langacker, R. W. (1982). Space grammar, analysability, and the English passive.  Language , 58(1), 22 Œ80.   26 Lansing, J. S., Cox, M. P., Downey, S. S., Gabler, B. M., Hallmark, B.,  Karafet, T. M., –  Hammer, M. F. (2007). Coevolution of languages and genes on the island of Sumba,  eastern Indonesia.  Proceedings of the National Academy of Sciences , 104(41), 16022 Œ16026. LaPolla, R. J. (2001). The role of migration and language contact i n the development of the  Sino -Tibetan language family. In A. Y. Aikhenvald & R. M. W. Dixon (Eds.),  Areal  Diffusion and Genetic Inheritance: Problems in Comparative Linguistics  (pp. 225 Œ254).  Oxford: Oxford University Press.  Levinson, S. C. (1996). Languag e and space.  Annual Review of Anthropology , 25, 353 Œ382. Levinson, S. C. (2003).  Space in Language and Cognition: Explorations in Cognitive  Diversity . Cambrid ge: Cambridge University Press.  Levinson, S. C., & Wilkins, D. P. (2006).  Grammars of Space:  Explorations in Cognitive  Diversity . Cambrid ge: Cambridge University Press.  Lucy, J. A. (1997). Linguistic Relativity.  Annual Review of Anthropology , 26, 291 Œ312. Lupyan, G., & Dale, R. (2010). Language structure is partly determined by social structure.  PLoS ONE , 5(1), e8559.  Mace, R., & Pagel, M. (1995). A latitudinal gradient in the density of human languages in  North America.  Proceedings of the Royal Society B: Biological Sciences , 261(1360),  117Œ121. Maffi, L. (2005). Linguistic, cultural, and biologi cal diversity.  Annual Review of  Anthropology , 34(1), 599 Œ617.  Manne, L. L. (2003). Nothing has yet lasted forever: current and threatened levels of  biological and cultural diversity.  Evolutionary Ecology Research , 5(4), 517 Œ527.  Marck, J. C. (1986). Micron esian dialects and the overnight voyage.  Journal of the  Polynesian Society , 95(2), 253 Œ258. Matras, Y., & Sakel, J. (Eds.). (2007).  Grammatical Borrowing in Cross -Linguistic  Perspective . Berlin: Mouton de Gruyter.  McConvell, P. (1985). The origin of subsec tions in Northern Australia.  Oceania , 56(1), 1 Œ33.  Milroy, J., & Milroy, L. (1985).  Authority in Language: Investigating Language Prescription  and Standardisation . London: Routledge.  Mithun, M. (2006). Grammars and the community.  Studies in Language , 30(2) , 281 Œ306. Moore, J. L., Manne, L., Brooks, T., Burgess, N. D., Davies, R., Rahbek, C., – Balmford, A.  (2002). The distribution of cultural and biological diversity in Africa.  Proceedings of the  Royal Society B: Biological Sciences , 269(1501), 1645 Œ1653. Mufwene, S. S. (2001).  The Ecology of Language Evolution . Cambridge: Cambridge  University Press.  Müller, M. (1870). Darwinism tested by the science of language. Translated from the  German of Professor August Schleicher.  Nature , 1(10), 256 Œ259.  Nettle, D. (1 996). Language diversity in West Africa: An ecological approach.  Journal of  Anthropological Archaeology , 15(4), 403 Œ438.  Nettle, D. (1998). Explaining global patterns of language diversity.  Journal of  Anthropological Archaeology , 17(4), 354 Œ374.  Nettle, D.  (2009). Ecological influences on human behavioural diversity: A review of recent  findings.  Trends in Ecology & Evolution , 24(11), 618 Œ624. Nettle, D. (2012). Social scale and structural complexity in human languages.  Philosophical  Transactions of the Roya l Society B: Biological Sciences , 367(1597), 1829 Œ1836.    27 Nevins, A., Pesetsky, D., & Rodrigues, C. (2009). Pirahã exceptionality: A reassessment.  Language , 85(2), 355 Œ404.  Nichols, J. (1984). Functional theories of grammar.  Annual Review of Anthropology , 13, 97 Œ117. Nichols, J. (1992).  Linguistic Diversity in Space and Time . Chicago: University of Chicago  Press.  O™Meara, C., & Pérez Báez, G. (2011). Spatial frames of reference in Mesoamerican  languages.  Language Sciences , 33(6), 837 Œ852.  Oppenheimer, S., & R ichards, M. (2001). Fast trains, slow boats, and the ancestry of the  Polynesian islanders.  Science Progress , 84(3), 157 Œ181. Ostler, N. (2006).  Empires of the Word: A Language History of the World . New York:  HarperCollins.  Pullum, G. (2012, March 28). The  rise and fall of a venomous dispute.  Lingua Franca  - The  Chronicle of Higher Education . Retrieved from  https://chronicle.com/blogs/linguafranca/2012/03/28/poisonous -dispute/  Radford, A. (2009).  Transformational Grammar: A First Course . Cambrid ge: Cambridge  University Press.  Renfrew, C. (1987).  Archaeology and Language: The Puzzle of Indo -European Origins . Cambrid ge: Cambridge University Press.  Sapir, E. (1912). Language and environment.  American Anthropologist , 14(2), 226 Œ242. Sapir, E. (1921).  Language: An  Introduction to the Study of Speech . New Yor k: Harcourt,  Brace and Company.  Schebeck, B. (1973). The Adnjamathanha personal pronoun and the ﬁWailpi kinship system.ﬂ  Papers in Australian Linguistics , 6, 1Œ45.  Schreier, D. (2009). Language in isolation, and  its implications for variation and change.  Language and Linguistics Compass , 3(2), 682 Œ699.  Senft, G. (2006). Prolegomena to a Kilivila grammar of space. In S. C. Levinson & D. P.  Wilkins (Eds.),  Grammars of Space: Explorations in Cognitive Diversity  (pp. 206Œ229).  Cambrid ge: Cambridge University Press.  Siegel, J. (2009).  Language Contact in a Plantation Environment: A Sociolinguistic History  of Fiji . Cambrid ge: Cambridge University Press.  Silva, E. J. S., & de Oliveira, V. M. (2008). Evolution of the ling uistic diversity on correlated  landscapes.  Physica a  - Statistical Mechanics and Its Applications , 387(22), 5597 Œ5601.  Storch, A. (2011).  Secret Manipulations: Language and Context in Africa . Oxford: Oxford  University Press.  Trudgill, P. (2001).  Sociolingu istics: An Introduction to Language and Society, Fourth  Edition . London: Penguin.  Trudgill, P. (2011).  Sociolinguistic Typology: Social Determinants of Linguistic Complexity . Oxford: Oxford University Press.  Ward, R. H., Redd, A., Valencia, D., Frazier, B. , & Pääbo, S. (1993). Genetic and linguistic  differentiation in the Americas.  Proceedings of the National Academy of Sciences , 90(22),  10663Œ10667. Wardhaugh, R. (2006).  An Introduction to Sociolinguistics, Fifth Edition . Malden, MA:  Blackwell Publishing.  Whorf, B. L. (1940). Science and linguistics.  Technology Review , 42, 229 Œ231, 247 Œ248.  28 Wichmann, S., Stauffer, D., Schulze, C., & Holman, E. W. (2008). Do language change rates  depend on population size?  Advances in Complex Systems , 11(3), 357 Œ369. Wierzbi cka, A. (1979). Ethno -syntax and the philosophy of grammar.  Studies in Language , 3(3), 313 Œ383.  Wierzbicka, A. (1996).  Semantics: Primes and Universals . Oxford: Oxford University Press.
Syntax	Autonomous syntax	Grammar	Generative grammar
What is Bilingualism?	Bilingualism is a term used to refer to a person who is able to speak and understand two different languages at a native level. This  is a common occurrence when a person born into a household where two languages are spoken.	A person needs to be completely fluent in two languages in order to be considered a bilingual	False	True	Bilingualism||slides||ˇˆˆ ˆˇ     ˛ˇ  ˘     ˛˚ ˚ ˛!ˇ   #˛ ˆ!$ ˛ ˆˆ     ˚$ˆ ˆ!%˚ %" "&$ ˆ!%˚ %" ˛  &$ˆ  ˆ ˆˆ %!%   ˜ !" )*$  ˆ+*$,˚-(˚-.-/ %0ˆ 12ˆ ˆ˛ 3$2 ˆˆ ˛& ˆ  ˘ ,4 ˆ/ ˆ  ˆ˙    5˛   & ˙ & ˇˇ    ˆ˙ $ˆ ˆˇ   ˆ  ˛ $˛  $   ˆ˙ ˆˇ   ˛  !  '! ˆˇ ˆˇˆ  ˛    !    ˆ˙ 3ˆ & + %˛˛'   ) (+& ˘ ) /01,  ˆ˙ ˘7 ' 7 'ˆ  ˆ    ˚˙ ˘ˇˇ  ˘ˇˇ ˛$˛&86ˆ ˚-9./  &˛ˆˇ ˆ ,  . + !ˆˆ ˆ ,:$ˆ% :; ˆˆˇ ,$;  ˙ ˘ ˇˆ  ˛ %ˆ˚-9 ˘ˆ ˇˆ   ˆ7   ˆˆˆˇ ˛ ˆˇ ˆ ˆ     ˆ ˆ ˘ ˆˆ  6ˆ ˆˇ ˆˆˇ ˇ  ˆˆ ˆ$   ˛ˇˇ      2$'& ˇ ˇˆ ˆ @ ,1 ˆ"CC./  ˆ ,"CC(/ ˆ ˛ˆ &˛˛ˇ˛ ˆ˛ &,$ˆ ;"CC4/        ˙ ˘! ˆ 1 D2ˆ ˆ     ˇ445 ˛˚ ˚˘!˘ ˇ   ,   /˘ˆˆ ˆ   " ˘!    ,   /!         6      6       6      6     6      ˛  E<  &;1:,˚--.5+$ˇ ˇ$ˇ2ˇ  ˇ  *,˚--H/˘ˆ&ˇ% ,"=+ˆ3˜ ˆ<,˚--˚ˇ6'$ˇ ˜ˇ + 3˜ :,"CC45& I&0    >,˚-94˜ %ˆ   ˝ 1 % 1˘"CC9/ˆ * 1 --H/ 1 ˚ ˆ J F%&ˆ2 #$; J˚-9./˙     J&˚-94/ˇˇ˘% % ˇ ˇ˝˛ F<%ˇˆ 2 ˆJ,˚-94/'=ˇ˘= %ˆ ˛  F<%ˇ&ˆˇ ˜˛=Jˆ5,˚-99/ ' =˘ˆˇ&  !"ˇˇ ˜J:˛ˇˆ% ,˚-9./ %˙ˇ=ˇ#    ˝ $ˇ*%0%==,˚-9./ˇ %˙ˇ2ˇˇ 5#ˇ ˜   ˇ 0,"C˚"/%˘ #˛<˚H"C˚"ˆ ˇˇ'E"C˚"CHEˇ' '''' ˝˛˚˙ 2Fˇ˘ ˚-H4'"CCC ,J˜"CC-/  ˆˆ&)0˜ˇˇ' #Jˇ,J#$˘ "CC./  %˜ˇ,˘ˆ*ˆK *CCH/ ˘#*0,0>/ ˘ ˆ,˚--C/  ˚ˆˆ!ˆˆ ˆˆˇ * ,ˆ/ 'ˆ$  ˇ ˜$ ˚C/ % 1ˆ* % #%2ˆ/  2ˇ˝ D,F Jˇ%$ˆˆ
Bilingualism	Branches of linguistics	Applied linguistics	Psycholinguistics	Key terms in linguistics
What is Propbank?	Propbank (proposition bank) is a digital collection of parsed sentences – a treebank – based on the Penn TreeBank, with other treebanks for languages other than English. The sentences are parsed and annotated with the semantic roles described in Verbnet, another major resource.  Each sentence in Propbank is linked to a list of its numbered arguments, each with a semantic (thematic) role and selection restrictions, and labeled with its Verbnet class. Verb adjuncts such as adverbials are also labeled with roles, such as temporal and manner. Propbank was made primarily for the training of automatic semantic role labelers through machine learning.	Which of the following is NOT true about Propbank?	 Only nuclear arguments of verbs are assigned semantic roles.	Propbank arguments are labeled with verb-specific roles|||There is an Arabic propbank|||All subjects in Propbank are assumed to be proto-agents	Lecture on Semantic Role Labeling||slides||L4+.,"#6)1'&5)D5"#$&'()*+,"6 O1$&6#%6($,1#,68#1"&1$&6 8,#6%>6 $%B,86%$6>%$/1BB;6&,>)",6#.,/ U>#,"6$%B,86",,&6#%6=,6>$1+/,"#,&6#%6=,6&,>)",&: 2,E)"61"&6I17717%$#6 O%E1E @fa\FCM6#<%69)"&86%>6 !JH-IPGWJ-H '%&"4#"9'$4= '%6&43#"%&6) #.1#6(1"6177,1$61868'=N,(#86 -.,6(%%96%7,",&6#.,6N1$6<)#.6#.,6",<6+1&+,#:6 -.,6",<6+1&+,#6%7,",&6#.,6N1$:6 "%$.,'%/) '%6&43#"%&6) #.1#6 (1""%# H.,BB;6 1#,6#.,68B)(,&6=1"1"16<)#.616>%$9:6 g-.,6 >%$961#,6#.,68B)(,&6=1"1"1:6 \e ;,&"4%$&'7"6)&+)&5"#$&'()4+,"6 IMN"1"4)4+,"6 M6+,",$1B)d,&6 8,/1"#)(6 $%B,8A6&,>)",&6186 7$%#%#;7,86@ L%<#; \]]\C0IU-U K*VWJ-6 0IU-U K0*-!WJ-6 OMP+4")4+,"6 M6L,>)",6$%B,8687,()>)(6#%616+$%'76%>67$,&)(1#,8 \?N4$#"Q"& L4+:@$%A  !"#$%&'()*+,") -$.",'%/ -.,60$%7%8)#)%"6Y1"96 @0$%7Y1"9 C L4+:@$%A ¥01B/,$A6G1$#.1A6L1"),B6 V)B&,1 A61"&601'B6h)"+8='$;:6faaF:6-.,6 0$%7%8)#)%"6Y1"9M6*"6*""%#1#,&6Z%$7'86%>6H,/1"#)(6I%B,8:6 @*61:)$).*+$4'A.+/:.&).;& A6e\@\CMR\ i\a^6\^ L4+:@$%A *+,"6 0$%#% K*+,"# ¥T%B)#)%"1B6)"E%BE,/,"#6)"6,E,"#6%$68#1#, ¥H,"#),"(,6 @1"&j%$6 7,$(,7#)%"C ¥Z1'8,86 1"6,E,"#6%$6(.1"+,6%>68#1#,6)"61"%#.,$671$#)()71"#6 ¥G%E,/,"#6 @$,B1#)E,6#%67%8)#)%"6%>61"%#.,$671$#)()71"# C0$%#% K01#),"# ¥P"&,$+%,86(.1"+,6%>6 8#1#, ¥Z1'81BB;6 1>>,(#,&6=;61"%#.,$6 71$#)()71"# ¥H#1#)%"1$;6 $,B1#)E,6#%6/%E,/,"#6%>61"%#.,$6 71$#)()71"# \RFollowing  Dowty 1991 L4+:@$%A *+,"6 ¥[%BB%<)"+6 L%<#; \]]\¥I%B,6&,>)")#)%"86 &,#,$/)",&6 E,$=6=;6E,$=A6 <)#.6 $,87,(#6#%6#.,6%#.,$6$%B,86 ¥H,/1"#)(6 $%B,86)"6 0$%7Y1"9 1$,6 #.'86E,$= K8,"8,687,()>)(: ¥W1(.6E,$=68,"8,6.186"'/=,$,&61$+'/,"#M6 *$+aA6*$+\A6 *$+fAk *$+aM60IU-U K*VWJ- *$+\ M0IU-U K0*-!WJ- *$+fM6'8'1BB;M6 =,",>1(#)E, A6)"8#$'/,"#A61##$)='#,A6%$6,"&6 8#1#, *$+eM6'8'1BB;M68#1$#6 7%)"#A6 =,",>1(#)E, A6)"8#$'/,"#A6%$6 1##$)='#, *$+?6 #.,6,"&6 7%)"# BC"/D EC"/F'$"#'+*)'"#$448')-$)';*+&.&)#+)<';$:&#&'$'1"*G4#6'>*"'4$G#4.+/H \X L4+:@$%A N4$#")N',"6 \]22.4¥THEPROPOSITION BANK 5thattheargumentcanbelabeleda PROTO -AGENT .Themorepatient-liketheproper- ties(undergoingchangeofstate,causallyaffectedbyanotherparticipant,stationary relativetootherparticipants,etc.),thegreaterthelikelihoodthattheargumentcan belabeleda PROTO -PATIENT .TheseconddirectionisinsteadtodeÞnesemanticrolesthatarespeciÞctoa particularverboraparticulargroupofsemanticallyrelatedverbsornouns. Inthenexttwosectionswedescribetwocommonlyusedlexicalresourcesthat makeuseofthesealternativeversionsofsemanticroles. PropBank usesbothproto- rolesandverb-speciÞcsemanticroles. FrameNetusessemanticrolesthatarespe- ciÞctoageneralsemanticideacalleda frame .22.4ThePropositionBank ThePropositionBank ,generallyreferredtoas PropBank ,isaresourceofsen- PropBank tencesannotatedwithsemanticroles.TheEnglishPropBanklabelsallthesentences inthePennTreeBank;theChinesePropBanklabelssentencesinthePennChinese TreeBank.BecauseofthedifÞcultyofdeÞningauniversalsetofthematicroles, thesemanticrolesinPropBankaredeÞnedwithrespecttoanindividualverbsense. EachsenseofeachverbthushasaspeciÞcsetofroles,whicharegivenonlynumbers ratherthannames: Arg0 ,Arg1 ,Arg2 ,andsoon.Ingeneral, Arg0 representsthe PROTO -AGENT ,and Arg1 ,the PROTO -PATIENT .Thesemanticsoftheotherroles arelessconsistent,oftenbeingdeÞnedspeciÞcallyforeachverb.Nonethelessthere aresomegeneralization;the Arg2 isoftenthebenefactive,instrument,attribute,or endstate,the Arg3 thestartpoint,benefactive,instrument,orattribute,andthe Arg4 theendpoint. HerearesomeslightlysimpliÞedPropBankentriesforonesenseeachofthe verbs agree andfall.SuchPropBankentriesarecalled frameÞles ;notethatthe deÞnitionsintheframeÞleforeachrole(ÒOtherentityagreeingÓ,ÒExtent,amount fallenÓ)areinformalglossesintendedtobereadbyhumans,ratherthanbeingformal deÞnitions.(22.11)agree.01 Arg0:Agreer Arg1:Proposition Arg2:Otherentityagreeing Ex1:[ Arg0 Thegroup] agreed [Arg1 itwouldnÕtmakeanoffer]. Ex2:[ ArgM-TMP Usually][ Arg0 John]agrees [Arg2 withMary] [Arg1 oneverything]. (22.12)fall.01Arg1:Logicalsubject,patient,thingfalling Arg2:Extent,amountfallen Arg3:startpoint Arg4:endpoint,endstateofarg1 Ex1:[ Arg1 Sales]fell[Arg4 to$25million][ Arg3 from$27million]. Ex2:[ Arg1 Theaveragejunkbond] fell[Arg2 by4.2%]. NotethatthereisnoArg0rolefor fall,becausethenormalsubjectof fallisa PROTO -PATIENT .22.4¥THEPROPOSITION BANK 5thattheargumentcanbelabeleda PROTO -AGENT .Themorepatient-liketheproper- ties(undergoingchangeofstate,causallyaffectedbyanotherparticipant,stationary relativetootherparticipants,etc.),thegreaterthelikelihoodthattheargumentcan belabeleda PROTO -PATIENT .TheseconddirectionisinsteadtodeÞnesemanticrolesthatarespeciÞctoa particularverboraparticulargroupofsemanticallyrelatedverbsornouns. Inthenexttwosectionswedescribetwocommonlyusedlexicalresourcesthat makeuseofthesealternativeversionsofsemanticroles. PropBank usesbothproto- rolesandverb-speciÞcsemanticroles. FrameNetusessemanticrolesthatarespe- ciÞctoageneralsemanticideacalleda frame .22.4ThePropositionBank ThePropositionBank ,generallyreferredtoas PropBank ,isaresourceofsen- PropBank tencesannotatedwithsemanticroles.TheEnglishPropBanklabelsallthesentences inthePennTreeBank;theChinesePropBanklabelssentencesinthePennChinese TreeBank.BecauseofthedifÞcultyofdeÞningauniversalsetofthematicroles, thesemanticrolesinPropBankaredeÞnedwithrespecttoanindividualverbsense. EachsenseofeachverbthushasaspeciÞcsetofroles,whicharegivenonlynumbers ratherthannames: Arg0 ,Arg1 ,Arg2 ,andsoon.Ingeneral, Arg0 representsthe PROTO -AGENT ,and Arg1 ,the PROTO -PATIENT .Thesemanticsoftheotherroles arelessconsistent,oftenbeingdeÞnedspeciÞcallyforeachverb.Nonethelessthere aresomegeneralization;the Arg2 isoftenthebenefactive,instrument,attribute,or endstate,the Arg3 thestartpoint,benefactive,instrument,orattribute,andthe Arg4 theendpoint. HerearesomeslightlysimpliÞedPropBankentriesforonesenseeachofthe verbs agree andfall.SuchPropBankentriesarecalled frameÞles ;notethatthe deÞnitionsintheframeÞleforeachrole(ÒOtherentityagreeingÓ,ÒExtent,amount fallenÓ)areinformalglossesintendedtobereadbyhumans,ratherthanbeingformal deÞnitions.(22.11)agree.01 Arg0:Agreer Arg1:Proposition Arg2:Otherentityagreeing Ex1:[ Arg0 Thegroup] agreed [Arg1 itwouldnÕtmakeanoffer]. Ex2:[ ArgM-TMP Usually][ Arg0 John]agrees [Arg2 withMary] [Arg1 oneverything]. (22.12)fall.01Arg1:Logicalsubject,patient,thingfalling Arg2:Extent,amountfallen Arg3:startpoint Arg4:endpoint,endstateofarg1 Ex1:[ Arg1 Sales]fell[Arg4 to$25million][ Arg3 from$27million]. Ex2:[ Arg1 Theaveragejunkbond] fell[Arg2 by4.2%]. NotethatthereisnoArg0rolefor fall,becausethenormalsubjectof fallisa PROTO -PATIENT . ;97$%&$/")+2)$) L4+.@$%A -$.",'%/ 6CHAPTER 22¥SEMANTIC ROLE LABELING ThePropBanksemanticrolescanbeusefulinrecoveringshallowsemanticin- formationaboutverbalarguments.Considertheverb increase :(22.13)increase.01 ÒgoupincrementallyÓ Arg0:causerofincrease Arg1:thingincreasing Arg2:amountincreasedby,EXT,orMNR Arg3:startpoint Arg4:endpoint APropBanksemanticrolelabelingwouldallowustoinferthecommonalityin theeventstructuresofthefollowingthreeexamples,thatis,thatineachcase BigFruitCo. isthe AGENT andthepriceofbananas isthe THEME ,despitethediffering surfaceforms. (22.14)[Arg0 BigFruitCo.]increased[ Arg1 thepriceofbananas]. (22.15)[Arg1 Thepriceofbananas]wasincreasedagain[ Arg0 byBigFruitCo.] (22.16)[Arg1 Thepriceofbananas]increased[ Arg2 5%].PropBankalsohasanumberofnon-numberedargumentscalled ArgMs ,(ArgM- TMP,ArgM-LOC,etc)whichrepresentmodiÞcationoradjunctmeanings.Theseare relativelystableacrosspredicates,soarenÕtlistedwitheachframeÞle.Datalabeled withthesemodiÞerscanbehelpfulintrainingsystemstodetecttemporal,location, ordirectionalmodiÞcationacrosspredicates.SomeoftheArgMÕsinclude: TMPwhen?yesterdayevening,now LOCwhere?atthemuseum,inSanFrancisco DIRwhereto/from?down,toBangkok MNRhow?clearly,withmuchenthusiasm PRP/CAU why?because...,inresponsetotheruling RECthemselves,eachother ADV miscellaneousPRDsecondarypredication...atethemeatraw WhilePropBankfocusesonverbs,arelatedproject,NomBank (Meyersetal., 2004)addsannotationstonounpredicates.Forexamplethenoun agreement inAppleÕsagreementwithIBM wouldbelabeledwithAppleastheArg0andIBMas theArg2.Thisallowssemanticrolelabelerstoassignlabelstoargumentsofboth verbalandnominalpredicates. 22.5FrameNet Whilemakinginferencesaboutthesemanticcommonalitiesacrossdifferentsen- tenceswith increase isuseful,itwouldbeevenmoreusefulifwecouldmakesuch inferencesinmanymoresituations,acrossdifferentverbs,andalsobetweenverbs andnouns.Forexample,weÕdliketoextractthesimilarityamongthesethreesen- tences:(22.17)[Arg1 Thepriceofbananas]increased[ Arg2 5%].(22.18)[Arg1 Thepriceofbananas]rose[ Arg2 5%].(22.19)Therehasbeena[ Arg2 5%]rise[ Arg1 inthepriceofbananas]. Notethatthesecondexampleusesthedifferentverb rise,andthethirdexample usesthenounratherthantheverb rise.WeÕdlikeasystemtorecognizethat thefa6CHAPTER 22¥SEMANTIC ROLE LABELING ThePropBanksemanticrolescanbeusefulinrecoveringshallowsemanticin- formationaboutverbalarguments.Considertheverb increase :(22.13)increase.01 ÒgoupincrementallyÓ Arg0:causerofincrease Arg1:thingincreasing Arg2:amountincreasedby,EXT,orMNR Arg3:startpoint Arg4:endpoint APropBanksemanticrolelabelingwouldallowustoinferthecommonalityin theeventstructuresofthefollowingthreeexamples,thatis,thatineachcase BigFruitCo. isthe AGENT andthepriceofbananas isthe THEME ,despitethediffering surfaceforms. (22.14)[Arg0 BigFruitCo.]increased[ Arg1 thepriceofbananas]. (22.15)[Arg1 Thepriceofbananas]wasincreasedagain[ Arg0 byBigFruitCo.] (22.16)[Arg1 Thepriceofbananas]increased[ Arg2 5%].PropBankalsohasanumberofnon-numberedargumentscalled ArgMs ,(ArgM- TMP,ArgM-LOC,etc)whichrepresentmodiÞcationoradjunctmeanings.Theseare relativelystableacrosspredicates,soarenÕtlistedwitheachframeÞle.Datalabeled withthesemodiÞerscanbehelpfulintrainingsystemstodetecttemporal,location, ordirectionalmodiÞcationacrosspredicates.SomeoftheArgMÕsinclude: TMPwhen?yesterdayevening,now LOCwhere?atthemuseum,inSanFrancisco DIRwhereto/from?down,toBangkok MNRhow?clearly,withmuchenthusiasm PRP/CAU why?because...,inresponsetotheruling RECthemselves,eachother ADV miscellaneousPRDsecondarypredication...atethemeatraw WhilePropBankfocusesonverbs,arelatedproject,NomBank (Meyersetal., 2004)addsannotationstonounpredicates.Forexamplethenoun agreement inAppleÕsagreementwithIBM wouldbelabeledwithAppleastheArg0andIBMas theArg2.Thisallowssemanticrolelabelerstoassignlabelstoargumentsofboth verbalandnominalpredicates. 22.5FrameNet Whilemakinginferencesaboutthesemanticcommonalitiesacrossdifferentsen- tenceswith increase isuseful,itwouldbeevenmoreusefulifwecouldmakesuch inferencesinmanymoresituations,acrossdifferentverbs,andalsobetweenverbs andnouns.Forexample,weÕdliketoextractthesimilarityamongthesethreesen- tences:(22.17)[Arg1 Thepriceofbananas]increased[ Arg2 5%].(22.18)[Arg1 Thepriceofbananas]rose[ Arg2 5%].(22.19)Therehasbeena[ Arg2 5%]rise[ Arg1 inthepriceofbananas]. Notethatthesecondexampleusesthedifferentverb rise,andthethirdexample usesthenounratherthantheverb rise.WeÕdlikeasystemtorecognizethat the-.)86<%'B&61BB%<6'86#%68,,6#.,6(%//%"1B)#),86)"6#.,8,6e68,"#,"(,8M  P+9'2'"46)+4)$9R3%(&6)+2)&5"):4"9'($&"<) ;4/ >P6CHAPTER 22¥SEMANTIC ROLE LABELING ThePropBanksemanticrolescanbeusefulinrecoveringshallowsemanticin- formationaboutverbalarguments.Considertheverb increase :(22.13)increase.01 ÒgoupincrementallyÓ Arg0:causerofincrease Arg1:thingincreasing Arg2:amountincreasedby,EXT,orMNR Arg3:startpoint Arg4:endpoint APropBanksemanticrolelabelingwouldallowustoinferthecommonalityin theeventstructuresofthefollowingthreeexamples,thatis,thatineachcase BigFruitCo. isthe AGENT andthepriceofbananas isthe THEME ,despitethediffering surfaceforms. (22.14)[Arg0 BigFruitCo.]increased[ Arg1 thepriceofbananas]. (22.15)[Arg1 Thepriceofbananas]wasincreasedagain[ Arg0 byBigFruitCo.] (22.16)[Arg1 Thepriceofbananas]increased[ Arg2 5%].PropBankalsohasanumberofnon-numberedargumentscalled ArgMs ,(ArgM- TMP,ArgM-LOC,etc)whichrepresentmodiÞcationoradjunctmeanings.Theseare relativelystableacrosspredicates,soarenÕtlistedwitheachframeÞle.Datalabeled withthesemodiÞerscanbehelpfulintrainingsystemstodetecttemporal,location, ordirectionalmodiÞcationacrosspredicates.SomeoftheArgMÕsinclude: TMPwhen?yesterdayevening,now LOCwhere?atthemuseum,inSanFrancisco DIRwhereto/from?down,toBangkok MNRhow?clearly,withmuchenthusiasm PRP/CAU why?because...,inresponsetotheruling RECthemselves,eachother ADV miscellaneousPRDsecondarypredication...atethemeatraw WhilePropBankfocusesonverbs,arelatedproject,NomBank (Meyersetal., 2004)addsannotationstonounpredicates.Forexamplethenoun agreement inAppleÕsagreementwithIBM wouldbelabeledwithAppleastheArg0andIBMas theArg2.Thisallowssemanticrolelabelerstoassignlabelstoargumentsofboth verbalandnominalpredicates. 22.5FrameNet Whilemakinginferencesaboutthesemanticcommonalitiesacrossdifferentsen- tenceswith increase isuseful,itwouldbeevenmoreusefulifwecouldmakesuch inferencesinmanymoresituations,acrossdifferentverbs,andalsobetweenverbs andnouns.Forexample,weÕdliketoextractthesimilarityamongthesethreesen- tences:(22.17)[Arg1 Thepriceofbananas]increased[ Arg2 5%].(22.18)[Arg1 Thepriceofbananas]rose[ Arg2 5%].(22.19)Therehasbeena[ Arg2 5%]rise[ Arg1 inthepriceofbananas]. Notethatthesecondexampleusesthedifferentverb rise,andthethirdexample usesthenounratherthantheverb rise.WeÕdlikeasystemtorecognizethat thef\ArgM - L4+:@$%A'%/ $)!"%&"%(" PropBank - A  TreeBanked  Sentence  Analysts  S NP-SBJ  VP  have  VP  been  VP  expecting  NP  a GM-Jaguar  pact  NP  that  SBAR  WHNP-1 *T*-1  S NP-SBJ  VP  would  VP  give  the US car  maker  NP  NP  an eventual  30% stake  NP  the British  company  NP  PP-LOC  in  (S (NP-SBJ  Analysts )      (VP  have           (VP  been              (VP  expecting             (NP (NP  a GM-Jaguar pact )                    (SBAR (WHNP -1 that)                  (S (NP-SBJ  *T*-1 )                             (VP  would               (VP  give                                     (NP  the U.S. car maker )                  (NP (NP  an eventual  (ADJP  30 %) stake )              (PP-LOC  in (NP  the British company )))))))))))) Analysts have been expecting a GM-Jaguar   pact that  would give the U.S. car maker an   eventual 30% stake in the British company.  ffG1$#.1601B/,$6fa\e A sample  parse  tree  D5") 6$#"):$46")&4"") L4+:@$%A"9 The same sentence, PropBanked  Analysts  have been expecting  a GM-Jaguar  pact  Arg0  Arg1  (S Arg0  (NP-SBJ  Analysts )      (VP  have           (VP  been              (VP  expecting             Arg1  (NP (NP  a GM-Jaguar pact )                    (SBAR (WHNP -1 that)                        (S  Arg0  (NP-SBJ  *T*-1 )                             (VP  would                     (VP  give                                           Arg2  (NP  the U.S. car maker )                     Arg1  (NP (NP  an eventual  (ADJP  30 % ) stake)               (PP-LOC  in (NP  the British  company )))))))))))) that would give  *T*-1  the US car  maker  an eventual 30% stake in the  British company   Arg0  Arg2  Arg1  expect(Analysts, GM-J pact)  give(GM-J pact, US car maker, 30% stake)  feG1$#.1601B/,$6fa\e  ;%%+&$&"9) L4+:@$%A C$&$ ¥0,""6W"+B)8.6 -$,,Y1"9 A6U"#%J%#,8 F:a:6 ¥-%#1B6lf6/)BB)%"6<%$&8 ¥0,""6Z.)",8,6 -$,,Y1"9 ¥O)"&)jP$&'6 0$%7Y1"9 ¥*$1=)(6 0$%7Y1"9 f?Verb Frames Coverage By Language Ð   Current Count of Senses (lexical units)  Language Final Count  Estimated Coverage  in Running Text  English    10,615*  99% Chinese  24, 642  98% Arabic      7,015  99%  ¥!Only 111 English adjectives  54 fa\e6T,$=6 [$1/,86Z%E,$1+,6 Z%'"#6%>6<%$&68,"8,6@B,D)(1B6'")#8C [$%/6G1$#.1601B/,$6fa\e6-'#%$)1B  L,36)%+3%6)$%9),'/5&)7"4.6 English Noun and LVC annotation  !!Example Noun:  Decision  !!Roleset:  Arg0: decider , Arg1: decision É !!ÒÉ[ your ARG0 ] [ decision REL]       [to say look I don't want to go through this anymore ARG1 ]Ó  !!Example within an LVC:  Make a decision  !!ÒÉ[ the President ARG0 ] [ made REL-LVB ]        the [ fundamentally  correct ARGM -ADJ ]       [decision REL]  [ to get on offense ARG1 ]Ó  57 fFHB)&,6 >$%/601B/,$6fa\e
Propbank	Treebanks	Semantic role labeling (SRL)	Thematic roles	Semantic roles	Machine learning
Explain the steps involved in calculating the probability of the phrase “tricks are for kids” occurring in a particular corpus using a bigram count-based model, relative frequencies, and log probabilities.	First one needs to count the numbers of occurrences of all the word-types and bigrams in one's corpus. Then one needs to calculate the relative frequencies of each bigram appearing in the given phrase (“tricks are”, “are for”, and “for kids”).  The relative frequency of each bigram is the raw count of the bigram divided by the raw count of the first word in the bigram.  Then to find the log probability, one multiplies the logs of each relative frequency and takes the exponential of the result. That's it.	Using bigram models to predict the probabilities of words occurring in context does NOT depend on which of the following assumptions?	That the probabilities of words occurring depend mainly on their immediate contexts	That the language from which the bigram model was derived came from a source similar to that of the language being predicted|||That the bigram corpus is sufficiently large|||Using bigram models successfully depends on all these assumptions	Chapter Section on N-gram modeling||publication||SpeechandLanguageProcessing.DanielJurafsky&JamesH.Martin.Copyright c  2019.All rightsreserved.DraftofOctober2,2019. CHAPTER 3 N-gramLanguageModels ﬁYouareuniformlycharming!ﬂcriedhe,withasmileofassociatingandnow andthenIbowedandtheyperceivedachaiseandfourtowishfor. RandomsentencegeneratedfromaJaneAustentrigrammodel Predictingisdifaboutthefuture,astheoldquipgoes.Buthow aboutpredictingsomethingthatseemsmucheasier,likethenextfewwordssomeone isgoingtosay?Whatword,forexample,islikelytofollow Pleaseturnyourhomework... Hopefully,mostofyouconcludedthataverylikelywordis in ,orpossibly over , butprobablynot refrigerator or the .Inthefollowingsectionswewillformalize thisintuitionbyintroducingmodelsthatassigna probability toeachpossiblenext word.Thesamemodelswillalsoservetoassignaprobabilitytoanentiresentence. Suchamodel,forexample,couldpredictthatthefollowingsequencehasamuch higherprobabilityofappearinginatext: allofasuddenInoticethreeguysstandingonthesidewalk thandoesthissamesetofwordsinadifferentorder: onguysallIofnoticesidewalkthreeasuddenstandingthe Whywouldyouwanttopredictupcomingwords,orassignprobabilitiestosen- tences?Probabilitiesareessentialinanytaskinwhichwehavetoidentifywordsin noisy,ambiguousinput,like speechrecognition .Foraspeechrecognizertorealize thatyousaid Iwillbebacksoonish andnot Iwillbebassoondish ,ithelpstoknow that backsoonish isamuchmoreprobablesequencethan bassoondish .Forwriting toolslike spellingcorrection or grammaticalerrorcorrection ,weneedtoand correcterrorsinwritinglike Theiraretwomidterms ,inwhich There wasmistyped as Their ,or Everythinghasimprove ,inwhich improve shouldhavebeen improved . Thephrase Thereare willbemuchmoreprobablethan Theirare ,and hasimproved than hasimprove ,allowingustohelpusersbydetectingandcorrectingtheseerrors. Assigningprobabilitiestosequencesofwordsisalsoessentialin machinetrans- lation .SupposewearetranslatingaChinesesourcesentence: Ö  °  Ë Í ƒ; † – ¹ Hetoreportersintroducedmaincontent Aspartoftheprocesswemighthavebuiltthefollowingsetofpotentialrough Englishtranslations: heintroducedreporterstothemaincontentsofthestatement hebriefedtoreportersthemaincontentsofthestatement hebriefedreportersonthemaincontentsofthestatement  2 C HAPTER 3  N- GRAM L ANGUAGE M ODELS Aprobabilisticmodelofwordsequencescouldsuggestthat briefedreporterson isamoreprobableEnglishphrasethan briefedtoreporters (whichhasanawkward to after briefed )or introducedreportersto (whichusesaverbthatisless Englishinthiscontext),allowingustocorrectlyselecttheboldfacedsentenceabove. Probabilitiesarealsoimportantfor augmentativeandalternativecommuni- cation systems( Trnkaetal.2007 , Kaneetal.2017 ).Peopleoftenusesuch AAC AAC devicesiftheyarephysicallyunableorsignbutcaninsteadusingeyegazeorother movementstoselectwordsfromamenutobespokenbythesystem.Word predictioncanbeusedtosuggestlikelywordsforthemenu. Modelsthatassignprobabilitiestosequencesofwordsarecalled languagemod- els or LMs .Inthischapterweintroducethesimplestmodelthatassignsprobabilities languagemodel LM tosentencesandsequencesofwords,the n-gram .Ann-gramisasequenceof N  words:a2-gram(or bigram )isatwo-wordsequenceofwordslikeﬁpleaseturnﬂ, ﬁturnyourﬂ,orﬂyourhomeworkﬂ,anda3-gram(or trigram )isathree-wordse- quenceofwordslikeﬁpleaseturnyourﬂ,orﬁturnyourhomeworkﬂ.We'llseehow tousen-grammodelstoestimatetheprobabilityofthelastwordofann-gramgiven thepreviouswords,andalsotoassignprobabilitiestoentiresequences.Inabitof terminologicalambiguity,weusuallydropthewordﬁmodelﬂ,andthustheterm n- gram isusedtomeaneitherthewordsequenceitselforthepredictivemodelthat assignsitaprobability.Inlaterchapterswe'llintroducemoresophisticatedlanguage modelsliketheRNNLMsofChapter9. 3.1N-Grams Let'sbeginwiththetaskofcomputing P ( w j h ) ,theprobabilityofaword w given somehistory h .Supposethehistory h isﬁ itswaterissotransparentthat ﬂandwe wanttoknowtheprobabilitythatthenextwordis the : P ( the j itswaterissotransparentthat ) : (3.1) Onewaytoestimatethisprobabilityisfromrelativefrequencycounts:takea verylargecorpus,countthenumberoftimeswesee itswaterissotransparentthat , andcountthenumberoftimesthisisfollowedby the .Thiswouldbeansweringthe questionﬁOutofthetimeswesawthehistory h ,howmanytimeswasitfollowedby theword w ﬂ,asfollows: P ( the j itswaterissotransparentthat )= C ( itswaterissotransparentthatthe ) C ( itswaterissotransparentthat ) (3.2) Withalargeenoughcorpus,suchastheweb,wecancomputethesecountsand estimatetheprobabilityfromEq. 3.2 .Youshouldpausenow,gototheweb,and computethisestimateforyourself. Whilethismethodofestimatingprobabilitiesdirectlyfromcountsworksin manycases,itturnsoutthateventhewebisn'tbigenoughtogiveusgoodestimates inmostcases.Thisisbecauselanguageiscreative;newsentencesarecreatedallthe time,andwewon'talwaysbeabletocountentiresentences.Evensimpleextensions oftheexamplesentencemayhavecountsofzeroontheweb(suchasﬁ Walden Pond'swaterissotransparentthatthe ﬂ;well, usedto havecountsofzero).  3.1  N-G RAMS 3 Similarly,ifwewantedtoknowthejointprobabilityofanentiresequenceof wordslike itswaterissotransparent ,wecoulddoitbyaskingﬁoutofallpossible sequencesofvewords,howmanyofthemare itswaterissotransparent ?ﬂWe wouldhavetogetthecountof itswaterissotransparent anddividebythesumof thecountsofallpossiblevewordsequences.Thatseemsratheralottoestimate! Forthisreason,we'llneedtointroduceclevererwaysofestimatingtheproba- bilityofaword w givenahistory h ,ortheprobabilityofanentirewordsequence W . Let'sstartwithalittleformalizingofnotation.Torepresenttheprobabilityofapar- ticularrandomvariable X i takingonthevalueﬁtheﬂ,or P ( X i = ﬁtheﬂ ) ,wewilluse the P ( the ) .We'llrepresentasequenceof N wordseitheras w 1 ::: w n or w n 1 (sotheexpression w n  1 1 meansthestring w 1 ; w 2 ;:::; w n  1 ).Forthejointprob- abilityofeachwordinasequencehavingaparticularvalue P ( X = w 1 ; Y = w 2 ; Z = w 3 ;:::; W = w n ) we'lluse P ( w 1 ; w 2 ;:::; w n ) . Nowhowcanwecomputeprobabilitiesofentiresequenceslike P ( w 1 ; w 2 ;:::; w n ) ? Onethingwecandoisdecomposethisprobabilityusingthe chainruleofproba- bility : P ( X 1 ::: X n )= P ( X 1 ) P ( X 2 j X 1 ) P ( X 3 j X 2 1 ) ::: P ( X n j X n  1 1 ) = n Y k = 1 P ( X k j X k  1 1 ) (3.3) Applyingthechainruletowords,weget P ( w n 1 )= P ( w 1 ) P ( w 2 j w 1 ) P ( w 3 j w 2 1 ) ::: P ( w n j w n  1 1 ) = n Y k = 1 P ( w k j w k  1 1 ) (3.4) Thechainruleshowsthelinkbetweencomputingthejointprobabilityofase- quenceandcomputingtheconditionalprobabilityofawordgivenpreviouswords. Equation 3.4 suggeststhatwecouldestimatethejointprobabilityofanentirese- quenceofwordsbymultiplyingtogetheranumberofconditionalprobabilities.But usingthechainruledoesn'treallyseemtohelpus!Wedon'tknowanywayto computetheexactprobabilityofawordgivenalongsequenceofprecedingwords, P ( w n j w n  1 1 ) .Aswesaidabove,wecan'tjustestimatebycountingthenumberof timeseverywordoccursfollowingeverylongstring,becauselanguageiscreative andanyparticularcontextmighthaveneveroccurredbefore! Theintuitionofthen-grammodelisthatinsteadofcomputingtheprobabilityof awordgivenitsentirehistory,wecan approximate thehistorybyjustthelastfew words. The bigram model,forexample,approximatestheprobabilityofawordgiven bigram allthepreviouswords P ( w n j w n  1 1 ) byusingonlytheconditionalprobabilityofthe precedingword P ( w n j w n  1 ) .Inotherwords,insteadofcomputingtheprobability P ( the j WaldenPond'swaterissotransparentthat ) (3.5) weapproximateitwiththeprobability P ( the j that ) (3.6)  4 C HAPTER 3  N- GRAM L ANGUAGE M ODELS Whenweuseabigrammodeltopredicttheconditionalprobabilityofthenext word,wearethusmakingthefollowingapproximation: P ( w n j w n  1 1 ) ˇ P ( w n j w n  1 ) (3.7) Theassumptionthattheprobabilityofaworddependsonlyonthepreviousword iscalleda Markov assumption.Markovmodelsaretheclassofprobabilisticmodels Markov thatassumewecanpredicttheprobabilityofsomefutureunitwithoutlookingtoo farintothepast.Wecangeneralizethebigram(whichlooksonewordintothepast) tothetrigram(whichlookstwowordsintothepast)andthustothe n-gram (which  looks n  1wordsintothepast). Thus,thegeneralequationforthisn-gramapproximationtotheconditional probabilityofthenextwordinasequenceis P ( w n j w n  1 1 ) ˇ P ( w n j w n  1 n  N + 1 ) (3.8) Giventhebigramassumptionfortheprobabilityofanindividualword,wecan computetheprobabilityofacompletewordsequencebysubstitutingEq. 3.7 into Eq. 3.4 : P ( w n 1 ) ˇ n Y k = 1 P ( w k j w k  1 ) (3.9) Howdoweestimatethesebigramorn-gramprobabilities?Anintuitivewayto estimateprobabilitiesiscalled maximumlikelihoodestimation or MLE .Weget maximum likelihood estimation theMLEestimatefortheparametersofann-grammodelbygettingcountsfroma corpus,and normalizing thecountssothattheyliebetween0and1. 1 normalize Forexample,tocomputeaparticularbigramprobabilityofaword y givena previousword x ,we'llcomputethecountofthebigram C ( xy ) andnormalizebythe sumofallthebigramsthatsharethesameword x : P ( w n j w n  1 )= C ( w n  1 w n ) P w C ( w n  1 w ) (3.10) Wecansimplifythisequation,sincethesumofallbigramcountsthatstartwith agivenword w n  1 mustbeequaltotheunigramcountforthatword w n  1 (thereader shouldtakeamomenttobeconvincedofthis): P ( w n j w n  1 )= C ( w n  1 w n ) C ( w n  1 ) (3.11) Let'sworkthroughanexampleusingamini-corpusofthreesentences.We'll needtoaugmenteachsentencewithaspecialsymbol < s > atthebeginning ofthesentence,togiveusthebigramcontextoftheword.We'llalsoneeda specialend-symbol. </s> 2 <s>IamSam</s> <s>SamIam</s> <s>Idonotlikegreeneggsandham</s> 1 Forprobabilisticmodels,normalizingmeansdividingbysometotalcountsothattheresultingprob- abilitiesfalllegallybetween0and1. 2 Weneedtheend-symboltomakethebigramgrammaratrueprobabilitydistribution.Withoutan end-symbol,thesentenceprobabilitiesforallsentencesofagivenlengthwouldsumtoone.Thismodel wouldansetofprobabilitydistributions,withonedistributionpersentencelength.See Exercise3. 5 .  3.1  N-G RAMS 5 Herearethecalculationsforsomeofthebigramprobabilitiesfromthiscorpus P ( I|<s> )= 2 3 = : 67 P ( Sam|<s> )= 1 3 = : 33 P ( am|I )= 2 3 = : 67 P ( </s>|Sam )= 1 2 = 0 : 5 P ( Sam|am )= 1 2 = : 5 P ( do|I )= 1 3 = : 33 ForthegeneralcaseofMLEn-gramparameterestimation: P ( w n j w n  1 n  N + 1 )= C ( w n  1 n  N + 1 w n ) C ( w n  1 n  N + 1 ) (3.12) Equation 3.12 (likeEq. 3.11 )estimatesthen-gramprobabilitybydividingthe observedfrequencyofaparticularsequencebytheobservedfrequencyofa Thisratioiscalleda relativefrequency .Wesaidabovethatthisuseofrelative relative frequency frequenciesasawaytoestimateprobabilitiesisanexampleofmaximumlikelihood estimationorMLE.InMLE,theresultingparametersetmaximizesthelikelihood ofthetrainingset T giventhemodel M (i.e., P ( T j M ) ).Forexample,supposethe word Chinese occurs400timesinacorpusofamillionwordsliketheBrowncorpus. Whatistheprobabilitythatarandomwordselectedfromsomeothertextof,say, amillionwordswillbetheword Chinese ?TheMLEofitsprobabilityis 400 1000000 or : 0004.Now : 0004isnotthebestpossibleestimateoftheprobabilityof Chinese occurringinallsituations;itmightturnoutthatinsomeothercorpusorcontext Chinese isaveryunlikelyword.Butitistheprobabilitythatmakesit mostlikely thatChinesewilloccur400timesinamillion-wordcorpus.Wepresentwaysto modifytheMLEestimatesslightlytogetbetterprobabilityestimatesinSection 3.4 . Let'smoveontosomeexamplesfromaslightlylargercorpusthanour14-word exampleabove.We'llusedatafromthenow-defunctBerkeleyRestaurantProject, adialoguesystemfromthelastcenturythatansweredquestionsaboutadatabase ofrestaurantsinBerkeley,California (Jurafskyetal.,1994) .Herearesometext- normalizedsampleuserqueries(asampleof9332sentencesisonthewebsite): canyoutellmeaboutanygoodcantoneserestaurantscloseby midpricedthaifoodiswhati'mlookingfor tellmeaboutchezpanisse canyougivemealistingofthekindsoffoodthatareavailable i'mlookingforagoodplacetoeatbreakfast wheniscaffeveneziaopenduringtheday Figure 3.1 showsthebigramcountsfromapieceofabigramgrammarfromthe BerkeleyRestaurantProject.Notethatthemajorityofthevaluesarezero.Infact, wehavechosenthesamplewordstocoherewitheachother;amatrixselectedfrom arandomsetofsevenwordswouldbeevenmoresparse. Figure 3.2 showsthebigramprobabilitiesafternormalization(dividingeachcell inFig. 3.1 bytheappropriateunigramforitsrow,takenfromthefollowingsetof unigramprobabilities): i want to eat chinese food lunch spend 2533 927 2417 746 158 1093 341 278 Hereareafewotherusefulprobabilities: P ( i|<s> )= 0 : 25 P ( english|want )= 0 : 0011 P ( food|english )= 0 : 5 P ( </s>|food )= 0 : 68 Nowwecancomputetheprobabilityofsentenceslike IwantEnglishfood or IwantChinesefood bysimplymultiplyingtheappropriatebigramprobabilitiesto- gether,asfollows:  6 C HAPTER 3  N- GRAM L ANGUAGE M ODELS iwanttoeatchinesefoodlunchspend i 5827 0 9 0 0 0 2 want 2 0 60816651 to 2 0 46862 0 6211 eat 0 0 2 0 16242 0 chinese 1 0 0 0 0 821 0 food 15 0 15 0 14 0 0 lunch 2 0 0 0 0 1 0 0 spend 1 0 1 0 0 0 0 0 Figure3.1 Bigramcountsforeightofthewords(outof V = 1446)intheBerkeleyRestau- rantProjectcorpusof9332sentences.Zerocountsareingray. iwanttoeatchinesefoodlunchspend i 0.0020.33 0 0.0036 0 0 0 0.00079 want 0.0022 0 0.660.00110.00650.00650.00540.0011 to 0.00083 0 0.00170.280.00083 0 0.00250.087 eat 0 0 0.0027 0 0.0210.00270.056 0 chinese 0.0063 0 0 0 0 0.520.0063 0 food 0.014 0 0.014 0 0.000920.0037 0 0 lunch 0.0059 0 0 0 0 0.0029 0 0 spend 0.0036 0 0.0036 0 0 0 0 0 Figure3.2 BigramprobabilitiesforeightwordsintheBerkeleyRestaurantProjectcorpus of9332sentences.Zeroprobabilitiesareingray. P ( <s>iwantenglishfood</s> ) = P ( i|<s> ) P ( want|i ) P ( english|want ) P ( food|english ) P ( </s>|food ) = : 25  : 33  : 0011  0 : 5  0 : 68 = : 000031 WeleaveitasExercise3. 2 tocomputetheprobabilityof iwantchinesefood . Whatkindsoflinguisticphenomenaarecapturedinthesebigramstatistics? Someofthebigramprobabilitiesaboveencodesomefactsthatwethinkofasstrictly syntactic innature,likethefactthatwhatcomesafter eat isusuallyanounoran adjective,orthatwhatcomesafter to isusuallyaverb.Othersmightbeafactabout thepersonalassistanttask,likethehighprobabilityofsentencesbeginningwith thewords I .Andsomemightevenbeculturalratherthanlinguistic,likethehigher probabilitythatpeoplearelookingforChineseversusEnglishfood. Somepracticalissues: Althoughforpedagogicalpurposeswehaveonlydescribed bigrammodels,inpracticeit'smorecommontouse trigram models,whichcon- trigram ditionontheprevioustwowordsratherthanthepreviousword,or 4-gram oreven  5-gram models,whenthereissuftrainingdata.Notethatfortheselargern-  grams,we'llneedtoassumeextracontextforthecontextstotheleftandrightofthe sentenceend.Forexample,tocomputetrigramprobabilitiesattheverybeginningof thesentence,wecanusetwopseudo-wordsforthetrigram(i.e., P ( I|<s><s> ) . Wealwaysrepresentandcomputelanguagemodelprobabilitiesinlogformat as logprobabilities .Sinceprobabilitiesare(bylessthanorequalto log probabilities 1,themoreprobabilitieswemultiplytogether,thesmallertheproductbecomes. Multiplyingenoughn-gramstogetherwouldresultinnumericalw.Byusing logprobabilitiesinsteadofrawprobabilities,wegetnumbersthatarenotassmall.  3.2  E VALUATING L ANGUAGE M ODELS 7 Addinginlogspaceisequivalenttomultiplyinginlinearspace,sowecombinelog probabilitiesbyaddingthem.Theresultofdoingallcomputationandstorageinlog spaceisthatweonlyneedtoconvertbackintoprobabilitiesifweneedtoreport themattheend;thenwecanjusttaketheexpofthelogprob: p 1  p 2  p 3  p 4 = exp ( log p 1 + log p 2 + log p 3 + log p 4 ) (3.13) 3.2EvaluatingLanguageModels Thebestwaytoevaluatetheperformanceofalanguagemodelistoembeditin anapplicationandmeasurehowmuchtheapplicationimproves.Suchend-to-end evaluationiscalled extrinsicevaluation .Extrinsicevaluationistheonlywayto extrinsic evaluation knowifaparticularimprovementinacomponentisreallygoingtohelpthetask athand.Thus,forspeechrecognition,wecancomparetheperformanceoftwo languagemodelsbyrunningthespeechrecognizertwice,oncewitheachlanguage model,andseeingwhichgivesthemoreaccuratetranscription. Unfortunately,runningbigNLPsystemsend-to-endisoftenveryexpensive.In- stead,itwouldbenicetohaveametricthatcanbeusedtoquicklyevaluatepotential improvementsinalanguagemodel.An intrinsicevaluation metricisonethatmea- intrinsic evaluation suresthequalityofamodelindependentofanyapplication. Foranintrinsicevaluationofalanguagemodelweneeda testset .Aswithmany ofthestatisticalmodelsinourtheprobabilitiesofann-grammodelcomefrom thecorpusitistrainedon,the trainingset or trainingcorpus .Wecanthenmeasure trainingset thequalityofann-grammodelbyitsperformanceonsomeunseendatacalledthe testset ortestcorpus.Wewillalsosometimescalltestsetsandotherdatasetsthat testset arenotinourtrainingsets heldout corporabecauseweholdthemoutfromthe heldout trainingdata. Soifwearegivenacorpusoftextandwanttocomparetwodifferentn-gram models,wedividethedataintotrainingandtestsets,traintheparametersofboth modelsonthetrainingset,andthencomparehowwellthetwotrainedmodelsthe testset. Butwhatdoesitmeantothetestsetﬂ?Theanswerissimple:whichever modelassignsa higherprobability tothetestsetŠmeaningitmoreaccurately predictsthetestsetŠisabettermodel.Giventwoprobabilisticmodels,thebetter modelistheonethathasatightertothetestdataorthatbetterpredictsthedetails ofthetestdata,andhencewillassignahigherprobabilitytothetestdata. Sinceourevaluationmetricisbasedontestsetprobability,it'simportantnotto letthetestsentencesintothetrainingset.Supposewearetryingtocomputethe probabilityofaparticularﬁtestﬂsentence.Ifourtestsentenceispartofthetraining corpus,wewillmistakenlyassignitanhighprobabilitywhenitoccurs inthetestset.Wecallthissituation trainingonthetestset .Trainingonthetest setintroducesabiasthatmakestheprobabilitiesalllooktoohigh,andcauseshuge inaccuraciesin perplexity ,theprobability-basedmetricweintroducebelow. Sometimesweuseaparticulartestsetsooftenthatweimplicitlytunetoits characteristics.Wethenneedafreshtestsetthatistrulyunseen.Insuchcases,we calltheinitialtestsetthe development testsetor, devset .Howdowedivideour development test dataintotraining,development,andtestsets?Wewantourtestsettobeaslarge aspossible,sinceasmalltestsetmaybeaccidentallyunrepresentative,butwealso wantasmuchtrainingdataaspossible.Attheminimum,wewouldwanttopick
Count-based models	N-gram model	Bigrams	Relative frequency	Log probability
What is meant by "scaffolding" in second language learning?	Scaffolding refers to giving contextual support in the process of language learning by using graphics, visuals, and simplified language. This support is provided by the teacher. The scaffold is gradually removed as students become more proficient.	Which method of foreign language teaching can scaffolding NOT be a part of? 	Grammar translation	The audio-lingual method|||The direct method|||The structural approach	Scaffolding Academic Learning for Second Language Learners||document||The Internet TESL Journal Scaffolding Academic Learning for Second Language Learners Karen Sue Bradley & Jack Alden Bradley kfksb00 [at] tamuk.edu Texas A&M University (Kingsville, Texas, USA) Introduction What is meant by the term scaffolding? "Scaffolding refers to providing contextual supports for meaning through the use of simplified language, teacher modeling, visuals and graphics, cooperative learning and hands-on learning" (Ovando, Collier, & Combs, 2003, p. 345). The teacher of second language learners has to facilitate that support. Then, "as students become more proficient, the scaffold is gradually removed" (Diaz-Rico & Weed, 2002, p. 85). Three types of scaffolding have been identified as being especially effective for second language learners. 1. Simplifying the language: The teacher can simplify the language by shortening selections, speaking in the present tense, and avoiding the use of idioms. 2. Asking for completion, not generation: The teacher can have students choose answers from a list or complete a partially finished outline or paragraph. 3. Using visuals: The teacher can present information and ask for students to respond through the use of graphic organizers, tables, charts, outlines, and graphs. The development of academic language is vital to student success in the classroom. Each of the content area subjects contain a unique and demanding technical vocabulary. In addition, familiar words are used in completely different ways. The purpose of this paper is to share strategies that can facilitate a teacher's scaffolding of difficult academic vocabulary. Active student involvement is the key to success. "The overriding drive in current changes occurring in second language teaching is the need to teach language through something essential and meaningful to the student. When the goal is to prepare students for academic success in classes taught in English, then ESL is best taught through lessons that teach meaningful mathematics, science, social studies, and language arts concepts simultaneously with second language objectives" (Ovando, Collier, & Combs, 2003, p. 310). This drive supports efforts toward planning thematic instruction. Theme studies provide a meaningful context for learning technical, academic vocabulary. In the sequence of activities described, a group of fifth graders are involved in the theme of great inventions. The lesson design format integrates reading and writing and leads students from the pre-reading stage through the post-writing reflection stage. I. OverviewHelping second language learners master academic content can be challenging. Scaffolding reading/writing  lessons emphasizing active student involvement provides the setting for success in this area. Theme: Great Inventions Using the book: Great Inventions Demonstration Lesson: Transportation and Under Water Overview: After a study of great inventions related to the history of transportation, students will research a topic, create a poster, and orally present it to the class. II. Objectives Connect student background by making predictions about text. Predict text content through pictures. Make connections through personal experiences to text content. Interrelate concepts using a structured overview and visuals. Keep notes in margins while reading. Self-question as sections of the text are read. Work collaboratively in a group. Create a poster to present the most important information about your group's selected topic. III. Pre-reading Activities 1. Pre-Reading #1 - Think About the Title: Ask student to think about what they already know about transportation on and under water. Give them a couple of minutes to share their predictions of the content of the text with a partner. Debrief as a total class, writing the responses on the board. Ask students to look at the pictures in the text (The text has vivid pictures of a sailboat, an aqualung, a submarine, a propeller, and several types of ships, as well as a lighthouse. Ask students to write down what they think the text is about, based on the pictures. Debrief as a class and add those ideas to the list on the board. Before actually reading the text, ask students if anyone has ever traveled anywhere in a boat or ship. Then ask the class if anyone has had any experiences in a boat. Students share their stories. 2. Pre-Reading #2: Provide a structured overview that previews and highlights important information and the interrelationships of ideas.  For this activity, students can be placed in groups and given a set of index cards containing the inventions related to the reading selection. If inventions for land and air transportation have been previously studied, they could be included. Students sort the inventions under the appropriate category as shown in the structured overview. This can be done as a prediction prior to reading. IV. During-Reading: Monitoring Comprehension English language learners need to have an established purpose for reading something so they can evaluate whether they are successful readers. The purpose of during-reading strategies revolves around the teacher's modeling of questioning techniques in order to develop the self-questioning ability of students. For example, the purpose for reading the selection "On or Under the Water," which was established during the pre-reading phase, was to find out what inventions promoted the development of different types of boatsthroughout history. Two during-reading strategies that effectively assist students in monitoring their own comprehension are using subheadings and headings and analyzing captions. During Reading #1: Analyzing Captions For example: One caption from "On and Under the Water" says, "Finding the Way." The pictures surrounding the text are of a lighthouse, which might be within the students' background knowledge, and a gold, circular object that they probably will not be able to identify (It is a mariner's ASTROLABE). The teacher's role is to discuss the purpose of the lighthouse (or something students already know about) and then suggest that perhaps the other object is also something that will help ships "find their way," as the text says. During-reading #2: Turn Headings and Subheadings into Questions Using the same example, one of the subheadings reads, "Beneath the Surface." The teacher should guide students in the process of changing the subheadings into questions. The question should be a  prediction of what the text will say below that subheading. For "Beneath the Surface," the question might be, "What invention made it possible to take a boat beneath the surface, or under the water?" Because expository, or non-fiction texts usually have headings and subheadings, determining where to ask questions is easier. Good readers use those signals and self-question as they read. During Reading #3:Read "On or Under Water" aloud, asking students to join in as they are able. (This is a short, two pages of text) Students have a copy of the chosen text so they can write notes as the selection is studied. During the reading, specific vocabulary words are identified as follows: navigation, invention, lighthouse, relied, chronometer, accurate, and satellite signals. Students write the words in the margin. Before having students reread the text silently, pre-teach this key vocabulary using a variety of techniques. Examples: a. Clustering -- In clustering, students guess the words meaning by the context of its use. From "On or Under the Water" the cluster might look like this: b. Word Scroll or Graphic Completing the graphic helps readers visually see relationships that they might otherwise overlook. When completing a vocabulary graphic, a portion of the visual should include personal connections to the word, as in the previous example. During Reading #4: Students Should Reread Either Silently, in Partners, or in Groups. V. Post-reading  The goal of this activity is to have students actually participate in a process that has traditionally been used by teachers to modify text for English language learners. Through the use of simplification, expansions, direct explanations, and comparisons, comprehension is built in to create a clearer, more understandable text, as in the following example: Simplification: The government's funds were depleted. (It was almost out of money.) Expansion of ideas: The government funds were depleted. (It had spent a lot of money on things; equipment, help of the poor. It did not have any more money to spend on anything else.) Direct definition: The government's funds were depleted. (This means that the government had spent all of its money. (Diaz-Rico, & Weed, 2003, p. 230). For this activity, the teacher models the first work after creating a bulletin board which is labeled as follows: The end goal of this activity is to have students do this activity in groups, with the teacher circulating as facilitator. The amount of teacher-directed instruction is going to vary depending on the students involved. Organize students into small groups and have them brainstorm ideas about the history of inventions related to transportation. Then, have them share with the whole class. As ideas are shared, create a graphic on the board or overhead. Each cooperative group is to select an area for further research. An example of a graphic which might culminate study of transportation inventions follows: VI. Writing  Each group is to create a poster "showing" their topic in an organized fashion. 1. Planning The group has to decide how best to organize their information. For example, a group who selects a study of inventions used throughout history to guide ships in the open sea, might present their information using a time line as a focus.  1200      1300       1400       1500       1600       1700        1800  |__________|__________|__________|__________|__________|__________| Students select materials to use to best present their topic. (A variety of art materials should be made available as well as magazine pictures) 2. Writing The first draft is produced. 3. Sharing Posters are shared with the rest of the class. Each person in the group tells what his/her part was in the creation of the project. 4. RevisingThe first draft is reshaped, incorporating the feedback from the sharing. 5. Editing Students proofread for conformity to the conventions of the English language. 6. Evaluation The writing is judged to determine if it meets the criteria on the evaluation or rubric and if it satisfies the writer and the reader. 7. Writing Log Reßection Students write in their individual logs. Give them a guided question to get them started. Example: Review the activities you have participated in during our study of great inventions. Which of the activities do you think most helped you understand the information? Why? References Diaz-Rico, L.T., & Weed, K.Z. (2002). The crosscultural, language, and academic development handbook: A complete K-12 reference guide (2nd ed.). Boston: Ally & Bacon. Herrell, A. (2000). Fifty strategies for teaching English language learners. Upper Saddle River, NJ: Prentice-Hall.  Olson, C.B. (2003). The reading/writing connection. New York: Allyn & Bacon. Ovando, C., Collier, V., & Combs, M. (2003). Bilingual and ESL classrooms: Teaching multicultural contexts (3rd ed.). Boston: McGraw-Hill. Peregoy, S. Boyle, O. (2001). Reading, writing, and learning in ESL (3rd ed.). New York: Addison Wesley Longman. Spangenberg-Urbschat, K., & Prichart, R. (eds.). (1994). Kids come in all languages: Reading instruction for ESL students. Newark, DE: International Reading Association. Wood, R. (2003). Great inventions. New York: Barnes & Noble. The Internet TESL Journal, Vol. X, No. 5, May 2004 http://iteslj.org/ http://iteslj.org/Articles/Bradley-Scaffolding/|||The Audio Lingual Method clip||youtube||N/A|||The Direct Method clip||youtube||N/A
Scaffolding	Language acquisition	Foreign languages	Second language learning
What is the "silent" period in language acquisition/learning?	Children acquiring their first language go through a period in which they learn what language is. They listen to the language they are exposed to and do not speak in this period. When learning a second language, a person will go through a period of silence, in which they aren't required to produce speech immediately. If urged to speak, they will refuse or make a lot of mistakes. This is the silent period, existing in both first and second language learners.	What is a suitable approach to shortening or overcoming the silent period?	Inquiry-based learning	Grammar translation|||Accelerated learning|||Limiting instruction to verbal participation strategies	"What is inquiry based learning?" clip||youtube||N/A
Inquiry-based learning	Language acquisition	Foreign languages	Silent period
Of the branches of linguistics, which one studies how languages become similar over time?	Diachronic linguistics studies how languages develop over time, how they become more different or more similar.	Which of the statements below would NOT explain why languages are becoming more similar with time?	Certain language words or rules are harder to learn than others.	Groups of language users lose contact with one another, then regain contact.|||Globalization is changing language.|||Languages are becoming different due to the areal effect.	Branches of Linguistics presentation||slides||SECOND DISTINCTION Diachronic (Historical)  Linguistics  Traces the historical  development of the  language and records  the changes that have  taken place in it between  successive points in time:   to historical  Of particular interest to  linguists throughout the  nineteenth century Synchronic Linguistics  Non - historical: presents  an account of the  language as it is at  some particular point in  time
Development of languages	Branches of linguistics	Diachronic linguistics
Write a program using Python and NLTK that will find all adverbs in “document.txt” that do not end in “-ly” and print out the 100 most frequent ones in order of decreasing frequency, with their frequencies.	import nltk doc = open(“document.txt”, “r”).read() data = nltk.sentence_tokenize(doc) targets = [] for sent in data: 	tagged = nltk.pos_tag(sent) 	targets += [w[0] for w in tagged if w[1] = RB and not w.endswith(“ly”)] fd = nltk.FreqDist(targets) fd.most_common(100)	Why is it necessary to do this sentence by sentence, rather than the whole document at once?	Because nltk.pos_tag() will not tag correctly unless it operates on sentences.	Because then one has to loop through every word in the data, individually, which is slow. |||Because it is the only way to get the data word-tokenized.|||It is not.	NONE
Coding	Python	NLTK	Data analytics	Searching corpora	Part-of-speech (POS) tagging
Which branch of microlinguistics studies the interpretation of utterances in context?	Sociolinguistics - the branch studying language and society - shows how our use of language is determined by factors such as location, class, gender, race, etc. A subsection of this area is anthropological linguistics which is concerned with form and use of language in different cultures and to what extent the development of language has been influenced by cultural environment.	Which of the following is irrelevant to sociolinguistics?	Women's tone of voice tends to be softer than men's	Language varieties|||Pronunciation of fricatives in a certain area|||The study of artificial languages	"Language and Society" publication||publication||Language and society 1.1    Methods in sociolinguistics  1.2    The development of sociolinguistics   1.2.1    Sociolinguistic data   1.2.2    The linguistic variable   1.2.3    The question of co-variation   1.2.4    Indicators and markers   1.2.5    Register and hypercorrection 1.3    Sociolinguistics and language change  1.3.1    Social networks  1.3.2    The Belfast investigations 1.4    Types of speech communities   1.4.1    Where do standards come from?   1.4.2    Artificial languages 1.5    Language and gender  1.5.1    Growing into a gender role  1.5.2    Gender roles in adulthood  1.5.3    Gender and power  1.5.4    Language used by women  1.5.5    Gender and standard  1.5.6    Gender-neutral language  1.5.7    Desexification of language  1.5.8    Gender and language change 1.6    Language and culture  1.6.1    The ethnography of communication  1.6.2    Colour terms  1.6.3    Kinship terms  1.6.4    Counting systems1 Language and societyLanguage is both a system of communication between individuals and a socialphenomenon. The area of language and society Œ sociolinguistics Œ is intendedto show how our use of language is governed by such factors as class, gender,race, etc. A subsection of this area is anthropological linguistics which isconcerned with form and use of language in different cultures and to what extentthe development of language has been influenced by cultural environment.  Raymond Hickey  Language and Society   Page 2 of  37The study of language and society Œ sociolinguistics Œ can be dated to about themiddle of the twentieth century. Before that there were authors who commentedon how language use was influenced or indeed guided by socially relevantfactors, such as class, profession, age or gender. Indeed the father of modernlinguistics, Ferdinand de Saussure (1857-1913), saw language as a type ofsocial behaviour and in this he reflected French sociological thinking of his day,above all that of his contemporary Emile Durkheim (1858-1917). But a set ofindependent, objective principles, in short a methodology for investigatingsocial factors in language use, was not available until some decades after theadvent of Saussurean structuralism. In the early 1960s a number of linguists in America began to investigateEnglish usage in the United States from a social point of view. Since then therehas been a flood of publications in this vein, primarily in America but soonafterwards in Europe as well (notably in Britain).1.1 Methods in sociolinguistics The roots of sociolinguistics are to be found in traditional dialectology. Thecommon denominator between the two disciplines is their concern with languagevariation, the one with that on a social level and the other with geographicallydetermined variation. However, many aspects of dialectological research areunacceptable to modern sociolinguists.  The chief deficiency of dialect investigations in the nineteenth centuryand early twentieth century is that they were unrepresentative, i.e. theirinformants consisted of a skewed selection of speakers. Older, male, rural,non-mobile speakers were given preference as informants. Because manydialectologists were trained as historical linguists they were frequentlyconcerned with discovering the most archaic forms of language still spoken attheir time, often on the assumption that the older forms were somehow more‚genuine™. The kinds of speakers just alluded to were regarded as those whowould speak the most conservative, hence most genuine form of a language atany given time. This standpoint is quite different from that of present-daysociolinguistics. Language use in society applies to all groups, young and old,male and female, rural and urban. Indeed because the majority of inhabitants ofwestern countries now live in cities and because such concentrations of peopletend to induce high amounts of language variation, sociolinguists are more oftenthan not concerned with language use in cities. In order to realise impartial investigations of language in society it isnecessary to employ objective methods. Care must be take that the choice ofinformants be random and thus not subject to the possible bias of the fieldworker / linguist. Furthermore, consciously interviewing informants often has the  Raymond Hickey  Language and Society   Page 4 of  37in casual speech the attention paid is less, 3) degree of formality, determined bythe nature of the interview, this can vary depending on the way informants reactto the interviewer and the situations they are placed in.  The difficulty referred to above, namely that people™s linguisticbehaviour changes while being recorded, has been dubbed the observer™sparadox by Labov. His answer to this problem was to develop the Rapid andAnonymous Interview in which informants were not aware they were beinginterviewed by a linguist. The essence of this technique can be seen byconsidering how Labov collected data on English in New York city. To beginwith one should say that he was interested in the following linguistic variables:1) the presence or absence of syllable-final /r/, 2) the pronunciation of thefricatives / / and /!/ and 3) the quality of various vowels. He chose two wordsin which these sound occurred, namely fourth floor, and then went around to anumber of department stores in New York. Each of these was typical of a certainsocial class, and going on the assumption that employees use the pronunciationwhich holds for their typical customers, he could then examine the kind ofEnglish used in each store. To get samples without people knowing that theywere acting as informants for a linguist, Labov checked in advance what itemswere for sale on the fourth floor and then asked a store employee where he couldfind these items. After the individual responded ‚on the fourth floor™ he askedagain, pretending that he did not hear the first time. This supplied him with amore careful pronunciation of the two words. Labov saw in this technique ameans of gaining genuine pronunciations which were not spoiled by speakers™awareness of providing data for an investigating linguist. Of course, there aredisadvantages to this method, above all the small quantity of data which can begleaned at any one time and the inability to do a sound recording which onecould listen to afterwards.1.2.1 Sociolinguistic data Whereas traditional dialectology focussed on the relationship between languageand geography, urban dialectology is more concerned with the relationshipbetween language and social factors. Furthermore, the methods of traditionaldialectology differ from those of urban dialectology in terms of the selection ofinformants. The data of traditional dialectology was often obtained by askinginformants to fill in questionnaires, consisting of questions for words used in arural setting. Very often the outcome of this method consisted of one-wordanswers. Since sociolinguistics is more interested in phonology and grammarrather than in vocabulary, the methods of eliciting data have to ensure stretchesof free spoken speech which are suitable for evaluation.  Depending on the size of the survey and on its objectives, the number ofinformants and thus the amount of data might vary. Where large amounts of dataare involved informants must be classified along various lines. With urbangroupings, traditional divisions of class may not always be suitable. At the very Raymond Hickey  Language and Society   Page 5 of  37least the basis for these divisions must be made explicit. For that reason,linguists often categorise speakers by factors such as occupation, income,education and housing, quite apart from age and gender. 1.2.2 The linguistic variable When examining sociolinguistic behaviour linguists have found that somefeatures of a variety tend to vary more than others. Not only that, there arefeatures for which the variation has special social significance. In order tocapture such features and describe them, the term linguistic variable is used.This refers to a specific feature which can be used as a tag for classifying aspeaker™s speech. For example, as William Labov has pointed out in hisinvestigation of English in New York, the realisation of /r/ is just such avariable. The realisation of /r/ varies significantly across the groups within thecity. The traditional lower class in New York do not pronounce /r/ after a voweland not before another, so a word like car or card are realised as [ka"] and[ka"d] respectively. Speakers from groups further up the social ladder in NewYork do tend to pronounce the /r/ in this position, i.e. they would have [ka"] and[ka"#d] for the word just mentioned. In order to refer to sounds which areregarded as linguistic variables, round brackets are used, e.g. (r). Linguistic variables differ from city to city or from region to region. Innorthern England, for instance, the vowel in the word cut is an example. Somespeakers use a high back vowel here while others have a lower vowel andmaintain a distinction between the vowels in words like but [b$t] and bush[b%&]. In London, users of colloquial speech have a glottal stop for intervocalic/t/ in a word like butter [b$'(] whereas others maintain the [t] pronunciation inall positions.  A linguistic variable need not only be phonological. Examples ofgrammatical variables are double negation, the use of ain™t and the lack ofmarking with verbs in the 3rd person singular present tense among AfricanAmericans.  There exists a common non-linguistic label for a linguistic variable,shibboleth. This term stems from the Book of Judges (12: 5-6) in the OldTestament which recounts how Jephthah and the Gileadites defeated theEphraimites at the banks of the Jordan. The Gileadites managed to cross theriver before the Ephraimites. To check whether those behind them were actuallyfrom their group they asked each to pronounce the word shibboleth (whichmeant either ‚stream in flood™ or ‚ear of corn™). Those who pronounced it assibboleth, i.e. with [s] and not [&], were not Gileadites and regarded as enemies.1.2.3 The question of co-variation The realisation of linguistic variables is not a matter of either/or. For many Raymond Hickey  Language and Society   Page 6 of  37speakers, one can notice a preference to use one form over another, but not to theexclusion of one of these. The linguistic term for this situation is co-variation.To take one of the examples given above, some people in northern England use[b$t] sometimes and [b%&] sometimes, both as realisastions of but. Wheninvestigating a variety which shows such co-variation the first thing is toestablish is relative frequencies for the one realisation over the other. The nexttask is to determine, if possible, the conditions under which one form is usedrather than another. For the example just given, various motives can berecognised: northern speakers seeking acceptance by more standard-speakingsoutherners are likely to favour [b$t] over [b%t]. On the other hand, northenerswho glady identify with the north can be seen to favour [b%t] over [b$t]. Thereare various shades between these two poles. Furthermore, factors such as ageand gender are important considerations, quite apart from those of class,occupation, place or residence, etc. Co-variation is also a characteristic of transitions over time. If alanguage or variety has changed from form A to B, then one usually finds thatthat the pathway has been A à  [A + B] à  B, where the stage in brackets showsan increasing incidence of B over A. Such transitions can last a considerablelength of time and be influenced by many factors. For instance, the shift fromwhom to who as the oblique form of the relative pronoun has lasted a couple ofcenturies and is not complete yet. A similar change in progress, this time fromphonology, is the loss of initial /h/ in urban varieties of English, e.g. /a%/ for/ha%/ how. This, like the example of who(m), is retarded by the restraininginfluence of standard English.1.2.4 Indicators and markers The extent to which linguistic variables correlate with social features has beeninvestigated in detail by several linguists. One of the investigated items is thevariable (ng), alternating between [)] and [n] in many varieties of English. InNorwich, some distance north of London, this variation is found and words likewalking can be pronounced either as [*w+"k,)] or [*w+"k-] (the stroke under the[n] in the transcription indicates that it is syllable-bearing). This is commonlyknown as ‚dropping one™s g™s™.  An investigation of speech differentiation in Norwich was carried out bythe English linguist Peter Trudgill in the late 1960s. The informants of the surveywere classified into five social groups from middle middle class (MMC)through lower middle class (LMC), upper working class (UWC), middleworking class (MWC) to lower working class (LWC) The parameters used forthis classification included income, housing, education and occupation. Trudgillfound that the highest incidence of (ng) = [n] occurred in the bottom social groupand the lowest incidence, that is the greatest occurrence of (ng) = [)], wastypical of the highest social group. He also found that the scores for (ng) = [n] Raymond Hickey  Language and Society   Page 7 of  37tend to decrease as the formality of the speech situation increased, no matterwhich particular social group was involved. One explanation for this is thatwhenever there is class differentiation with a linguistic variable, speakers of allsocial groups gravitate towards the higher status variants in more formal thesituation.  However, not all variables which are subject to class differentiationshow stylistic variation as well. There are variables which correlate with socialclass variation but which do not vary when the speech situation changes.Variables which are subject to stylistic variation as well as variation acrossclass, gender or age are referred to as markers. Variables which are notinvolved in style variation are called indicators, an example would be thefricative t [.] of southern Irish English, in a word like put [p%.], which is foundin all styles of this variety of English. Indicators do not contribute to thedescription of class differences as markers do, since speakers appear to be lessaware of the social implications of an indicator than of a marker.  According to the observer™s paradox, in tape-recorded interviewsinformants pay more attention to the way they speak and for that reason producea formal style (FS) rather than a casual style (CS). In many cases linguistschoose to study the increase of stylistic level in formal situations to see whatlinguistic correlates such a shift has. By asking informants to read a passage ofconnected prose aloud one can induce a style shift, since reading causes peopleto be more conscious of their speech and hence to move away from theirvernacular mode. The style elicited in this manner can be labelled ‚readingpassage style™ (RPS). An even more formal style is the so called ‚word liststyle™ (WLS) found when informants read one word at a time from of a list. Index scores for (ng) variable across different styles and social groups shift from [)] to [n] (percentages represent occurrences of [n])   WLS  RPS  FS  CS MMC  0  0  3  28 LMC  0  10  15  42 UWC  5  15  74  87 MWC  23  44  88  95 LWC  29  66  98  100Using the methodology of index scores the quantitative distribution of(ng)-alternatives can be displayed. It is striking that scores increase regularlyacross the rows and down the columns. Despite the different values in each rowand column, there is a general increase of [n] values when moving both from topto bottom and from left to right. These and similar results reveal clear quantitative correlations betweenpronunciation and social class. Relationships of this sort were observed, albeit Raymond Hickey  Language and Society   Page 8 of  37unscientifically, long before urban dialectology arose as a discipline. Theadvantage of quantitative research of this kind is that it gives more detailedinsight into the nature of these relationships. Above all, it shows that therealisations of linguistic variables are a question of more-or-less rather thaneither-or.1.2.5 Register and hypercorrectionThe stylistic differences referred to in the previous paragraphs are related to thenotion of register. By this is meant a specific style which speakers use inspecific situations. For example, when talking to the postman most people woulduse a different kind of language than when one is holding a public address. Suchregisters may have different grammatical rules and different lexical items. Forinstance, in colloquial registers of Irish English one is likely to find fellah [f/l(](from fellow) for boyfriend, the lads for one™s friends. Speakers frequently usesuch items or grammatical forms to shift downwards,  to move (often justbriefly) into a vernacular mode. For instance, a special form for the secondperson plural pronoun does not exist in standard English but many colloquialvarieties do so that saying something like Where are youse goin™ tonight wouldbe a colloquial way of saying Where are you going tonight (again in IrishEnglish). Speakers whose vernacular mode is a local variety of a language maynot feel at home when using the standard of this language. They may nonethelessshift to this standard in certain situations, e.g. when talking to people fromoutside their community. A frequent phenomenon in this kind of situation is whatis called hypercorrection. This term refers to what happens when speakersovergeneralises a feature which they does not have in their native variety. Forexample, in standard English short /%/ as in put has been lowered to /$/ in otherwords such as cut, but not in all words which contained the earlier /%/. Innorthern English dialects /%/ has remained /%/ in all cases. When dialectspeakers attempt to use standard English they may use /$/ in too many instances,i.e. they are hypercorrect: in their efforts to speak ‚correct™ English, they overdoit. Thus they may say /b$t&0/ for /b%t&0/ butcher probably on analogy withwords like /b$t/ but although in their native dialect they have no /$/ sound at all.1.3 Sociolinguistics and language changeWhen a system is dynamic and shows movement, there is change. This truismapplies to language as much as it does to any other area. During the heyday ofhistorical linguistics, in the nineteenth century, change which had taken placeover the centuries was described in great detail. However, the mechanisms oflanguage change, were only partially considered. Only internal change in sound Raymond Hickey  Language and Society   Page 9 of  37systems and grammar were investigated. But the large area of externallymotivated change, determined by currents in society, was never given its duerecognition. With the advent of sociolinguistics, the process of language changebecame the subject of close scrutiny. Sociolinguists maintained that the minuteshifts in language use which can be observed in contemporary societies are, inaccumulation, no different from the large-scale changes observed over centuries. In particular William Labov concerned himself with the process andstages of language change. Here he recognised three phases which can besummarised as follows.Labov™s three stages of language change1)   origin A period in which alternative variants for estaliblishedvariants begin to appear.2)   propagation The stage at which the new variants establish themselvesto the detriment of the older ones which are sidelined. 3)   conclusion  The stages at which the remaining variants are (i)replaced completely by new variants or (ii) remain as aresidue after the change has terminated.Various external factors can accelerate the process of language change, aboveall social pressure from above or below. Additional factors are the degree ofliteracy in a community, the restraining influence of a standard of a language, therelative ‚prestige™ of speakers favouring new forms, etc.  The course of language change shows a typical rate which involves aslow beginning, a relatively rapid middle section and a deceleration towards theend. Schematically, these three phases correspond to the beginning, middle andend of an S-curve which is frequently used as a visualisation of language change. A good example to illustrate the course of a change is the development ofthe vowel in but in early modern English. In the south of England (and inScotland, but not in the north of England) this was lowered to an unrounded,fairly central vowel, something like present-day but [b$t]. The change probablystarted in the early 17th century and after a slow start, gained speed, involvingmore and more words, and slowed down again. It did not encompass allinstances of the input /%/ vowel, specificially before /&/ and /l/, which is whyone still has push [p%&] and pull [p%1] in southern British English. This grouprepresents the gap between the top of the S and the limit of 100% in the abovegraph. Sociolinguistics has provided insights not just into the course of languagechange, but also into its motivation. One powerful motive is the desire ofindividuals to make their speech more like that of a group they aspire to.Typically, this could involve a higher social class, an urban dialect vis a vis a Raymond Hickey  Language and Society   Page 10 of  37rural one, the language of a powerful neighbour vis a vis that of a smallercountry. Changing one™s language to make it more like that of another group iscalled accommodation: speakers attempt, in face to face interaction, toapproximate their speech to that of their partners in conversation most probablyto increase their acceptance by the group who language features are beingadopted. Such accommodation can be short-term or long-term. If the latter, and ifit is community-wide, then it can lead to language change. The reverse side ofthe coin is dissociation where speakers attempt to make their speech moredifferent from that of a group they wish to distance themselves from. A clearexample of this can be seen in present-day Dublin English where a newpronunciation has arisen with a raising of back vowels which is diametricallyopposed to the low realisations of popular Dublin English, e.g. a word like Corkwould now be pronounced [ko"#k] and not [k2"3k] as it still is colloquially. In both accommodation and dissociation, speakers are normally notaware of the alterations to their language which they make, i.e. one is dealinghere with unconscious linguistic behaviour.1.3.1 Social networksJames and Lesley Milroy began investigating language use in Belfast in the1970s and continued for over a decade. At the centre of the Milroys™ work is thenotion of social network, adapted from work on sociology. All speakers have aplace in the network of their social environment. This network consists of ties ofvarying strength depending on the social bonds speakers entertain within theirneighbourhood. There is a general assumption that for those on the lower end ofthe socio-economic scale the ties are stronger than for those further up this scale.Networks can be defined by how dense and multiplex they are. For instance, if aspeaker A not only knows other speakers B, C, D, E, etc. but the latter also knoweach other then the network is dense. If the individuals in a network are moreisolated and not mutual acquaintances then it shows low-density. A network ismultiplex if its members interact in more than one way, e.g. if members have anumber of work colleagues in their network with whom they also spend theirspare time, through communal neighbourhood activities or sports for example,then the network is multiplex because there is more than one factor unitingmembers.  A focussed and bound network can impose rigid linguistic norms on itsmembers which in turn acquire a defining character, albeit an unconscious one,for the network itself. Such networks tend to be impervious to influence fromoutside, specifically from the prestigious norm of the society of which they arepart. Speakers who engage in loose-knit networks, such as the suburbanmiddle-classes, are relatively more accessible to language norms. Becauseloose networks do not show clear defining features, these speakers adopt thenorms of the socially prestigious standard. Conversely, working-class sections Raymond Hickey  Language and Society   Page 11 of  37of society Œ those with strong networks Œ do not see middle-class speech as amodel because they have their own linguistic norms from within their network.  Certain generalisations about networks can be made. The relativestrength of a network depends on the weight accorded by speakers to twoconflicting forces in society: status and solidarity. If speakers opts for statusthen they are likely to have weak ties, to try to move upwards on a social scale,striving to achieve professional status and economic success. Should speakersopt for solidarity then they generally remain in their surroundings, maintainingties with neighbours and participating in the life of the community. Solidarity isan aspect of social behaviour which has a linguistic component: speakers whodemonstrate solidarity exhibit allegiance to the vernacular norms of theirneighbourhood, frequently in contradistinction to those of the sociallyprestigious form of language. The linguistic norms of a community are localwhereas status features are diffuse and hold for a much wider area, typically foran entire country. The number of defining features of low-status, high-solidarityvarieties is usually quite high. The linguistic norms of such communities can bedifficult, if not impossible, for non-natives to acquire. The identity function ofhigh-solidarity varieties implies that one can exclude those who are not native tothe community. Dense multiplex network ties seem furthermore to holdespecially for young males. Women and middle-aged speakers in general tend toopt more for status and to tone down the linguistic signs of strong network ties.  A network is much smaller than a class which is a generalcharacterisation of social status whereas a network is an areal reality, typicallya part of a city. There is of course a correlation to class inasmuch as people in anetwork usually belong to a single socioeconomic group and those in thestrongest networks tend to be lowest on this scale. Given their relatively smallscale, networks form a consensus-based microlevel within society. The bondingwithin a class is achieved through similarity in socio-political outlook and notby identification with a certain locale. 1.3.2 The Belfast investigationsThe insights into sociolinguistic behaviour just sketched were gained by a closestudy of the following three areas of Belfast.  1)  Ballymacarett (Protestant East Belfast) 2)  The Hammer (Lower Shankhill Road, Protestant West-Central Belfast) 3)  Clonard   (Lower Falls Road, Catholic West-Central Belfast)The three areas are different in the social importance they attach to certainfeatures. For instance, the palatalisation of /k, g/, as in cap /kjap/ and gap /gjap/,is generally regarded as a rural feature of Ulster English and in Belfast is foundmost with older males (40-55) chiefly in Catholic west Belfast. In the easternsection of the city Œ which has had a largely Ulster Scots input from north Co.Down where the palatalisation is not an indigenous feature Œ this trait is avoided Raymond Hickey  Language and Society   Page 12 of  37as can be seen from its percentual representation in the data collected by theMilroys.  Clonard  62% palatalisation  Hammer  14% Ballymacarrett  0%   (J. Milroy 1981: 94)This pattern would also seem to apply to the dental realisation of /t/ before /r/ inunstressed syllables, i.e. the pronunciation [b$40] rather than [b$t0] for butter,which occurred with older males more than with younger men and women inwest Belfast (Clonard and Hammer).  The raising of /a/ before velars, as in bag /b/g/, is quite common and ismore frequent in west Belfast then in east Belfast, young males in east Belfastseem to have lost this raised vowel entirely, perhaps as the result of peer-grouppressure. With the variable ($) the Milroys found that young males displayvernacular loyalty and preserve this dialect feature in words such as pull /p$l/. Correlates of community structure Lesley Milroy has analysed the relationbetween realisations of three phonological variables, (th), ($) and (a) within theframework of the network model of language ties. With regard to (th) Milroynotes that the deletion of [!] intervocalically Œ as in brother [br$(r] Œ has beena stable marker of lower-class speech for some time and shows little change. ($)on the other hand would appear for young men Œ particularly in Catholic westBelfast Œ to have overcome its inherent stigma in the community and be usedmore frequently. With regard to (a) the situation is more complex as there is aphonetically conditioned front raising before velars and back raising beforelabials, as in hand [h2nd]. It too is subject to socially determined variationwithin the phonotactic constraints just mentioned.  The insight here is that not just the use of one realisation as opposed toanother is sociolinguistically significant but the numbers of tokens for a givenrealisation are relevant, i.e. both the qualitative and the quantitative aspects of avariable are important as linguistic markers within a social network. Wider implications The Milroys™ work on vernacular speech in Belfast has hadwider implications for linguistic studies, for instance for the development ofphonological norms in English. James Milroy has devoted his attention to therelationship between standard and vernacular norms, stressing the uniformnature of the standard and the essentially variant structure of vernaculars whererules governing the variation are understood by those within the vernacularcommunity but not those outside.What is prestige? All too often vague references to ‚prestige™ are made inlinguistic treatments of language change. James Milroy has criticised that the Raymond Hickey  Language and Society   Page 13 of  37notion of prestige has often been appealed to too lightly in explanations oflanguage variation and change and that such appeals frequently lead tocontradiction and confusion. Instead he suggests that explanations based on theidentity-function of language appear to be more successful. The nature of the vernacular The work of the Milroys has heightened thelinguists™ awareness of the vernacular. Its norms oppose standardisation andprescription for reasons of identity maintenance. Such norms are not codified butpassed on orally. The Milroys distinguish between grammaticality, typical ofstandards, and norms of usage, characteristic of vernaculars. In-group variationcan be quite complicated and not necessarily accessible to outside groups.Supra-local varieties, which tend towards koinés, are simpler in structure asvariation no longer shows any in-group function. Lower-status varieties oflanguage are dynamic and the locus of change. Little or no change results fromstandard varieties. Gender and network The interaction of gender and network affiliation has beena concern of Lesley Milroy. She defines clearly how a network score is arrivedat for an individual by considering membership in a high-density,territorially-based group, ties of kinship, similar place of work to other groupmembers and voluntary association with others during leisure time. LesleyMilroy then examined three vernacular variables indicative of network strengthin Belfast, (a), (th) and ($). The conclusion she drew is that the first twovariables are important as gender markers for men but as network markers forwomen. The last variable is a weaker gender marker but important to men as anetwork marker which shows the essential interdependence of both types ofmarking and the complex relationship between them. 1.4 Types of speech communities When linguists talk about the people who use languages they refer to speechcommunities. Lay people tend much more to refer to countries when talkingabout such groups. Certainly in the western world, countries are stronglyassociated with single languages and vice versa. Linguists see this as due to therise of nation states in the past few centuries where single languages attainedofficial status in individual countries, English in the United Kingdom, French inFrance, Italian in Italy, etc. However beneficial the official status of onelanguage may be for a modern country in terms of government, public discourse,higher eductation, etc., it is not helpful to other languages which may be foundwithin the borders of a particular country. Furthermore, even in countries whereonly one language is spoken by nearly everybody, say Dutch in the Netherlands,there may well be different dialects of this language which themselves stand in acertain relationship to the standard of that country. Raymond Hickey  Language and Society   Page 14 of  37 Even within Europe, and most certainly outside of it, the equation of onecountry with one language breaks down. Switzerland is a good example of acountry where a single national identity exists across three major and one minorspeech community: Swiss German, French, Italian and Rhaeto-Romance. BeyondEurope it is more the rule than the exception for countries to have severallanguages within their borders. One need only think of such countries as Indiaand China or Siberian Russia to see how many languages can be integrated into asingle state. In such instances, there is of course an official language whichserves the function of a lingua franca, that is, a language which is used as ameans of communication among those groups who do not speak each other™slanguage. A sub-set of bilingual countries can be recognised in those regions of theworld where former European colonial powers were once to be found. Manysuch countries have conferred official status on the European language they havebeen exposed to, with the languages of their native peoples having a precariousstatus. This is true for all South America (with the exception of Paraguay):Spanish or Portuguese (in Brazil) has official status and local languages arespoken to varying degrees, particularly in rural communities. Some countries have been fairly successful in reconciling the status ofthe colonial language with that of local languages. In India, English is a linguafranca but indigenous languages, primarily Hindi but many others as well, havelarge numbers of speakers and are used in all spheres of society.  Still other countries have had mixed colonial backgrounds and variousEuropean languages may or may not survive from earlier periods. Cameroonwas subject to British and French rule, even German rule briefly. Both Englishand French are spoken in present-day Cameroon. South Africa has maintainedboth the English and Dutch legacies with forms of English and Afrikaans (aformer of early colonial Dutch) being spoken by large numbers alongside manyindigenous languages. In the Philippines Spanish was more or less ousted byEnglish (from America) at the end of the nineteenth and in the early twentiethcentury. In the past 50 years of so, forms of the major indigenous language,Tagalog, have asserted themselves, above all around the capital Manila. Occasionally the role of lingua franca can be played by a non-Europeanlanguage. In large stretches of east Africa the Bantu language Kiswahili(influenced in its vocabulary by Arabic) serves as a general means ofcommunication, for instance in Kenya and Tanzania.Diglossia This is a type of situation in which there is a division between twolanguages or two varieties of a language such that one variety, the so-called‚high™ or H variety, is used in public life Œ in addresses, in the media, in schoolsand universities, etc. Œ and another variety, the so-called ‚low™ variety or Lvariety, is used in domestic life Œ with family and friends. Examples of diglossicsituations are to be found in Switzerland (Hochdeutsch and Schwyzerdütsch), inGreece with the literary form of the language Katharevousa and the colloquial Raymond Hickey  Language and Society   Page 15 of  37form Dhimotiki, in various Arabian countries (Classical Arabic and the localdialect of Arabic), Paraguay (Spanish and Guaraní). Bilingualism A type of linguistic situation in which two languages co-exist in acountry or language community without there being a notable distributionaccording to function or social class. Within Europe, Belgium, in those partswhere French and Flemish are spoken side by side, provides an example ofbilingualism. Finland, in the area of Helsinki, with Finnish and Swedish is afurther example. In North America, Canada with English and Quebec French isanother example. Not all countries offer official recognition to the languagesspoken within its borders. France has not accord Breton the same status asFrench. Turkey does not recognise Kurdish as an official language.  In discussions of bilingualism it is normal to distinguish societalbilingualism and individual bilingualism. Naturally, a bilingual society consistsof bilingual individuals. But such a society is not necessary for many bilingualswho speak two languages almost equally and do not show a functionaldistribution of the languages. Bilinguals often illustrate a feature which can contribute over time tolanguage change. This is what is called code-switching which occurs whenspeakers move from one language to another and back again within the samesentence, e.g. J™ai mangé toujours oatcakes en Angleterre ‚I always ate[oatcakes] in England™. There are many speculations about why this takes place,for example, speakers have first become acquainted with some phenomenon inthe second language and switch to it when talking about it, or maybe they switchbecause they just feel the second language is momentarily more appropriate.  The switching may involve single words or whole clauses, e.g. Ma c™estvrai, she wants to play cricket, incroyable! ‚But it is true [she wants to playcricket] unbelievable!™. The latter type is governed by strict rules about whatpoint in a sentence can act as a pivot for the switch-over. If code-switching iswidespread in a community and becomes socially accepted then it may lead inthe fullness of time to changes in the original language just as borrowing orstructural transfer has done (see section on language contact below). Minority languages This is a reference to languages which are spoken by smallnumbers of people within the borders of a country which has another languagespoken by the majority. Irish in a minority language in Ireland, Rhaeto-Romanceis in Switzerland and is Saami in Finland (in all these cases the languages haveofficial status). Sorbian is a minority language in Germany, though withoutofficial status (it is spoken by a dwindling number of speakers). A minoritylanguage in one country may be a national language in another. For instance,Danish in northern Germany is spoken by a small minority as is Turkish inBulgaria or Swedish in Finland. Within the European Union a degree of officialstatus is accorded to languages which the European administration recognises asliving languages, irrespective of numbers or of status in the host country. Raymond Hickey  Language and Society   Page 16 of  37 Some minority languages are spread across more than one countrywithout having official status in any (though there are varying degrees ofrecognition). Basque, for instance, is found in both north-east Spain andsouth-west France. Kurdish is spread across a number of countries: Turkey,Armenia, Syria, Iraq and Iran, to mention the main ones. Although the Kurdishcultural region has over 25 million speakers of Kurdish, about half of whom livein Turkey, the language does not have official status.Language split This term is used to refer to the type of situation which obtainswhen for political reasons two varieties of a language, which are scarcelydistinguishable, are forcibly differentiated to maximise differences between twocountries. This applies to the Moldavian dialect of Rumanian, which is nowwritten in Cyrillic and is the language of the Republic of Moldavia, and theremaining dialects of Rumanian. It also applies to Hindi, the official language ofIndia, alongside English, and Urdu, the official language of Pakistan. In thesesituations much use is made of different writing systems. Thus Hindi is writtenfrom left to right in the Devanagari script while Urdu, the language of a Muslimcountry, is written right to left in the Persian variant of Arabic. Once languagesplit has been introduced the differences may become real with time, e.g. withHindi and Urdu the different religions make for different vocabulary which helpsthe originally artificial distinction between the languages to become real.Historically in Europe, Dutch and the Lower Rhenish dialects represent a caseof language split. Language maintenance This refers to the extent to which immigrant speakers ofa certain language retain knowledge of the original language in the host countryinto following generations. Here language communities vary. The Irish, forexample, gave up their native language immediately in the United States whereasthe Estonians have shown a remarkable degree of language maintenance. Thereasons for this often have to do with the attitude of the respective groups totheir original language. For the Irish their native language was associated with abackground of poverty and so they switched rapidly to English in America. Language preservation This is the extent to which a country has officialinstitutions to preserve the language in an ostensibly pure form. For example, inFrance an academy has existed since 1634 which acts as a watchdog over thesupposed purity of French. There is no corresponding institution in England orGermany (though South Africa, as the only anglophone country, does have alanguage academy). In the latter two countries, major publishing houses play therole of language academies, the Oxford University Press in England and theBibliographisches Institut (Mannheim) in Germany, the publishers of the Dudenseries of reference books. One should add that the value of prescriptive organsis very much disputed as they cannot stop language change in the form of Raymond Hickey  Language and Society   Page 17 of  37borrowing (cf. the influence of English on French despite the efforts of theacademy). Language death This emotive term is sometimes applied to those situations inwhich a language ceases to exist. The fact itself is not of major concern tolinguists, it is rather the stages which the language goes through which are ofinterest. A well-studied instance of language death is Scottish Gaelic in EastSutherland in the north-east of Scotland. The language was progressivelyabandoned from one generation to the next and during this process the grammarof the language showed clear signs of disintegration, for example in itsmorphological system. In some instances the very last speaker of a language maybe identified. It is reported that Dolly Pentreath was the last native speaker ofCornish in the late eighteenth century. For Ubykh, a north-west Caucasianlangauge, the last native speaker of which, Tevfik Esenç, died in 1992.Language revival Occasionally a language which has died out may be revived.Such a process can be triggered by a number of circumstances. The most famousinstance is that of Modern Hebrew which was formed from Classical Hebrew (awritten language) during the later nineteenth and early twentieth century whenJews began to settle in Palestine. It then became the language of the state ofIsrael and is now a fully fledged language adapted to the needs of a modernsociety. A less spectacular instance, and one on which opinions are divided, isthat of Cornish which has ardent supporters who claim to have revived thelanguage as a viable medium of communication in their community.1.4.1 Where do standards come from? The term ‚standard™ has been used frequently in the present chapter and it mightbe worth considering for a moment how standard forms of language have arisenin various countries. If one looks at European nations for a moment it becomesevident that different historical developments have led to a particular variety ofa language attaining status as standard. A small selection of countries is offeredbelow to illustrate typical situations which have given rise to standardlanguages.EnglandIn the present-day United Kingdom (England, Wales and to a much lesser extentScotland and Northern Ireland) there is a single type of pronunciation whichenjoys the highest social prestige. Linguists call this Received Pronunciationfrom a label first used by the early twentieth century phonetician Daniel Jones torefer to that modified accent of English in London and the Home Counties whichis accepted and used on higher levels of society, typically in private education,among the higher military and clergy and by figures in public life. Other termsfor this accent are Queen™s English, Oxford English or indeed BBC English. All Raymond Hickey  Language and Society   Page 18 of  37of these are inaccurate (in different ways, Oxford has its own accent andnowadays not everyone at the BBC speaks Received Pronunciation).Historically, Received Pronunciation is a modification of London speech of theearly modern period which from the eighteenth century onwards became moreand more a sociolect and distinct from Cockney, the verncular of London city.The acceptance of Received Pronunciation varies, extreme forms are regardedas snobbish, ‚plummy™ and socially condescending. Intermediate varietiesbetween RP and Cockney Œ often referred to loosely as Estuary English Œ havearisen and gained increasing acceptance in the latter half of the twentiethcentury.  The situation just described has meant that in Britain the question ofstandard is defined primarily via pronunciation. Using standard grammar is notenough, the accent must be non-regional too (compare the situation withGermany below). The grammar of standard English in Britain derives from usage in thesouth-east in the early modern period. Certainly the language of the AuthorizedVersion of the Bible (the King James Bible of 1611) played a role, but many ofthe features of standard English derive from prescriptive writings and attitudesin the eighteenth century. Normative, prescriptive grammars appeared, such asthe Short introduction to English grammar (1762) by Bishop Robert Lowth(1710-1787) which achieved great popularity by laying down the law withregard to grammatical usage. For instance, the prohibition on double negative (acommon feature of Shakespeare™s) is stated clearly by Lowth. He was not aloneis his crusade against grammatical ‚incorrectness™: he had an even more popularsuccessor in Lindley Murray (1745-1826) whose English grammar (1794) wentinto several editions and was influential in the teaching of English in schools. This kind of prescriptivism meant that many widespread features ofEnglish dialects did not make it into the standard, for instance, the use of them asa demonstrative, e.g. Them clothes are wet, the use of seen and done as pastforms of do, e.g. I seen it; He done it, or the lack of the subject relative(although this is possible with the object relative), contrast There is a manwants to speak to you with This is the man you met yesterday.FranceBefore the Middle Ages the French language began to divide into two separateregions north and south of the Loire river, called the langue d™oïl (the northernform) and the langue d™oc (the southern form) respectively. The labels are basedon the word for ‚yes™ in each case. The langue d™oïl fed into what was later tobecome modern French with its centre around the city of Paris. There were otherdialects, such as that of Normandy which influence English after 1066, but forthe French the language of the capital and the court became the prestige variety.This situation has obtained ever since and was no doubt reinforced by the factthat France is a highly centralised country. Raymond Hickey  Language and Society   Page 19 of  37ItalyVulgar Latin yielded to Italian during the second half of the first millenium AD.Various dialects developed and some of these were used for writing, such asSicilian for poetry. In the early fourteenth century, Italy™s greatest writer DanteAlighieri (1265-1321), a Florentine by birth and inclination, composed hisgreatest literary works, the collection of love poetry La vita nuova ‚The newlife™ and the epic La divina commedia ‚The divine comedy™ in his native TuscanItalian. Due to his stature as a poet, his language became a yardstick for standardItalian in the following centuries when Italy was divided into manyprincipalities. It was not until 1871 that Rome became the capital of united Italybut without imposing its dialect as standard.SpainStandard Spanish is the dialect of Castilian which is also spoken in the capitalMadrid. This variety has been that of the Spanish court and government forcenturies. The region of Castile once stetched from the north coast of Spaindown to Andalusia and achieved dominance in Spain in the late Middle Ages.Dialectally it includes Madrid, though in present-day Spain it is split into twoprovinces Castile-León in the north and Castile-La Mancha in the south withMadrid city in an autonomous region of the same between the two.  There are other languages and major dialects in modern Spain, chieflyCatalan in the north-east of the Mediterranean and on the Balearic Islands,Basque in the north and over to the border with France on the Bay of Biscay,Galician (close to Portugues) in the north-west and Andalusian in the south.The NetherlandsDuring the sixteenth century Spain ruled over the Low Countries. The northernprovinces revolted against this dominance and The Dutch Republic was formedin 1648. In the early nineteenth century the southern parts seceeded and in 1830Belgium was formed. The Dutch language, its close relative Flemish inpresent-day Belgium, derives from the Old Low Franconian dialect of Germanwhich was spoken in the Lower Rhenish area and along the North Sea coastnorth the estuary of the Rhine. This area formed the centre of the new state andthe name of the province, Holland yielded the unofficial name of the country. The language of the capital Amsterdam (in this province) was the basisfor standard Dutch which established its position as a language independentfrom German Œ with its own orthography Œ during the centuries of Dutchmaritime power. West Frisian is a further Germanic language which is spoken inthe north of the Netherlands.NorwayThere are two forms of Norwegian which have official status in present-dayNorway. The one is Bokmål ‚book language™ and the other is Nynorsk ‚newNorwegian™. Bokmål is the language of the majority, spoken in the north and in Raymond Hickey  Language and Society   Page 20 of  37the south including, importantly the capital city Oslo. It is derived from anextraterritorial form of Danish spoken in Norway during its dominance byDanish and Sweden (to a lesser extent).  After independence in 1905 Bokmål was given official status as wasNynorsk, the set of varieties spoken in the west of the country which wereassumed to be a purer and more original form of Norwegian. The tensionbetween both main versions of Norwegian continued throughout the twentiethcentury and has not been fully resolved yet.GermanyLike Italy, Germany was not united as a single nation until 1871. Before thatthere were many individualities with their own dialects. These have survived tothis day, but the pronunciation standard in the country is derived from a generalnorth German accent which prevailed with Prussian hegemony in the nineteenthcentury and Berlin as capital of the country. The status of the present-daydialects is connected with the relationship of the region to the north of thecountry. Thus Bavaria has a marked dialect with an unmistakable pronunciationwhich many people use in public to stress the independent stance of this statewithin modern Germany.  In grammar, the situation is slightly different. Standard written German Œin morphology and syntax Œ derives from the translation of the Bible by MartinLuther (1483-1546) in the early sixteenth century. For Germans today, usingstandard German means adhering to this grammar. Local accents have a hightolerance in public, much more so than in Britain.FinlandThe significance of translations into vernaculars of the holy scriptures, mainlythe Old and New Testament, should not be underestimated in the development ofstandard varieties in the early modern period across Europe. Finland is anothercountry which, like Germany, received an authorative translation of the Bible inthe sixteenth century which established a norm for written Finnish. Thistranslation was made by Michael Agricola (c. 1509-1557), a later contemporaryof Martin Luther who produced a Finnish version of the New Testament in 1548.The pronunciation of Finnish which has greatest prestige today is roughly that ofthe southern coast, especially of the Helsinki area.1.4.2 Artificial languagesIn the late nineteenth and at the beginning of the twentieth century variousscholars turned to the task of devising an artificial language which could be usedas a general means of communication among speakers who do not understandeach other™s language. This function is fulfilled by English nowadays but beforethe spread of the latter as a world-wide lingua franca, other practicalsuggestions were put forward about how to fill the perceived gap.  Raymond Hickey  Language and Society   Page 21 of  37 A proposal which enjoyed brief, if intense, popularity was Volapük‚world talk™ (in this language) put forward by the German priest JohannesMartin Schleyer in 1879. He devised the language ‚out of pure love for troubledand divided humanity™. It was initially very successful with conferences andpublic discussions. Even an academy for Volapük was founded in Paris in 1889.However, the language was very cumbersome with too many endings (it waslargely agglutinative in structure, see section on typology below). Thecommunity of Volapük supporters found the language too much of an ordeal tolearn and around 1890 most of them switched allegiances to the following. Esperanto is another artificial language invented by the Polish scholarLudwig Zamenhof (1859-1917), again in the late nineteenth century. It wasintended as an easy-to-learn, regular language which would, like Volapük,further international communication and understanding. The language is based onRomance elements and was intended to be easy for Europeans to learn. Thename clearly suggests Spanish esperanza ‚hope™. Of all the various proposalsfor an artificial language, Esperanto is the only one which can be said to surviveto this day. Although it is not used widely, there is nonetheless a dedicatedcommunity of Esperanto users world-wide. In the course of the twentieth century various modifications of Esperantohave been put forward, largely to rid it of what were perceived as unnecessarydifficulties in the language. Even the great historian of English, the Dane OttoJespersen, offered a greatly modified version called Novial which, like so manyothers, did not catch on.  Various suggestions have also come from Romance scholars, such asInterlingua, to devise a sort of common-core language which would becomprehensible for all speakers of Romance languages. In the English-speaking world, attempts have been made to providenon-natives with simplified versions of the language for rudimentarycommunication. The English literary scholar C. K. Ogden put forward hisproposal for Basic English in 1930. This consisted of only 850 words butinterest in the suggestion quickly waned because it was impracticable.1.5 Language and genderThe word ‚gender™, originally a grammatical term, has come to refer to thesocial roles and behaviour of individuals arising from their classification asbiologically male or female. This is a huge complex embracing virtually allaspects of social behaviour of which language is only one. In the past threedecades or so intensive research has been carried out into the relationship oflanguage and gender, largely by female scholars who have felt drawn to thetopic because of the obvious discrimination against women which has takenplace in the past and which is still to be observed today. Raymond Hickey  Language and Society   Page 22 of  37 Within linguistics the initial impulse was the work of the Americanlinguist Robin Lakoff who in the early 1970s focussed her attention on certainthemes with the language and gender complex above those which she rightly feltwere in need of rectifying. Her work stimulated other scholars to engage in thisstudy and soon language and gender was a burgeoning research area inuniversities across the western world. It is in the nature of language and gender studies that they are concernedwith contrasting language as use by men and by women. Various opinionsemerged on this relationship with two gaining particular focus. One is thedifference approach which established that male and female language isdissimilar without attributing this to the nature of the social relationship betweenmen and women. The other is the dominance approach which saw language useby females and males as reflecting established relationship of social control ofthe latter over the former. With the maturation of research on language andgender the simple ‚difference Œ dominance™ dichotomy was increasinglyregarded as unsatisfactory and insufficiently nuanced. Before beginning a discussion of language and gender it is important tostress that statements made in this context are taken to apply to groups of people,in this case men and women. They are generalisations whose justification, if thisholds, derives from their applicability to majorities with groups. As any field ofinquiry, statements made about groups are not taken to apply to each member Œthat would be to use clichees. The personality of the individual can always leadher or him not to conform to a pattern claimed for her or his group. For example,to maintain that men have a competitive style of social behaviour in westerncountries is a generalisation which vitually everyone would agree on. However,they are men would are not competitive in this respect. Because they form arelatively small minority the generalisation still holds. The opening sentence to this section contains the phrase ‚biologicallymale or female™. In a book of this nature it is not possible to discuss the issuesaround possible transitions between the unambiguously female and theunambiguously male in the physiological sense. Lastly, it should be stressed that all statements in this section areintended to be non-evaluative. That the social discrimination of women,including its linguistic dimension, is to be deplored is taken as given. 1.5.1 Growing into a gender roleIt is assumed by all researchers on language and gender that men and women uselanguage differently. This is taken to result from what is called socialisation, thegrowing into a society from early childhood onwards. Gender roles arepresented to infants and lead them along paths full of preconceived opinions.  The first act of genderisation is the giving of names. Females and malesare usually recognisable by their firstnames although a few English names, e.g.Hilary; Chris, Pat (abbreviated forms), can apply to both genders. Infants are Raymond Hickey  Language and Society   Page 23 of  37then treated as one gender or the other, though occasionally parents resist theallocation of gender to children if this does not match what they were wishingfor. So much seems fairly innocuous but the reinforcement of gender allocationcan take on subtle forms based on whether children comply to the role they areassigned. Parental approval or disapproval is often expressed according to howone conforms to one™s gender role: good boy, good girl; bad boy, bad girl arecommon assessments made of children. While many parents use such labelswithout thinking it may well show how much they in turn internalised notions ofgender roles during their own youth, i.e. in their own formative period. Attributes of the two genders are conveyed early on to children. Thereare essential differences between boys and girls, not just in external physiology,but in mental makeup. Reason is for boys while emotions are for girls. Cryingbecomes increasingly unacceptable for boys who are supposed to come to termswith unpleasant situations in a rational manner. Girls are allowed displays ofemotions which is not a male thing, only in extreme circumstances which areregarded as exceptional. Girls can show (or feign) fear, for example when theyclassify films as ‚spooky and scary™. Boys learn early on to hide fear andanxiety.  There are linguistic manifestations of these slots for the genders.Diminutive formations are commonly used for girls, which stresses their lack ofpower and conversely their need for protection. For instance, there is girl andgirlie but no boy and boyie. Some labels seem to imply that one gender isparticularly well adapted to the expected role, e.g. the word lad is an approvingterm which refers to a boy who is clearly boyish in character. Different ways of passing their time are related to children at an earlystage. Think of the toys given to children to play with: guns for boys, dolls forgirls. A competitive, public role is suggested for boys and a more domestic,docile role is assumed for girls. Already in early childhood there is anasymmetry which increasingly widens into many areas of social behaviour. What happens to those individuals who do not conform to the gender rolewhich is expected of them? Boys are taunted for being sissy and girls can bereferred to as tomboyish. There would seem to be another asymmetry here,though this time of a different kind. In western societies the taint of effeminatebehaviour is more serious than that of boyish behaviour for girls. Oneexplanation for this is that male patterns are unmarked, they are the yardstick forgeneral behaviour while female patterns a special subset. Deviation from thegeneral norm is therefore less tolerated than that within a subset. There is a separation of boys and girls which is often found in childhood,at least in terms of games and outdood activities (though in the home one cannotavoid contact with opposite sex siblings). This situation changes quitedramatically just before puberty when boys and girls enter what has been called‚the heterosexual market™. Boys and girls engage in pairing off, they test theirvalue for the opposite and the feedback they receive in this activity plays acrucial in their later self-image as adults. Western societies assume adolescents Raymond Hickey  Language and Society   Page 24 of  37partake in this behaviour and those who do not Œ for whatever reasons Œ aremarginalised by their peers. For adolescents heteresexual desirability is a strongindication of the individual™s value, even if that individual is not especiallyinterested in this market. Peer pressure is enormous among adolescents andthose who do not conform to it, such as homosexuals, suffer stigma and exclusionwhich can mark them for life. Adolescence is about conformity. To adults this may seem strange atfirst, after all adolescents continually criticise their parents for their socialconformity. Nonetheless, adolescents show the most extreme forms of groupconformity themselves, probably because their personalities are not yet firmlyestablished and individuals are not prepared to risk rejection by the group.Those instances of individuals who appear to stand out among their peersusually embody established roles to a heightened extent, for instance in sports,dress or as active heterosexual couples in schools.  In industrialised nations, the commercial sector is a powerful factor inmaintaining perceived notions of mainstream behaviour. It gears its productstowards the statistical majority in society and thus plays a pivotal role inreinforcing gender roles. A good illustration for the mainstream character of thecommerical sector is provided by clothing. Certain patterns and colours arepromoted as typical of one or other gender, flowers and frills are definitely forgirls as are the colours orange, pink and lilac (in ascending order).  Fashions and styles come and go, exaggerated shoulder pads in womens™blazers and blouses were common in the 1980s but not anymore. Low cutwaistlines are now (early 2000s) common in slacks for women. The naming of objects is also closely linked to gender distinctions. Forclothes one finds not just skirts and dresses (which only apply to women inwestern countries) but blouses for females and shirts for men although the itemsof clothing are essentially similar. 1.5.2 Gender roles in adulthoodBy and large gender roles are kept throughout one™s life. It is true that somepeople put their assumed role aside in later life, frequently as a result ofquestioning the basis for such roles. But such major re-orientations in life are notthe rule. Instead most people continue along the path which was set for them inchildhood and adolescence. There are advantages to this. The gender rolessupplied by society allow those who conform them to enjoy the benefits. Mencan assert themselves in public and occupy good jobs and women can accept thesupport and economic backing from men which goes with a domestic role. Suchdescriptions are close to stereotypes but they are accurate for large sections ofwestern societies and many others as well, if they were not, these societieswould be organised differently. The problems arise with the discriminationswhich are endemic to such a socio-economic system. The very public presence Raymond Hickey  Language and Society   Page 25 of  37of males can be interpreted as hegemony over women. Even if not all womenwish to assume such public roles, the opportunity should be open to them. Theextent to which we carry assumptions about what gender should occupy whatpositions in society can be seen in the unmarked or default use of many titles.Surgeons are taken to be male unless specified otherwise. Nurses are femalesunless one speaks of male nurses. Now while the work of neither of theseindividuals is inherently of greater value that that of the other, the social prestigeof the surgeon is far greater. Traditionally, the justification for different employment patterns alonggender lines appealed to the larger physique of men and their greater strength visa vis women. These physiological differences were often compared to those inthe animal world. It is true that in the mammal world, the males of species aregenerally stronger, one just needs to look at a bull or a boar to see that. Butdivision of labour according to gender would only make sense in manual jobs,but even there the ever increasing automation of labour has meant that physicalstrength is becoming less and less of a consideration. The employmentopportunities of women versus men have to do with power in society.1.5.3 Gender and powerIf gender consists of roles allocated to males and females and determined bysociety then it is not a given, at least very much less than one™s biological sex isa given. Scholars nowadays refer to gender as constructed, it is something whichis performed by individuals who adopt categories of beliefs and behaviourwhich are associated in a given society with their biological sex. Gender roles are perceptually merged with biological sex and unless onestops to think about the matter this seems to be perfectly natural. The genderroles of males and those of females have been established for centuries insocieties across the world. Such roles have the power of the familiar and theconventional. For example, in various religions, it seem ‚natural™ that menshould occupy the most senior positions. In governments and public institutionsit again seems ‚natural™ than men should lead. For instance, all 43 presidents ofthe United States have been male (and white for that matter). How often doesone hear pseudo-arguments like ‚that™s the way it has always been™ whensomeone questions the legitimacy of male hegemony. Women who rightly insist on equal rights as men are often regarded as‚ungrateful™, as betraying their partners, parents, society or whatever, asrejecting male protection and kindness. The assumption here is that if the peoplein power are nice to you, you are supposed to be thankful.1.5.4 Language used by womenEarly researchers into language and gender, above all Robin Lakoff, stressed thethe lack of power which the kind of language women use seems to embody. Raymond Hickey  Language and Society   Page 26 of  37Various features were listed which were intended to document thispowerlessness. These are listed in the following table.Putative features of women™s languagePowerless, non-confrontational language indirect statements  It would save a lot of money if we bought a smaller car. tag questions  It™s not that much to ask, is it? use of hedges, alternatives  It™s not really that difficult. Well, why not?  We could go for a drive or a walk this afternoon. high rising intonation at end of sentence  We could go away for the weekend.äEmotional, ‚genteel™ language use of augmentatives  I™m *delighted you™re going to help. They™re *so kind! use of euphemisms  Peter™s gone to wash his hands.  It is important to note that, if at all, such features indicate vague tendencies in thelanguage of women. Individuals may or may not show these features.Furthermore, it would be wrong to maintain that those who do should adopt amore assertive style in order to overcome social powerlessness. Offers ofremedial instruction to lend more authority to the language of individual womenmiss the point that it is the general position of women which needs to be changedand not the speech of single persons.1.5.5 Gender and standardThe standard of any language is the one with greatest prestige. It is chosen forofficial usage, taught in schools, used in universities and is the universal mediumfor writing. All western societies have a standard form of their officiallanguages, standard French, Italian, Spanish, English, etc. Dialects of the samelanguage are usually viewed unfavourably in relation to the standard. In somecountries, e.g. Spain with Catalan and Galician (Gallego), there may be otherlanguages spoken. These normally have more status than ‚mere™ dialects butnever the clout of the standard. An observation made by researchers on gender-specific language usageis that women tend to use more standard language than men. Not everyone agreeswith this, but the evidence for it is very powerful and many investigations haveshown this objectively. Those who disagree with this should remember that theobservation is not a criticism of female linguistic behaviour. At any rate the key Raymond Hickey  Language and Society   Page 27 of  37question is, why do women use more standard forms of language? Anexplanation which appeals to a power differential between men and womenwould seem to give an acceptable answer. If one group has been traditionallydiscriminated against, then assuming the accent of prestige will afford somepower by association. Some data to illustrate the more standard use of language by women canbe taken from the study of English in Norwich city which was referred to above(see section ???). Trudgill in his investigation noted that besides stylisticdifferentiation, linguistic differences correlated very closely with the gender ofspeakers. This correlation can be illustrated by the figures for (ng)-alternation,already touched upon above. In a class and gender array for (ng) based onformal speech, (ng)-pronunciation can be shown to have the followingdistribution.Index scores for (ng) variable across social group and gender shift from [)] to [n] (percentages represent occurrences of [n])    Male  Female MMC  4  0 LMC  27  3 UWC  81  68 MWC  91  81 LWC  100  97It can be noted that there is a consistency in the pattern of this array. Even though[n]-scores increase down the columns in both male and female cases, in eachsocial class male speakers tend to use more lower status [n]-variants thanfemale speakers. The same kind of pattern was found for other variables as well.The result common to all these findings is that on average women consistentlyprefer more standard variants where a choice is possible Œ irrespective ofsocial class and style. The striking point is that gender differentiation evenoccurs in the speech of children. In an investigation of the pronunciation ofpostvocalic /r/ in Edinburgh, for instance, it was shown that there was a patternof gender differentiation even in the speech of six year old children.  There is another reason why women, certainly in English-speakingcountries like the United Kingdom, tend to use more standard forms of language.There is, or at least was, an association of dialect with working-class maleculture in which women, again at least previously, did not participate directly.Another aspect of this complex concerns the use of ‚strong language™, e.g. cursesand swear words, like ‚four-letter words™. Using such language has been part ofthe ‚rough and tough™ male image in many western countries. Women weresupposed to be more ‚genteel™, strong language was regarded as not ‚ladylike™. Raymond Hickey  Language and Society   Page 28 of  371.5.6 Gender-neutral languageEven a cursory glance at English shows that it, like other western languages, areinherently sexist, i.e. embody discrimination in their structure and/or vocabulary.This may be by assuming that the default case is always male as in The linguistmust gather data and be careful that he organises it properly. One could,when making a generic reference to the linguist or reader, assume that theindividual is female and use ‚she™ as pronoun. It is perfectly understandable forfemale authors to do this. However, it may appear slightly ingratiating for maleauthors to do so. One could vary the pronominal references at will, but this mayleave the reader wondering why ‚she™ is used in one instance and ‚he™ in thenext. Apart from the issue of generic usage, language may be sexist in thelabels sometimes used for women. Animal comparisons are common inmetaphoric usage, e.g. strong as an ox, meek as a lamb, sly as a fox. But manyof these are also used derisively for women, e.g. stupid cow, silly duck, awfulbitch.  A further type of sexism is found in expressions which stress women asobjects of sexual desire, e.g. chick or peach. Even the fairly innocuous honeyhas its origin in the association of women with sweetness. The converse of thissituation is the use of special vocabulary to describe women who, because ofage, are not regarded as sexually desirable and hence viewed negatively by men,consider such expressions as old hag. Other abusive labels for women with asexual origin are also found, e.g. cunt. Such terms stem from derisory maleattitudes to women.1.5.7 Desexification of languageThere have been many attempts to desexify language, that is to remove inherentlysexist structures. One obvious means is just not to use such language, as in thecase of abusive terms like those just mentioned. But there are many instanceswhere one cannot avoid the issue and speech communities have reacted invarious ways, for example by creating new generic forms. Humankind forformer mankind, chairperson instead of chairman / chairwoman or indeedchair. However, this type of reduction does not always work, e.g. one couldhardly call a spokesperson a spoke. The use of a different word to indicateoccupation is sometimes possible, e.g. police officier for policeman. In othercases a form has been replaced, e.g. air hostess by flight attendant.  The goal of such creations is to arrive at a neutral label which can beused for either gender without highlighting this. There are difficulties where alexical replacement is not possible. The word doctor has sometimes beenqualified by an adjective or further noun to indicate the gender of the individual.Apart from the distinctly quaint lady doctor, there is female doctor or womandoctor. There are differences, too, within the anglophone world. What is Raymond Hickey  Language and Society   Page 29 of  37acceptable in the British Isles does not necessarily meet with approval in theUnited States or Canada. Different strategies can be employed to avoid being overtly sexist inlanguage use. Instead of using a generic masculine reference of the type thelinguist Œ he... (see remarks above) one can use a plural: linguists Œ they...English is forgiving in this respect as it allows the use of plural anaphora withsingular antecedents (not permitted in synthetic languages such as German) sothat one can have sentences like Anyone interesting in taking part should seetheir tutor. Other pronouns like one or you are an another option.Reformulations are often a solution man the position can be re-formulated asoccupy/staff/fill the position. Using formally marked feminine forms is decreasing in popularity. Oftenone finds actresses referred to as actors (generic for both genders); poetess isdefinitely antiquated. However, specific feminine forms of titles tend to beretained, for instance, The Duchess of Kent, Baroness Thatcher. In the area of written address English has had considerable problems, forinstance, with the forms Mrs. and Miss Œ stressing the marital status of thewoman, but not of the man Œ and which are now rightly regarded asunacceptable. The use of Ms. shows some of the difficulties with attempts todesexify language: the success depends on whether the new form is accepted inthe society in question. A new form can also backfire which is obviously notintended by its inventors. However, if official guidelines are adamant, as is forinstance the case with Ms. for official correspondence in Britain, then individualresistance may give way to a general acceptance and the form may becomeneutral through constant use. In current British English usage it is also commonto address people (in writing and on formal occasions) by simply using theirfirstname and surname, avoiding the issue entirely. Finally one should mention that some of the suggestions for desexifyinglanguage border on the comical because of the seeming sexist attitudes which areimputed to words which are not related to gender reference. For example,human does not contain the word man nor does history mean ‚his story™ (bothwords derive from Latin stems). Indeed with the normal initial stress of Englishthe elements -man and -story both have an indeterminate central vowel [(]. Thisfact raises another issue which deserves more serious attention. Take the wordpostman. This does contain the element -man meaning ‚man™ but it too ispronounced with a schwa, i.e. the word is [*p(%sm(n]. It is a moot point whetherall speakers of English using this word think that it consists morphologically ofpost and man. If the word postman is lexicalised in English to mean ‚anyonewho delivers the post™ then one can hardly classify it as sexist language.Nonetheless, if the conscious perception of some people is that the word isdiscriminatory, then one could change it in order not to offend sensitivities. Raymond Hickey  Language and Society   Page 30 of  371.5.8 Gender and language changeIt was noted above that women tend to use more standard forms of language. Butthere is another valid observation which seems paradoxical in this light: whenlanguage change is taking place then women seem to be the vanguard of suchchange. This can be illustrated clearly by considering change in present-dayDublin English. A new pronunciation of English has been evolving in the pastfifteen or twenty years which was probably triggered by dissociation from localDublin English accents by fashionable individuals in the city enjoying theprosperity and status attendant on the recent economic boom in Ireland. Thereare number of features in this change, two of which are relevant to thisdiscussion. The first is the diphthongisation of the vowel in GOAT and thesecond is the retroflex realisation of /r/. The GOAT vowel has been developinga central starting point, very different from the back or low initial position fortraditional Dublin English. Equally the retroflex /r/ Œ [#] Œ can be seen as amovement away from the low rhoticity of conservative Dublin English. Thefigures below are from recordings made by the present author in recent years.They show a clear preference by females for the new pronunciation.Change in present-day Dublin English and gender-related differiationGOAT diphthongisation  [g(%t] versus [go%t, g$%t]    n = 40, 40 yes  no   Male  18 (45%)      22 (55%)   Female  34 (85%)  6 (15%)  Test sentence: They had a GOAT on their farm. R-retroflexion  [n+"#4] versus [n2"34]   n = 40, 40 yes  no   Male  17 (43%)     23 (57%)   Female  36 (90%)   4 (10%)  Test sentence: They™re travelling up NORTH.Findings like those above have been reached in many investigations of languagechange in different countries. The question of interest here is why women are atthe forefront of ongoing change? One explanation which connects this fact with agreater use of standard forms of language by women has to do with power.Individuals with relatively little power compared to others, have a highersensitivity to aspects of social behaviour which can give them more power. Raymond Hickey  Language and Society   Page 31 of  37Where a language/variety is fairly stable and there is a standard, using thisstandard confers more power on women speakers by increasing their relativesocial status. In a situation where a language/variety is changing, as inpresent-day Dublin, participating actively in this change, indeed pushing itsomewhat, also confers power on women because the change is somethingwhich has status associated with it, especially in Dublin where it is motivatedby dissociation from those conservative speakers who do not have much socialclout.1.6 Language and cultureSociolinguistics can be given a broader remit and consider questions oflanguage and culture and/or ethnicity rather than just of language and society.This broader approach is labelled anthropological linguistics. Anthropology isa holistic science which encompasses every aspect of human society and cultureat present. It can also trace human evolution and development stretching backinto prehistory. There are two main branches of anthropology: 1) Cultural orsocial anthropology which studies living human societies and their culturalsystems; 2) Physical or biological anthropology which is primarily concernedwith human evolution at a much greater time depth. Typical issues inanthropological linguistics are linguistic relativity, kinship terms, colour terms,systems of address, honorifics, politeness or different modes of communicationacross cultures.1.6.1 The ethnography of communicationThe ethnography of communication is concerned with cultural differences in actsof communication, in particular what additional features accompany speech. Thecultural assumptions which we have internalised in our childhood as part of theprocess of socialisation guide our social and linguistic behaviour for the rest ofour lives. However, these assumptions must be relativised, indeed questioned,when we come into contact with other cultures. The following are some featuresof non-verbal behaviour which differ across cultures. The distance which speakers keep to their interlocutors (proxemics) isimportant. Italians tolerate a smaller distance than do English people who mayfeel uncomfortable in situations where what they regard as the minimum distancebetween speakers is not kept.  Countries also differ is the use of the hands when talking. The bodymovements used in communication (kinesics) can vary over relatively smalldistances. Northern Europeans in general use their hands sparingly when talking,indeed Irish men very often just keep them in their pockets. Hand movements andfacial expressions can often reinforce what is being said. Italians often presstheir elbows to their sides and hold both hands in front of their chest with the Raymond Hickey  Language and Society   Page 32 of  37tips of the fingers and thumbs together when explaining something, touching thechest when referring to themselves and opening the hands when making generalstatements or referring to their interlocutor. A common facial gesture amongMediterraneans is to protude the chin somewhat and draw the corners of themouth downwards to express uncertainty or lack of knowledge. In manycountries raising the eyebrows with deliberate eye contact and a slight twist ofthe head is used when seeking approval or asking a question. Touching the leftcorner of the forehead with the index finger of the left hand usually signalsincredulity.  The angle one stands at and whether it is polite to seek eye contact isanother factor. In certain African cultures it is regarded as impertinent to looksomeone in the eye who is perceived as socially superior. The lack of eyecontact can be disconcerting for Europeans who may regard it as a sign ofdeviousness. The question of touch is another sensitive issue. In most Europeancultures, shaking the hand of someone you meet is normal at the beginning of anencounter. Women may hug each other and men on occasions too, though it is byno means established behaviour. This does not apply outside Europe to any likethe same extent. In east and south-east Asia one might make a slight bow insteador press one one™s hands together under one™s chin and nod as in Thailand.There are strong taboos on touching others in various cultures. This question isrelated to gender as well. It is not allowed for strangers to touch a woman theydo not know in Arabic countries so one should not attempt to shake the hand ofanother man™s wife. Every conversation has a beginning and an end. The right to initiate aconversation may not apply to everyone, but only to an older person or a socialsuperior. The termination of a conversation is less strongly codified though thistoo may be done by the person with relatively more social status. In all cultures the language used to open and close an exchange isfrequently formulaic. It is proverbial to say that the British and Irish can open anexchange by a reference to the weather. In west European languages hello or arelated form is almost universal as an opener. The origin is French holà (fromthe exclamation ho! and là ‚there™) which entered English in the sixteenthcentury. There are other instances of expressions from one language whichspread across countries, e.g. Italian ciao ‚goodbye™ (from a dialect wordschiavo ‚(I am your) slave™). Greeting and parting formulas may derive from religious usage, althoughthe words used may be opaque nowadays. The word goodbye comes from ‚Godbe with you™, though this is not obvious to all speakers of English. In Irish(Gaelic) one greets with Dia dhuit ‚God be with you™ and replies with Dia isMuire dhuit ‚God and Mary be with you™. It is also common to thank the deity insuch formulas, as in Lá maith, buíochas le Dia ‚lovely day, thank God™.  Raymond Hickey  Language and Society   Page 33 of  371.6.2 Colour termsIn the late 1960s the American anthroplogists Brent Berlin and Paul Kaypublished the results of an investigation into basic colour terms in some 98languages from across the world. They wished to discover which colours hadthis basic function in each language and hence determine if there were overallpatterns in colours. For a word to be a basic colour term (i) it must be a singlelexeme and morphologically simple, so redish is not such a term, (ii) it must notbe included in another term, e.g. sepia which is a kind of brown, (iii) it is notlimited to a subset of object which it can classify, e.g. blond which only refersto hair, (iv) it must be wellknown and listed among the first colour terms in alanguage, something that would exclude English turquoise or lilac.  From their investigation they concluded that there are eleven basiccolours which occur in a specific implicational order, for instance the authorsclaimed that if a language had a word for ‚green™ then it had a word for ‚red™, if‚brown™ then ‚blue™, etc. The ordering for their eleven colours is as follows.Implicational hierarchy of colours (items on right imply those on left)    ß  ß   ß  ß   ß white  red  green  blue brown  purple black    yellow       pink              orange              greyIn order to find out what colours the words of individual languages referred to,Berlin and Kay presented their informants with colour chips which had 320equally spaced hues and eight degrees of brightness. They found remarkableconsistency in the colour chip chosen as best match to basic colour terms acrossthe selection of languages. The hue of the colour chips chosen was termed the‚focal hue™ for that colour. With red, for instance, the focal hue across languageswas a fire-engine red. Apart from the existence of basic colour terms, many languages showdifferent ranges for specific colour terms. In this respect one can say that theyencode distinctions in the ‚outside world™ differently, assuming that the visualperception of colour is the same for all individuals with normal vision. Thefollowing table offers some examples from a selection of European languages.Range of colours in selected languagesIn French there are two words for ‚brown™: brun and marron, the former refersto a shade of brown intermediary between yellow and black whereas the latterrefers to a reddish type of brown.  Raymond Hickey  Language and Society   Page 34 of  37In Russian there are two words for ‚blue™: goluboi denotes (sky)blue while sinijis used for a darker shade of blue.In Irish the word glas can mean ‚blue™ when referring to the sea (in literaryusage), ‚green™ when referring to grass, plants, etc. and ‚grey™ with reference toan object/being which cannot be green, e.g. caora ghlas ‚grey sheep™, spéirghlas ‚grey sky™ (the same is true of the etymologically identical word glas inWelsh). There are also other words for these colours: gorm ‚blue™ and uaine‚(vivid or artificial) green™ and liath ‚grey™ (Welsh llwyd). 1.6.3 Kinship termsThe basis of all kinship systems is the nuclear family, consisting of mothers andfathers, brothers and sisters. Within this unit the relationship of mother to childis the centre. The role of the father is more socially constructed than biologicallygiven, because in some cases the father is not known and another man may act asfather, one mother may have children by more than one father or no father orother man need not be present when the child is reared by the mother. Therelationship of parents to children is a genealogical one which lays out whoone™s ancestors are. Husbands and wives enter a mating relationship which isdifferent in kind (not a blood relationship), hence languages have differentwords for these than for fathers and mothers. By and large languages furtherdifferntiate between relationships between generations, as with parents andchildren and those within a generation as with siblings. The kinship terms found in a given language will first have lexicalisedterms for the members of the nuclear family. Relations beyond this unit, eithervertically backwards, i.e. grandparents and great grandparents, or verticallyforwards, i.e. grandchildren or grandchildren, may have lexicalised terms. Butthese can also be constructed from elements of the nuclear family, e.g. somethinglike ‚mother-mother™ for grandmother. Horizontal distinctions are a further planeon which relationships can be distinguished: one has uncles and aunts on thelevel of parents, nephews and nieces on that of one™s children. From thechildren™s point of view, i.e. within a generation, these individuals are cousins.Where languages have cover terms, e.g. cousins for all children of parents™siblings, gender distinctions may or not be made, in English they are not but inFrench, with cousins ‚cousins™-MASC and cousines ‚cousins™-FEM, they are. Languages furthermore distinguish between neutral and familiar terms forrelatives. In German, Großvater is ‚grandfather™ as is the familiar term Opa,Großmutter is ‚grandmother™ as is Oma. Sometimes the familiar terms arerestricted to parents, e.g. mum/mom/ma and dad/da/pa in English and byextension are used for grandparents as well, e.g. grandma, grandpa. English isunusual in having a scientific or academic level of usage where the term‚sibling™ is used. This is not normally found in colloquial usage, one saysinstead ‚my brothers and sisters™, except in some set expressions like siblingrivalry. Raymond Hickey  Language and Society   Page 35 of  37 A further axis of distinctions in kinship systems is the side on whichrelations are located. All relations can be identified as either paternal ormaternal. Language will always have a way of indicating this. English uses aspecial adjective before the kinship term in question. But some languages havelexicalised terms, i.e. single words, for paternal and maternal relationsrespectively. Swedish, for instance, uses single-word combinations of ‚mother™and ‚father™ to indicate the side on which grandparents are located, e.g. morfar‚mother-father™, i.e. maternal grandfather, farfar ‚father-father™, i.e. ‚paternalgrandfather™. In the following the kinship terms in Thai are shown in a table as anexample of a relatively complex system (compared to west Europeanlanguages). Note that the terms for grandfather/mother/child are lexicalised.There are also terms for members within a generation who may be older oryounger than others (a special feature of the Thai system).Basic kinship terms in Thaisami   ‚spouse™  bida marn-da ‚parents™phoua  ‚husband™  bida, phaw ‚father™phanraya  ‚wife™ (polite)  mae, marn-da ‚mother™mia  ‚wife™ (informal) nong, phi ‚brother, sister™phi-shaai ‚elder brother™  phi-sao ‚elder sister™nong-shaai ‚younger brother™ nong-sao ‚younger sister™louk-shaai, bout-shaai  ‚son™louk-sao, louk-ying, bout-ying ‚daughter™laan  ‚grandchild™  lenn  ‚great grandchild™pou  ‚grandfather™  ta  ‚maternal grandfather™ta-thuad ‚maternal great grandfather™ya, khun-yaai ‚grandmother™  shouad  ‚great grandmother™yaai shouad ‚maternal great grandmother™aa  ‚father™s younger brother or sister™na  ‚mother™s younger brother or sister™yart  ‚cousin™  yart haang-haang ‚distant relations™The word for ‚parents™, bida marn-da, consists of the word for ‚father™ and‚mother™ together. Other combinations yield other new terms, e.g. the masculineending shaai with laan ‚grandchild™ gives the word for nephew laan-shaai. Thefeminine ending sao forms a similar compound: laan-sao ‚niece™. Thecombinations of age groups seem strange here. Consider phaw-ta ‚father-in-law™which consists of the word phaw ‚father™ and ta ‚maternal grandfather™. Equallythe word mae-yaai ‚mother-in-law™ is made up of mae ‚mother™ and yaai‚grandmother™. But the reasoning here seems to be that grandchildren andnephews/nieces are twice removed from oneself, in the first instance verticallythrough one™s own children and in the second horizontally through one™s sibling Raymond Hickey  Language and Society   Page 36 of  37whose children they are. The same holds for parents-in-law: they are twiceremoved from oneself, they are a generation older and (horizontally) the parentsof one™s wife whereas one™s grandparents are also at two removes, this timeonly vertically.1.6.4 Counting systemsAs a final example of variation across languages and cultures a remarkablefeature of the counting systems of a few European languages will be considered.In most languages the base 10 which is used when counting. This system derivesfrom the ten fingers of both hands, in fact the word English digit is from Latindigitus ‚finger™.  Some cultures (and their languages) use a base 20, going on the fingersand the toes. The perceptual status of the toes vis à vis the fingers is interesting:some languages see them independently and have separate words for toes(English, German, etc.). Others like Latin, Turkish, Irish call the toes the ‚fingersof the foot™. French does this as well, ‚toe™ is doigt de pied, but it does givespecial status to the big toe: orteil, analogous to the thumb (lexicalised word forthis finger). The base 20 is found for counting in some languages in western Europe.French sometimes uses multiples of twenty for what are multiples of ten in otherlanguages, e.g. quatre vingt six ‚eighty six™, i.e. four-twenty-six. The Celticlanguages also show remnants of this counting system, e.g. the Welsh for twentyis ugain and deugain ‚40™ is morphologically ‚two-twenties™. Similarly in Irishtwenty is fichead and daichead is etymologically dá fichead, again‚two-twenties™. Basque also uses the ventigesimal system and some historicallinguists believe that the Celtic-French system has its origin in the languagegroup of which Basque is the sole present-day survivor.SummaryŁ Sociolinguistics looks at the role which language plays in society, theidentity function it has in communities and how attitudes frequentlydetermine language use. Vernacular forms of language are important forthe internal cohesion of social networks.Ł Socially driven language change can be observed by minuteinvestigation of variation. Such change can be triggered by the imitationof prestige groups and their forms of language.Ł Lower middle class speakers figure prominently in language change asthey are at the interface of the working and the middle class and striveupwards. Dissociation from people further down the social scale is astrong motivating factor in language change.Ł A change can start with a small set of words and can spread through thelexicon of the language (lexical diffusion). Alternatively, minute Raymond Hickey  Language and Society   Page 37 of  37variation for all instances of a sound may occur in a community(Neogrammarian advance). The difference between the forms with thechange and those without increases in time, due to a process known asphonologisation whereby small differences are exaggerated to make themdistinct from others. Ł Only a subset of variations in a language at any one time lead to laterchange. Just what variations result in change depends on their status forthe speakers. This status may be conscious in the case of identificationmarkers or subconscious, the latter not being any less important than theformer for language change.Ł Women tend to use more standard language than men (perhaps due totheir position in western societies). On the other hand they also tend tobe at the forefront of linguistic innovations.Ł The ethnography of communication concerns itself with discoursestrategies in cultures which differ considerably from each other.
Microlinguistics	Sociolinguistics
