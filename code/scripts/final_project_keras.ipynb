{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.7.5-final"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python37564bit8ef63aba3f24491db99035316b265dfd",
      "display_name": "Python 3.7.5 64-bit"
    },
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny3oQ6hbhFl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "247e6efe-6f6c-4159-82b0-9d835612fa43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-Ov4vXObCqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b984b824-8aee-48f2-ecd8-1fc336f4c36a"
      },
      "source": [
        "data = [i.strip('\\n').split('\\t') for i in open('/content/drive/My Drive/Colab Notebooks/openclass_prize.train', 'r', encoding='utf8', errors='ignore')]\n",
        "print(data[0])\n",
        "print(\" \")\n",
        "print(data[1])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['What is the Turing Test?', 'The original Turing test involved three entities communicating with each other through computer terminals in separate rooms - a computer program trying to pass itself off as a human, a human trying to prove that she or he is human, and a human judge deciding which of the competitors is human and which a machine. This is different from the popular idea that it\\'s about a human judging whether one subject is human or machine. The reason it\\'s different is that Turing\\'s original proposal was not intended as a practical test, but rather as a thought experiment making a philosophical argument - that it is impossible to determine whether something is \"intelligent\" better than by judging its linguistic interactions, and that therefore a machine that sounds intelligent has to be considered truly intelligent. And by \"intelligent,\" Turing meant, like a human, with consciousness.', 'Which of the following is NOT an objection to the Turing Test mentioned in the reading?', 'Intelligence is not the same thing as consciousness.', 'Many human beings might not pass the Turing test.|||Language is not necessarily the only way to demonstrate high intelligence.|||Cognitive ability is cognitive ability whether demonstrated through behavior or not.', 'The Turing Test web page||document||N/A|||The Turing test paragraph from Stanford Encyclopedia of Philosophy||image||N/A']\n",
            " \n",
            "['Turing test', 'Alan Turing', 'Computational linguistics', 'Language philosophy', 'Artificial Intelligence', 'Philosophy of artificial intelligence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YZjUG1QbCqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "eeb84306-776b-48cb-de09-b54cb90f153d"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "for i in range(len(data)):\n",
        "    if i % 2 == 0:\n",
        "        X.append(' '.join(data[i]))\n",
        "    else:\n",
        "        Y.append(data[i])\n",
        "\n",
        "print(X[0:3])\n",
        "print(\" \")\n",
        "print(Y[0:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['What is the Turing Test? The original Turing test involved three entities communicating with each other through computer terminals in separate rooms - a computer program trying to pass itself off as a human, a human trying to prove that she or he is human, and a human judge deciding which of the competitors is human and which a machine. This is different from the popular idea that it\\'s about a human judging whether one subject is human or machine. The reason it\\'s different is that Turing\\'s original proposal was not intended as a practical test, but rather as a thought experiment making a philosophical argument - that it is impossible to determine whether something is \"intelligent\" better than by judging its linguistic interactions, and that therefore a machine that sounds intelligent has to be considered truly intelligent. And by \"intelligent,\" Turing meant, like a human, with consciousness. Which of the following is NOT an objection to the Turing Test mentioned in the reading? Intelligence is not the same thing as consciousness. Many human beings might not pass the Turing test.|||Language is not necessarily the only way to demonstrate high intelligence.|||Cognitive ability is cognitive ability whether demonstrated through behavior or not. The Turing Test web page||document||N/A|||The Turing test paragraph from Stanford Encyclopedia of Philosophy||image||N/A', 'What is inflectional morphology? Inflectional morphology is when we have a change in word form, for example \"perform\" and \"performed.\" What is the difference between inflectional and derivational morphology? Derivational morphology is where we have a new word. One is part of micro- and the other is part of macro-linguistics.|||Inflectional morphology means the suffix changes.|||Derivational morphology is where morphemes are derived from phonemes. Inflection in Morphology presentation||slides||Inflection Bayu Jaka Magistra 180120130006 Indah  Mustika S. M . 180120130003  Inflection  Inflection  What is Inflection ?  Regular  & Irregular  Inflection  Forms of Nouns   Forms of Pronouns  & Determiners  Forms of Verbs  Forms of Adjectives   `   What is Inflection ?  What is Inflection?  1. The pianist performs in the local café  every month. 2. The pianist performed in the local café last night.  3. The performance was extraordinary.  What is Inflection? 1. The pianist performs in the local café  every month. 2. The pianist performed in the local café last night.  3. The performance was extraordinary.  What is Inflection? 1. The pianist  performs in the local café  every month. 2. The pianist  performed in the local café last night.  3. The  performance was extraordinary.  The words performs, performed & performance belong to the same root which is perform .  However, the word performs & performed in sentence ( 1 ) & ( 2 ) belong to the same word class i . e . verb .  The word Performance in sentence ( 3 ) , on the other hand, belongs to the different word class i . e . noun .  What is Inflection? 1. The pianist  performs in the local café  every month. 2. The pianist  performed in the local café last night.   What happens in sentence ( 1 ) and ( 2 ) is the process of word formation called inflection .  Inflection does not change the word class (parts of speech) and meaning of a word  Instead, Inflection is grammatically conditioned (McCarthy, 2002 ), or expresses grammatical categories like tense, mood, voice, aspect, person, number, gender and case . 1 1  Inflection . (2013, September 12).  Retrieved November 24, 2013,  from www.wikipedia.org : http://en.wikipedia.org/wiki/Inflection  What is Inflection? 1. The pianist  performs in the local café  every month. 2. The pianist  performed in the local café last night.  Grammatically condition or expresses grammatical categories like  tense, mood, voice, aspect, person, number, gender and case.  In sentence ( 1 ) the suffix - s is added to the root because of being grammatically conditioned by third - person singular subject the pianist .  In sentence ( 2 ) the suffix - ed is added to the root to express past tense .  What is Inflection? 3. The  performance was extraordinary.  What happens in sentence ( 3 ), on the other hand, is the process of word formation called derivation .  Derivation is the process of  new words by adding affixes to existing words .  ( Trask , 2007 ) .  Derivation changes the word class and/or meaning of the root .  What is Inflection? ROOT  Word class  Meaning Inflection Variants Variants Variants  What is Inflection? PERFORM  Verb  To execute Inflection Perform s Perform ing Perform ed  The variants still  belong to the  same word class  (verb), and have  the same  meaning  However, they are  grammatically  conditioned, or  express certain  grammatical  category grammatically  conditioned by third - person singular  subject Expressing continuous  and progressive  aspects Expressing past tense  What is Inflection? PERFORM  Verb  To execute Perform ance  Noun  The act of  performing Perform er  Noun  One who  performs Inflection Inflection Perform ance s Plural Perform er s Plural  Regular & Irregular Inflection Cats Guitars Hats Tables  Chairs Doors Windows  Regular & Irregular Inflection Cat s Guitar s Hat s Table s Chair s Door s Window s  Regular & Irregular Inflection Cat s Guitar s Hat s Table s Chair s Door s Window s  Adding suffix - s to a noun root is the regular method of forming plural .  Regular & Irregular Inflection Mice Children Women Teeth Oxen Men Knives are irregular plural forms of  Mouse Child Woman Tooth Ox Man Knife are allomorphs of   Regular & Irregular Inflection Went Better Worse are irregular inflection forms of  Go Good Bad Allomorphs ???  Regular & Irregular Inflection Went Better Worse Suppletion of  Go Good Bad Suppletion  Regular & Irregular Inflection Suppletion vs. Allomorph Allomorph Mice Children Women Teeth Oxen Men Knives Root Mouse Child Woman Tooth Ox Man Knife  Regular & Irregular Inflection Suppletion vs. Allomorph Allomorph M i ce Child ren Wom e n T ee th Ox en M e n Kni ves Root M ou se Child Wom a n T oo th Ox M a n Kni fe An allomorph has similar  phoneme(s) as its root  Regular & Irregular Inflection Suppletion vs. Allomorph Root Went Better Worse Suppletion Go Good Bad Suppletion and its root  does not have any similar  phoneme.  Forms of Nouns  Inflection in nouns expresses  grammatical category which is number.  Regular forms (adding the suffix  - s)  Irregular forms (Allophones, zero suffix  like  deer, fish, sheep )   4.4 Forms of Pronouns And  Determiners Open classes :  Nouns, Adjectives, Verbs,   Adverbs Determiners:  nouns , display a singular - plural  contrast  P ro - nouns  combine a singular - plural contrast  with contrast unique to them, between  subject and non - subject forms.   T he  distinction between this and  these.  These  are the singular and plural forms of the  determinest lexeme  this.  The  determiners THAT and THIS demonstrate  that number contrasts can have a grammatical  effect inside noun phrase as well as between  subject noun phrases and their accompanying  verbs.   In English, the same technique is used for one  small closed class of lexemes, namely personal  pronouns.  If  one replaces John and Mary with the  appropriate pronouns in these two examples,  the outcome is as in: 1. He  loves her. 2. She  loves him.   He and him are sometimes said to contrast in  case. 1. He belonging to the nominative  case 2. H im belonging to the accusative case .  It  is striking that the relationship between  nominative and accusative forms is  consistently  suppletive . >>  I/me, she/her, we/us, and they/them.    Corresponding words with a possessive  meaning: his and our, as well as my, her, your  and their.  Syntactically and semantically, these words  fulfill just the same role as noun phrases with  the apostrophe - s: 1.  His bicycle  means the bicycle belonging   to him. 2.   bicycle means the bicycle  belonging to that man.  4.5 Forms of  Verbs  In English, a verb lexeme has at most five distinct  forms, as illustrated here with GIVE .  Third person singular present tense e.g . Marry  gives a lecture every year.  Past tense e.g . Marry  gave a lecture last week.  Progressive participle e.g . Mary is  giving a lecture today.  Perfect or passive participle e.g . Mary has  given a lecture today.  Basic form (used everywhere else) e.g Mary wants to  give a lecture.  4.6  Forms of  Adjectives  Many  English adjectives exhibit three forms,  for example GREEN here: 1. Grass  is  green . 2. The  grass is  greener now than in winter. 3. The  grass is  greenest in early summer.  Other adjectives with  similar forms: Positive Comprative Superlative Happy happier happiest Long longer longest Pure purer purest Untidy untidier untidiest Good better best All  these exhibit a regular pattern of suffixation with   er and   est , except for better and best, which are  suppletive .', 'What English sentence could represent the following set of first order logic expressions? The answer is “What game do you want to play with me today?” Which of the following expressions could belong in an FOL representation of, â\\x80\\x9cWhat do you want to play with?â\\x80\\x9d with(play, $qvar) play(want, $qvar)|||want(play, $qvar)|||play(with, $qvar) NONE']\n",
            " \n",
            "[['Turing test', 'Alan Turing', 'Computational linguistics', 'Language philosophy', 'Artificial Intelligence', 'Philosophy of artificial intelligence'], ['Inflectional morphology', 'Inflection', 'Derivational morphology', 'Morphology', 'Macrolinguistics'], ['First-order logic', 'Predicate logic', 'Semantics', 'Sentence analysis'], ['Logistic regression', 'Text classification', 'Binary classification', 'Natural Language Processing (NLP)'], ['Naive Bayes', 'Text classification', 'Binary Naive Bayes (NB)', 'Sentiment analysis']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0LAMEvHbCq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#may not need any of this with the glove embedding I am using\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "def clean_data(dataset):\n",
        "    clean_data = []\n",
        "    sno = SnowballStemmer('english')\n",
        "\n",
        "    for entry in dataset:\n",
        "        lowered = entry.lower()\n",
        "        sans_special_chars = re.sub(r'\\W',' ', lowered)\n",
        "        sans_extra_spaces = re.sub(r'\\s+',' ', sans_special_chars)\n",
        "        sans_underscore = sans_extra_spaces.replace('_', ' ')\n",
        "        split = sans_underscore.split()\n",
        "        stemmed = ' '.join([sno.stem(word) for word in split])\n",
        "        clean_data.append(stemmed)\n",
        "\n",
        "    return clean_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ruy87JlbCq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean_X = clean_data(X)\n",
        "# print(clean_X[0:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7S595PMbCq-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11e369b0-9b7f-448a-9bae-066490493d40"
      },
      "source": [
        "labels = [item for sublist in Y for item in sublist]\n",
        "print(labels[0:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Turing test', 'Alan Turing', 'Computational linguistics', 'Language philosophy', 'Artificial Intelligence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEDN6fVibCrB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d15bf381-81bc-48d4-ca79-51dd1f66f54d"
      },
      "source": [
        "print(len(labels))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mALeoMqqbCrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "sample_labels = np.zeros((len(Y), len(labels)))\n",
        "\n",
        "for i in range(len(Y)):\n",
        "    for j in range(len(Y[i])):\n",
        "        for k in range(len(labels)):\n",
        "            if Y[i][j] == labels[k]:\n",
        "                sample_labels[i][k] = 1\n",
        "                break\n",
        "\n",
        "# sample_labels[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kte0D-cQbCrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d06e06d7-d900-4328-9e97-5f1a6d6d7ed0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, sample_labels, test_size=0.2)\n",
        "print(\"Length of X: \", len(X))\n",
        "print(\"Length of Y: \", len(Y))\n",
        "print(\"Length of X_train: \", len(X_train))\n",
        "print(\"Length of Y_train: \", len(Y_train))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of X:  163\n",
            "Length of Y:  163\n",
            "Length of X_train:  130\n",
            "Length of Y_train:  130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQKbeIMbbCrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1a712ba9-d6a7-492f-edcc-7123234be001"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import keras.preprocessing.sequence as kps\n",
        "NUM_WORDS = 120000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "train_indices = tokenizer.texts_to_sequences(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "train_indices = kps.pad_sequences(train_indices, padding='post')\n",
        "\n",
        "test_indices = tokenizer.texts_to_sequences(X_test)\n",
        "test_indices = kps.pad_sequences(test_indices, padding='post', maxlen=len(train_indices[0]))\n",
        "\n",
        "print(\"Train Data: \", train_indices[0:3])\n",
        "print(\"Test Data: \", test_indices[0:3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Data:  [[ 52   7   1 ...   0   0   0]\n",
            " [ 80  11 569 ...   0   0   0]\n",
            " [ 52   7   1 ...   0   0   0]]\n",
            "Test Data:  [[1062  881  889 ...    0    0    0]\n",
            " [  52   11 1549 ...    0    0    0]\n",
            " [  52    7    1 ...    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VL8YMaHbCri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "EMBEDDING_DIM = 100\n",
        "embeddings_index = {}\n",
        "\n",
        "f = open(\"/content/drive/My Drive/Colab Notebooks/glove.6B.100d.txt\", 'r', encoding='utf8', errors='ignore')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = ''.join(values[:-EMBEDDING_DIM])\n",
        "    coefs = np.asarray(values[-EMBEDDING_DIM:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = layers.Embedding(len(word_index) + 1,\n",
        "                        EMBEDDING_DIM,\n",
        "                        weights=[embedding_matrix],\n",
        "                        input_length=len(train_indices[0]),\n",
        "                        trainable=True,\n",
        "                        mask_zero=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YneNAu1-bCrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import callbacks\n",
        "def create_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(layers.Bidirectional(layers.GRU(500)))\n",
        "    model.add(layers.Dense(325, activation='relu'))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Dense(len(labels), activation ='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return [model, {'batch_size' : 16, 'callbacks' :[callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)]}] # callback only used here to restore best weights if need be"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt94GPBRbCrw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b2c39173-8172-432d-9531-38ef92c7cf24"
      },
      "source": [
        "model, kwargs = create_model()\n",
        "\n",
        "kwargs.update(x=train_indices, y=Y_train, epochs=2, validation_data=(test_indices, Y_test))\n",
        "model.fit(**kwargs)\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"modle.h5\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "9/9 [==============================] - 887s 99s/step - loss: 0.3488 - accuracy: 0.0000e+00 - val_loss: 0.0467 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 952s 106s/step - loss: 0.0495 - accuracy: 0.0385 - val_loss: 0.0444 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5ccJ1MLbCr2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5e32730-d2c5-4a6b-8980-70326e4e202d"
      },
      "source": [
        "predictions_probs = model.predict(test_indices)\n",
        "predictions_probs[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.02001429e-03, 4.30285931e-04, 1.89357996e-03, 4.92006540e-04,\n",
              "       8.14795494e-04, 1.37981772e-03, 2.12305784e-03, 1.98203325e-03,\n",
              "       2.25666165e-03, 8.69604945e-03, 7.90327787e-04, 1.12658739e-03,\n",
              "       8.46087933e-04, 8.63584876e-03, 1.50719285e-03, 1.42386943e-01,\n",
              "       1.72395408e-02, 4.36037779e-03, 2.17379630e-02, 4.36782837e-03,\n",
              "       1.98751688e-04, 1.02308393e-03, 3.84458899e-03, 8.41557980e-04,\n",
              "       2.42799520e-04, 4.16913629e-03, 4.46081161e-04, 1.67876482e-04,\n",
              "       1.23298756e-04, 2.25025415e-03, 1.26916170e-03, 4.23938036e-04,\n",
              "       1.48150325e-03, 2.14469433e-03, 2.72601843e-04, 3.38862240e-02,\n",
              "       6.61563873e-03, 9.60245728e-03, 6.42584264e-02, 1.02269650e-02,\n",
              "       1.65979981e-01, 3.00467014e-04, 1.42228603e-03, 1.45578384e-03,\n",
              "       2.34544277e-04, 4.81185317e-03, 3.47891450e-03, 1.59570575e-03,\n",
              "       6.26891851e-04, 5.55565953e-03, 2.21043825e-04, 2.24055648e-02,\n",
              "       1.20645761e-03, 2.99870968e-04, 6.73031807e-03, 4.08500433e-04,\n",
              "       5.39064407e-03, 1.35043263e-03, 2.89112329e-03, 3.30336588e-05,\n",
              "       1.66039169e-02, 5.46127558e-04, 1.30802393e-04, 3.56465578e-04,\n",
              "       3.61621380e-03, 5.71560860e-03, 1.01600587e-02, 1.26871467e-03,\n",
              "       8.58842832e-05, 1.86294317e-03, 2.68355012e-03, 2.06342340e-03,\n",
              "       1.60872936e-04, 1.73687935e-04, 7.64906406e-04, 2.37077475e-04,\n",
              "       3.33568454e-03, 1.18938088e-03, 5.87850009e-05, 5.20932674e-03,\n",
              "       1.55422091e-03, 9.95367765e-04, 4.43100929e-04, 2.73108482e-04,\n",
              "       3.88672352e-02, 1.09471497e-04, 2.78711319e-04, 1.00930789e-04,\n",
              "       1.50948763e-04, 2.99721956e-04, 4.15003300e-03, 4.91261482e-04,\n",
              "       5.03420830e-04, 7.46943115e-05, 2.09361315e-04, 2.79539800e-05,\n",
              "       2.04056501e-04, 1.06585026e-03, 1.86829269e-02, 2.76505947e-04,\n",
              "       4.56030975e-05, 1.02956517e-04, 5.40107489e-04, 2.18242407e-03,\n",
              "       8.69129872e-05, 1.79149611e-05, 6.09755516e-04, 4.79776859e-02,\n",
              "       1.14917755e-03, 1.61111355e-04, 1.80175900e-03, 3.94399322e-05,\n",
              "       1.23798847e-04, 6.92372560e-05, 8.08829718e-05, 7.74830580e-04,\n",
              "       5.19067049e-04, 9.22739506e-04, 8.53389502e-04, 1.92382932e-03,\n",
              "       1.43548846e-03, 8.99672508e-04, 4.40487266e-03, 8.27282667e-04,\n",
              "       3.63945961e-04, 1.49965286e-04, 1.00055337e-03, 3.98096442e-03,\n",
              "       1.72376633e-04, 3.68952751e-04, 2.28673220e-04, 1.41447783e-03,\n",
              "       1.27172470e-03, 9.54659390e-06, 1.81597471e-03, 8.03953409e-03,\n",
              "       1.09839439e-03, 4.23073769e-04, 2.48348713e-03, 2.78174877e-04,\n",
              "       5.91395910e-05, 3.80605459e-04, 2.25958228e-03, 4.26650047e-04,\n",
              "       2.11030245e-04, 2.18600035e-04, 8.55192702e-05, 1.39232874e-02,\n",
              "       2.34594481e-05, 3.14474106e-04, 5.06243110e-03, 2.92271376e-04,\n",
              "       2.22106391e-05, 9.31245886e-05, 1.59204006e-04, 1.92195177e-04,\n",
              "       6.28799200e-03, 2.48858333e-03, 4.62979078e-04, 2.91976333e-03,\n",
              "       1.86780744e-06, 2.28503346e-03, 1.50024891e-04, 1.85436010e-03,\n",
              "       2.00957060e-04, 7.51558327e-05, 2.16466188e-03, 1.39638782e-03,\n",
              "       1.61945820e-04, 6.58015560e-05, 3.89099121e-04, 5.84407826e-05,\n",
              "       5.80579042e-04, 2.39938498e-04, 1.60127878e-04, 7.36862421e-04,\n",
              "       9.33780393e-05, 5.31235237e-05, 3.91630238e-05, 1.59105659e-03,\n",
              "       3.57687473e-04, 4.33534384e-04, 7.74656939e-07, 2.85568833e-03,\n",
              "       1.40011311e-04, 1.21333600e-04, 3.84956598e-04, 9.88453627e-04,\n",
              "       1.04072690e-03, 1.34906173e-03, 7.50556588e-03, 1.39313936e-03,\n",
              "       2.32487917e-04, 1.06382370e-03, 1.51640177e-03, 2.74306536e-03,\n",
              "       1.58912226e-05, 2.99717722e-05, 1.25527382e-03, 6.99996948e-04,\n",
              "       1.14285946e-03, 1.52722001e-03, 3.99202108e-04, 1.47884190e-02,\n",
              "       1.42008066e-04, 1.96740031e-03, 1.86052918e-03, 2.28393078e-03,\n",
              "       5.39422035e-04, 2.46844556e-05, 9.04328845e-05, 7.96033346e-05,\n",
              "       9.04530287e-04, 3.53994966e-03, 1.39239430e-03, 1.03932619e-03,\n",
              "       9.67383385e-04, 1.31466985e-03, 1.78793967e-01, 1.69348717e-03,\n",
              "       1.28236413e-03, 8.50409269e-04, 1.23587251e-03, 3.11022997e-03,\n",
              "       4.53442335e-04, 2.68757343e-04, 4.35113907e-04, 1.48087740e-04,\n",
              "       9.05573368e-04, 7.32808912e-05, 3.45647335e-04, 5.12131774e-05,\n",
              "       2.54571438e-04, 4.21955665e-05, 5.72353601e-04, 9.38922167e-04,\n",
              "       3.36143374e-03, 5.29974699e-04, 6.31679213e-05, 8.54929312e-05,\n",
              "       5.30034304e-04, 1.14218390e-04, 9.27984715e-04, 1.83597207e-03,\n",
              "       3.11464071e-04, 4.87774611e-04, 1.03911757e-03, 1.15787983e-03,\n",
              "       4.03940678e-03, 7.51376152e-04, 1.35046244e-03, 4.28557396e-04,\n",
              "       3.29554081e-04, 1.61446915e-05, 1.45465136e-04, 6.38939964e-05,\n",
              "       5.84810972e-04, 1.89912319e-03, 3.02970409e-04, 5.59881330e-03,\n",
              "       3.51995230e-04, 5.67432580e-05, 1.59242749e-03, 1.37299299e-04,\n",
              "       7.53641129e-04, 2.63631344e-04, 1.45620108e-03, 6.06321510e-06,\n",
              "       2.05427408e-04, 2.82526016e-04, 9.12368298e-04, 1.53192878e-03,\n",
              "       6.56217337e-04, 4.14188326e-05, 1.46150589e-04, 3.33489879e-05,\n",
              "       1.55732036e-03, 3.85314226e-04, 1.50594413e-02, 9.73225251e-05,\n",
              "       3.32385302e-04, 8.30560923e-04, 3.99857759e-04, 9.22799110e-04,\n",
              "       1.66529417e-03, 1.05434656e-03, 6.72668219e-04, 9.15825367e-04,\n",
              "       5.38021326e-04, 5.00798225e-04, 8.37117434e-04, 8.94278288e-04,\n",
              "       2.47657299e-04, 2.32636929e-04, 3.52233648e-04, 3.51413146e-05,\n",
              "       3.48109007e-03, 7.82597065e-03, 3.67671251e-04, 6.30378723e-04,\n",
              "       3.29345465e-04, 1.37221813e-03, 1.34897232e-03, 4.12136316e-04,\n",
              "       2.66164541e-04, 1.37746334e-04, 1.59889460e-04, 9.47413137e-05,\n",
              "       6.41547813e-05, 1.12805159e-04, 1.38789415e-04, 8.60333443e-03,\n",
              "       1.36205554e-03, 4.23127413e-03, 3.03485990e-03, 6.97049499e-03,\n",
              "       8.62687826e-04, 9.48271845e-05, 4.60624695e-04, 1.64926052e-04,\n",
              "       1.48177147e-04, 6.02066517e-04, 1.54465437e-04, 6.48403584e-05,\n",
              "       1.21045113e-03, 1.23326041e-04, 9.76383686e-04, 7.17956282e-05,\n",
              "       9.98079777e-04, 1.89894438e-03, 1.90526247e-04, 3.31506133e-03,\n",
              "       1.43408775e-04, 1.24514103e-04, 4.25875187e-04, 8.56220722e-04,\n",
              "       9.55849886e-04, 2.06112862e-04, 7.90268183e-04, 4.47064303e-05,\n",
              "       3.23146582e-04, 2.48998404e-04, 1.18229837e-04, 9.52512026e-04,\n",
              "       2.30813026e-03, 2.32100487e-04, 1.93238258e-03, 1.73002481e-04,\n",
              "       2.54124403e-04, 1.81704760e-04, 3.74227762e-04, 3.50505114e-04,\n",
              "       2.76079774e-03, 7.01762401e-05, 5.27888536e-04, 7.76588917e-04,\n",
              "       1.03136947e-04, 1.17924809e-03, 6.89777880e-05, 3.98695469e-04,\n",
              "       5.09500504e-04, 6.58904682e-05, 1.60551071e-03, 3.87549400e-04,\n",
              "       2.79244781e-03, 1.12026930e-03, 7.52419583e-05, 7.30127096e-04,\n",
              "       1.47163868e-04, 5.41836023e-04, 2.49716640e-03, 5.83967566e-03,\n",
              "       1.63432956e-03, 3.04076076e-03, 1.04856491e-03, 1.96665525e-04,\n",
              "       1.81525946e-04, 1.93059444e-04, 3.39120626e-04, 3.56763601e-04,\n",
              "       1.60548091e-03, 2.91636586e-03, 6.56396151e-04, 1.58920884e-03,\n",
              "       6.94572926e-04, 1.26600266e-04, 5.20706177e-04, 3.97533178e-04,\n",
              "       2.09689140e-04, 1.58995390e-03, 1.02406477e-04, 4.34547663e-04,\n",
              "       1.29753351e-03, 1.18122436e-04, 1.13511086e-03, 7.74595683e-05,\n",
              "       1.47756934e-03, 3.35276127e-04, 5.70766606e-05, 5.43773174e-04,\n",
              "       2.44408846e-03, 3.45888734e-03, 6.85100240e-05, 9.68307257e-04,\n",
              "       1.43903494e-03, 2.59578228e-04, 7.59601593e-04, 2.22563744e-04,\n",
              "       1.62807107e-03, 3.88532877e-04, 9.23424959e-04, 5.34212886e-05,\n",
              "       3.73810530e-04, 3.57497302e-05, 6.92903996e-04, 2.57875326e-05,\n",
              "       3.99559736e-04, 1.37189031e-03, 1.11341476e-03, 2.95430422e-04,\n",
              "       2.23991275e-03, 3.49762649e-06, 7.45564699e-04, 3.45763564e-03,\n",
              "       4.56929207e-04, 1.11189485e-03, 5.25861979e-04, 2.54452229e-04,\n",
              "       8.83936882e-04, 2.19792128e-04, 1.05179468e-04, 3.50415707e-04,\n",
              "       1.32381916e-04, 9.78350639e-04, 1.37439370e-03, 4.79251146e-04,\n",
              "       8.44180584e-04, 2.55310535e-03, 8.50677490e-04, 2.07067787e-05,\n",
              "       5.33968210e-04, 2.59369612e-04, 9.95218754e-04, 2.74509192e-04,\n",
              "       5.81532717e-04, 2.32338905e-04, 2.38686800e-04, 1.43796206e-04,\n",
              "       5.40750807e-05, 1.13248825e-03, 9.98973846e-04, 6.08652830e-04,\n",
              "       3.71605158e-04, 5.27411699e-04, 7.19547272e-04, 3.61382961e-04,\n",
              "       1.45184726e-01, 1.90645456e-04, 3.15166690e-05, 1.83671713e-04,\n",
              "       1.93864107e-04, 1.41942501e-03, 1.45941973e-04, 1.89721584e-04,\n",
              "       4.61459160e-04, 4.26113605e-04, 2.05397606e-04, 1.33723021e-04,\n",
              "       2.41437554e-03, 4.04710190e-05, 4.42147255e-04, 1.33424997e-04,\n",
              "       1.14332819e-04, 1.01280212e-03, 2.59131193e-03, 5.78314066e-04,\n",
              "       2.27281451e-03, 2.76356936e-04, 6.11782074e-04, 2.89857388e-04,\n",
              "       3.42309475e-04, 3.57270241e-04, 2.95788050e-04, 2.02208757e-04,\n",
              "       2.69502401e-04, 2.49713659e-03, 2.04145908e-04, 1.55620564e-05,\n",
              "       4.04566526e-04, 8.70794058e-04, 6.25371933e-04, 1.30295753e-03,\n",
              "       1.71281426e-05, 1.83701515e-04, 5.20437956e-04, 1.14036731e-04,\n",
              "       1.77413225e-04, 1.37001276e-04, 1.03455782e-03, 1.11787558e-04,\n",
              "       8.24673916e-05, 1.01214369e-04, 6.31093979e-04, 7.03809928e-05,\n",
              "       7.10517168e-04, 1.20017728e-04, 4.74661589e-04, 1.23783946e-03,\n",
              "       5.32597303e-04, 7.95652304e-05, 1.17745996e-03, 1.58098974e-05,\n",
              "       1.16852418e-04, 1.83135271e-04, 8.15302134e-04, 1.10468268e-03,\n",
              "       2.27153301e-04, 2.44736671e-04, 3.07679176e-04, 1.00779587e-04,\n",
              "       2.28264928e-03, 5.54309700e-05, 3.95537973e-05, 2.25985050e-03,\n",
              "       1.14162191e-04, 9.84042883e-04, 9.29396920e-05, 8.57353210e-04,\n",
              "       2.42286921e-03, 1.44600868e-04, 6.16980469e-05, 7.38657182e-05,\n",
              "       9.28789377e-04, 2.95490026e-04, 3.01539898e-04, 1.20945942e-05,\n",
              "       1.76426768e-03, 3.41743231e-04, 2.34923809e-05, 3.24338675e-04,\n",
              "       3.88752669e-05, 7.34439091e-05, 6.71160221e-03, 6.16240141e-05,\n",
              "       3.84694338e-03, 6.81665540e-03, 1.38133764e-04, 1.89949870e-02,\n",
              "       7.29491294e-05, 3.56939272e-05, 1.20347738e-03, 8.38577747e-04,\n",
              "       1.97976828e-04, 1.56104565e-04, 2.07394361e-04, 1.14692215e-04,\n",
              "       8.39114189e-04, 1.23549407e-05, 2.52404461e-05, 9.02313914e-05,\n",
              "       7.59297109e-05, 4.17888165e-04, 8.01295042e-04, 5.50687313e-04,\n",
              "       2.38060951e-04, 4.06533480e-04, 1.61141157e-04, 2.23850329e-05,\n",
              "       3.07589769e-04, 3.52152892e-05, 2.15053558e-04, 1.28403306e-03,\n",
              "       5.67135030e-05, 6.89953566e-04, 1.66696310e-03, 2.21735239e-03,\n",
              "       4.83405602e-05, 8.54175451e-05, 1.14162736e-04, 1.02155107e-04,\n",
              "       2.62558460e-04, 4.19735909e-03, 1.96307898e-04, 6.77257776e-04,\n",
              "       1.48117542e-04, 4.56094742e-04, 1.25378370e-04, 6.76572323e-04,\n",
              "       2.20268965e-04, 1.26311183e-03, 2.28911638e-04, 5.79476357e-04,\n",
              "       7.65956938e-05, 2.29895115e-04, 5.33491373e-04, 1.04337931e-03,\n",
              "       1.67441368e-03, 1.34378672e-04, 7.39634037e-04, 7.56479712e-05,\n",
              "       2.28047371e-04, 1.36703253e-03, 2.34633684e-04, 6.56276941e-04,\n",
              "       2.89115033e-05, 3.25861573e-03, 3.28093767e-04, 5.20616770e-04,\n",
              "       1.74373388e-04, 2.42112637e-05, 2.31981277e-04, 1.82221575e-05,\n",
              "       2.16513872e-04, 5.93990088e-04, 3.22397646e-05, 1.92138553e-03,\n",
              "       6.75648451e-04, 2.21059645e-06, 3.03822126e-05, 3.22639942e-04,\n",
              "       1.71077251e-03, 4.38779593e-04, 2.09480524e-04, 6.37680292e-04,\n",
              "       4.28494811e-03, 4.24474478e-04, 5.62995672e-04, 3.02970409e-04,\n",
              "       9.31964605e-05, 3.50505114e-04, 1.39236450e-04, 6.34014606e-04,\n",
              "       1.80840492e-04, 6.63310289e-04, 3.87638807e-04, 6.21378422e-04,\n",
              "       7.63893127e-04, 2.11447477e-04, 2.18998321e-05, 8.61950980e-07,\n",
              "       3.14652920e-04, 5.58823347e-04, 3.67379471e-05, 6.95228577e-04,\n",
              "       2.20596790e-04, 2.10493803e-04, 2.27235432e-05, 2.51799822e-04,\n",
              "       1.55155994e-05, 1.93685293e-04, 1.11830632e-04, 8.86708498e-04,\n",
              "       1.95950270e-04, 5.60032931e-05, 1.08155608e-03, 9.24347842e-05,\n",
              "       7.30812550e-04, 1.52206303e-06, 5.22732735e-04, 1.92850828e-04,\n",
              "       1.39355659e-04, 4.62919474e-04, 1.06197156e-04, 2.29404009e-06,\n",
              "       1.56313181e-04, 1.24573708e-04, 1.13556940e-04, 4.29511070e-04,\n",
              "       1.52736902e-04, 1.79874897e-03, 1.06025429e-04, 3.98975611e-03,\n",
              "       1.05190277e-03, 1.88261271e-04, 4.45498081e-05, 1.55341625e-03,\n",
              "       6.01768494e-04, 1.85533499e-05, 1.04856491e-03, 7.57813454e-04,\n",
              "       9.52531846e-05, 2.68906355e-04, 5.77270985e-04, 1.14905834e-03,\n",
              "       1.69545412e-03, 2.16603279e-04, 3.27795744e-04, 7.58492315e-05,\n",
              "       2.77847052e-04, 6.16550446e-04, 1.45584345e-04, 2.44796276e-04,\n",
              "       2.30923295e-03, 1.15176364e-04, 1.00469589e-03, 1.32748485e-03,\n",
              "       4.38123941e-04, 1.45193934e-03, 3.70234251e-04, 1.81666017e-03,\n",
              "       8.22693110e-04, 2.85542428e-05, 3.27497721e-04, 1.61916018e-04,\n",
              "       4.03248123e-05, 8.68171453e-04, 3.62734900e-05, 4.50670719e-04,\n",
              "       6.06805086e-04, 1.07559562e-03, 3.24994326e-04, 5.43369642e-05,\n",
              "       1.91476941e-03, 2.17795372e-04, 3.44663858e-04, 4.87267971e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zJkoS0pr7sm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "45c5b7bd-4d25-43f0-93fb-06e4a2d89177"
      },
      "source": [
        "predictions = np.where(predictions_probs > 0.01, 1, 0)\n",
        "predictions[0]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnz2W7VJbCr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "6b6056bb-21f9-46df-f0a5-7f71a0b10a33"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, _, __ = precision_recall_fscore_support(Y_test, predictions, average='macro')\n",
        "print((5 *  precision * recall) / (4 * (precision+recall)))\n",
        "precision, recall, _, __ = precision_recall_fscore_support(Y_test, predictions, average='micro')\n",
        "print((5 *  precision * recall) / (4 * (precision+recall)))\n",
        "precision, recall, _, __ = precision_recall_fscore_support(Y_test, predictions, average='weighted')\n",
        "print((5 *  precision * recall) / (4 * (precision+recall)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.001437053255592691\n",
            "0.050699300699300696\n",
            "0.02435109904569315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGzGwYzbbCr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}